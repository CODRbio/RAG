This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    llm-calling-convention.mdc
config/
  __init__.py
  llm_module_construction.md
  rag_config.example.json
  rag_config.json
  rag_config.local.example.json
  settings.py
  Untitled
data/
  parsed/
    .gitkeep
  parsed_test/
    .gitkeep
  .gitkeep
  eval_mini.json
docs/
  api_reference.md
  architecture.md
  configuration.md
  dependency_matrix.md
  developer_guide.md
  operations_and_troubleshooting.md
  README.md
  release_migration_ubuntu.md
  scripts_guide.md
  testing_and_evaluation.md
frontend/
  public/
    vite.svg
  src/
    api/
      auth.ts
      auto.ts
      canvas.ts
      chat.ts
      client.ts
      compare.ts
      graph.ts
      health.ts
      index.ts
      ingest.ts
      models.ts
      projects.ts
    assets/
      react.svg
    components/
      canvas/
        CanvasPanel.tsx
        DraftingStage.tsx
        ExploreStage.tsx
        FloatingToolbar.tsx
        OutlineStage.tsx
        RefineStage.tsx
        StageStepper.tsx
      chat/
        ChatInput.tsx
        ChatWindow.tsx
        RetrievalDebugPanel.tsx
        ToolTracePanel.tsx
      compare/
        CompareView.tsx
      graph/
        GraphExplorer.tsx
      layout/
        Header.tsx
        Sidebar.tsx
      research/
        ResearchProgressPanel.tsx
      settings/
        SettingsModal.tsx
      ui/
        Modal.tsx
        PdfViewerModal.tsx
        Toast.tsx
      workflow/
        CommandPalette.tsx
        DeepResearchDialog.tsx
        DeepResearchSettingsPopover.tsx
        index.ts
        IntentConfirmPopover.tsx
        IntentModeSelector.tsx
        WorkflowStepper.tsx
    i18n/
      locales/
        en.json
        zh.json
      index.ts
    pages/
      AdminPage.tsx
      ChatPage.tsx
      IngestPage.tsx
      LoginPage.tsx
    stores/
      index.ts
      useAuthStore.ts
      useCanvasStore.ts
      useChatStore.ts
      useCompareStore.ts
      useConfigStore.ts
      useProjectsStore.ts
      useToastStore.ts
      useUIStore.ts
    types/
      index.ts
    App.tsx
    index.css
    main.tsx
  .gitignore
  eslint.config.js
  index.html
  package.json
  README.md
  tsconfig.app.json
  tsconfig.json
  tsconfig.node.json
  vite.config.ts
scripts/
  _test_ncbi_integration.py
  00_healthcheck_docker.sh
  00_preflight_check.sh
  01_init_env.py
  02_parse_papers.py
  03_index_papers.py
  03b_build_graph.py
  04_test_search.py
  05_test_rag.py
  06_ingest_langgraph.py
  07_test_web_search.py
  08_run_api.py
  08_test_logging.py
  09_test_google_search.py
  10_test_multiturn.py
  11_test_llm_providers.py
  12_test_workflow_stage.py
  13_test_canvas_api.py
  14_test_memory.py
  15_test_citations.py
  16_test_export.py
  17_test_chat_stream.py
  18_eval_rag.py
  19_cleanup_storage.py
  20_bootstrap_admin.py
  21_test_intent_override.py
  22_test_offline_models.py
  23_sync_local_models.py
  23_test_deep_research_e2e.py
  24_generate_eval_dataset.py
  25_extract_claims.py
  26_backfill_doi.py
  27_test_year_window_and_citation_style.py
  check_sonar_health.py
  debug_chat.py
  demo_llm_manager.py
  start.sh
  test_chat_hybrid_optimizer.py
  verify_dependencies.sh
src/
  api/
    __init__.py
    routes_auth.py
    routes_auto.py
    routes_canvas.py
    routes_chat.py
    routes_compare.py
    routes_export.py
    routes_graph.py
    routes_ingest.py
    routes_models.py
    routes_project.py
    schemas.py
    server.py
  auth/
    __init__.py
    password.py
    session.py
  chunking/
    __init__.py
    chunker.py
  collaboration/
    canvas/
      __init__.py
      canvas_manager.py
      canvas_store.py
      models.py
    citation/
      __init__.py
      formatter.py
      manager.py
    export/
      __init__.py
      formatter.py
    intent/
      __init__.py
      commands.py
      parser.py
    memory/
      __init__.py
      persistent_store.py
      session_memory.py
      working_memory.py
    research/
      __init__.py
      agent.py
      dashboard.py
      job_store.py
      trajectory.py
      verifier.py
    workflow/
      __init__.py
      graph.py
      states.py
      transitions.py
    __init__.py
    auto_complete.py
  evaluation/
    __init__.py
    dataset.py
    metrics.py
    runner.py
  generation/
    __init__.py
    context_packer.py
    evidence_synthesizer.py
    llm_client.py
  graph/
    __init__.py
    hippo_rag.py
  graphs/
    __init__.py
    ingestion_graph.py
  indexing/
    __init__.py
    embedder.py
    ingest_job_store.py
    milvus_ops.py
    paper_metadata_store.py
    paper_store.py
  llm/
    __init__.py
    llm_manager.py
    react_loop.py
    tools.py
  log/
    __init__.py
    log_manager.py
  mcp/
    __init__.py
    server.py
  observability/
    __init__.py
    metrics.py
    middleware.py
    setup.py
    tracing.py
  parser/
    __init__.py
    claim_extractor.py
    pdf_parser.py
  pipelines/
    __init__.py
  prompts/
    coherence_refine.txt
    evaluate_sufficiency.txt
    extract_claims.txt
    generate_abstract.txt
    generate_claims.txt
    generate_queries.txt
    limitations_section.txt
    open_gaps_agenda.txt
    plan_outline.txt
    scope_research.txt
    translate_content.txt
    verify_claims.txt
    write_section.txt
  retrieval/
    __init__.py
    colbert_reranker.py
    dedup.py
    evidence.py
    google_search.py
    hybrid_retriever.py
    ncbi_search.py
    query_optimizer.py
    semantic_scholar.py
    service.py
    smart_query_optimizer.py
    unified_web_search.py
    web_content_fetcher.py
    web_search.py
  utils/
    __init__.py
    cache.py
    limiter.py
    model_sync.py
    prompt_manager.py
    storage_cleaner.py
    task_runner.py
  __init__.py
tests/
  __init__.py
  conftest.py
  test_chunker.py
  test_citation_resolution.py
  test_hybrid_retriever.py
  test_intent_parser.py
  test_research_agent_state_compaction.py
  test_research_meta_analysis_guards.py
  test_tools_routing.py
  test_tracing_modes.py
.env.example
.gitignore
docker-compose.yml
index.jsx
install.md
pytest.ini
README.md
requirements.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/llm-calling-convention.mdc">
---
description: LLM 调用规范 - 使用 src/llm/llm_manager.py 统一管理
alwaysApply: true
---

# LLM 调用规范

本项目使用 `src/llm/llm_manager.py` 统一管理所有 LLM 调用。

## 推荐方式（新代码）

```python
from src.llm import LLMManager

# 加载配置
manager = LLMManager.from_json("config/rag_config.json")

# 获取客户端
client = manager.get_client("deepseek")  # 或 claude, openai, gemini, kimi 等

# 发送请求
resp = client.chat(
    messages=[
        {"role": "system", "content": "你是一个助手"},
        {"role": "user", "content": "问题内容"}
    ],
    model=None,           # 可选：覆盖默认模型
    max_tokens=2000,      # 可选：覆盖默认参数
)

# 使用响应
text = resp["final_text"]           # 最终回答（业务层使用）
reasoning = resp["reasoning_text"]  # 思考过程（可选，thinking 模式）
usage = resp["meta"]["usage"]       # token 用量
```

## 兼容方式（旧代码）

```python
from src.generation.llm_client import call_llm

result = call_llm(
    provider="deepseek",
    system="系统提示",
    user_prompt="用户问题",
    model_override=None,
    max_tokens=2000
)
# result 为字符串（final_text）
```

## 可用 Providers

- `openai` / `openai-thinking`
- `deepseek` / `deepseek-thinking`
- `gemini` / `gemini-thinking` / `gemini-vision`
- `claude` / `claude-thinking`
- `kimi` / `kimi-thinking` / `kimi-vision`

## 配置来源

- 配置文件：`config/rag_config.json`
- 环境变量覆盖 API Key：`RAG_LLM__{PROVIDER}__API_KEY`（如 `RAG_LLM__CLAUDE__API_KEY`）
- 日志目录：`logs/llm_raw/YYYY-MM-DD.jsonl`

## 禁止做法

- ❌ 直接使用 `openai.OpenAI()` 或 `anthropic.Anthropic()`
- ❌ 硬编码 API Key
- ❌ 绕过 LLMManager 直接发 HTTP 请求
</file>

<file path="config/__init__.py">
from config.settings import settings

__all__ = ["settings"]
</file>

<file path="config/llm_module_construction.md">
```markdown
# Cursor 开发指导（最终版）：单文件 `llm_manager.py` —— Provider 统一封装 + final/reasoning 隔离 + Raw JSON 落库与清理

## English query prompt (for Cursor)
Build a single-file Python module `llm_manager.py` that loads an LLM provider configuration from JSON (same schema as rag_config.example.json), implements a unified Provider base class with subclasses for OpenAI-compatible and Anthropic, deep-merges default params with per-call overrides, resolves model aliases via `models` mapping, supports API key env override, supports dry-run mode, always stores raw JSON responses to JSONL logs, and normalizes outputs by returning `final_text` by default while extracting `reasoning_text` (best-effort) without mixing it into `final_text`. Add log retention: delete logs older than 10 days and keep total log size under 100MB by deleting oldest files.

---

## 0. 背景/输入配置（必须兼容）

配置文件结构参考 `rag_config.example.json`，关键点如下：

- 默认 provider：`llm.default = "claude"` [1]
- dry_run 开关：`llm.dry_run = false` [1]
- providers 里包含多种 profile（如 `openai-thinking`, `claude-thinking`, `gemini-thinking`, `kimi-vision` 等）[1]
- provider 配置字段：
  - `api_key`, `base_url`, `default_model`, `models`（alias -> real model），`params`（默认参数）[1]
- thinking 相关默认参数示例：
  - `openai-thinking.params.reasoning_effort = "high"` [1]
  - `claude-thinking.params.thinking = {"type":"enabled","budget_tokens":16000}` 且 `max_tokens=32000` [1]
  - `gemini-thinking.params.thinkingConfig.thinkingBudget=16000`（走 OpenAI-compatible base_url）[1]
  - `kimi-thinking.params.enable_reasoning=true`, `reasoning_effort="high"` [1]
- 视觉/结构化输出示例：
  - `kimi-vision.params.response_format={"type":"json_object"}` [1]

---

## 1. 总目标（MVP）

做一个统一的 LLM 管理器，后续用于 LangGraph/RAG 节点调用，要求：

1) 单文件实现：`llm_manager.py`  
2) Provider 基类统一封装：不同模型/服务实现 `request()`  
3) 支持 OpenAI-compatible 与 Anthropic 两类 HTTP 客户端  
4) **输出必须规范化**：默认只返回最终答案 `final_text`；如果响应包含思考过程，则尽力抽取到 `reasoning_text`，但绝不混入 `final_text`  
5) **Raw JSON 必须落库**（jsonl），并做定量清理：总大小 >100MB 删除旧文件；超过 10 天删除旧文件  
6) API key 支持环境变量覆盖，且严禁明文打印 key  

---

## 2. 文件交付

- 必须：`llm_manager.py`
- 可选：`demo_llm_manager.py`（简单演示加载配置与 dry_run）

---

## 3. 统一 API 设计

### 3.1 `LLMManager`
必须实现：

- `LLMManager.from_json(path: str) -> LLMManager`
- `get_provider_names() -> list[str]`
- `get_client(provider: str | None = None, api_key: str | None = None) -> BaseChatClient`
- `resolve_model(provider: str, model: str | None) -> str`
  - model=None => provider.default_model [1]
  - model 在 `provider.models` 映射中 => 用映射值 [1]
  - 否则直接用传入 model

### 3.2 `BaseChatClient.chat()`
统一消息格式（OpenAI 风格）：

```python
messages = [{"role": "user", "content": "hello"}]
```

统一调用：

```python
resp = client.chat(
  messages=messages,
  model=None,                 # 可选：alias 或真实模型名
  return_reasoning=False,     # 默认 False：业务层只用 final_text
  **overrides                 # 覆盖 params
)
```

---

## 4. 输出规范化（你必须实现这层）

### 4.1 统一返回结构（所有 client 一致）
`chat()` 返回 dict：

```python
{
  "provider": "claude-thinking",
  "model": "resolved-model-name",
  "final_text": "...",            # 业务层默认使用
  "reasoning_text": None | "...", # 仅旁路信息，不混入 final_text
  "raw": {...},                   # 完整原始 JSON（必须）
  "params": {...},                # 实际使用的 merged params（必须）
  "meta": {
      "usage": {...} | None,
      "latency_ms": int | None,
      "refusal": bool | None
  }
}
```

### 4.2 `normalize_response(provider_name, raw)`（必须）
实现一个 best-effort 的提取器：

```python
def normalize_response(provider_name: str, raw: dict) -> dict:
    """
    Returns:
      final_text: str|None
      reasoning_text: str|None
      usage: dict|None
      refusal: bool|None
    """
```

要求：
- `final_text`：只抽取最终答案（可展示）
- `reasoning_text`：尽力抽取（如果有），但不影响 final_text
- 字段缺失时必须容错，不允许抛 KeyError
- 抽不到 reasoning_text 是允许的，因为 raw 会落库

#### OpenAI-compatible（/chat/completions）建议抽取规则
- `final_text`：优先 `choices[0].message.content`（str）  
  - 如果是 list（多段），只拼接 `type=="text"` 段落
- `reasoning_text`：多候选探测（抽不到也 OK）
  - `choices[0].message.reasoning` / `thoughts`（若存在）
  - content 为 list 时，拼接 `type in ("reasoning","thinking")` 段落

#### Anthropic（/v1/messages）建议抽取规则
- `final_text`：拼接 `raw["content"]` 中 `type=="text"` 的 `.text`
- `reasoning_text`：如果 content 中有 `type=="thinking"` 或类似块，抽取其文本；否则 None

---

## 5. Provider 基类与两类实现（你要求的统一封装）

在单文件中实现：

```python
class Provider:
    def request(self, payload: dict) -> dict:
        raise NotImplementedError
```

子类：
- `OpenAICompatProvider(Provider)`：POST `{base_url}/chat/completions`
  - Header：`Authorization: Bearer {api_key}`
- `AnthropicProvider(Provider)`：POST `{base_url}/v1/messages`
  - Headers：`x-api-key`, `anthropic-version: 2023-06-01`

然后 `ChatClient` 调用 `provider.request()`，拿到 raw，再走 `normalize_response()`。

> 注：`gemini/deepseek/kimi` 在示例里是 OpenAI-compatible base_url，可统一走 `OpenAICompatProvider` [1]。`claude/claude-thinking` 走 `AnthropicProvider` [1]。

---

## 6. 参数合并（必须深合并）

### 6.1 默认参数来源
- provider `params` 作为默认 [1]
- `chat()` 的 `**overrides` 覆盖默认

### 6.2 深合并实现
实现：

```python
def deep_merge(base: dict, override: dict) -> dict:
    # recursive dict merge
```

必须支持嵌套对象：
- `thinking`, `thinkingConfig`, `response_format` 等 [1]

---

## 7. API key 覆盖策略（必须）

优先级：
1) `get_client(..., api_key=...)`
2) 环境变量：`RAG_LLM__{PROVIDER}__API_KEY`
   - provider 名称转大写
   - `-` 替换为 `_`
   - 例：`claude-thinking` => `RAG_LLM__CLAUDE_THINKING__API_KEY`
3) JSON 里的 `api_key` [1]

安全：
- 不允许在 print/log/异常里输出完整 key
- 实现 `mask_secret()` 仅保留前后少量字符

---

## 8. dry_run（必须）

如果 `llm.dry_run == true` [1]：
- `LLMManager.get_client()` 返回 `DryRunChatClient`
- DryRun 返回同样结构，raw 里写：
  - `{"dry_run": true, "note": "..."}`
- 仍然执行参数合并与 model 解析，便于测试工作流一致性

---

## 9. Raw JSON 落库 + 清理策略（你要求必须做）

### 9.1 设计：jsonl 按天写文件
实现一个简单的 `RawLogStore`：

- 默认目录：`logs/llm_raw/`
- 文件名：`YYYY-MM-DD.jsonl`
- 每次 `chat()` 成功或失败都要记录（失败记录 error 信息 + request 摘要）

record 建议字段：
- `timestamp`
- `provider`, `model`
- `params`
- `messages_digest`（role + content 前 N 字符；默认 N=200，可配置）
- `final_text`（可选）
- `reasoning_text`（可选，可能很大；默认也可保存）
- `raw_response`（完整 raw JSON）
- `meta`（usage/latency/refusal）
- `error`（如果异常）

### 9.2 清理策略：超过 10 天 + 总大小 100MB
实现：

```python
cleanup(max_age_days: int = 10, max_total_mb: int = 100)
```

规则：
1) 删除超过 10 天的 log 文件
2) 若总大小 > 100MB，从最旧文件开始删除直到小于阈值

默认值必须是：10 days + 100MB（但允许调用者改）。


---

## 10. 建议的单文件代码骨架（Cursor 按这个顺序写）

1) imports / constants（anthropic version、默认超时等）
2) dataclasses：
   - `ProviderConfig`
   - `LLMConfig`
3) helper funcs：
   - `load_json()`
   - `deep_merge()`
   - `mask_secret()`
   - `provider_env_var()`
   - `now_iso()`
4) `RawLogStore`
5) Provider classes：
   - `Provider`
   - `OpenAICompatProvider`
   - `AnthropicProvider`
6) Response normalization：
   - `normalize_response()`
7) Chat clients：
   - `BaseChatClient`
   - `DryRunChatClient`
   - `HTTPChatClient`（内部持有 Provider、config、logstore）
8) `LLMManager`

---

## 11. 验收标准（我会这样验收）

- 能加载配置，默认 provider 为 `claude` [1]
- `resolve_model()` 能将 `claude-sonnet-4-5` 映射到版本化真实模型名（例如 `claude-sonnet-4-5-20250929`）[1]
- `openai-thinking` 会把 `reasoning_effort="high"` 作为默认参数透传 [1]
- `claude-thinking` 会把 `thinking.budget_tokens=16000` 与 `max_tokens=32000` 作为默认参数透传 [1]
- 返回结构永远包含：
  - `final_text`（不含 reasoning）
  - `reasoning_text`（尽力抽取）
  - `raw`（完整）
- Raw 日志会写 jsonl，且清理策略可用（10 天/100MB）

---

## 12. 备注（重要）
- 不要把任何 API key 写进 Git 仓库。
- 对 response 的 reasoning 字段提取只能 best-effort；抽不到不算失败，因为 raw 已保存，可后续离线解析。
- 若某 provider 的返回结构与预期不一致，请在代码中写 TODO 注释，保持可维护。

```

这份 MD 文档已把你的三条关键诉求都“落到可实现的工程要求”上了：
- Provider 基类统一封装（子类实现 request）
- Raw JSON 落库 + 10days/100MB 清理
- thinking 模式“抽取 final_text + 隔离 reasoning_text”，业务层默认只用 final_text，不污染输出

如果你还希望加一条：**`reasoning_text` 默认不写入 messages_digest（避免泄露敏感推理）**，我也可以帮你补一个更严格的脱敏/采样策略。
</file>

<file path="config/Untitled">
a7c37cae6ebc807ec4049d1f239eeaa2aa09
</file>

<file path="data/parsed/.gitkeep">
# Parsed paper output directory - structure preserved for local data
</file>

<file path="data/parsed_test/.gitkeep">
# Test parsed papers - structure preserved for local data
</file>

<file path="data/.gitkeep">
# This file ensures the data directory structure is preserved in git
# while actual data files are ignored by .gitignore
# 
# Allowed files in this directory:
# - .gitkeep (this file)
# - eval_mini.json (test dataset)
# - Any subdirectories with .gitkeep files
#
# All other data files should be generated locally and not committed to git
</file>

<file path="data/eval_mini.json">
{
  "name": "deepsea_eval_v1",
  "description": "Deep-sea RAG evaluation dataset — 50 cases across 14 papers, covering factual, synthesis, methodology, data_query, and unanswerable types",
  "version": "1.0.0",
  "cases": [
    {
      "id": "f01",
      "query": "What effect does artificial light at night (ALAN) have on oyster microbiota?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Botté_et_al_Artificial_Light_at_Night_Affects_Microbiota"],
      "expected_citations": [],
      "reference_answer": "ALAN exposure at low intensity (~1 lx) disrupts the gill microbiota of Crassostrea gigas, obliterating the day/night difference in microbial alpha diversity. A direct correlation was found between decreased daily rhythm robustness, limited shell growth, and changes in certain microbial strands."
    },
    {
      "id": "f02",
      "query": "What organisms dominate the cold seep ecosystem in the Krishna-Godavari basin?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Peketi_et_al_Cold_seep_induced_elevated_water_column"],
      "expected_citations": [],
      "reference_answer": "The cold seep ecosystem is predominantly populated with Bathymodiolus species, along with tube worms (Siboglinidae) and goose barnacles. The ecosystem was found at site SN184/KG-1 at approximately 1750 m water depth."
    },
    {
      "id": "f03",
      "query": "How does microplastic exposure affect viral communities in Mytilus coruscus?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Zhang_et_al_Microplastic_exposure_reshapes_the_virome_and"],
      "expected_citations": [],
      "reference_answer": "Microplastic exposure induces divergent responses in DNA and RNA viral communities. DNA viromes show suppressed diversity and downregulation of core viral metabolic pathways, while RNA viromes display metabolic activation including enriched glycan and nucleotide metabolism. Phage-bacteria interaction networks are restructured with increased associations with opportunistic pathogens like Vibrio cholerae and Enterobacter."
    },
    {
      "id": "f04",
      "query": "What is the Deep Ocean Omics (DOO) database and what data does it integrate?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_She_et_al_DOO_integrated_multi-omics_resources_for_deep"],
      "expected_citations": [],
      "reference_answer": "DOO (https://DeepOceanOmics.org) is a multi-omics atlas for deep ocean organisms. It integrates diverse omics resources from 68 species across seven phyla and 16 classes, encompassing 72 genomes, 950 bulk transcriptomes, 15 single-cell transcriptomes, and 1112 metagenomes, alongside functional support toolkits for functional and comparative analysis."
    },
    {
      "id": "f05",
      "query": "What cell types were identified in the oyster single-cell atlas?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Chen_et_al_Oyster_Cellular_Architecture_and_Innate_Immune"],
      "expected_citations": [],
      "reference_answer": "The tissue-resolved single-cell atlas of Crassostrea hongkongensis defined twenty cell types. This includes a diversified phagocyte lineage of five phagocytic cells with three terminal strategies (rapid extracellular containment, deep intracellular clearance, resolution program), three humoral immune populations, barrier epithelial cells, and a neural-related compartment comprising neural progenitor-like, neuron-like, and neuroendocrine cells."
    },
    {
      "id": "f06",
      "query": "Does the deep-sea mussel Bathymodiolus azoricus have an endogenous circadian rhythm?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Grace_et_al_Shell_trace_elemental_fingerprints_of_the"],
      "expected_citations": [],
      "reference_answer": "Yes. Isolated cells from B. azoricus displayed circadian oscillations under constant conditions despite tidal-dominant rhythms in situ. Reporter assays indicate a functional transcription-translation feedback loop (TTFL) underpins circadian timing even in the deep sea. Notably, BazPeriod lacks autonomous repressive activity but modulates BazCry2, and it oscillates tidally, which may explain how a single endogenous clock yields both tidal and diel rhythms."
    },
    {
      "id": "f07",
      "query": "What shell microstructures were found in Bathymodiolus marisindicus?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Gozeoglu_et_al_Assessment_of_nickel_nanoparticle_toxicity_on"],
      "expected_citations": [],
      "reference_answer": "Using TEM and SEM characterization, the shell of B. marisindicus from ~2700 m depth in the Southwest Indian Ocean Ridge was found to have a trilayered structure: periostracum (outer layer), prismatic layer, and nacreous layer."
    },
    {
      "id": "f08",
      "query": "What genetic connectivity patterns were found in Atlantic Rimicaris vent shrimp?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Portanier_et_al_High_gene_flow_in_Atlantic_vent"],
      "expected_citations": [],
      "reference_answer": "Using ddRAD sequencing, the study found high gene flow in Atlantic Rimicaris vent shrimp species. Rimicaris exoculata and R. chacei inhabit the Mid-Atlantic Ridge (MAR). The analysis examined genetic differentiation to understand past and present migration pathways and demographic history that led to their current distribution."
    },
    {
      "id": "f09",
      "query": "What are the key genes involved in ovarian development of Sepiella japonica?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Xu_et_al_Comparative_transcriptomic_analysis_across_ovarian_developmental"],
      "expected_citations": [],
      "reference_answer": "Key regulatory genes showing consistently high expression across four ovarian developmental stages include FOXL2, HSD17B2, and BMP1. A total of 13,319 differentially expressed genes were identified. KEGG pathway analysis revealed enrichment in Ribosome, Prolactin signaling, FOXO signaling, and Lysosome pathways."
    },
    {
      "id": "f10",
      "query": "What is the performance of UCE sequencing for deep-sea mussel population genomics?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Li_et_al_Sequencing_Ultraconserved_Elements_(UCEs)_for_Marine"],
      "expected_citations": [],
      "reference_answer": "UCE target capture sequencing was applied to 123 individuals of Gigantidas platifrons. The species has a panmictic genetic structure across its extensive distribution range in chemosynthetic ecosystems of the Western Pacific. UCEs were evaluated against published whole genome and 2b-RAD seq data as a tool for marine population genomics."
    },
    {
      "id": "f11",
      "query": "What dissolved methane concentrations were measured at SN184/KG-1?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Peketi_et_al_Cold_seep_induced_elevated_water_column"],
      "expected_citations": [],
      "reference_answer": "Dissolved methane concentrations at SN184/KG-1 varied from 0.18 to 1.48 µmol/L. At the second site SN184/KG-2, concentrations were lower, ranging from 0.14 to 0.5 µmol/L. The high concentrations at KG-1 were attributed to gas bubbling from the cold seeps."
    },
    {
      "id": "f12",
      "query": "Which circadian clock genes were impaired by ALAN in oysters?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Botté_et_al_Artificial_Light_at_Night_Affects_Microbiota"],
      "expected_citations": [],
      "reference_answer": "ALAN significantly impaired the expression of core circadian clock genes CgClock and CgBmal1 in Crassostrea gigas, affecting rhythmic characteristics such as robustness and daily profile."
    },
    {
      "id": "f13",
      "query": "What antibiotic resistance gene changes were caused by microplastic exposure in mussels?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Zhang_et_al_Microplastic_exposure_reshapes_the_virome_and"],
      "expected_citations": [],
      "reference_answer": "The expression of antibiotic resistance genes (ARGs) in viral genomes was differentially regulated by microplastic exposure, suggesting pollutant-induced microbial selection that may challenge host immune resilience."
    },
    {
      "id": "f14",
      "query": "How many species and genomes does the DOO database cover?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_She_et_al_DOO_integrated_multi-omics_resources_for_deep"],
      "expected_citations": [],
      "reference_answer": "DOO integrates data from 68 species across seven phyla and 16 classes, encompassing 72 genomes, 950 bulk transcriptomes, 15 single-cell transcriptomes, and 1112 metagenomes."
    },
    {
      "id": "f15",
      "query": "What phagocytic strategies were identified in oyster immune cells?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Chen_et_al_Oyster_Cellular_Architecture_and_Innate_Immune"],
      "expected_citations": [],
      "reference_answer": "Three terminal phagocytic strategies were identified: (1) rapid extracellular containment, (2) deep intracellular clearance, and (3) a resolution program that couples degradation with matrix restoration. These were carried out by five phagocytic cell types in a diversified phagocyte lineage."
    },
    {
      "id": "f16",
      "query": "What causes precocious sexual maturation in captive Sepiella japonica?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Xu_et_al_Comparative_transcriptomic_analysis_across_ovarian_developmental"],
      "expected_citations": [],
      "reference_answer": "Under captive conditions, S. japonica frequently exhibits precocious sexual maturation, which results in reduced adult body size. The molecular mechanisms involve genes like FOXL2, HSD17B2, and BMP1, with pathways including Prolactin signaling and FOXO signaling being enriched."
    },
    {
      "id": "f17",
      "query": "At what depth was the deep-sea mussel B. marisindicus discovered?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Gozeoglu_et_al_Assessment_of_nickel_nanoparticle_toxicity_on"],
      "expected_citations": [],
      "reference_answer": "Bathymodiolus marisindicus was discovered living at a depth of about 2,700 meters on the Southwest Indian Ocean Ridge."
    },
    {
      "id": "f18",
      "query": "What differentially expressed genes were identified across ovarian stages in cuttlefish?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Xu_et_al_Comparative_transcriptomic_analysis_across_ovarian_developmental"],
      "expected_citations": [],
      "reference_answer": "Pairwise comparisons identified 10,050 DEGs between stages I and II, 2,564 between stages II and III, and 2,278 between stages III and IV, with a total of 13,319 differentially expressed genes across all comparisons."
    },
    {
      "id": "f19",
      "query": "What is the BazPeriod gene's role in deep-sea mussel circadian timing?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Grace_et_al_Shell_trace_elemental_fingerprints_of_the"],
      "expected_citations": [],
      "reference_answer": "BazPeriod lacks autonomous repressive activity but modulates BazCry2. Since BazPeriod itself oscillates tidally, it may explain how a single endogenous clock yields both tidal and diel rhythms in B. azoricus."
    },
    {
      "id": "f20",
      "query": "What was the survey method used to study cold seep ecosystems in the K-G basin?",
      "mode": "local",
      "tags": ["factual"],
      "expected_doc_ids": ["2026_Peketi_et_al_Cold_seep_induced_elevated_water_column"],
      "expected_citations": [],
      "reference_answer": "An autonomous underwater vehicle (AUV) survey was conducted at two cold seep locations in the Krishna-Godavari basin. The survey included water column methane concentration measurements and seabed surveys using underwater photographic studies."
    },
    {
      "id": "s01",
      "query": "How do different marine pollutants (artificial light vs microplastics) affect biological rhythms and microbiota in marine organisms?",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_Botté_et_al_Artificial_Light_at_Night_Affects_Microbiota",
        "2026_Zhang_et_al_Microplastic_exposure_reshapes_the_virome_and"
      ],
      "expected_citations": [],
      "reference_answer": "Both ALAN and microplastics disrupt marine organism microbiota but through different mechanisms. ALAN disrupts circadian clock genes and obliterates day/night microbial diversity differences in oyster gills. Microplastics reshape the virome, differentially affecting DNA and RNA viral communities, and restructure phage-bacteria networks with increased pathogen associations. Both pollutants ultimately challenge host immune resilience."
    },
    {
      "id": "s02",
      "query": "What population genomics approaches have been applied to deep-sea vent organisms, and what connectivity patterns do they reveal?",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_Portanier_et_al_High_gene_flow_in_Atlantic_vent",
        "2026_Li_et_al_Sequencing_Ultraconserved_Elements_(UCEs)_for_Marine"
      ],
      "expected_citations": [],
      "reference_answer": "Two major approaches: ddRAD sequencing was used for Atlantic Rimicaris vent shrimp, revealing high gene flow along the Mid-Atlantic Ridge. UCE target capture sequencing was applied to Gigantidas platifrons in the Western Pacific, confirming panmictic genetic structure. Both studies demonstrate extensive connectivity in deep-sea vent organisms despite large geographic distances, likely due to long larval dispersal periods."
    },
    {
      "id": "s03",
      "query": "Compare the biological adaptations of deep-sea mussels across different environments (cold seeps vs hydrothermal vents).",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_Peketi_et_al_Cold_seep_induced_elevated_water_column",
        "2026_Grace_et_al_Shell_trace_elemental_fingerprints_of_the",
        "2026_Gozeoglu_et_al_Assessment_of_nickel_nanoparticle_toxicity_on"
      ],
      "expected_citations": [],
      "reference_answer": "Deep-sea mussels show distinct adaptations depending on their environment. In cold seeps (K-G basin), Bathymodiolus species dominate ecosystems with methane hydrates at ~1750 m depth. At hydrothermal vents, B. azoricus maintains both tidal and circadian rhythms at -1700 m despite lacking diel cues. B. marisindicus at 2700 m on the SW Indian Ocean Ridge exhibits specialized shell microstructures (periostracum, prismatic layer, nacreous layer) adapted to extreme depth."
    },
    {
      "id": "s04",
      "query": "What multi-omics approaches have been used to study deep ocean organisms?",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_She_et_al_DOO_integrated_multi-omics_resources_for_deep",
        "2026_Chen_et_al_Oyster_Cellular_Architecture_and_Innate_Immune"
      ],
      "expected_citations": [],
      "reference_answer": "Multiple omics approaches have been applied: The DOO database integrates genomes, bulk transcriptomes, single-cell transcriptomes, and metagenomes across 68 species. Single-cell RNA sequencing has been used to resolve cellular architecture in Crassostrea hongkongensis, identifying 20 cell types. Virome sequencing has revealed microbial interactions. These tools provide increasingly detailed views of marine organism biology from ecosystem to single-cell level."
    },
    {
      "id": "s05",
      "query": "How do different deep-sea species maintain biological rhythms in the absence of light?",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_Grace_et_al_Shell_trace_elemental_fingerprints_of_the",
        "2026_Botté_et_al_Artificial_Light_at_Night_Affects_Microbiota"
      ],
      "expected_citations": [],
      "reference_answer": "Deep-sea mussel B. azoricus maintains endogenous circadian rhythms via a functional TTFL involving BazPeriod and BazCry2, even at -1700 m without diel cues. Tidal rhythms appear dominant in situ. In contrast, shallow-water oysters C. gigas are highly sensitive to light disruption — even low-intensity ALAN (~1 lx) impairs circadian clock genes CgClock and CgBmal1, disrupting growth and microbiota rhythms."
    },
    {
      "id": "s06",
      "query": "What are the conservation concerns related to deep-sea vent ecosystems based on current research?",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_Portanier_et_al_High_gene_flow_in_Atlantic_vent",
        "2026_Peketi_et_al_Cold_seep_induced_elevated_water_column",
        "2026_She_et_al_DOO_integrated_multi-omics_resources_for_deep"
      ],
      "expected_citations": [],
      "reference_answer": "Key conservation concerns include potential mineral extraction near hydrothermal vents, which requires understanding species connectivity. High gene flow in Rimicaris shrimp suggests interconnected populations along the MAR. Cold seep ecosystems in the K-G basin host unique biodiversity dominated by Bathymodiolus. The DOO database highlights that deep ocean environments remain Earth's least explored frontiers, and comprehensive omics data are needed to understand genetic adaptations before potential industrial impacts."
    },
    {
      "id": "s07",
      "query": "How do immune systems differ between oyster species studied in recent research?",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_Chen_et_al_Oyster_Cellular_Architecture_and_Innate_Immune",
        "2026_Botté_et_al_Artificial_Light_at_Night_Affects_Microbiota"
      ],
      "expected_citations": [],
      "reference_answer": "Single-cell analysis of C. hongkongensis revealed a sophisticated innate immune system with 20 cell types including five phagocytic cells with three terminal strategies and three humoral immune populations. In C. gigas, research focused on how ALAN disrupts the gill microbiota and biological rhythms, which are linked to immune function. Together these studies show that oyster immunity involves complex cellular architecture susceptible to environmental perturbation."
    },
    {
      "id": "s08",
      "query": "What environmental stressors affect marine bivalve health based on recent studies?",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_Botté_et_al_Artificial_Light_at_Night_Affects_Microbiota",
        "2026_Zhang_et_al_Microplastic_exposure_reshapes_the_virome_and",
        "2026_Gozeoglu_et_al_Assessment_of_nickel_nanoparticle_toxicity_on"
      ],
      "expected_citations": [],
      "reference_answer": "Multiple stressors affect marine bivalves: (1) ALAN disrupts circadian rhythms and microbiota in oysters, reducing shell growth. (2) Microplastics reshape viral communities and increase antibiotic resistance gene expression in Mytilus coruscus. (3) Extreme depth environments require specialized shell microstructural adaptations in Bathymodiolus marisindicus. These stressors collectively challenge bivalve immune resilience and physiological function."
    },
    {
      "id": "s09",
      "query": "Summarize the advances in understanding deep-sea mussel biology from recent publications.",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_Grace_et_al_Shell_trace_elemental_fingerprints_of_the",
        "2026_Gozeoglu_et_al_Assessment_of_nickel_nanoparticle_toxicity_on",
        "2026_Li_et_al_Sequencing_Ultraconserved_Elements_(UCEs)_for_Marine",
        "2026_Peketi_et_al_Cold_seep_induced_elevated_water_column"
      ],
      "expected_citations": [],
      "reference_answer": "Recent advances span multiple dimensions: B. azoricus exhibits endogenous circadian rhythms driven by TTFL at hydrothermal vents (-1700 m). B. marisindicus has specialized trilayered shell microstructures at 2700 m. G. platifrons shows panmictic genetic structure across the Western Pacific via UCE genomics. Cold seep Bathymodiolus dominate K-G basin ecosystems. Collectively, these studies reveal remarkable adaptations to extreme environments in deep-sea mussels."
    },
    {
      "id": "s10",
      "query": "How has transcriptomics advanced our understanding of marine invertebrate reproduction?",
      "mode": "local",
      "tags": ["synthesis"],
      "expected_doc_ids": [
        "2026_Xu_et_al_Comparative_transcriptomic_analysis_across_ovarian_developmental",
        "2026_Chen_et_al_Oyster_Cellular_Architecture_and_Innate_Immune"
      ],
      "expected_citations": [],
      "reference_answer": "Transcriptomics has revealed key insights: RNA-seq across four ovarian stages in Sepiella japonica identified 13,319 DEGs including critical regulators FOXL2, HSD17B2, and BMP1. Single-cell transcriptomics in C. hongkongensis resolved 20 cell types including neural-related cells. These approaches uncover molecular mechanisms of reproduction and cellular organization in marine invertebrates."
    },
    {
      "id": "m01",
      "query": "What sequencing method was used to study genetic differentiation in Rimicaris shrimp?",
      "mode": "local",
      "tags": ["methodology"],
      "expected_doc_ids": ["2026_Portanier_et_al_High_gene_flow_in_Atlantic_vent"],
      "expected_citations": [],
      "reference_answer": "ddRAD (double digest restriction site-associated DNA) sequencing was used to examine genetic differentiation of three Atlantic Rimicaris species to understand their past and present migration pathways."
    },
    {
      "id": "m02",
      "query": "How was the single-cell atlas of oyster constructed?",
      "mode": "local",
      "tags": ["methodology"],
      "expected_doc_ids": ["2026_Chen_et_al_Oyster_Cellular_Architecture_and_Innate_Immune"],
      "expected_citations": [],
      "reference_answer": "A tissue-resolved single-cell atlas was generated using single-cell RNA sequencing of Crassostrea hongkongensis. Cell type identification was performed along with phagocyte lineage analysis, immune cell communication analysis, and ligand-receptor inference to resolve cellular architecture and innate immune programs."
    },
    {
      "id": "m03",
      "query": "What techniques were used to characterize the shell microstructure of deep-sea mussels?",
      "mode": "local",
      "tags": ["methodology"],
      "expected_doc_ids": ["2026_Gozeoglu_et_al_Assessment_of_nickel_nanoparticle_toxicity_on"],
      "expected_citations": [],
      "reference_answer": "Transmission electron microscopy (TEM) and scanning electron microscopy (SEM) were used. SEM provided detailed information on shell surface morphology and layered structure. TEM was used for investigating high-resolution microstructural and crystallographic details."
    },
    {
      "id": "m04",
      "query": "How was the virome analyzed in the microplastic exposure study?",
      "mode": "local",
      "tags": ["methodology"],
      "expected_doc_ids": ["2026_Zhang_et_al_Microplastic_exposure_reshapes_the_virome_and"],
      "expected_citations": [],
      "reference_answer": "Mytilus coruscus individuals were exposed to microplastics in situ for seven days. Virome sequencing and bioinformatic analyses were performed, separately analyzing DNA and RNA viral communities, phage-bacteria interaction networks, and antibiotic resistance gene expression in viral genomes."
    },
    {
      "id": "m05",
      "query": "What experimental design was used to study ALAN effects on oysters?",
      "mode": "local",
      "tags": ["methodology"],
      "expected_doc_ids": ["2026_Botté_et_al_Artificial_Light_at_Night_Affects_Microbiota"],
      "expected_citations": [],
      "reference_answer": "ALAN exposure at low and realistic intensity (~1 lx) was applied to the coastal keystone species Crassostrea gigas. Measurements included circadian clock gene expression (CgClock and CgBmal1), valve opening behavior, daily shell growth, and gill microbiota analysis with microbial alpha diversity assessment."
    },
    {
      "id": "m06",
      "query": "How does UCE sequencing compare with other genomic methods for marine population studies?",
      "mode": "local",
      "tags": ["methodology"],
      "expected_doc_ids": ["2026_Li_et_al_Sequencing_Ultraconserved_Elements_(UCEs)_for_Marine"],
      "expected_citations": [],
      "reference_answer": "UCE target capture sequencing was compared against published whole genome and 2b-RAD seq data using Gigantidas platifrons as a test species. UCEs have low DNA quality requirements and broad taxonomic applicability, making them suitable for marine taxa with metapopulations spanning thousands of kilometers."
    },
    {
      "id": "m07",
      "query": "What RNA sequencing approach was used for the cuttlefish ovarian development study?",
      "mode": "local",
      "tags": ["methodology"],
      "expected_doc_ids": ["2026_Xu_et_al_Comparative_transcriptomic_analysis_across_ovarian_developmental"],
      "expected_citations": [],
      "reference_answer": "RNA sequencing was performed on ovary tissue of female S. japonica collected across four key developmental stages: oogonium production (stage I), protoplasmic growth (stage II), interstitial growth (stage III), and trophoplasmic growth (stage IV). A total of 354,393,214 clean reads (Q20 > 98.58%) were obtained from 16 samples."
    },
    {
      "id": "m08",
      "query": "How was endogenous circadian rhythm demonstrated in deep-sea mussels?",
      "mode": "local",
      "tags": ["methodology"],
      "expected_doc_ids": ["2026_Grace_et_al_Shell_trace_elemental_fingerprints_of_the"],
      "expected_citations": [],
      "reference_answer": "Endogenous circadian rhythms were demonstrated using cell cultures under constant conditions — isolated cells displayed circadian oscillations. Reporter assays using genomic regions upstream of the mussel's per gene containing E-box motifs confirmed a functional transcription-translation feedback loop (TTFL)."
    },
    {
      "id": "d01",
      "query": "What methane concentration ranges were measured at the two cold seep sites?",
      "mode": "local",
      "tags": ["data_query"],
      "expected_doc_ids": ["2026_Peketi_et_al_Cold_seep_induced_elevated_water_column"],
      "expected_citations": [],
      "reference_answer": "At SN184/KG-1 (water depth ~1750 m): dissolved methane varied from 0.18 to 1.48 µmol/L. At SN184/KG-2 (water depth ~950 m): dissolved methane varied from 0.14 to 0.5 µmol/L."
    },
    {
      "id": "d02",
      "query": "How many differentially expressed genes were found between each pair of ovarian developmental stages?",
      "mode": "local",
      "tags": ["data_query"],
      "expected_doc_ids": ["2026_Xu_et_al_Comparative_transcriptomic_analysis_across_ovarian_developmental"],
      "expected_citations": [],
      "reference_answer": "Stage I vs II: 10,050 DEGs; Stage II vs III: 2,564 DEGs; Stage III vs IV: 2,278 DEGs. Total across all comparisons: 13,319 DEGs."
    },
    {
      "id": "d03",
      "query": "How many species and omics datasets are in the DOO database?",
      "mode": "local",
      "tags": ["data_query"],
      "expected_doc_ids": ["2026_She_et_al_DOO_integrated_multi-omics_resources_for_deep"],
      "expected_citations": [],
      "reference_answer": "68 species across 7 phyla and 16 classes. Datasets: 72 genomes, 950 bulk transcriptomes, 15 single-cell transcriptomes, and 1,112 metagenomes."
    },
    {
      "id": "d04",
      "query": "How many cell types were identified in the oyster single-cell atlas?",
      "mode": "local",
      "tags": ["data_query"],
      "expected_doc_ids": ["2026_Chen_et_al_Oyster_Cellular_Architecture_and_Innate_Immune"],
      "expected_citations": [],
      "reference_answer": "Twenty cell types were defined in the tissue-resolved single-cell atlas of Crassostrea hongkongensis, including five phagocytic cells, three humoral immune populations, barrier epithelial cells, and neural-related cells."
    },
    {
      "id": "d05",
      "query": "What was the ALAN light intensity used in the oyster experiment?",
      "mode": "local",
      "tags": ["data_query"],
      "expected_doc_ids": ["2026_Botté_et_al_Artificial_Light_at_Night_Affects_Microbiota"],
      "expected_citations": [],
      "reference_answer": "ALAN exposure was at low and realistic intensity of approximately 1 lux (~1 lx)."
    },
    {
      "id": "d06",
      "query": "How many individuals were sequenced in the UCE study of Gigantidas platifrons?",
      "mode": "local",
      "tags": ["data_query"],
      "expected_doc_ids": ["2026_Li_et_al_Sequencing_Ultraconserved_Elements_(UCEs)_for_Marine"],
      "expected_citations": [],
      "reference_answer": "UCE target capture sequencing was conducted on 123 individuals of Gigantidas platifrons."
    },
    {
      "id": "d07",
      "query": "What is the depth of the cold seep site KG-2 in the Krishna-Godavari basin?",
      "mode": "local",
      "tags": ["data_query"],
      "expected_doc_ids": ["2026_Peketi_et_al_Cold_seep_induced_elevated_water_column"],
      "expected_citations": [],
      "reference_answer": "SN184/KG-2 is at approximately 950 m water depth."
    },
    {
      "id": "u01",
      "query": "What is the effect of deep-sea mining on hydrothermal vent biodiversity?",
      "mode": "local",
      "tags": ["unanswerable"],
      "expected_doc_ids": [],
      "expected_citations": [],
      "reference_answer": ""
    },
    {
      "id": "u02",
      "query": "What is the global distribution of microplastic pollution in the Arctic Ocean?",
      "mode": "local",
      "tags": ["unanswerable"],
      "expected_doc_ids": [],
      "expected_citations": [],
      "reference_answer": ""
    },
    {
      "id": "u03",
      "query": "How does climate change affect coral reef bleaching rates in the Caribbean?",
      "mode": "local",
      "tags": ["unanswerable"],
      "expected_doc_ids": [],
      "expected_citations": [],
      "reference_answer": ""
    },
    {
      "id": "u04",
      "query": "What pharmaceutical compounds have been derived from deep-sea organisms?",
      "mode": "local",
      "tags": ["unanswerable"],
      "expected_doc_ids": [],
      "expected_citations": [],
      "reference_answer": ""
    },
    {
      "id": "u05",
      "query": "What are the latest IPCC predictions for sea level rise by 2100?",
      "mode": "local",
      "tags": ["unanswerable"],
      "expected_doc_ids": [],
      "expected_citations": [],
      "reference_answer": ""
    }
  ]
}
</file>

<file path="docs/architecture.md">
# 系统架构

本文档描述 DeepSea RAG 的真实代码架构与核心数据流，便于开发、排障和二次扩展。

## 一、总体分层

```text
Frontend (React + Zustand + i18n)
  → API Layer (FastAPI routes_*.py)
    → Collaboration Layer (workflow / canvas / memory / research / intent / citation / export)
    → Agent Layer (llm_manager + tools + react_loop)
    → Retrieval Layer (hybrid + web + graph + rerank)
    → Persistence / Infra (Milvus + SQLite + files + observability)
```

## 二、后端模块映射（`src/`）

### 接口层

| 模块 | 职责 |
|---|---|
| `api/server.py` | FastAPI 应用入口，路由注册，生命周期管理 |
| `api/routes_chat.py` | 聊天、意图检测、Deep Research 全部接口 |
| `api/routes_canvas.py` | 画布 CRUD、大纲、草稿、快照、AI 编辑、引用管理 |
| `api/routes_ingest.py` | 在线入库（上传/Collections/任务管理） |
| `api/routes_compare.py` | 多文档对比 |
| `api/routes_graph.py` | 图谱统计、实体、邻居、chunk 详情 |
| `api/routes_export.py` | 导出（Markdown） |
| `api/routes_auto.py` | 自动补全（一键综述） |
| `api/routes_auth.py` | 认证与用户管理 |
| `api/routes_project.py` | 项目管理（存档/删除） |
| `api/routes_models.py` | 模型状态与同步 |
| `api/schemas.py` | Pydantic 请求/响应模型 |

### LLM / Agent 层

| 模块 | 职责 |
|---|---|
| `llm/llm_manager.py` | 统一 LLM 客户端管理（多 provider、日志、指标、结构化输出、tool-calling） |
| `llm/tools.py` | 工具定义与路由 |
| `llm/react_loop.py` | ReAct Agent 循环 |
| `generation/llm_client.py` | 兼容层（旧 `call_llm()` 接口） |
| `generation/evidence_synthesizer.py` | 证据综合（时间线排序、来源分组、强度分级、交叉验证） |
| `generation/context_packer.py` | 上下文打包 |

### 协作层

| 模块 | 职责 |
|---|---|
| `collaboration/canvas/` | 画布管理（canvas_manager + canvas_store + models） |
| `collaboration/memory/session_memory.py` | 会话级记忆（历史、引用、滚动摘要） |
| `collaboration/memory/working_memory.py` | 画布级工作记忆 |
| `collaboration/memory/persistent_store.py` | 持久用户偏好 |
| `collaboration/intent/parser.py` | 意图解析（Chat vs Deep Research） |
| `collaboration/intent/commands.py` | 命令处理 |
| `collaboration/research/agent.py` | Deep Research LangGraph Agent（核心） |
| `collaboration/research/verifier.py` | 声明验证 |
| `collaboration/research/job_store.py` | 后台任务与事件存储 |
| `collaboration/research/dashboard.py` | 研究进度仪表盘 |
| `collaboration/research/trajectory.py` | 研究轨迹追踪 |
| `collaboration/workflow/` | 状态机（states + transitions + graph） |
| `collaboration/citation/manager.py` | 引用解析（ref_hash → cite_key） |
| `collaboration/citation/formatter.py` | 引用格式化 |
| `collaboration/export/formatter.py` | 导出格式化 |
| `collaboration/auto_complete.py` | 一键综述服务 |

### 检索层

| 模块 | 职责 |
|---|---|
| `retrieval/hybrid_retriever.py` | 混合检索（dense + sparse + graph + RRF 融合） |
| `retrieval/service.py` | 检索服务编排 |
| `retrieval/unified_web_search.py` | 统一网络检索聚合器 |
| `retrieval/web_search.py` | Tavily 搜索 |
| `retrieval/web_content_fetcher.py` | 网页内容提取 |
| `retrieval/google_search.py` | Google / Scholar 搜索 |
| `retrieval/ncbi_search.py` | NCBI 文献搜索 |
| `retrieval/semantic_scholar.py` | Semantic Scholar API |
| `retrieval/colbert_reranker.py` | ColBERT 重排 |
| `retrieval/query_optimizer.py` | 规则查询优化 |
| `retrieval/smart_query_optimizer.py` | LLM 查询优化 |
| `retrieval/evidence.py` | 证据数据结构（EvidenceChunk / EvidencePack） |
| `retrieval/dedup.py` | 去重与多样化 |

### 数据处理层

| 模块 | 职责 |
|---|---|
| `parser/pdf_parser.py` | PDF 解析（Docling，含表格/图片/公式提取与 LLM 增强） |
| `parser/claim_extractor.py` | 声明提取 |
| `chunking/chunker.py` | 结构化切块（section-aware，表格行切块，句级 overlap） |
| `indexing/embedder.py` | 文本向量化（BGE-M3 dense + sparse，BGE-Reranker） |
| `indexing/milvus_ops.py` | Milvus 向量数据库操作（schema v1/v2，hybrid search） |
| `indexing/paper_store.py` | 论文元数据持久化（SQLite） |
| `indexing/paper_metadata_store.py` | 扩展元数据存储 |
| `indexing/ingest_job_store.py` | 入库任务追踪 |
| `graph/hippo_rag.py` | HippoRAG 知识图谱（实体/关系提取 + PPR 检索） |
| `graphs/ingestion_graph.py` | LangGraph 入库流水线 |

### 基础设施层

| 模块 | 职责 |
|---|---|
| `auth/session.py` | 会话管理 |
| `auth/password.py` | 密码哈希（bcrypt） |
| `observability/metrics.py` | Prometheus 指标（LLM 调用计数/延迟/token 用量） |
| `observability/tracing.py` | OpenTelemetry 分布式追踪 |
| `observability/middleware.py` | FastAPI 可观测中间件 |
| `observability/setup.py` | 可观测性初始化 |
| `mcp/server.py` | MCP Server（对外暴露工具与资源） |
| `evaluation/runner.py` | 评测执行器 |
| `evaluation/metrics.py` | 评测指标 |
| `evaluation/dataset.py` | 评测数据集 |
| `utils/cache.py` | TTL 缓存 |
| `utils/limiter.py` | 并发限流 |
| `utils/storage_cleaner.py` | 存储清理 |
| `utils/prompt_manager.py` | 提示词模板管理 |
| `utils/task_runner.py` | 后台任务运行器 |
| `utils/model_sync.py` | 模型同步 |
| `log/log_manager.py` | 统一日志管理 |
| `prompts/` | LLM 提示词模板文件（研究/写作/验证/翻译等） |

## 三、前端结构（`frontend/src/`）

### 页面层

- `pages/ChatPage.tsx`：主聊天界面
- `pages/IngestPage.tsx`：文档入库
- `pages/LoginPage.tsx`：登录认证
- `pages/AdminPage.tsx`：管理后台

### 组件层

| 目录 | 内容 |
|---|---|
| `components/chat/` | ChatWindow、ChatInput、ToolTracePanel、RetrievalDebugPanel |
| `components/canvas/` | CanvasPanel、ExploreStage、OutlineStage、DraftingStage、RefineStage、StageStepper、FloatingToolbar |
| `components/compare/` | CompareView |
| `components/graph/` | GraphExplorer |
| `components/workflow/` | DeepResearchDialog、DeepResearchSettingsPopover、WorkflowStepper、CommandPalette、IntentModeSelector、IntentConfirmPopover |
| `components/research/` | ResearchProgressPanel |
| `components/settings/` | SettingsModal |
| `components/layout/` | Header、Sidebar |
| `components/ui/` | Modal、Toast、PdfViewerModal |

### 状态管理

`stores/`：Zustand 状态管理（useChatStore、useCanvasStore、useConfigStore、useAuthStore、useProjectsStore、useCompareStore、useUIStore、useToastStore）

### 其他

- `api/`：后端接口封装（chat、canvas、compare、graph、ingest、models、auth、projects、auto、health）
- `types/`：TypeScript 类型定义
- `i18n/`：国际化（en.json、zh.json）

## 四、核心数据流

### 1) 离线入库流

```text
raw PDF
 → scripts/02_parse_papers.py
 → src/parser/pdf_parser.py（Docling + LLM 增强）
 → scripts/03_index_papers.py
 → src/chunking/chunker.py（section-aware 切块）
 → src/indexing/embedder.py（BGE-M3 dense + sparse）
 → src/indexing/milvus_ops.py（写入 Milvus）
 → (optional) scripts/03b_build_graph.py
 → src/graph/hippo_rag.py（构建知识图谱）
```

### 2) 在线入库流

```text
POST /ingest/upload
 → routes_ingest.py
 → PDF 解析 + 切块 + 向量化 + 写入 Milvus
 → 任务状态追踪（job_store）
 → SSE 事件流上报进度
```

### 3) 在线聊天检索流

```text
POST /chat or /chat/stream
 → routes_chat.py
 → (route decision: chat / rag)
 → build_search_query_from_context()
    - reference gate: 无指代时仅使用当前问题
    - context resolve: 存在指代时用 rolling_summary 解析主语
    - post-validation: 关键词重叠校验，否则回退原问题
 → RetrievalService.search()
    → HybridRetriever（dense + sparse + RRF）
    → UnifiedWebSearcher（Tavily + Google + Scholar + NCBI + Semantic Scholar）
    → HippoRAG（图检索，条件触发）
 → Evidence 综合 + context packing
 → LLMManager client.chat()
 → response / SSE events
```

### 4) Agent ReAct 流

```text
react_loop(messages, tools)
 → llm_manager.chat(tools=...)
 → parse_tool_calls(...)
 → execute_tool_call(...)（检索/画布/图谱/对比等工具）
 → append tool result message
 → next iteration / final_text
```

### 5) Deep Research 流（LangGraph + 后台任务）

```text
Phase 1: /deep-research/start
  Scope → Plan → 返回 brief + outline（前端可确认/编辑）

Phase 2: /deep-research/submit（推荐）
  提交后台任务 → 返回 job_id
  前端轮询 /deep-research/jobs/{job_id} 与 /events
  任务在后端持续运行（不依赖前端连接）

  研究循环（per section）：
    Research → Evaluate → (gap? → Research) → Write → Verify
    - Recall + Precision 双类查询
    - 分层 search_top_k
    - 3 级验证分流
    - 收益曲线早停

  Drafting 审核门：
    - 用户审核各章节（approve / revise）
    - 支持缺口补充（material / direct_info）
    - 审核门指数退避 + 早停

  最终整合：
    Synthesize → Abstract → Limitations → Open Gaps → Global Refine → Citation Guard

兼容模式: /deep-research/confirm（SSE 直连）
```

### Deep Research 前端交互

- **配置前置化**：输入区 `⚙` 设置弹窗，持久化到 `useConfigStore + localStorage`
  - 配置项：`depth`、`outputLanguage`、`stepModels`、`stepModelStrict`
- **启动链路**：澄清问题优先使用 `scope` 步骤模型
- **Drafting 审核区**：
  - 通过 / 修改 / 重新确认 / 一键全部通过并触发整合
  - 章节级缺口补充弹窗
- **运行期监测**：
  - Research Monitor（graph steps、成本状态、coverage 曲线、效率评分）
  - 低效率提示与章节优化提示词模板
- **人工介入**：
  - `user_context`（supporting / direct_injection）
  - 临时材料上传（pdf/md/txt → 不写入持久库）

### Research Depth 双级别

| | lite | comprehensive |
|---|---|---|
| 定位 | 快速但学术可用 | 全面学术综述 |
| 预计耗时 | ~5-15 min | ~20-60 min |
| 迭代预算 | 3 × 章节数 | 6 × 章节数 |
| 每章研究轮次 | max 3 | max 5 |
| 覆盖度阈值 | 0.60 | 0.80 |
| 查询策略 | 2 recall + 2 precision + gaps | 4 recall + 4 precision + gaps |
| search_top_k（首轮/补缺/写作） | 18 / 10 / 8 | 30 / 15 / 10 |
| 写作二次取证 `verification_k` | 12 | 16 |
| 验证（轻/中/重） | 20% / 40% / 45% | 15% / 30% / 35% |
| 审核门（指数退避 + 早停） | 80 轮 / 8 次无变化 | 200 轮 / 12 次无变化 |
| LangGraph 递归上限 | 200 | 500 |
| 成本预警 / 强制摘要步数 | 120 / 180 | 300 / 420 |

阈值定义在 `src/collaboration/research/agent.py` 的 `DEPTH_PRESETS`，可通过 `config/rag_config.json` → `deep_research.depth_presets` 覆盖。

### 循环防护机制

- **动态迭代预算**：`max_iterations = max_iterations_per_section × num_sections`
- **每章研究轮次上限**：`max_section_research_rounds`
- **Self-Correction**：第 3 轮及以后，coverage 达标时自动衰减 `search_top_k_gap`
- **收益曲线早停**：最近两轮 coverage 增益低且接近目标时提前进入写作
- **审核门指数退避 + 早停**：sleep 从 2s 指数增长；连续 N 轮无变化自动放行
- **3 级验证分流**：只有"严重"级才触发回滚
- **LangGraph 递归限制**：编译时显式设置 `recursion_limit`
- **成本步数监控**：达到 `cost_warn_steps` 提示人工介入；达到 `cost_force_summary_steps` 强制摘要
- **成本心跳上报**：每 `cost_tick_interval` 步发出 `cost_monitor_tick`

### 最终整合链路（Synthesize + Global Refine）

1. 生成 `Abstract`
2. 基于 insights + evidence-scarce sections 生成 `Limitations and Future Directions`
3. 聚合 open gaps 生成 `Open Gaps and Future Research Agenda`
4. 全篇连贯性重写（跨章节衔接、术语一致性、冗余消除）
5. 引用保护（citation guard）：检测到引用/证据标签丢失时自动回退
6. 写回最终结果并切换 Canvas 到 `refine` 阶段

## 五、存储与状态

| 存储 | 用途 |
|---|---|
| Milvus | 向量索引与 chunk 检索 |
| SQLite (`src/data/sessions.db`) | 会话、画布、用户与项目状态 |
| SQLite (`src/data/deep_research_jobs.db`) | Deep Research 后台任务与事件 |
| 文件系统 | `data/raw_papers`、`data/parsed`、`artifacts`、`logs` |
| NetworkX (in-memory + JSON) | HippoRAG 知识图谱 |
| 会话记忆 | `rolling_summary` / `summary_at_turn` 用于跨轮主语解析 |

## 六、可观测性

| 端点 | 功能 |
|---|---|
| `GET /metrics` | Prometheus 指标导出 |
| `GET /health` | 基础健康检查 |
| `GET /health/detailed` | 组件级健康状态（检索/LLM/图谱等） |
| `GET /storage/stats` | 存储统计 |
| OTel | 通过环境变量启用 tracing |
| LangSmith | 可选集成（LangGraph 执行追踪） |

## 七、设计约束

- LLM 调用必须通过 `src/llm/llm_manager.py`
- 新工具必须在 `src/llm/tools.py` 与 `src/mcp/server.py` 同步注册
- 配置新增字段必须同步 `config/rag_config.json` 与 `config/rag_config.example.json`
- 敏感配置放 `config/rag_config.local.json` 或环境变量
- 依赖方向：上层调用下层，避免反向耦合
- 新 API 路由统一放 `src/api/routes_*.py`，在 `server.py` 注册
</file>

<file path="docs/configuration.md">
# 配置说明

本文档描述配置文件与环境变量的加载逻辑、关键配置块和推荐实践。

更新时间：2026-02-19

## 一、配置来源与优先级

配置入口：`config/settings.py`

- 主配置：`config/rag_config.json`
- 本地覆盖：`config/rag_config.local.json`（可选，gitignored）
- 环境变量覆盖（敏感配置优先）

加载顺序：

1. 读取 `rag_config.json`
2. 若存在 `rag_config.local.json`，深度合并覆盖
3. 部分字段再由环境变量覆盖（如 API Key / 端口）

## 二、关键配置块（`rag_config.json`）

### `llm`

LLM 统一调度配置。

- `default`：默认 provider
- `dry_run`：是否启用 Mock 模式（测试用）
- `providers`：各 provider 的 `api_key / base_url / default_model / models / params`
- 支持 provider：
  - `openai` / `openai-thinking`
  - `deepseek` / `deepseek-thinking`
  - `gemini` / `gemini-thinking` / `gemini-vision`
  - `claude` / `claude-thinking`
  - `kimi` / `kimi-thinking` / `kimi-vision`
  - `sonar`

### `parser`

PDF 解析相关配置。

- `ocr`：OCR 开关
- `detect_columns`：多栏检测
- `caption_pattern` / `table_caption_pattern`：图表标题正则
- `llm_text_provider` / `llm_vision_provider`：解析增强使用的 LLM provider
- `enrich_tables` / `enrich_figures`：表格/图表 LLM 增强开关

### `chunk`

切块粒度配置。

- `target_chars`：目标字符数（默认 1000）
- `min_chars`：最小字符数（默认 200）
- `max_chars`：最大字符数（默认 1800）
- `overlap_sentences`：句级重叠（默认 2）

### `search`

本地检索配置。

- `top_k`：召回数量
- `rerank_top_k`：重排后保留数量
- Dense/Sparse 召回参数
- `reranker_mode`：重排模式（bge / colbert / cascade）
- ColBERT 开关与模型配置

### `web_search`

Tavily 网络搜索配置。

- `enabled`：开关
- `api_key`：Tavily API Key
- `max_results`：最大结果数
- `query_optimizer`：查询优化选项

### `google_search`

Google / Scholar 搜索配置。

- `enabled` / `google_enabled` / `scholar_enabled`：开关
- `browser_headless`：无头模式
- `max_results`：最大结果数

### `semantic_scholar`

Semantic Scholar API 配置。

- `enabled`：开关
- `api_key`：API Key

### `ncbi`

NCBI 文献搜索配置。

- `enabled`：开关
- `email`：NCBI API 邮箱
- `api_key`：API Key

### `content_fetcher`

网页全文抓取配置。

- `enabled`：开关
- 缓存与策略配置

### `api`

API 服务配置。

- `host`：监听地址（默认 `127.0.0.1`）
- `port`：端口（默认 `9999`）

### `auth`

认证配置。

- `secret_key`：JWT 密钥（生产环境必须替换）
- `token_expire_hours`：Token 过期时间（默认 24h）
- 初始化管理员账户字段（`default_admin_*`）

### `logging`

日志配置。

- `level`：日志级别（默认 INFO）
- `max_size`：单文件大小上限
- `retention_days`：保留天数

### `storage`

存储清理配置。

- `max_age_days`：最大保留天数
- `max_size_gb`：最大存储大小
- `cleanup_on_startup`：启动时是否自动清理
- `cleanup_batch_size`：清理批次大小

### `citation`

引用格式配置。

- 引用样式与格式化选项

### `deep_research`

Deep Research 配置。

- `depth_presets`：`lite` / `comprehensive` 的完整阈值集合
  - 迭代预算：`max_iterations_per_section`、`max_section_research_rounds`
  - 覆盖阈值：`coverage_threshold`
  - 查询预算：`recall_queries_per_section`、`precision_queries_per_section`
  - 分层召回：`search_top_k_first` / `search_top_k_gap` / `search_top_k_write`
  - 写作二次取证：`verification_k`
  - 自校正补检索：`self_correction_trigger_coverage`、`self_correction_min_round`、`search_top_k_gap_decay_factor`、`search_top_k_gap_min`
  - 收益曲线早停：`coverage_plateau_floor`、`coverage_plateau_min_gain`
  - 验证分层：`verify_light_threshold` / `verify_medium_threshold` / `verify_severe_threshold`
  - 审核门：`review_gate_max_rounds`、`review_gate_base_sleep`、`review_gate_max_sleep`、`review_gate_early_stop_unchanged`
  - 图上限：`recursion_limit`
  - 成本监控：`cost_warn_steps`、`cost_force_summary_steps`、`cost_tick_interval`

### `auto_complete`

一键综述配置。

- 自动补全相关参数

### `performance`

性能配置（统一控制超时、重试、并发和缓存）。

- `retrieval`：检索超时与并发
- `llm`：LLM 调用超时与重试
- `web_search`：Tavily 超时
- `unified_web_search`：统一网络搜索并发
  - `browser_providers_max_parallel`：浏览器类搜索并发数
- `google_search`：Google 搜索超时

## 三、重要环境变量

### API Key 注入（推荐）

- `RAG_LLM__{PROVIDER}__API_KEY`：覆盖对应 provider key
  - 示例：`RAG_LLM__OPENAI__API_KEY`、`RAG_LLM__DEEPSEEK__API_KEY`
- `RAG_LLM__SONAR__API_KEY`：Perplexity Sonar key
- 兼容旧变量：`OPENAI_API_KEY`、`DEEPSEEK_API_KEY`、`GEMINI_API_KEY`、`ANTHROPIC_API_KEY`

### 服务配置

- `API_HOST`、`API_PORT`：API 监听地址
- `MILVUS_HOST`、`MILVUS_PORT`：Milvus 地址
- `RAG_ENV`：运行环境（dev / prod）
- `COMPUTE_DEVICE`：计算设备（mps / cuda / cpu）

### 模型与缓存

- `HF_LOCAL_FILES_ONLY`：离线加载 HuggingFace 模型
- `MODEL_CACHE_ROOT`：模型缓存根目录
- `EMBEDDING_CACHE_DIR`：Embedding 模型缓存
- `RERANKER_CACHE_DIR`：Reranker 模型缓存
- `COLBERT_CACHE_DIR`：ColBERT 模型缓存
- `EMBEDDING_MODEL`：Embedding 模型名（默认 `BAAI/bge-m3`）
- `RERANKER_MODEL`：Reranker 模型名（默认 `BAAI/bge-reranker-v2-m3`）
- `INDEX_TYPE`：索引类型（`IVF_FLAT` / `GPU_IVF_FLAT`）

### 可观测性

- OpenTelemetry 环境变量（`OTEL_*`）
- LangSmith 环境变量（可选）

## 四、推荐配置实践

- `rag_config.json` 存结构化配置，不写真实密钥
- `rag_config.local.json` 存开发机私密配置，不提交仓库
- CI/服务器使用环境变量注入密钥
- 修改配置后优先检查：
  - `/health/detailed`
  - `GET /llm/providers`
  - `GET /models/status`

## 五、示例（最小可运行）

```json
{
  "llm": {
    "default": "deepseek",
    "providers": {
      "deepseek": {
        "api_key": "sk-xxx",
        "base_url": "https://api.deepseek.com/v1",
        "default_model": "deepseek-chat",
        "models": {"deepseek-chat": "deepseek-chat"},
        "params": {}
      }
    }
  },
  "api": {"host": "127.0.0.1", "port": 9999},
  "search": {"top_k": 20}
}
```
</file>

<file path="docs/dependency_matrix.md">
# 依赖矩阵

本文档基于当前仓库依赖文件核对：

- `requirements.txt`
- `frontend/package.json`
- `frontend/package-lock.json`

更新时间：2026-02-19

## 1) Python 依赖（后端）

### 关键框架

| 包 | 版本约束 | 用途 |
|---|---|---|
| FastAPI | `>=0.100.0,<1.0.0` | Web 框架 |
| Uvicorn | `>=0.22.0` | ASGI 服务器 |
| Pydantic | `>=2.0.0,<3.0.0` | 数据验证 |
| LangGraph | `>=0.2.0` | Agent 图编排 |
| MCP SDK | `>=1.26.0` | MCP 协议 |

### LLM 客户端

| 包 | 版本约束 | 用途 |
|---|---|---|
| openai | `>=1.0.0,<3.0.0` | OpenAI / DeepSeek / Kimi 等 |
| anthropic | `>=0.18.0` | Claude |

### 检索与模型

| 包 | 版本约束 | 用途 |
|---|---|---|
| pymilvus[model] | `>=2.5.0` | Milvus 向量数据库 |
| transformers | `>=4.42.0,<5.0.0` | HuggingFace 模型 |
| huggingface-hub | `==0.36.0` | 模型下载 |
| sentence-transformers | `>=2.2.0` | 句向量 |
| FlagEmbedding | `>=1.2.0` | BGE 系列模型 |
| ragatouille | `>=0.0.8` | ColBERT 重排 |
| einops | `>=0.7.0` | 张量操作 |

### PDF 解析

| 包 | 版本约束 | 用途 |
|---|---|---|
| docling | `>=2.0.0` | PDF 解析 |
| pymupdf | `>=1.24.0` | PDF 操作 |
| pillow | `>=10.0.0` | 图像处理 |

### 搜索与抓取

| 包 | 版本约束 | 用途 |
|---|---|---|
| tavily-python | `>=0.5.0` | Tavily 搜索 |
| playwright | `>=1.40.0` | 浏览器自动化 |
| playwright-stealth | `>=1.0.0` | 反检测 |
| beautifulsoup4 | `>=4.12.0` | HTML 解析 |
| trafilatura | `>=2.0.0` | 网页提取 |
| aiohttp | `>=3.9.0` | 异步 HTTP |

### 可观测性

| 包 | 版本约束 | 用途 |
|---|---|---|
| prometheus-client | `>=0.20.0` | Prometheus 指标 |
| opentelemetry-api/sdk | — | 分布式追踪 |

### 图与知识

| 包 | 版本约束 | 用途 |
|---|---|---|
| networkx | — | 知识图谱 |
| langgraph-checkpoint-sqlite | `>=0.0.20` | LangGraph 检查点 |

### 测试

| 包 | 版本约束 | 用途 |
|---|---|---|
| pytest | `>=8.0.0` | 测试框架 |

## 2) JavaScript / TypeScript 依赖（前端）

### Runtime dependencies

| 包 | 版本 | 用途 |
|---|---|---|
| react | `^19.2.0` | UI 框架 |
| react-dom | `^19.2.0` | DOM 渲染 |
| react-router-dom | `^7.13.0` | 路由 |
| zustand | `^5.0.11` | 状态管理 |
| axios | `^1.13.4` | HTTP 客户端 |
| react-markdown | `^10.1.0` | Markdown 渲染 |
| react-force-graph-2d | `^1.29.1` | 图谱可视化 |
| react-pdf | `^10.3.0` | PDF 预览 |
| lucide-react | — | 图标库 |
| i18next | — | 国际化 |
| react-i18next | — | React 国际化绑定 |

### Dev dependencies

| 包 | 版本 | 用途 |
|---|---|---|
| vite | `^7.2.4` | 构建工具 |
| typescript | `~5.9.3` | 类型系统 |
| eslint | `^9.39.1` | 代码检查 |
| tailwindcss | `^4.1.18` | CSS 框架 |
| @tailwindcss/vite | — | Tailwind Vite 插件 |

## 3) 运行时要求

| 依赖 | 版本 |
|---|---|
| Python | `3.10+`（推荐 3.10） |
| Node.js | `^20.19.0 \|\| >=22.12.0` |
| PyTorch | `>=2.6.0`（Conda 安装） |
| Torchvision | `>=0.21.0`（Conda 安装） |
| Docker / Compose | 最新稳定版 |

## 3.1) 系统依赖（Ubuntu 推荐）

- `docker.io`、`docker-compose-plugin`
- `git`、`curl`、`wget`、`unzip`
- `build-essential`、`pkg-config`、`python3-dev`
- `libssl-dev`、`libffi-dev`、`libsqlite3-dev`
- 可选：`xvfb`（仅 headful 浏览器场景）

## 3.2) 依赖配置映射

### LLM 相关

- 依赖：`openai`、`anthropic`、`mcp`
- 配置：`llm.default`、`llm.providers.*`
- 环境变量：`RAG_LLM__{PROVIDER}__API_KEY`

### Web 搜索相关

- 依赖：`playwright`、`playwright-stealth`、`tavily-python`、`trafilatura`、`aiohttp`
- 配置：`web_search.enabled`、`google_search.enabled`、`semantic_scholar.enabled`、`ncbi.enabled`、`content_fetcher.enabled`

### 检索/索引相关

- 依赖：`pymilvus[model]`、`sentence-transformers`、`FlagEmbedding`、`ragatouille`
- 配置：`search.*`、`MILVUS_HOST`、`MILVUS_PORT`

### API/服务相关

- 依赖：`fastapi`、`uvicorn`、`pydantic`
- 配置：`api.host`、`api.port`、`auth.*`

### 可观测性

- 依赖：`opentelemetry-*`、`prometheus-client`
- 端点：`/metrics`、`/health/detailed`

## 4) 一致性检查建议

- Python：`pip install -r requirements.txt --no-cache-dir` 后跑一次关键导入检查
- 前端：优先 `npm ci` 确保与 lock 一致
- CI / 发布前执行：
  - `pytest -q`
  - `npm run build`（`frontend/`）
  - `bash scripts/verify_dependencies.sh`

## 5) 注意事项

- `torch` / `torchvision` / `timm` 不在 `requirements.txt`，需 Conda 安装
- 密钥相关依赖配置不要写死在代码，统一走 `rag_config.local.json` 或环境变量
- `huggingface-hub` 版本锁定为 `0.36.0`，升级前需验证 `transformers` 与 `FlagEmbedding` 兼容性
</file>

<file path="docs/developer_guide.md">
# DeepSea RAG 开发指南

本指南面向接手开发的工程师，聚焦"如何在当前项目里正确开发和扩展"。

## 1. 先读什么

建议阅读顺序：

1. `../README.md`（项目入口）
2. `architecture.md`（架构与数据流）
3. `api_reference.md`（接口边界）
4. `configuration.md`（配置与环境变量）
5. `scripts_guide.md`（脚本与执行顺序）
6. `dependency_matrix.md`（依赖矩阵与运行时要求）

## 2. 代码分层约定

```text
src/
├── api/              # 接口层：HTTP 路由，请求/响应模型
├── collaboration/    # 协作层：canvas / memory / intent / research / workflow / citation / export
├── llm/              # Agent 层：LLMManager / tools / react_loop
├── retrieval/        # 检索层：混合检索 / web 聚合 / 重排 / 去重
├── parser/           # 数据处理：PDF 解析
├── chunking/         #            结构化切块
├── indexing/         #            向量化与 Milvus 读写
├── generation/       # 生成层：证据综合 / 上下文打包 / LLM 兼容层
├── graph/            # 图谱：HippoRAG
├── graphs/           # 流水线：LangGraph 入库图
├── auth/             # 认证：session / password
├── observability/    # 可观测：metrics / tracing / middleware
├── evaluation/       # 评测：runner / metrics / dataset
├── mcp/              # MCP Server
├── utils/            # 工具：缓存 / 限流 / 清理 / 提示词 / 任务运行器
├── log/              # 日志管理
└── prompts/          # LLM 提示词模板
```

保持依赖方向：上层调用下层，避免反向耦合。

## 3. LLM 调用规范（强约束）

所有 LLM 调用必须走 `src/llm/llm_manager.py`。

### 推荐写法

```python
from src.llm import LLMManager

manager = LLMManager.from_json("config/rag_config.json")
client = manager.get_client("deepseek")
resp = client.chat(
    messages=[
        {"role": "system", "content": "你是一个助手"},
        {"role": "user", "content": "问题内容"},
    ],
    model=None,       # 可选：覆盖默认模型
    max_tokens=2000,  # 可选：覆盖默认参数
)
text = resp["final_text"]        # 最终回答
reasoning = resp["reasoning_text"]  # 思考过程（thinking 模式）
usage = resp["meta"]["usage"]    # token 用量
```

### 兼容写法（旧代码）

```python
from src.generation.llm_client import call_llm

result = call_llm(
    provider="deepseek",
    system="系统提示",
    user_prompt="用户问题",
)
# result 为字符串（final_text）
```

### 可用 Providers

- `openai` / `openai-thinking`
- `deepseek` / `deepseek-thinking`
- `gemini` / `gemini-thinking` / `gemini-vision`
- `claude` / `claude-thinking`
- `kimi` / `kimi-thinking` / `kimi-vision`
- `sonar`

### 禁止做法

- 直接实例化 `openai.OpenAI()` / `anthropic.Anthropic()`
- 业务代码里硬编码 API Key
- 绕过 LLMManager 直接发 HTTP 请求

## 4. Agent 与工具扩展

核心文件：

- `src/llm/tools.py`：工具定义与路由
- `src/llm/react_loop.py`：ReAct 循环
- `src/mcp/server.py`：MCP Server

新增工具步骤：

1. 在 `tools.py` 定义 `ToolDef`
2. 将工具加入核心工具列表
3. 在 `mcp/server.py` 注册对应 MCP tool
4. 必要时补充前端工具轨迹展示（`ToolTracePanel.tsx`）

## 5. API 开发约定

- 新路由统一放 `src/api/routes_*.py`
- 在 `src/api/server.py` 注册 `include_router`
- 请求/响应模型优先放 `src/api/schemas.py`
- 新增接口后同步更新：
  - `docs/api_reference.md`
  - 前端 `frontend/src/api/*` 客户端
  - 前端 `frontend/src/types/index.ts` 类型

## 6. 配置与密钥

- 公共配置：`config/rag_config.json`
- 本地覆盖：`config/rag_config.local.json`（gitignored）
- 环境变量覆盖：`RAG_LLM__{PROVIDER}__API_KEY`

新增配置字段时必须同步：

- `config/rag_config.json`
- `config/rag_config.example.json`
- `docs/configuration.md`

## 7. 数据与存储

| 路径 | 用途 |
|---|---|
| `data/raw_papers/` | 原始 PDF 文档 |
| `data/parsed/` | 解析后结构化数据 |
| `src/data/sessions.db` | 会话数据库 |
| `src/data/deep_research_jobs.db` | Deep Research 任务数据库 |
| `logs/` | 运行日志 |
| `logs/llm_raw/` | LLM 原始响应日志（JSONL） |
| `artifacts/` | 评测/任务产物 |

上线前确认：

- `storage.max_age_days`
- `storage.max_size_gb`
- `storage.cleanup_on_startup`

## 8. 前端联动约定

当你改后端协议时，至少检查：

- `frontend/src/api/*` 调用参数与返回类型
- `frontend/src/types/index.ts` 类型定义
- `frontend/src/stores/*` 状态处理
- SSE 事件处理组件（聊天与研究相关）
- `frontend/src/i18n/locales/*.json` 翻译文件

## 9. 测试与回归

### 最小回归集

```bash
pytest -q
python scripts/04_test_search.py
python scripts/05_test_rag.py
python scripts/13_test_canvas_api.py
python scripts/17_test_chat_stream.py
```

### 效果回归

```bash
python scripts/18_eval_rag.py
```

### 前端构建验证

```bash
cd frontend && npm run build && cd ..
```

## 10. 常见扩展场景

### 新增 LLM Provider

1. 配置 `rag_config.json` 的 `llm.providers`
2. 若非 OpenAI 兼容协议，在 `llm_manager.py` 增加 provider 适配
3. 补充文档与测试脚本

### 新增检索源

1. `src/retrieval/` 新增 searcher
2. 在 `unified_web_search.py` 注册聚合逻辑
3. 在工具层与 API 层暴露可选参数
4. 更新 `docs/configuration.md`

### 新增 API 业务域

1. 新建 `routes_xxx.py`
2. 在 `server.py` 注册 router
3. schemas 中补齐模型
4. 前端加 API 封装与页面入口
5. 更新文档与测试

### 新增提示词模板

1. 在 `src/prompts/` 新增 `.txt` 模板文件
2. 通过 `src/utils/prompt_manager.py` 加载
3. 在业务代码中引用

## 11. 文档维护职责

任何业务变更至少更新 1 份文档；跨模块变更更新 2 份以上。优先维护：

- `../README.md`
- `api_reference.md`
- `configuration.md`
- `scripts_guide.md`

这可以保证新成员可以在 1 小时内完成项目冷启动与关键功能定位。
</file>

<file path="docs/README.md">
# 文档中心

本目录是 DeepSea RAG 的统一文档入口。建议按角色阅读。

## 按角色阅读

| 角色 | 推荐路径 |
|---|---|
| 新成员 / 首次部署 | `../README.md` → `../install.md` → `scripts_guide.md` |
| 后端开发 | `developer_guide.md` → `architecture.md` → `api_reference.md` |
| 前端开发 | `../frontend/README.md` → `api_reference.md` |
| 运维 / 排障 | `configuration.md` → `operations_and_troubleshooting.md` |
| 质量保障 | `testing_and_evaluation.md` → `dependency_matrix.md` |
| 生产部署 | `release_migration_ubuntu.md` → `operations_and_troubleshooting.md` |

## Deep Research 相关文档导航

| 方面 | 文档 |
|---|---|
| 前后端流程与节点策略 | `architecture.md` |
| 任务接口、审核/补充接口与事件 | `api_reference.md` |
| 配置项与环境变量 | `configuration.md` |
| 运行与排障（review gate / synthesize / resume queue） | `operations_and_troubleshooting.md` |
| 安装后快速验证（后台任务模式） | `../install.md` |

## 文档清单

| 文件 | 内容 |
|---|---|
| `developer_guide.md` | 开发总指南（模块职责、约定、扩展路径） |
| `architecture.md` | 系统架构与关键数据流 |
| `api_reference.md` | 按前缀分组的完整 API 参考 |
| `configuration.md` | 配置项与环境变量说明 |
| `scripts_guide.md` | 脚本用途、参数、推荐执行顺序 |
| `operations_and_troubleshooting.md` | 启动、监控、运维、故障处理 |
| `release_migration_ubuntu.md` | Ubuntu 发布与迁移全流程（systemd + Nginx） |
| `testing_and_evaluation.md` | pytest 与评测体系 |
| `dependency_matrix.md` | Python / 前端依赖矩阵与运行时要求 |

## 维护规范

- 新增功能时，至少更新以下任一文档：
  - 新 API → `api_reference.md`
  - 新配置 → `configuration.md`
  - 新脚本 → `scripts_guide.md`
  - 新模块/重要设计变更 → `architecture.md` + `developer_guide.md`
- 与用户直接相关的变更（启动方式、主要能力）同步更新根目录 `README.md`。
- 依赖版本变更时更新 `dependency_matrix.md`。
</file>

<file path="docs/release_migration_ubuntu.md">
# Ubuntu 发布与迁移手册（完整版）

本文档用于生产发布与跨机器迁移，重点覆盖：系统依赖、Python/前端依赖、配置基线、数据迁移、启动托管与验收。

更新时间：2026-02-19

## 1. 适用范围

- 目标系统：Ubuntu 22.04/24.04
- 部署方式：`docker compose`（Milvus 依赖）+ `systemd`（应用托管）
- 项目路径示例：`/opt/deepsea-rag`

## 2. 系统层依赖（Ubuntu）

建议先安装系统依赖：

```bash
sudo apt update
sudo apt install -y \
  git curl wget unzip ca-certificates gnupg lsb-release \
  build-essential pkg-config python3-dev \
  libssl-dev libffi-dev libsqlite3-dev \
  docker.io docker-compose-plugin
```

可选（仅在服务器需要 headful 浏览器时）：

```bash
sudo apt install -y xvfb
```

## 3. 运行时版本基线

- Python：`3.10+`（推荐 3.10）
- Conda：建议 Miniconda/Anaconda（用于隔离 `deepsea-rag` 环境）
- Node.js：`^20.19.0 || >=22.12.0`
- npm：建议与 Node LTS 配套
- Docker/Compose：可用 `docker compose version` 检查

## 4. 代码与目录

```bash
sudo mkdir -p /opt/deepsea-rag
sudo chown -R "$USER":"$USER" /opt/deepsea-rag
cd /opt/deepsea-rag
git clone <YOUR_REPO_URL> .
```

## 5. Python 环境与依赖

```bash
conda create -n deepsea-rag python=3.10 -y
conda activate deepsea-rag

# 必须走 Conda 安装 PyTorch 生态
conda install -c pytorch -c conda-forge "pytorch>=2.6.0" "torchvision>=0.21.0" timm -y

# 项目 Python 依赖
pip install -r requirements.txt --no-cache-dir

# 浏览器自动化依赖
playwright install chromium
```

## 6. 前端依赖

```bash
cd frontend
npm ci
npm run build
cd ..
```

## 7. 配置文件（必须项）

```bash
cp config/rag_config.example.json config/rag_config.json
cp config/rag_config.local.example.json config/rag_config.local.json
```

### 7.1 推荐密钥注入方式

优先环境变量（避免明文落盘）：

- `RAG_LLM__OPENAI__API_KEY`
- `RAG_LLM__DEEPSEEK__API_KEY`
- `RAG_LLM__GEMINI__API_KEY`
- `RAG_LLM__CLAUDE__API_KEY`
- `RAG_LLM__KIMI__API_KEY`

### 7.2 发布前关键配置核对

- `config/rag_config.json`
  - `api.host`、`api.port`
  - `llm.default` + provider 列表
  - `storage.max_age_days`、`storage.max_size_gb`
  - `performance.*`（超时/并发）
- `config/rag_config.local.json`
  - 各 provider 密钥
  - `auth.secret_key`（生产必须替换）

## 8. 基础服务（Milvus/MinIO/etcd）

```bash
# 生产 profile
docker compose --profile prod up -d
bash scripts/00_healthcheck_docker.sh
```

## 9. 数据迁移（如从旧机迁移）

建议迁移目录/文件：

- `data/raw_papers/`
- `data/parsed/`
- `src/data/sessions.db`
- `src/data/deep_research_jobs.db`
- `artifacts/`（可选）

示例（源机执行）：

```bash
tar -czf deepsea-rag-data.tgz data/raw_papers data/parsed src/data/sessions.db src/data/deep_research_jobs.db artifacts
```

目标机解压：

```bash
tar -xzf deepsea-rag-data.tgz -C /opt/deepsea-rag
```

## 10. 依赖与版本核对（强烈建议）

```bash
bash scripts/verify_dependencies.sh
```

通过标准：

- `FAIL=0`
- Python 关键约束通过（`transformers/huggingface-hub/fastapi/pydantic/openai/mcp`）

## 11. 初始化与索引（首次或重建）

```bash
python scripts/01_init_env.py
python scripts/02_parse_papers.py
python scripts/03_index_papers.py
python scripts/03b_build_graph.py
```

> 若已迁移完成 `data/parsed` 且 Milvus 数据卷完整迁移，可按需跳过部分步骤。

## 12. systemd 托管（推荐）

### 12.1 后端服务

`/etc/systemd/system/deepsea-rag-api.service`

```ini
[Unit]
Description=DeepSea RAG FastAPI Service
After=network.target docker.service
Wants=docker.service

[Service]
Type=simple
User=YOUR_USER
Group=YOUR_USER
WorkingDirectory=/opt/deepsea-rag
Environment=PYTHONUNBUFFERED=1
Environment=API_HOST=0.0.0.0
Environment=API_PORT=9999
EnvironmentFile=-/opt/deepsea-rag/.env
Environment=CONDA_EXE=conda
ExecStart=/bin/bash -lc '$CONDA_EXE run -n deepsea-rag python scripts/08_run_api.py --host 0.0.0.0 --port 9999'
Restart=always
RestartSec=5
TimeoutStopSec=20

[Install]
WantedBy=multi-user.target
```

### 12.2 前端服务（preview 方案，过渡可用）

`/etc/systemd/system/deepsea-rag-frontend.service`

```ini
[Unit]
Description=DeepSea RAG Frontend Preview
After=network.target

[Service]
Type=simple
User=YOUR_USER
Group=YOUR_USER
WorkingDirectory=/opt/deepsea-rag/frontend
Environment=NODE_ENV=production
Environment=CONDA_EXE=conda
ExecStart=/bin/bash -lc '$CONDA_EXE run -n deepsea-rag npm run preview -- --host 0.0.0.0 --port 5173'
Restart=always
RestartSec=5
TimeoutStopSec=20

[Install]
WantedBy=multi-user.target
```

启用与启动：

```bash
sudo systemctl daemon-reload
sudo systemctl enable deepsea-rag-api deepsea-rag-frontend
sudo systemctl start deepsea-rag-api deepsea-rag-frontend
```

说明：

- 不建议写死 `node/npm` 路径；优先通过 conda 环境统一管理。
- 若 systemd 下 `conda` 不在 PATH，请将 `CONDA_EXE` 设置为实际路径（如 `/opt/anaconda3/bin/conda`）。
- 当 Node 位于 `.../envs/deepsea-rag/bin/node` 时，上述 `conda run -n deepsea-rag npm ...` 可直接使用该环境内 node/npm。

### 12.3 Nginx 生产推荐方案（建议）

生产更推荐：**后端由 systemd 托管，前端用 Nginx 托管静态文件**（替代 `npm preview`）。

1) 构建前端：

```bash
cd /opt/deepsea-rag/frontend
conda run -n deepsea-rag npm ci
conda run -n deepsea-rag npm run build
```

2) 安装 Nginx：

```bash
sudo apt update
sudo apt install -y nginx
```

3) Nginx 配置（示例：`/etc/nginx/sites-available/deepsea-rag`）：

```nginx
server {
    listen 80;
    server_name your.domain.com;

    # 前端静态资源
    root /opt/deepsea-rag/frontend/dist;
    index index.html;

    # SPA 路由回退
    location / {
        try_files $uri $uri/ /index.html;
    }

    # 后端 API 反向代理
    location /api/ {
        proxy_pass http://127.0.0.1:9999/;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # SSE/长连接建议配置
        proxy_buffering off;
        proxy_read_timeout 3600s;
        proxy_send_timeout 3600s;
    }

    # 直接暴露 docs（可按需加白名单）
    location /docs {
        proxy_pass http://127.0.0.1:9999/docs;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }

    location /openapi.json {
        proxy_pass http://127.0.0.1:9999/openapi.json;
        proxy_http_version 1.1;
        proxy_set_header Host $host;
    }
}
```

4) 启用站点并检查：

```bash
sudo ln -s /etc/nginx/sites-available/deepsea-rag /etc/nginx/sites-enabled/deepsea-rag
sudo nginx -t
sudo systemctl restart nginx
sudo systemctl enable nginx
```

5) HTTPS（Let's Encrypt）：

```bash
sudo apt install -y certbot python3-certbot-nginx
sudo certbot --nginx -d your.domain.com
```

> 使用 Nginx 方案后，可以停止前端 preview service，仅保留后端 API systemd。

## 13. 验收清单（Go/No-Go）

- `GET /health` 返回 `ok`
- `GET /health/detailed` 无关键组件失败
- `GET /metrics` 可访问
- `GET /llm/providers` 列表正常
- `POST /chat` 可返回
- `POST /chat/stream` 可流式返回
- `GET /compare/papers` 正常
- `GET /ingest/collections` 正常
- Deep Research 可启动并返回 job_id
- 前端页面可打开并调用后端
- Nginx `nginx -t` 通过且证书自动续期可用（若启用 HTTPS）

## 14. 回滚策略

- 保留旧环境与数据快照
- 新版本发布前备份：
  - `config/rag_config*.json`
  - `data/parsed/`
  - `src/data/sessions.db`
  - `src/data/deep_research_jobs.db`
- 回滚时：
  1. 停服务（systemd）
  2. 切回旧代码/旧镜像
  3. 恢复配置与数据快照
  4. 重启并验收关键接口

## 15. 常见发布问题

- `node: command not found`：Node 未安装或 PATH 未配置
- `mcp/trafilatura missing`：未在 `deepsea-rag` 环境执行安装
- API 401：密钥未注入或 `.local.json` 覆盖错误
- 检索为空：Milvus profile 未启动或 collection 未初始化
- Google/Scholar 不稳定：Playwright 依赖/并发参数需调整
</file>

<file path="docs/scripts_guide.md">
# 脚本指南

本文档按使用场景组织 `scripts/` 下脚本，帮助快速定位执行顺序与用途。

更新时间：2026-02-19

## 一、环境与基础设施

| 脚本 | 用途 |
|---|---|
| `00_preflight_check.sh` | 环境预检（Python、Docker、端口等） |
| `00_healthcheck_docker.sh` | Milvus / MinIO / etcd 健康检查 |
| `01_init_env.py` | 初始化 Milvus Collections |
| `verify_dependencies.sh` | 一键核对 Python 依赖 + 前端 Node 运行时 |

## 二、离线数据流水线

| 脚本 | 用途 |
|---|---|
| `02_parse_papers.py` | 解析 PDF 生成结构化数据 |
| `03_index_papers.py` | 切块、向量化并写入 Milvus |
| `03b_build_graph.py` | 构建 HippoRAG 图谱 |
| `06_ingest_langgraph.py` | 基于 LangGraph 的入库流程 |

## 三、服务启动

| 脚本 | 用途 |
|---|---|
| `start.sh` | 一键启动后端 + 前端（`--backend-only` / `--frontend-only`） |
| `08_run_api.py` | 只启动 FastAPI（支持 `--host --port --reload`） |

## 四、检索与功能测试脚本

| 脚本 | 用途 |
|---|---|
| `04_test_search.py` | 本地检索测试 |
| `05_test_rag.py` | RAG 端到端测试 |
| `07_test_web_search.py` | Tavily 网络搜索测试 |
| `09_test_google_search.py` | Google / Scholar 搜索测试 |
| `10_test_multiturn.py` | 多轮对话测试 |
| `11_test_llm_providers.py` | LLM provider 切换测试 |
| `12_test_workflow_stage.py` | 工作流状态测试 |
| `13_test_canvas_api.py` | Canvas API 测试 |
| `14_test_memory.py` | 记忆系统测试 |
| `15_test_citations.py` | 引用管理测试 |
| `16_test_export.py` | 导出功能测试 |
| `17_test_chat_stream.py` | SSE 流式聊天测试 |
| `21_test_intent_override.py` | 意图覆盖测试 |
| `23_test_deep_research_e2e.py` | Deep Research 端到端测试 |
| `27_test_year_window_and_citation_style.py` | 年份窗口与引用风格测试 |
| `test_chat_hybrid_optimizer.py` | 混合检索优化测试 |
| `_test_ncbi_integration.py` | NCBI 集成测试 |

## 五、评测与运维脚本

| 脚本 | 用途 |
|---|---|
| `18_eval_rag.py` | RAG 评测入口 |
| `19_cleanup_storage.py` | 存储清理与压缩（`--vacuum`） |
| `20_bootstrap_admin.py` | 首次管理员初始化 |
| `22_test_offline_models.py` | 离线模型检查 |
| `23_sync_local_models.py` | 本地模型同步 |
| `24_generate_eval_dataset.py` | 生成评测数据集 |
| `25_extract_claims.py` | 声明提取 |
| `26_backfill_doi.py` | DOI 元数据回填 |

## 六、调试与演示脚本

| 脚本 | 用途 |
|---|---|
| `08_test_logging.py` | 日志系统测试 |
| `debug_chat.py` | 聊天调试工具 |
| `demo_llm_manager.py` | LLMManager 功能演示 |
| `check_sonar_health.py` | Sonar 健康检查 |

## 七、推荐执行顺序

### 冷启动（首次）

```bash
bash scripts/00_preflight_check.sh
docker compose --profile dev up -d
bash scripts/00_healthcheck_docker.sh
python scripts/01_init_env.py
python scripts/02_parse_papers.py
python scripts/03_index_papers.py
python scripts/03b_build_graph.py
python scripts/20_bootstrap_admin.py
bash scripts/start.sh
```

### 功能回归（开发中）

```bash
pytest -q
python scripts/04_test_search.py
python scripts/05_test_rag.py
python scripts/13_test_canvas_api.py
python scripts/17_test_chat_stream.py
python scripts/18_eval_rag.py
cd frontend && npm run build && cd ..
```

### 数据维护

```bash
python scripts/19_cleanup_storage.py --vacuum
python scripts/26_backfill_doi.py
python scripts/23_sync_local_models.py
```

## 八、常见注意事项

- 需要浏览器自动化的脚本（Google / Scholar）先执行：`playwright install chromium`
- 并发或网络不稳定时，优先调小配置中的 timeout / concurrency 参数
- 清理数据前，先备份 `data/` 与 `src/data/sessions.db`
- Deep Research 端到端测试（`23_test_deep_research_e2e.py`）耗时较长，建议在非高峰时段运行
- 脚本编号可能不连续，属于历史遗留，不影响使用
</file>

<file path="docs/testing_and_evaluation.md">
# 测试与评测

本文档说明项目的自动化测试与效果评测入口。

更新时间：2026-02-19

## 一、单元测试（pytest）

测试目录：`tests/`

| 测试文件 | 覆盖范围 |
|---|---|
| `test_chunker.py` | 切块策略 |
| `test_hybrid_retriever.py` | 混合检索 |
| `test_intent_parser.py` | 意图解析 |
| `test_citation_resolution.py` | 引用解析 |
| `test_tools_routing.py` | 工具路由 |
| `test_tracing_modes.py` | 追踪模式 |
| `test_research_meta_analysis_guards.py` | 研究元分析防护 |
| `test_research_agent_state_compaction.py` | Agent 状态压缩 |

执行：

```bash
pytest -q
```

详细输出：

```bash
pytest -v
```

指定测试文件：

```bash
pytest tests/test_hybrid_retriever.py -v
```

## 二、脚本级功能测试

| 脚本 | 覆盖范围 |
|---|---|
| `scripts/04_test_search.py` | 本地检索 |
| `scripts/05_test_rag.py` | RAG 端到端 |
| `scripts/07_test_web_search.py` | Tavily 网络搜索 |
| `scripts/09_test_google_search.py` | Google / Scholar |
| `scripts/10_test_multiturn.py` | 多轮对话 |
| `scripts/11_test_llm_providers.py` | LLM provider 切换 |
| `scripts/12_test_workflow_stage.py` | 工作流状态 |
| `scripts/13_test_canvas_api.py` | Canvas API |
| `scripts/14_test_memory.py` | 记忆系统 |
| `scripts/15_test_citations.py` | 引用管理 |
| `scripts/16_test_export.py` | 导出功能 |
| `scripts/17_test_chat_stream.py` | SSE 流式聊天 |
| `scripts/21_test_intent_override.py` | 意图覆盖 |
| `scripts/22_test_offline_models.py` | 离线模型 |
| `scripts/23_test_deep_research_e2e.py` | Deep Research 端到端 |
| `scripts/27_test_year_window_and_citation_style.py` | 年份窗口与引用风格 |
| `scripts/test_chat_hybrid_optimizer.py` | 混合检索优化 |
| `scripts/_test_ncbi_integration.py` | NCBI 集成 |

## 三、评测体系

评测脚本：`scripts/18_eval_rag.py`

默认数据集：`data/eval_mini.json`

关注指标（由评测模块输出）：

- 检索：Recall / Hit 类指标
- 生成：ROUGE / 文本重叠类指标
- 引用：引用命中与有效性

评测数据集生成：

```bash
python scripts/24_generate_eval_dataset.py
```

## 四、推荐回归流程

每次较大改动后建议执行：

### 最小回归集

```bash
pytest -q
python scripts/04_test_search.py
python scripts/05_test_rag.py
python scripts/13_test_canvas_api.py
python scripts/17_test_chat_stream.py
```

### 效果回归

```bash
python scripts/18_eval_rag.py
```

### 前端构建验证

```bash
cd frontend && npm run build && cd ..
```

### 完整回归（发布前）

```bash
pytest -q
python scripts/04_test_search.py
python scripts/05_test_rag.py
python scripts/10_test_multiturn.py
python scripts/13_test_canvas_api.py
python scripts/15_test_citations.py
python scripts/17_test_chat_stream.py
python scripts/18_eval_rag.py
cd frontend && npm run build && cd ..
```

## 五、评测数据集扩展建议

- 固定领域子集（同主题论文）与开放问题集分开维护
- 对每条样本同时标注：
  - 期望命中文档
  - 关键事实点
  - 可接受答案范围
- 对高价值样本建立长期基线，避免回归退化
- 建议定期从实际使用中提取新样本补充数据集
</file>

<file path="frontend/public/vite.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="31.88" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 257"><defs><linearGradient id="IconifyId1813088fe1fbc01fb466" x1="-.828%" x2="57.636%" y1="7.652%" y2="78.411%"><stop offset="0%" stop-color="#41D1FF"></stop><stop offset="100%" stop-color="#BD34FE"></stop></linearGradient><linearGradient id="IconifyId1813088fe1fbc01fb467" x1="43.376%" x2="50.316%" y1="2.242%" y2="89.03%"><stop offset="0%" stop-color="#FFEA83"></stop><stop offset="8.333%" stop-color="#FFDD35"></stop><stop offset="100%" stop-color="#FFA800"></stop></linearGradient></defs><path fill="url(#IconifyId1813088fe1fbc01fb466)" d="M255.153 37.938L134.897 252.976c-2.483 4.44-8.862 4.466-11.382.048L.875 37.958c-2.746-4.814 1.371-10.646 6.827-9.67l120.385 21.517a6.537 6.537 0 0 0 2.322-.004l117.867-21.483c5.438-.991 9.574 4.796 6.877 9.62Z"></path><path fill="url(#IconifyId1813088fe1fbc01fb467)" d="M185.432.063L96.44 17.501a3.268 3.268 0 0 0-2.634 3.014l-5.474 92.456a3.268 3.268 0 0 0 3.997 3.378l24.777-5.718c2.318-.535 4.413 1.507 3.936 3.838l-7.361 36.047c-.495 2.426 1.782 4.5 4.151 3.78l15.304-4.649c2.372-.72 4.652 1.36 4.15 3.788l-11.698 56.621c-.732 3.542 3.979 5.473 5.943 2.437l1.313-2.028l72.516-144.72c1.215-2.423-.88-5.186-3.54-4.672l-25.505 4.922c-2.396.462-4.435-1.77-3.759-4.114l16.646-57.705c.677-2.35-1.37-4.583-3.769-4.113Z"></path></svg>
</file>

<file path="frontend/src/api/auth.ts">
import client from './client';
import type { LoginRequest, LoginResponse, UserItem } from '../types';

export async function login(data: LoginRequest): Promise<LoginResponse> {
  const res = await client.post<LoginResponse>('/auth/login', data);
  return res.data;
}

export async function listUsers(): Promise<UserItem[]> {
  const res = await client.get<UserItem[]>('/admin/users');
  return res.data;
}

export async function createUser(data: {
  user_id: string;
  password: string;
  role: string;
}): Promise<{ user_id: string; role: string }> {
  const res = await client.post('/admin/users', data);
  return res.data;
}
</file>

<file path="frontend/src/api/auto.ts">
import client from './client';

export interface AutoCompleteRequest {
  topic: string;
  session_id?: string;
  canvas_id?: string;
  search_mode?: 'local' | 'web' | 'hybrid';
  max_sections?: number; // 1-12, default 6
}

export interface AutoCompleteResponse {
  session_id: string;
  canvas_id: string;
  markdown: string;
  outline: string[];
  citations: string[];
  total_time_ms: number;
}

/**
 * 自动完成综述：根据主题检索 -> 生成大纲 -> 逐章写作 -> 返回完整 Markdown。
 */
export async function autoComplete(data: AutoCompleteRequest): Promise<AutoCompleteResponse> {
  const res = await client.post<AutoCompleteResponse>('/auto-complete', data);
  return res.data;
}
</file>

<file path="frontend/src/api/chat.ts">
import client, { streamChat } from './client';
import type {
  ChatRequest,
  ChatResponse,
  SessionInfo,
  SessionListItem,
  IntentDetectRequest,
  IntentDetectResponse,
  ClarifyResponse,
  DeepResearchStartRequest,
  DeepResearchStartResponse,
  DeepResearchConfirmRequest,
  DeepResearchSubmitResponse,
  DeepResearchJobInfo,
  DeepResearchJobEvent,
  GapSupplement,
  ResearchInsight,
} from '../types';

export async function chat(data: ChatRequest): Promise<ChatResponse> {
  const res = await client.post<ChatResponse>('/chat', data);
  return res.data;
}

/**
 * 意图检测 API（简化版）：Chat vs Deep Research
 */
export async function detectIntent(data: IntentDetectRequest): Promise<IntentDetectResponse> {
  const res = await client.post<IntentDetectResponse>('/intent/detect', data);
  return res.data;
}

/**
 * Deep Research 澄清问题生成
 */
export async function clarifyForDeepResearch(data: {
  message: string;
  session_id?: string;
  search_mode?: string;
  llm_provider?: string;
  model_override?: string;
}): Promise<ClarifyResponse> {
  const res = await client.post<ClarifyResponse>('/deep-research/clarify', data);
  return res.data;
}

export function chatStream(data: ChatRequest, signal?: AbortSignal) {
  return streamChat('/chat/stream', data, signal);
}

export async function deepResearchStart(data: DeepResearchStartRequest): Promise<DeepResearchStartResponse> {
  const res = await client.post<DeepResearchStartResponse>(
    '/deep-research/start',
    data,
    {
      // Phase-1 deep research may run hybrid retrieval + planning,
      // which can exceed the global 60s default timeout.
      timeout: 300000,
    }
  );
  return res.data;
}

export function deepResearchConfirmStream(data: DeepResearchConfirmRequest, signal?: AbortSignal) {
  return streamChat('/deep-research/confirm', data, signal);
}

export async function deepResearchSubmit(data: DeepResearchConfirmRequest): Promise<DeepResearchSubmitResponse> {
  const res = await client.post<DeepResearchSubmitResponse>('/deep-research/submit', data, { timeout: 30000 });
  return res.data;
}

export async function getDeepResearchJob(jobId: string): Promise<DeepResearchJobInfo> {
  const res = await client.get<DeepResearchJobInfo>(`/deep-research/jobs/${encodeURIComponent(jobId)}`);
  return res.data;
}

export async function listDeepResearchJobEvents(
  jobId: string,
  afterId: number,
  limit: number = 200,
): Promise<DeepResearchJobEvent[]> {
  const res = await client.get<{ job_id: string; events: DeepResearchJobEvent[] }>(
    `/deep-research/jobs/${encodeURIComponent(jobId)}/events`,
    { params: { after_id: afterId, limit } },
  );
  return res.data.events || [];
}

export async function cancelDeepResearchJob(jobId: string): Promise<{ ok: boolean; job_id: string; status: string }> {
  const res = await client.post<{ ok: boolean; job_id: string; status: string }>(
    `/deep-research/jobs/${encodeURIComponent(jobId)}/cancel`,
  );
  return res.data;
}

export async function submitSectionReview(
  jobId: string,
  data: { section_id: string; action: 'approve' | 'revise'; feedback?: string },
): Promise<{ ok: boolean; job_id: string; section_id: string; action: string }> {
  const res = await client.post<{ ok: boolean; job_id: string; section_id: string; action: string }>(
    `/deep-research/jobs/${encodeURIComponent(jobId)}/review`,
    data,
  );
  return res.data;
}

export async function listSectionReviews(
  jobId: string,
): Promise<Array<{ job_id: string; section_id: string; action: string; feedback?: string; created_at?: number }>> {
  const res = await client.get<{
    job_id: string;
    reviews: Array<{ job_id: string; section_id: string; action: string; feedback?: string; created_at?: number }>;
  }>(
    `/deep-research/jobs/${encodeURIComponent(jobId)}/reviews`,
  );
  return res.data.reviews || [];
}

// ── Gap Supplement APIs ──

export async function submitGapSupplement(
  jobId: string,
  data: {
    section_id: string;
    gap_text: string;
    supplement_type: 'material' | 'direct_info';
    content: Record<string, unknown>;
  },
): Promise<{ ok: boolean; id: number; job_id: string; section_id: string; status: string }> {
  const res = await client.post<{ ok: boolean; id: number; job_id: string; section_id: string; status: string }>(
    `/deep-research/jobs/${encodeURIComponent(jobId)}/gap-supplement`,
    data,
  );
  return res.data;
}

export async function listGapSupplements(
  jobId: string,
  sectionId?: string,
): Promise<GapSupplement[]> {
  const params: Record<string, string> = {};
  if (sectionId) params.section_id = sectionId;
  const res = await client.get<{ job_id: string; supplements: GapSupplement[] }>(
    `/deep-research/jobs/${encodeURIComponent(jobId)}/gap-supplements`,
    { params },
  );
  return res.data.supplements || [];
}

// ── Research Insights APIs ──

export async function listInsights(
  jobId: string,
  insightType?: string,
  status?: string,
): Promise<ResearchInsight[]> {
  const params: Record<string, string> = {};
  if (insightType) params.insight_type = insightType;
  if (status) params.status = status;
  const res = await client.get<{ job_id: string; insights: ResearchInsight[] }>(
    `/deep-research/jobs/${encodeURIComponent(jobId)}/insights`,
    { params },
  );
  return res.data.insights || [];
}

export async function updateInsightStatus(
  jobId: string,
  insightId: number,
  status: 'open' | 'addressed' | 'deferred',
): Promise<{ ok: boolean }> {
  const res = await client.post<{ ok: boolean }>(
    `/deep-research/jobs/${encodeURIComponent(jobId)}/insights/${insightId}/status`,
    { status },
  );
  return res.data;
}

export async function extractDeepResearchContextFiles(files: File[]): Promise<Array<{ name: string; content: string }>> {
  const fd = new FormData();
  files.forEach((f) => fd.append('files', f));
  const res = await client.post<{ documents: Array<{ name: string; content: string }> }>(
    '/deep-research/context-files',
    fd,
    {
      headers: {
        'Content-Type': null as unknown as string,
      },
      timeout: 120000,
    },
  );
  return res.data.documents || [];
}

export async function getSession(sessionId: string): Promise<SessionInfo> {
  const res = await client.get<SessionInfo>(`/sessions/${sessionId}`);
  return res.data;
}

export async function deleteSession(sessionId: string): Promise<void> {
  await client.delete(`/sessions/${sessionId}`);
}

export async function listSessions(limit: number = 100): Promise<SessionListItem[]> {
  const res = await client.get<SessionListItem[]>('/sessions', { params: { limit } });
  return res.data;
}
</file>

<file path="frontend/src/api/client.ts">
import axios, { type AxiosInstance, type InternalAxiosRequestConfig } from 'axios';

// 基础 URL：开发时通过 Vite proxy 转发，生产时直接访问后端
const BASE_URL = import.meta.env.VITE_API_BASE_URL || '/api';

const client: AxiosInstance = axios.create({
  baseURL: BASE_URL,
  timeout: 60000,
  headers: {
    'Content-Type': 'application/json',
  },
});

// 请求拦截器：自动附加 token
client.interceptors.request.use(
  (config: InternalAxiosRequestConfig) => {
    const token = localStorage.getItem('token');
    if (token && config.headers) {
      config.headers.Authorization = `Bearer ${token}`;
    }
    return config;
  },
  (error) => Promise.reject(error)
);

// 响应拦截器：处理 401
client.interceptors.response.use(
  (response) => response,
  (error) => {
    if (error.response?.status === 401) {
      localStorage.removeItem('token');
      // 可以在这里触发全局登出逻辑
      window.dispatchEvent(new CustomEvent('auth:logout'));
    }
    return Promise.reject(error);
  }
);

export default client;

// SSE 流式请求辅助
export async function* streamChat(
  url: string,
  body: object,
  signal?: AbortSignal
): AsyncGenerator<{ event: string; data: unknown }> {
  const token = localStorage.getItem('token');
  const response = await fetch(`${BASE_URL}${url}`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      ...(token ? { Authorization: `Bearer ${token}` } : {}),
    },
    body: JSON.stringify(body),
    signal,
  });

  if (!response.ok) {
    throw new Error(`HTTP ${response.status}: ${response.statusText}`);
  }

  const reader = response.body?.getReader();
  if (!reader) {
    throw new Error('No response body');
  }

  const decoder = new TextDecoder();
  let buffer = '';

  while (true) {
    const { done, value } = await reader.read();
    if (done) break;

    buffer += decoder.decode(value, { stream: true });
    const lines = buffer.split('\n');
    buffer = lines.pop() || '';

    let currentEvent = '';
    for (const line of lines) {
      if (line.startsWith('event: ')) {
        currentEvent = line.slice(7).trim();
      } else if (line.startsWith('data: ')) {
        const dataStr = line.slice(6);
        try {
          const data = JSON.parse(dataStr);
          yield { event: currentEvent, data };
        } catch {
          yield { event: currentEvent, data: dataStr };
        }
      }
    }
  }
}
</file>

<file path="frontend/src/api/compare.ts">
import client from './client';

export interface PaperSummary {
  paper_id: string;
  title: string;
  year?: number | null;
  abstract: string;
}

export interface CompareCandidate {
  paper_id: string;
  title: string;
  year?: number | null;
  abstract: string;
  citation_count: number;
  last_cited_turn_index: number;
  is_local_ready: boolean;
}

export interface CandidatesResponse {
  candidates: CompareCandidate[];
  total: number;
}

export interface PapersResponse {
  papers: PaperSummary[];
  total: number;
}

export interface CompareRequest {
  paper_ids: string[];
  aspects?: string[];
  llm_provider?: string;
  model_override?: string;
}

export interface CompareResponse {
  papers: PaperSummary[];
  comparison_matrix: Record<string, Record<string, string>>;
  narrative: string;
}

export async function comparePapers(body: CompareRequest): Promise<CompareResponse> {
  const res = await client.post<CompareResponse>('/compare', body);
  return res.data;
}

export async function listCompareCandidates(
  sessionId: string,
  opts?: { scope?: string; limit?: number; offset?: number }
): Promise<CandidatesResponse> {
  const params: Record<string, string | number> = { session_id: sessionId };
  if (opts?.scope != null) params.scope = opts.scope;
  if (opts?.limit != null) params.limit = opts.limit;
  if (opts?.offset != null) params.offset = opts.offset;
  const res = await client.get<CandidatesResponse>('/compare/candidates', { params });
  return res.data;
}

export async function listAvailablePapers(opts?: {
  limit?: number;
  offset?: number;
  q?: string;
}): Promise<PapersResponse> {
  const params: Record<string, string | number> = {};
  if (opts?.limit != null) params.limit = opts.limit;
  if (opts?.offset != null) params.offset = opts.offset;
  if (opts?.q != null && opts.q !== '') params.q = opts.q;
  const res = await client.get<PapersResponse>('/compare/papers', { params });
  return res.data;
}
</file>

<file path="frontend/src/api/health.ts">
import client from './client';

export async function checkHealth(): Promise<{ status: string }> {
  const res = await client.get<{ status: string }>('/health');
  return res.data;
}

export async function getStorageStats(): Promise<Record<string, unknown>> {
  const res = await client.get('/storage/stats');
  return res.data;
}
</file>

<file path="frontend/src/api/index.ts">
export * from './auth';
export * from './chat';
export * from './canvas';
export * from './projects';
export * from './health';
export * from './auto';
export * from './models';
export * from './ingest';
export * from './graph';
export * from './compare';
export { default as client } from './client';
export { streamChat } from './client';
</file>

<file path="frontend/src/api/ingest.ts">
import client from './client';

const BASE_URL = import.meta.env.VITE_API_BASE_URL || '/api';

// ---- Collections ----

export interface CollectionInfo {
  name: string;
  count: number;
}

export async function listCollections(): Promise<CollectionInfo[]> {
  const res = await client.get<{ collections: CollectionInfo[] }>('/ingest/collections');
  return res.data.collections;
}

export async function createCollection(name: string, recreate = false): Promise<void> {
  await client.post('/ingest/collections', { name, recreate });
}

export async function deleteCollection(name: string): Promise<void> {
  await client.delete(`/ingest/collections/${encodeURIComponent(name)}`);
}

// ---- Papers (文件级管理) ----

export interface PaperInfo {
  paper_id: string;
  filename: string;
  file_size: number;
  chunk_count: number;
  row_count: number;
  enrich_tables_enabled?: number;
  enrich_figures_enabled?: number;
  table_count?: number;
  figure_count?: number;
  table_success?: number;
  figure_success?: number;
  status: string;
  error_message: string;
  created_at: number;
  content_hash?: string;
}

export async function listPapers(collection: string): Promise<PaperInfo[]> {
  const res = await client.get<{ papers: PaperInfo[] }>(
    `/ingest/collections/${encodeURIComponent(collection)}/papers`,
  );
  return res.data.papers;
}

export async function deletePaper(
  collection: string,
  paperId: string,
): Promise<{ deleted_chunks: number }> {
  const res = await client.delete<{ deleted_chunks: number }>(
    `/ingest/collections/${encodeURIComponent(collection)}/papers/${encodeURIComponent(paperId)}`,
  );
  return res.data;
}

// ---- Upload ----

export interface UploadedFile {
  filename: string;
  path: string;
  size: number;
  content_hash?: string;
}

export async function uploadFiles(
  files: File[],
  collection: string,
): Promise<UploadedFile[]> {
  const fd = new FormData();
  for (const f of files) {
    fd.append('files', f);
  }
  fd.append('collection', collection);

  console.log('[ingest] uploadFiles called, files:', files.length, 'collection:', collection);

  // 使用独立 axios 实例发送 multipart，避免默认 Content-Type: application/json 干扰
  const token = localStorage.getItem('token');
  const res = await client.post<{ uploaded: UploadedFile[]; count: number }>(
    '/ingest/upload',
    fd,
    {
      // axios 1.x: 传 null 才能真正移除默认 header，让浏览器自动加 multipart boundary
      headers: {
        'Content-Type': null as unknown as string,
        ...(token ? { Authorization: `Bearer ${token}` } : {}),
      },
      timeout: 300000,
    },
  );
  console.log('[ingest] uploadFiles response:', res.data);
  return res.data.uploaded;
}

// ---- Process (SSE) ----

export interface IngestProgressEvent {
  event: string;
  data: Record<string, unknown>;
}

export interface IngestJobInfo {
  job_id: string;
  collection: string;
  status: string;
  total_files: number;
  processed_files: number;
  failed_files: number;
  total_chunks: number;
  total_upserted: number;
  current_file: string;
  current_stage: string;
  message: string;
  error_message: string;
  created_at: number;
  updated_at: number;
  finished_at?: number | null;
}

export interface EnrichmentOptions {
  enrich_tables: boolean;
  enrich_figures: boolean;
  llm_text_provider?: string | null;
  llm_text_model?: string | null;
  llm_text_concurrency?: number | null;
  llm_vision_provider?: string | null;
  llm_vision_model?: string | null;
  llm_vision_concurrency?: number | null;
}

export interface LLMProviderInfo {
  id: string;
  default_model: string;
  models: string[];
}

export interface LLMProvidersResponse {
  default: string;
  providers: LLMProviderInfo[];
  parser_defaults?: {
    llm_text_provider?: string | null;
    llm_text_model?: string | null;
    llm_text_concurrency?: number | null;
    llm_vision_provider?: string | null;
    llm_vision_model?: string | null;
    llm_vision_concurrency?: number | null;
  };
}

export async function listLLMProviders(): Promise<LLMProvidersResponse> {
  const res = await client.get<LLMProvidersResponse>('/llm/providers');
  return res.data;
}

export async function listIngestJobs(limit = 20, status?: string): Promise<IngestJobInfo[]> {
  const params = new URLSearchParams();
  params.set('limit', String(limit));
  if (status) params.set('status', status);
  const res = await client.get<{ jobs: IngestJobInfo[] }>(`/ingest/jobs?${params.toString()}`);
  return res.data.jobs || [];
}

export async function getIngestJob(jobId: string): Promise<IngestJobInfo> {
  const res = await client.get<{ job: IngestJobInfo }>(`/ingest/jobs/${encodeURIComponent(jobId)}`);
  return res.data.job;
}

export async function cancelIngestJob(jobId: string): Promise<{ ok: boolean; status: string }> {
  const res = await client.post<{ ok: boolean; status: string }>(
    `/ingest/jobs/${encodeURIComponent(jobId)}/cancel`,
  );
  return res.data;
}

export async function startIngestJob(
  filePaths: string[],
  collection: string,
  enrichment: EnrichmentOptions,
  content_hashes?: Record<string, string>,
): Promise<{ job_id: string }> {
  const token = localStorage.getItem('token');
  const skipEnrichment = !enrichment.enrich_tables && !enrichment.enrich_figures;
  const response = await fetch(`${BASE_URL}/ingest/process`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      ...(token ? { Authorization: `Bearer ${token}` } : {}),
    },
    body: JSON.stringify({
      file_paths: filePaths,
      collection,
      skip_enrichment: skipEnrichment,
      enrich_tables: enrichment.enrich_tables,
      enrich_figures: enrichment.enrich_figures,
      llm_text_provider: enrichment.llm_text_provider ?? null,
      llm_text_model: enrichment.llm_text_model ?? null,
      llm_text_concurrency: enrichment.llm_text_concurrency ?? null,
      llm_vision_provider: enrichment.llm_vision_provider ?? null,
      llm_vision_model: enrichment.llm_vision_model ?? null,
      llm_vision_concurrency: enrichment.llm_vision_concurrency ?? null,
      content_hashes: content_hashes || {},
    }),
  });
  if (!response.ok) {
    const errBody = await response.text().catch(() => '');
    throw new Error(`HTTP ${response.status}: ${response.statusText} - ${errBody}`);
  }
  const body = (await response.json()) as { job_id?: string };
  if (!body.job_id) throw new Error('No job_id returned from /ingest/process');
  return { job_id: body.job_id };
}

export async function* streamIngestJobEvents(
  jobId: string,
  signal?: AbortSignal,
  afterId = 0,
): AsyncGenerator<IngestProgressEvent> {
  const token = localStorage.getItem('token');
  const response = await fetch(
    `${BASE_URL}/ingest/jobs/${encodeURIComponent(jobId)}/events?after_id=${afterId}`,
    {
      method: 'GET',
      headers: {
        ...(token ? { Authorization: `Bearer ${token}` } : {}),
      },
      signal,
    },
  );
  if (!response.ok) {
    const errBody = await response.text().catch(() => '');
    throw new Error(`HTTP ${response.status}: ${response.statusText} - ${errBody}`);
  }
  const reader = response.body?.getReader();
  if (!reader) throw new Error('No response body');

  const decoder = new TextDecoder();
  let buffer = '';
  while (true) {
    const { done, value } = await reader.read();
    if (done) break;
    buffer += decoder.decode(value, { stream: true });
    const lines = buffer.split('\n');
    buffer = lines.pop() || '';

    let currentEvent = '';
    for (const line of lines) {
      if (line.startsWith('event: ')) {
        currentEvent = line.slice(7).trim();
      } else if (line.startsWith('data: ')) {
        const dataStr = line.slice(6);
        try {
          const data = JSON.parse(dataStr);
          yield { event: currentEvent, data };
        } catch {
          yield { event: currentEvent, data: { raw: dataStr } };
        }
      }
    }
  }
}

/**
 * 调用 /ingest/process，以 SSE 方式返回进度事件。
 * content_hashes: path -> SHA256 hex，用于 paper 元数据与去重。
 */
export async function* processFiles(
  filePaths: string[],
  collection: string,
  enrichment: EnrichmentOptions,
  signal?: AbortSignal,
  content_hashes?: Record<string, string>,
): AsyncGenerator<IngestProgressEvent> {
  const started = await startIngestJob(filePaths, collection, enrichment, content_hashes);
  yield { event: 'job_created', data: { job_id: started.job_id, collection } };
  for await (const evt of streamIngestJobEvents(started.job_id, signal, 0)) {
    yield evt;
  }
}
</file>

<file path="frontend/src/api/models.ts">
import client from './client';
import type { ModelSyncRequest, ModelSyncResponse, ModelStatusResponse } from '../types';

export async function getModelStatus(): Promise<ModelStatusResponse> {
  const res = await client.get<ModelStatusResponse>('/models/status');
  return res.data;
}

export async function syncModels(payload: ModelSyncRequest = {}): Promise<ModelSyncResponse> {
  const res = await client.post<ModelSyncResponse>('/models/sync', payload);
  return res.data;
}
</file>

<file path="frontend/src/api/projects.ts">
import client from './client';
import type { Project } from '../types';

export async function listProjects(
  includeArchived = false
): Promise<Project[]> {
  const res = await client.get<Project[]>('/projects', {
    params: { include_archived: includeArchived },
  });
  return res.data;
}

export async function archiveProject(
  canvasId: string
): Promise<{ canvas_id: string; archived: boolean }> {
  const res = await client.post(`/projects/${canvasId}/archive`);
  return res.data;
}

export async function unarchiveProject(
  canvasId: string
): Promise<{ canvas_id: string; archived: boolean }> {
  const res = await client.post(`/projects/${canvasId}/unarchive`);
  return res.data;
}

export async function deleteProject(
  canvasId: string
): Promise<{ canvas_id: string; deleted: boolean }> {
  const res = await client.delete(`/projects/${canvasId}`);
  return res.data;
}
</file>

<file path="frontend/src/assets/react.svg">
<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" aria-hidden="true" role="img" class="iconify iconify--logos" width="35.93" height="32" preserveAspectRatio="xMidYMid meet" viewBox="0 0 256 228"><path fill="#00D8FF" d="M210.483 73.824a171.49 171.49 0 0 0-8.24-2.597c.465-1.9.893-3.777 1.273-5.621c6.238-30.281 2.16-54.676-11.769-62.708c-13.355-7.7-35.196.329-57.254 19.526a171.23 171.23 0 0 0-6.375 5.848a155.866 155.866 0 0 0-4.241-3.917C100.759 3.829 77.587-4.822 63.673 3.233C50.33 10.957 46.379 33.89 51.995 62.588a170.974 170.974 0 0 0 1.892 8.48c-3.28.932-6.445 1.924-9.474 2.98C17.309 83.498 0 98.307 0 113.668c0 15.865 18.582 31.778 46.812 41.427a145.52 145.52 0 0 0 6.921 2.165a167.467 167.467 0 0 0-2.01 9.138c-5.354 28.2-1.173 50.591 12.134 58.266c13.744 7.926 36.812-.22 59.273-19.855a145.567 145.567 0 0 0 5.342-4.923a168.064 168.064 0 0 0 6.92 6.314c21.758 18.722 43.246 26.282 56.54 18.586c13.731-7.949 18.194-32.003 12.4-61.268a145.016 145.016 0 0 0-1.535-6.842c1.62-.48 3.21-.974 4.76-1.488c29.348-9.723 48.443-25.443 48.443-41.52c0-15.417-17.868-30.326-45.517-39.844Zm-6.365 70.984c-1.4.463-2.836.91-4.3 1.345c-3.24-10.257-7.612-21.163-12.963-32.432c5.106-11 9.31-21.767 12.459-31.957c2.619.758 5.16 1.557 7.61 2.4c23.69 8.156 38.14 20.213 38.14 29.504c0 9.896-15.606 22.743-40.946 31.14Zm-10.514 20.834c2.562 12.94 2.927 24.64 1.23 33.787c-1.524 8.219-4.59 13.698-8.382 15.893c-8.067 4.67-25.32-1.4-43.927-17.412a156.726 156.726 0 0 1-6.437-5.87c7.214-7.889 14.423-17.06 21.459-27.246c12.376-1.098 24.068-2.894 34.671-5.345a134.17 134.17 0 0 1 1.386 6.193ZM87.276 214.515c-7.882 2.783-14.16 2.863-17.955.675c-8.075-4.657-11.432-22.636-6.853-46.752a156.923 156.923 0 0 1 1.869-8.499c10.486 2.32 22.093 3.988 34.498 4.994c7.084 9.967 14.501 19.128 21.976 27.15a134.668 134.668 0 0 1-4.877 4.492c-9.933 8.682-19.886 14.842-28.658 17.94ZM50.35 144.747c-12.483-4.267-22.792-9.812-29.858-15.863c-6.35-5.437-9.555-10.836-9.555-15.216c0-9.322 13.897-21.212 37.076-29.293c2.813-.98 5.757-1.905 8.812-2.773c3.204 10.42 7.406 21.315 12.477 32.332c-5.137 11.18-9.399 22.249-12.634 32.792a134.718 134.718 0 0 1-6.318-1.979Zm12.378-84.26c-4.811-24.587-1.616-43.134 6.425-47.789c8.564-4.958 27.502 2.111 47.463 19.835a144.318 144.318 0 0 1 3.841 3.545c-7.438 7.987-14.787 17.08-21.808 26.988c-12.04 1.116-23.565 2.908-34.161 5.309a160.342 160.342 0 0 1-1.76-7.887Zm110.427 27.268a347.8 347.8 0 0 0-7.785-12.803c8.168 1.033 15.994 2.404 23.343 4.08c-2.206 7.072-4.956 14.465-8.193 22.045a381.151 381.151 0 0 0-7.365-13.322Zm-45.032-43.861c5.044 5.465 10.096 11.566 15.065 18.186a322.04 322.04 0 0 0-30.257-.006c4.974-6.559 10.069-12.652 15.192-18.18ZM82.802 87.83a323.167 323.167 0 0 0-7.227 13.238c-3.184-7.553-5.909-14.98-8.134-22.152c7.304-1.634 15.093-2.97 23.209-3.984a321.524 321.524 0 0 0-7.848 12.897Zm8.081 65.352c-8.385-.936-16.291-2.203-23.593-3.793c2.26-7.3 5.045-14.885 8.298-22.6a321.187 321.187 0 0 0 7.257 13.246c2.594 4.48 5.28 8.868 8.038 13.147Zm37.542 31.03c-5.184-5.592-10.354-11.779-15.403-18.433c4.902.192 9.899.29 14.978.29c5.218 0 10.376-.117 15.453-.343c-4.985 6.774-10.018 12.97-15.028 18.486Zm52.198-57.817c3.422 7.8 6.306 15.345 8.596 22.52c-7.422 1.694-15.436 3.058-23.88 4.071a382.417 382.417 0 0 0 7.859-13.026a347.403 347.403 0 0 0 7.425-13.565Zm-16.898 8.101a358.557 358.557 0 0 1-12.281 19.815a329.4 329.4 0 0 1-23.444.823c-7.967 0-15.716-.248-23.178-.732a310.202 310.202 0 0 1-12.513-19.846h.001a307.41 307.41 0 0 1-10.923-20.627a310.278 310.278 0 0 1 10.89-20.637l-.001.001a307.318 307.318 0 0 1 12.413-19.761c7.613-.576 15.42-.876 23.31-.876H128c7.926 0 15.743.303 23.354.883a329.357 329.357 0 0 1 12.335 19.695a358.489 358.489 0 0 1 11.036 20.54a329.472 329.472 0 0 1-11 20.722Zm22.56-122.124c8.572 4.944 11.906 24.881 6.52 51.026c-.344 1.668-.73 3.367-1.15 5.09c-10.622-2.452-22.155-4.275-34.23-5.408c-7.034-10.017-14.323-19.124-21.64-27.008a160.789 160.789 0 0 1 5.888-5.4c18.9-16.447 36.564-22.941 44.612-18.3ZM128 90.808c12.625 0 22.86 10.235 22.86 22.86s-10.235 22.86-22.86 22.86s-22.86-10.235-22.86-22.86s10.235-22.86 22.86-22.86Z"></path></svg>
</file>

<file path="frontend/src/components/canvas/DraftingStage.tsx">
import { FileText, BookOpen, AlertTriangle, ChevronDown, ChevronUp, Send, CheckCircle2, RefreshCw } from 'lucide-react';
import { useEffect, useMemo, useState } from 'react';
import { listSectionReviews, submitSectionReview, submitGapSupplement, listGapSupplements } from '../../api/chat';
import { useToastStore } from '../../stores';
import { useChatStore } from '../../stores';
import type { Canvas, GapSupplement } from '../../types';

interface DraftingStageProps {
  canvas: Canvas;
}

export function DraftingStage({ canvas }: DraftingStageProps) {
  const { outline, drafts, citation_pool, identified_gaps, skip_draft_review } = canvas;
  const [expandedSections, setExpandedSections] = useState<Set<string>>(new Set());
  const [reviewingSectionId, setReviewingSectionId] = useState<string | null>(null);
  const [reviewFeedback, setReviewFeedback] = useState<string>('');
  const [submittingFor, setSubmittingFor] = useState<string | null>(null);
  const [reviewBySection, setReviewBySection] = useState<Record<string, { action: string; feedback?: string; created_at?: number }>>({});
  const [gapSupplements, setGapSupplements] = useState<GapSupplement[]>([]);
  const [bulkApproving, setBulkApproving] = useState(false);
  const [showSupplementModal, setShowSupplementModal] = useState(false);
  const [supplementSectionTitle, setSupplementSectionTitle] = useState('');
  const [supplementGapText, setSupplementGapText] = useState('');
  const [supplementType, setSupplementType] = useState<'material' | 'direct_info'>('direct_info');
  const [supplementText, setSupplementText] = useState('');
  const [supplementSubmitting, setSupplementSubmitting] = useState(false);
  const addToast = useToastStore((s) => s.addToast);
  const researchDashboard = useChatStore((s) => s.researchDashboard);
  const setShowDeepResearchDialog = useChatStore((s) => s.setShowDeepResearchDialog);
  const setDeepResearchTopic = useChatStore((s) => s.setDeepResearchTopic);

  const sortedOutline = [...(outline || [])].sort((a, b) => a.order - b.order);
  const activeJobId = localStorage.getItem('deep_research_active_job_id') || '';

  const toggleSection = (id: string) => {
    setExpandedSections((prev) => {
      const next = new Set(prev);
      if (next.has(id)) {
        next.delete(id);
      } else {
        next.add(id);
      }
      return next;
    });
  };

  if (!sortedOutline.length) {
    return (
      <div className="flex flex-col items-center justify-center h-full text-[var(--text-tertiary)] text-sm px-6">
        <FileText size={36} className="mb-3 opacity-30" />
        <p className="font-medium mb-1">尚未开始撰写</p>
        <p className="text-xs text-center">
          确认大纲后开始撰写，<br />
          每个章节的草稿将显示在此处。
        </p>
      </div>
    );
  }

  const citationMap = new Map((citation_pool || []).map((c) => [c.cite_key || c.id, c]));
  const draftedCount = Object.keys(drafts || {}).length;
  const approvedCount = useMemo(
    () => sortedOutline.filter((s) => (reviewBySection[s.title]?.action || '').toLowerCase() === 'approve').length,
    [sortedOutline, reviewBySection],
  );

  // Poll reviews
  useEffect(() => {
    if (skip_draft_review || !activeJobId) {
      setReviewBySection({});
      return;
    }
    let cancelled = false;
    const syncReviews = async () => {
      try {
        const reviews = await listSectionReviews(activeJobId);
        if (cancelled) return;
        const next: Record<string, { action: string; feedback?: string; created_at?: number }> = {};
        reviews.forEach((r) => {
          next[r.section_id] = { action: r.action, feedback: r.feedback, created_at: r.created_at };
        });
        setReviewBySection(next);
      } catch {
        // ignore transient errors
      }
    };
    syncReviews();
    const timer = window.setInterval(syncReviews, 5000);
    return () => {
      cancelled = true;
      window.clearInterval(timer);
    };
  }, [skip_draft_review, activeJobId]);

  // Poll gap supplements
  useEffect(() => {
    if (!activeJobId) return;
    let cancelled = false;
    const syncSupplements = async () => {
      try {
        const sups = await listGapSupplements(activeJobId);
        if (cancelled) return;
        setGapSupplements(sups);
      } catch {
        // ignore
      }
    };
    syncSupplements();
    const timer = window.setInterval(syncSupplements, 8000);
    return () => {
      cancelled = true;
      window.clearInterval(timer);
    };
  }, [activeJobId]);

  const handleSubmitReview = async (sectionId: string, action: 'approve' | 'revise', feedback: string = '') => {
    const jobId = localStorage.getItem('deep_research_active_job_id') || '';
    if (!jobId) {
      addToast('未检测到活跃任务，无法提交审核', 'error');
      return;
    }
    try {
      setSubmittingFor(sectionId);
      await submitSectionReview(jobId, { section_id: sectionId, action, feedback });
      setReviewBySection((prev) => ({
        ...prev,
        [sectionId]: { action, feedback, created_at: Date.now() / 1000 },
      }));
      if (action === 'approve') {
        const next = {
          ...reviewBySection,
          [sectionId]: { action, feedback, created_at: Date.now() / 1000 },
        };
        const nextApproved = sortedOutline.filter(
          (s) => (next[s.title]?.action || '').toLowerCase() === 'approve',
        ).length;
        if (nextApproved >= sortedOutline.length) {
          addToast('已全部通过，正在触发最终整合（synthesize）...', 'info');
        }
      }
      addToast(
        action === 'approve'
          ? '已通过，任务继续执行'
          : '已提交修改意见，已加入优先重写队列',
        'success',
      );
      if (action === 'revise') {
        setReviewFeedback('');
      }
      setReviewingSectionId(null);
    } catch (e) {
      addToast('提交审核失败，请重试', 'error');
    } finally {
      setSubmittingFor(null);
    }
  };

  const openSupplementModal = (
    sectionTitle: string,
    gapText: string = '',
    type: 'material' | 'direct_info' = 'direct_info',
  ) => {
    setSupplementSectionTitle(sectionTitle);
    setSupplementGapText(gapText);
    setSupplementType(type);
    setSupplementText('');
    setShowSupplementModal(true);
  };

  const closeSupplementModal = () => {
    setShowSupplementModal(false);
    setSupplementSectionTitle('');
    setSupplementGapText('');
    setSupplementText('');
    setSupplementType('direct_info');
    setSupplementSubmitting(false);
  };

  const handleSubmitSupplement = async () => {
    const text = supplementText.trim();
    if (!text) {
      addToast('请输入要补充的资料或观点', 'error');
      return;
    }
    const jobId = localStorage.getItem('deep_research_active_job_id') || '';
    const effectiveGap = (supplementGapText || 'Section-level supplement').trim();
    if (!jobId) {
      const payload = [
        `Section: ${supplementSectionTitle || 'N/A'}`,
        `Gap: ${effectiveGap}`,
        `Supplement type: ${supplementType}`,
        'User supplemental input:',
        text,
      ].join('\n');
      localStorage.setItem('deep_research_pending_user_context', payload);
      setDeepResearchTopic(canvas.topic || supplementSectionTitle || 'Deep Research');
      setShowDeepResearchDialog(true);
      addToast('未检测到活跃任务：已将补充内容带入对话框继续研究', 'info');
      closeSupplementModal();
      return;
    }

    try {
      setSupplementSubmitting(true);
      await submitGapSupplement(jobId, {
        section_id: supplementSectionTitle,
        gap_text: effectiveGap,
        supplement_type: supplementType,
        content: { text },
      });
      const sups = await listGapSupplements(jobId);
      setGapSupplements(sups);
      addToast('已提交补充信息，Agent 将在重写时采纳', 'success');
      closeSupplementModal();
    } catch {
      addToast('补充提交失败，请重试', 'error');
      setSupplementSubmitting(false);
    }
  };

  const handleSupplementGapWithMaterial = (sectionTitle: string, gap: string) => {
    openSupplementModal(sectionTitle, gap, 'material');
  };

  const handleSupplementGapWithDirectInfo = (sectionTitle: string, gap: string) => {
    openSupplementModal(sectionTitle, gap, 'direct_info');
  };

  const getGapStatus = (sectionTitle: string, gap: string): 'unhandled' | 'submitted' | 'consumed' => {
    const matching = gapSupplements.filter(
      (s) => s.section_id === sectionTitle && s.gap_text === gap,
    );
    if (!matching.length) return 'unhandled';
    if (matching.some((s) => s.status === 'consumed')) return 'consumed';
    return 'submitted';
  };

  const gapStatusLabel = (status: 'unhandled' | 'submitted' | 'consumed'): string => {
    switch (status) {
      case 'consumed': return '已采纳进本章重写';
      case 'submitted': return '已补充，待采纳';
      default: return '未处理';
    }
  };

  const gapStatusClass = (status: 'unhandled' | 'submitted' | 'consumed'): string => {
    switch (status) {
      case 'consumed': return 'text-emerald-200 bg-emerald-900/30 border-emerald-500/30';
      case 'submitted': return 'text-sky-200 bg-sky-900/30 border-sky-500/30';
      default: return 'text-slate-300 bg-slate-800/80 border-slate-600/60';
    }
  };

  const handleApproveAllSections = async () => {
    const jobId = localStorage.getItem('deep_research_active_job_id') || '';
    if (!jobId) {
      addToast('未检测到活跃任务，无法批量通过', 'error');
      return;
    }
    const pendingTitles = sortedOutline
      .map((s) => s.title)
      .filter((title) => (reviewBySection[title]?.action || '').toLowerCase() !== 'approve');
    if (!pendingTitles.length) {
      addToast('所有章节已是通过状态', 'info');
      return;
    }
    setBulkApproving(true);
    const success: string[] = [];
    const failed: string[] = [];
    for (const title of pendingTitles) {
      try {
        await submitSectionReview(jobId, { section_id: title, action: 'approve' });
        success.push(title);
      } catch {
        failed.push(title);
      }
    }

    if (success.length) {
      const ts = Date.now() / 1000;
      const merged = { ...reviewBySection };
      success.forEach((title) => {
        merged[title] = { action: 'approve', feedback: '', created_at: ts };
      });
      setReviewBySection(merged);
      const nextApproved = sortedOutline.filter(
        (s) => (merged[s.title]?.action || '').toLowerCase() === 'approve',
      ).length;
      addToast(`已批量通过 ${success.length} 个章节`, 'success');
      if (nextApproved >= sortedOutline.length) {
        addToast('已全部通过，正在触发最终整合（synthesize）...', 'info');
      }
    }
    if (failed.length) {
      addToast(`有 ${failed.length} 个章节提交失败，请重试`, 'error');
    }
    setBulkApproving(false);
  };

  return (
    <div className="p-4 space-y-3 overflow-y-auto h-full">
      <div className="bg-[var(--bg-surface)] rounded-lg p-3 border border-[var(--border-subtle)]">
        <div className="flex items-center justify-between">
          <span className="text-xs text-[var(--text-tertiary)]">章节进度</span>
          <span className="text-xs font-medium text-[var(--text-primary)]">
            {draftedCount} / {sortedOutline.length}
          </span>
        </div>
        {!skip_draft_review && draftedCount > 0 && (
          <div className="mt-2 space-y-0.5">
            <div className="text-[10px] text-amber-200 font-medium">
              审核进度：已通过 {approvedCount} / {sortedOutline.length}
            </div>
            <div className="text-[10px] text-amber-300">规则：章节可继续生成，但需全部通过后才会进入最终整合。</div>
            <div className="pt-1">
              <button
                onClick={handleApproveAllSections}
                disabled={bulkApproving || approvedCount >= sortedOutline.length}
                className="px-2.5 py-1 text-[11px] rounded-md border border-emerald-500/40 text-emerald-200 hover:bg-emerald-900/30 disabled:opacity-50 cursor-pointer"
              >
                {bulkApproving ? '批量提交中...' : '全部通过并触发整合'}
              </button>
            </div>
          </div>
        )}
      </div>

      {sortedOutline.map((section) => {
        const draft = drafts[section.id];
        const isExpanded = expandedSections.has(section.id);
        const hasDraft = !!draft?.content_md;
        const usedCitations = (draft?.used_citation_ids || [])
          .map((key) => citationMap.get(key))
          .filter(Boolean);
        const sectionGaps = (researchDashboard?.sections || [])
          .find((s) => (s.title || '').trim().toLowerCase() === (section.title || '').trim().toLowerCase())
          ?.gaps || [];
        const reviewAction = (reviewBySection[section.title]?.action || '').toLowerCase();
        const reviewStatusText =
          reviewAction === 'approve' ? '已通过' : reviewAction === 'revise' ? '待重写' : '待审核';
        const reviewStatusClass =
          reviewAction === 'approve'
            ? 'text-emerald-200 bg-emerald-900/30 border-emerald-500/30'
            : reviewAction === 'revise'
              ? 'text-amber-200 bg-amber-900/30 border-amber-500/30'
              : 'text-slate-300 bg-slate-800/80 border-slate-600/60';

        return (
          <div
            key={section.id}
            className={`bg-[var(--bg-panel)] rounded-lg border overflow-hidden transition-all ${
              hasDraft ? 'border-emerald-500/30 shadow-sm' : 'border-[var(--border-subtle)]'
            }`}
          >
            <button
              onClick={() => hasDraft && toggleSection(section.id)}
              className={`w-full flex items-center justify-between p-3 text-left ${
                hasDraft ? 'cursor-pointer hover:bg-[var(--bg-surface-hover)]' : 'cursor-default'
              }`}
            >
              <div className="flex items-center gap-2 min-w-0">
                <span className="text-sm shrink-0">
                  {hasDraft ? '✅' : section.status === 'drafting' ? '✍️' : '⬜'}
                </span>
                <div className="min-w-0">
                  <h4 className="text-sm font-medium text-[var(--text-primary)] truncate">
                    {section.title}
                  </h4>
                  {hasDraft && (
                    <div className="flex items-center gap-1.5">
                      <span className="text-[10px] text-[var(--text-tertiary)]">
                        v{draft.version} · {usedCitations.length} citations
                      </span>
                      {!skip_draft_review && (
                        <span className={`text-[10px] px-1.5 py-0.5 rounded border ${reviewStatusClass}`}>
                          {reviewStatusText}
                        </span>
                      )}
                    </div>
                  )}
                </div>
              </div>
              {hasDraft && (
                <span className="text-[var(--text-tertiary)]">
                  {isExpanded ? <ChevronUp size={14} /> : <ChevronDown size={14} />}
                </span>
              )}
            </button>

            {hasDraft && isExpanded && (
              <div className="border-t border-[var(--border-subtle)]">
                <div className="px-3 py-2">
                  <pre className="text-xs text-[var(--text-primary)] whitespace-pre-wrap font-sans leading-relaxed">
                    {draft.content_md}
                  </pre>
                </div>

                {usedCitations.length > 0 && (
                  <div className="px-3 py-2 border-t border-[var(--border-subtle)] bg-[var(--bg-muted)]">
                    <div className="flex items-center gap-1.5 mb-1.5">
                      <BookOpen size={10} className="text-teal-600" />
                      <span className="text-[10px] font-medium text-teal-300">References</span>
                    </div>
                    <div className="space-y-0.5">
                      {usedCitations.slice(0, 5).map((cite) => (
                        <div key={cite!.cite_key} className="text-[10px] text-[var(--text-tertiary)] truncate">
                          [{cite!.cite_key}] {cite!.title}
                          {cite!.year ? ` (${cite!.year})` : ''}
                        </div>
                      ))}
                    </div>
                  </div>
                )}

                {!skip_draft_review && (
                <div className="px-3 py-2 border-t border-amber-500/30 bg-amber-900/20">
                    {reviewingSectionId === section.id ? (
                      <div className="space-y-2">
                        <textarea
                          value={reviewFeedback}
                          onChange={(e) => setReviewFeedback(e.target.value)}
                          className="w-full min-h-20 border border-amber-500/30 rounded-md px-2 py-1.5 text-xs bg-slate-900/80 text-slate-100"
                          placeholder="请输入这章的修改意见（例如：补充方法对比、补引文等）"
                        />
                        <div className="flex items-center gap-2">
                          <button
                            onClick={() => handleSubmitReview(section.title, 'revise', reviewFeedback)}
                            disabled={!reviewFeedback.trim() || submittingFor === section.id}
                            className="px-2.5 py-1 text-xs rounded-md bg-amber-500 text-white hover:bg-amber-600 disabled:opacity-50 cursor-pointer flex items-center gap-1"
                          >
                            <Send size={12} />
                            提交修改意见
                          </button>
                          <button
                            onClick={() => {
                              setReviewingSectionId(null);
                              setReviewFeedback('');
                            }}
                            className="px-2.5 py-1 text-xs rounded-md border border-amber-500/30 text-amber-200 hover:bg-amber-900/30 cursor-pointer"
                          >
                            取消
                          </button>
                          <button
                            onClick={() => openSupplementModal(section.title, '', 'direct_info')}
                            className="px-2.5 py-1 text-xs rounded-md border border-indigo-500/40 text-indigo-200 hover:bg-indigo-900/30 cursor-pointer"
                          >
                            补充资料/观点
                          </button>
                        </div>
                      </div>
                    ) : (
                      <div className="flex items-center gap-2">
                        {reviewAction === 'approve' ? (
                          <>
                            <span className="px-2.5 py-1 text-xs rounded-md bg-emerald-900/30 text-emerald-200 border border-emerald-500/30 flex items-center gap-1">
                              <CheckCircle2 size={12} />
                              已通过
                            </span>
                            <button
                              onClick={() => handleSubmitReview(section.title, 'approve')}
                              disabled={submittingFor === section.id || bulkApproving}
                              className="px-2.5 py-1 text-xs rounded-md border border-emerald-500/40 text-emerald-200 hover:bg-emerald-900/30 disabled:opacity-50 cursor-pointer"
                            >
                              重新确认
                            </button>
                          </>
                        ) : (
                          <button
                            onClick={() => handleSubmitReview(section.title, 'approve')}
                            disabled={submittingFor === section.id || bulkApproving}
                            className="px-2.5 py-1 text-xs rounded-md bg-emerald-500 text-white hover:bg-emerald-600 disabled:opacity-50 cursor-pointer flex items-center gap-1"
                          >
                            <CheckCircle2 size={12} />
                            通过此章
                          </button>
                        )}
                        <button
                          onClick={() => setReviewingSectionId(section.id)}
                          className="px-2.5 py-1 text-xs rounded-md border border-amber-500/40 text-amber-200 hover:bg-amber-900/30 cursor-pointer flex items-center gap-1"
                        >
                          <RefreshCw size={12} />
                          需要修改
                        </button>
                        <button
                          onClick={() => openSupplementModal(section.title, '', 'direct_info')}
                          className="px-2.5 py-1 text-xs rounded-md border border-indigo-500/40 text-indigo-200 hover:bg-indigo-900/30 cursor-pointer"
                        >
                          补充资料/观点
                        </button>
                      </div>
                    )}
                  </div>
                )}

                {/* 章节级信息缺口（重点：每个大纲下都能看到自己的 gaps） */}
                {sectionGaps.length > 0 && (
                  <div className="px-3 py-2 border-t border-red-500/30 bg-red-900/20">
                    <div className="text-xs font-semibold text-red-200 mb-1.5">本章信息缺口</div>
                    <div className="space-y-1.5">
                      {sectionGaps.map((gap, i) => (
                        <div key={`${section.id}-gap-${i}`} className="rounded-md border border-red-500/30 bg-slate-900/70 p-2">
                          <div className="text-xs text-red-200 leading-relaxed">❗ {gap}</div>
                          {(() => {
                            const st = getGapStatus(section.title, gap);
                            return (
                              <span className={`inline-block mt-1 text-[10px] px-1.5 py-0.5 rounded border ${gapStatusClass(st)}`}>
                                {gapStatusLabel(st)}
                              </span>
                            );
                          })()}
                          <div className="mt-2 flex items-center gap-2">
                            <button
                              onClick={() => handleSupplementGapWithMaterial(section.title, gap)}
                              className="px-2.5 py-1 text-[11px] rounded-md bg-amber-500 text-white hover:bg-amber-600 cursor-pointer"
                            >
                              补材料并继续
                            </button>
                            <button
                              onClick={() => handleSupplementGapWithDirectInfo(section.title, gap)}
                              className="px-2.5 py-1 text-[11px] rounded-md border border-indigo-500/40 text-indigo-200 hover:bg-indigo-900/30 cursor-pointer"
                            >
                              直接补充信息
                            </button>
                          </div>
                        </div>
                      ))}
                    </div>
                  </div>
                )}
              </div>
            )}
          </div>
        );
      })}

      {showSupplementModal && (
        <div className="fixed inset-0 bg-black/40 z-[70] flex items-center justify-center p-4">
          <div className="w-full max-w-xl bg-slate-900 rounded-xl shadow-xl border border-slate-700 p-4 space-y-3">
            <div className="text-sm font-semibold text-slate-100">补充资料 / 观点</div>
            <div className="text-xs text-slate-300">
              章节：<span className="font-medium text-slate-100">{supplementSectionTitle || '-'}</span>
              {supplementGapText ? <> · 缺口：<span className="text-slate-100">{supplementGapText}</span></> : null}
            </div>
            <div className="flex items-center gap-3 text-xs">
              <label className="flex items-center gap-1 cursor-pointer">
                <input
                  type="radio"
                  checked={supplementType === 'direct_info'}
                  onChange={() => setSupplementType('direct_info')}
                />
                作为直接观点
              </label>
              <label className="flex items-center gap-1 cursor-pointer">
                <input
                  type="radio"
                  checked={supplementType === 'material'}
                  onChange={() => setSupplementType('material')}
                />
                作为材料线索
              </label>
            </div>
            <textarea
              value={supplementText}
              onChange={(e) => setSupplementText(e.target.value)}
              className="w-full min-h-32 border border-slate-600 rounded-md px-3 py-2 text-sm bg-slate-950 text-slate-100"
              placeholder={supplementType === 'material'
                ? '粘贴文献线索、URL、关键词、数据库来源、实验线索等...'
                : '补充你对该章节的观点、约束、反例或解释...'}
            />
            <div className="flex items-center justify-between">
              <button
                onClick={() => {
                  const seed = [
                    `Section: ${supplementSectionTitle || 'N/A'}`,
                    `Gap: ${(supplementGapText || 'Section-level supplement').trim()}`,
                    `Supplement type: ${supplementType}`,
                    'User supplemental input:',
                    supplementText.trim(),
                  ].join('\n');
                  localStorage.setItem('deep_research_pending_user_context', seed);
                  setDeepResearchTopic(canvas.topic || supplementSectionTitle || 'Deep Research');
                  setShowDeepResearchDialog(true);
                  addToast('已带入补充内容，可在对话框上传附件后继续研究', 'info');
                  closeSupplementModal();
                }}
                className="px-2.5 py-1.5 text-xs rounded-md border border-amber-500/40 text-amber-200 hover:bg-amber-900/30 cursor-pointer"
              >
                打开对话框上传附件
              </button>
              <div className="flex items-center gap-2">
                <button
                  onClick={closeSupplementModal}
                className="px-3 py-1.5 text-xs rounded-md border border-slate-600 text-slate-200 hover:bg-slate-800 cursor-pointer"
                >
                  取消
                </button>
                <button
                  onClick={handleSubmitSupplement}
                  disabled={supplementSubmitting || !supplementText.trim()}
                  className="px-3 py-1.5 text-xs rounded-md bg-indigo-600 text-white hover:bg-indigo-700 disabled:opacity-50 cursor-pointer"
                >
                  {supplementSubmitting ? '提交中...' : '提交补充'}
                </button>
              </div>
            </div>
          </div>
        </div>
      )}

      {identified_gaps.length > 0 && (
        <div className="bg-amber-900/20 border border-amber-500/30 rounded-lg p-3">
          <h4 className="text-xs font-semibold text-amber-200 mb-1.5 flex items-center gap-1.5">
            <AlertTriangle size={12} />
            信息缺口
          </h4>
          <ul className="space-y-0.5">
            {identified_gaps.map((gap, i) => (
              <li key={i} className="text-[11px] text-amber-300 flex items-start gap-1">
                <span className="shrink-0 mt-0.5">❗</span>
                <span>{gap}</span>
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/canvas/ExploreStage.tsx">
import { Target, FlaskConical, HelpCircle, Ban, BookOpen, Rocket, Clock, Edit3 } from 'lucide-react';
import type { Canvas } from '../../types';

interface ExploreStageProps {
  canvas: Canvas;
}

/**
 * Explore 阶段：Survey Canvas 结构化卡片（类似 Business Model Canvas）
 * 展示研究简报的各个板块：Goal / Hypothesis / Questions / Exclusions / Sources / Action Plan
 */
export function ExploreStage({ canvas }: ExploreStageProps) {
  const brief = canvas.research_brief;

  if (!brief) {
    return (
      <div className="flex flex-col items-center justify-center h-full text-[var(--text-tertiary)] text-sm px-6">
        <Target size={36} className="mb-3 opacity-30" />
        <p className="font-medium mb-1">研究简报尚未生成</p>
        <p className="text-xs text-center">
          通过 Deep Research 的澄清阶段生成研究简报后，<br />
          结构化的研究规划将显示在此处。
        </p>
      </div>
    );
  }

  const cards = [
    {
      icon: <Target size={16} />,
      title: '研究目标 (Goal)',
      color: 'blue',
      content: (
        <div className="space-y-1.5">
          <p className="text-sm text-[var(--text-secondary)] leading-relaxed">
            {brief.scope || canvas.topic || '—'}
          </p>
          {canvas.keywords.length > 0 && (
            <div className="flex flex-wrap gap-1 mt-2">
              {canvas.keywords.map((kw, i) => (
                <span key={i} className="px-2 py-0.5 text-[10px] bg-blue-50 text-blue-700 rounded-full">
                  {kw}
                </span>
              ))}
            </div>
          )}
        </div>
      ),
    },
    {
      icon: <FlaskConical size={16} />,
      title: '假设与标准 (Hypothesis)',
      color: 'purple',
      content: (
        <ul className="space-y-1">
          {brief.success_criteria.length > 0 ? (
            brief.success_criteria.map((c, i) => (
              <li key={i} className="text-sm text-[var(--text-secondary)] flex items-start gap-1.5">
                <span className="text-purple-500 mt-0.5 shrink-0">•</span>
                <span>{c}</span>
              </li>
            ))
          ) : (
            <li className="text-xs text-[var(--text-tertiary)]">尚未定义</li>
          )}
        </ul>
      ),
    },
    {
      icon: <HelpCircle size={16} />,
      title: '核心问题 (Key Questions)',
      color: 'amber',
      content: (
        <ul className="space-y-1">
          {brief.key_questions.length > 0 ? (
            brief.key_questions.map((q, i) => (
              <li key={i} className="text-sm text-[var(--text-secondary)] flex items-start gap-1.5">
                <span className="text-amber-500 mt-0.5 shrink-0 font-mono text-xs">Q{i + 1}</span>
                <span>{q}</span>
              </li>
            ))
          ) : (
            <li className="text-xs text-[var(--text-tertiary)]">尚未定义</li>
          )}
        </ul>
      ),
    },
    {
      icon: <Ban size={16} />,
      title: '排除范围 (Exclusions)',
      color: 'red',
      content: (
        <ul className="space-y-1">
          {brief.exclusions.length > 0 ? (
            brief.exclusions.map((e, i) => (
              <li key={i} className="text-sm text-[var(--text-secondary)] flex items-start gap-1.5">
                <span className="text-red-400 mt-0.5 shrink-0">✗</span>
                <span>{e}</span>
              </li>
            ))
          ) : (
            <li className="text-xs text-[var(--text-tertiary)]">无排除项</li>
          )}
        </ul>
      ),
    },
    {
      icon: <BookOpen size={16} />,
      title: '文献来源 (Source Plan)',
      color: 'teal',
      content: (
        <div className="space-y-1.5">
          {brief.time_range && (
            <div className="flex items-center gap-1.5 text-sm text-[var(--text-secondary)]">
              <Clock size={12} className="text-teal-500" />
              <span>时间范围: {brief.time_range}</span>
            </div>
          )}
          {brief.source_priority.length > 0 ? (
            <div className="flex flex-wrap gap-1.5">
              {brief.source_priority.map((s, i) => (
                <span key={i} className="px-2 py-0.5 text-[10px] bg-teal-50 text-teal-700 rounded-full">
                  {s}
                </span>
              ))}
            </div>
          ) : (
            <p className="text-xs text-[var(--text-tertiary)]">使用默认检索策略</p>
          )}
        </div>
      ),
    },
    {
      icon: <Rocket size={16} />,
      title: '行动计划 (Action Plan)',
      color: 'indigo',
      content: (
        <p className="text-sm text-[var(--text-secondary)] leading-relaxed">
          {brief.action_plan || '根据研究结果生成综述文档，覆盖所有大纲章节。'}
        </p>
      ),
    },
  ];

  const colorMap: Record<string, string> = {
    blue: 'border-l-blue-400',
    purple: 'border-l-purple-400',
    amber: 'border-l-amber-400',
    red: 'border-l-red-400',
    teal: 'border-l-teal-400',
    indigo: 'border-l-indigo-400',
  };

  const iconColorMap: Record<string, string> = {
    blue: 'text-blue-500',
    purple: 'text-purple-500',
    amber: 'text-amber-500',
    red: 'text-red-500',
    teal: 'text-teal-500',
    indigo: 'text-indigo-500',
  };

  return (
    <div className="p-4 space-y-3 overflow-y-auto h-full">
      {/* Topic Header */}
      <div className="bg-[var(--bg-surface)] rounded-lg p-4 border border-[var(--border-subtle)]">
        <div className="flex items-center gap-2 mb-2">
          <Edit3 size={14} className="text-[var(--primary)]" />
          <span className="text-xs font-medium text-[var(--text-tertiary)] uppercase tracking-wider">Research Topic</span>
        </div>
        <h3 className="text-base font-semibold text-[var(--text-primary)] leading-snug">
          {canvas.working_title || canvas.topic || '未命名研究'}
        </h3>
      </div>

      {/* 6-Card Grid */}
      <div className="grid grid-cols-1 gap-3">
        {cards.map((card) => (
          <div
            key={card.title}
            className={`bg-[var(--bg-panel)] rounded-lg border border-[var(--border-subtle)] border-l-4 ${colorMap[card.color]} p-3.5 shadow-sm hover:shadow transition-shadow`}
          >
            <div className="flex items-center gap-2 mb-2">
              <span className={iconColorMap[card.color]}>{card.icon}</span>
              <h4 className="text-xs font-semibold text-[var(--text-primary)] uppercase tracking-wide">
                {card.title}
              </h4>
            </div>
            <div>{card.content}</div>
          </div>
        ))}
      </div>

      {/* Identified Gaps */}
      {canvas.identified_gaps.length > 0 && (
        <div className="bg-amber-900/20 border border-amber-500/30 rounded-lg p-3.5">
          <h4 className="text-xs font-semibold text-amber-200 mb-2 uppercase tracking-wide">
            Information Gaps
          </h4>
          <ul className="space-y-1">
            {canvas.identified_gaps.map((gap, i) => (
              <li key={i} className="text-sm text-amber-300 flex items-start gap-1.5">
                <span className="shrink-0">⚠️</span>
                <span>{gap}</span>
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/canvas/FloatingToolbar.tsx">
import { RefreshCw, Maximize2, Minimize2, BookOpen, Loader2 } from 'lucide-react';

export type AIAction = 'rewrite' | 'expand' | 'condense' | 'add_citations' | 'targeted_refine';

interface Props {
  position: { top: number; left: number };
  isLoading: boolean;
  onAction: (action: AIAction) => void;
}

const ACTIONS: { action: AIAction; label: string; icon: React.ReactNode }[] = [
  { action: 'rewrite', label: '重写', icon: <RefreshCw size={12} /> },
  { action: 'expand', label: '扩展', icon: <Maximize2 size={12} /> },
  { action: 'condense', label: '精简', icon: <Minimize2 size={12} /> },
  { action: 'add_citations', label: '添加引用', icon: <BookOpen size={12} /> },
  { action: 'targeted_refine', label: '定向精炼', icon: <RefreshCw size={12} /> },
];

export function FloatingToolbar({ position, isLoading, onAction }: Props) {
  return (
    <div
      className="fixed z-[100] flex items-center gap-1 bg-slate-900 border border-slate-700 rounded-lg shadow-lg px-1.5 py-1 animate-in fade-in slide-in-from-bottom-1 duration-150"
      style={{ top: position.top, left: position.left }}
    >
      {isLoading ? (
        <div className="flex items-center gap-1.5 px-2 py-1 text-xs text-slate-300">
          <Loader2 size={12} className="animate-spin" />
          AI 编辑中...
        </div>
      ) : (
        ACTIONS.map(({ action, label, icon }) => (
          <button
            key={action}
            onClick={() => onAction(action)}
            className="flex items-center gap-1 px-2 py-1 text-xs text-slate-300 hover:text-sky-200 hover:bg-sky-900/40 rounded transition-colors cursor-pointer whitespace-nowrap"
            title={label}
          >
            {icon}
            {label}
          </button>
        ))
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/canvas/OutlineStage.tsx">
import { GripVertical, ChevronRight, AlertTriangle, BookOpen } from 'lucide-react';
import type { Canvas } from '../../types';

interface OutlineStageProps {
  canvas: Canvas;
}

/**
 * Outline 阶段：可视化的章节树 + Gap 标注
 * 展示大纲结构、每个章节的状态和关联信息
 */
export function OutlineStage({ canvas }: OutlineStageProps) {
  const { outline, drafts, identified_gaps } = canvas;

  if (!outline || outline.length === 0) {
    return (
      <div className="flex flex-col items-center justify-center h-full text-[var(--text-tertiary)] text-sm px-6">
        <ChevronRight size={36} className="mb-3 opacity-30" />
        <p className="font-medium mb-1">大纲尚未生成</p>
        <p className="text-xs text-center">
          在 Explore 阶段确认研究计划后，<br />
          大纲结构将显示在此处。
        </p>
      </div>
    );
  }

  const statusConfig: Record<string, { icon: string; label: string; className: string }> = {
    todo: { icon: '⬜', label: '待开始', className: 'text-gray-400' },
    drafting: { icon: '✍️', label: '撰写中', className: 'text-orange-500' },
    done: { icon: '✅', label: '已完成', className: 'text-emerald-600' },
  };

  // 按 level 分组，构建层级视图
  const sortedOutline = [...outline].sort((a, b) => a.order - b.order);

  return (
    <div className="p-4 space-y-3 overflow-y-auto h-full">
      {/* Working Title */}
      {canvas.working_title && (
        <div className="bg-[var(--bg-surface)] rounded-lg p-3 border border-[var(--border-subtle)]">
          <span className="text-xs text-[var(--text-tertiary)]">Working Title</span>
          <h3 className="text-sm font-semibold text-[var(--text-primary)] mt-0.5">
            {canvas.working_title}
          </h3>
        </div>
      )}

      {/* Outline Tree */}
      <div className="space-y-1.5">
        {sortedOutline.map((section) => {
          const draft = drafts[section.id];
          const status = statusConfig[section.status] || statusConfig.todo;
          const hasGaps = section.guidance?.includes('gap') || section.guidance?.includes('Gap');
          const citationCount = draft?.used_citation_ids?.length || 0;
          const indent = (section.level - 1) * 16;

          return (
            <div
              key={section.id}
              className="group bg-[var(--bg-panel)] rounded-lg border border-[var(--border-subtle)] hover:border-[var(--border-highlight)] hover:shadow-sm transition-all"
              style={{ marginLeft: indent }}
            >
              <div className="flex items-start gap-2 p-3">
                {/* Drag Handle (视觉占位，未来可启用拖拽) */}
                <div className="pt-0.5 text-[var(--text-tertiary)] opacity-0 group-hover:opacity-50 transition-opacity cursor-grab">
                  <GripVertical size={14} />
                </div>

                {/* Status Icon */}
                <span className="pt-0.5 text-sm shrink-0" title={status.label}>
                  {status.icon}
                </span>

                {/* Content */}
                <div className="flex-1 min-w-0">
                  <div className="flex items-center gap-2">
                    <h4 className="text-sm font-medium text-[var(--text-primary)] truncate">
                      {section.title}
                    </h4>
                    {section.level > 1 && (
                      <span className="text-[10px] text-[var(--text-tertiary)] bg-[var(--bg-muted)] px-1.5 py-0.5 rounded">
                        L{section.level}
                      </span>
                    )}
                  </div>

                  {/* Guidance */}
                  {section.guidance && (
                    <p className="text-xs text-[var(--text-tertiary)] mt-0.5 line-clamp-2">
                      {section.guidance}
                    </p>
                  )}

                  {/* Meta */}
                  <div className="flex items-center gap-3 mt-1.5">
                    {citationCount > 0 && (
                      <span className="flex items-center gap-1 text-[10px] text-teal-600">
                        <BookOpen size={10} />
                        {citationCount} citations
                      </span>
                    )}
                    {hasGaps && (
                      <span className="flex items-center gap-1 text-[10px] text-amber-600">
                        <AlertTriangle size={10} />
                        有信息缺口
                      </span>
                    )}
                  </div>
                </div>
              </div>
            </div>
          );
        })}
      </div>

      {/* Global Gaps */}
      {identified_gaps.length > 0 && (
        <div className="bg-amber-900/20 border border-amber-500/30 rounded-lg p-3">
          <h4 className="text-xs font-semibold text-amber-200 mb-2 flex items-center gap-1.5">
            <AlertTriangle size={12} />
            全局信息缺口 ({identified_gaps.length})
          </h4>
          <ul className="space-y-1">
            {identified_gaps.map((gap, i) => (
              <li key={i} className="text-xs text-amber-300 flex items-start gap-1.5">
                <span className="shrink-0 mt-0.5">❗</span>
                <span>{gap}</span>
              </li>
            ))}
          </ul>
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/canvas/RefineStage.tsx">
import { useRef, useState, useCallback, useEffect } from 'react';
import {
  Eye,
  Pencil,
  Undo2,
  Redo2,
  MessageSquarePlus,
  Send,
  Trash2,
  Plus,
  X,
  CheckCircle2,
  XCircle,
  Lightbulb,
  ChevronDown,
  ChevronUp,
} from 'lucide-react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import { useCanvasStore, useToastStore } from '../../stores';
import {
  aiEditCanvas,
  createSnapshot,
  exportCanvas,
  getCanvas,
  listCanvasSnapshots,
  refineCanvasFull,
  restoreSnapshot,
  updateCanvas,
} from '../../api/canvas';
import { listInsights, updateInsightStatus } from '../../api/chat';
import type { Canvas, Annotation, ResearchInsight, InsightType, InsightStatus } from '../../types';

interface RefineStageProps {
  canvas: Canvas;
}

interface LockedRange {
  id: string;
  start: number;
  end: number;
  text: string;
}

/**
 * Refine 阶段：全文预览 + 行内批注 + 全局指令 + AI 编辑
 */
export function RefineStage({ canvas }: RefineStageProps) {
  const {
    setCanvas,
    canvasContent,
    editMode,
    isAIEditing,
    versionHistory,
    currentVersionIndex,
    pendingAnnotations,
    setCanvasContent,
    setEditMode,
    pushVersion,
    undo,
    redo,
    setIsAIEditing,
    addAnnotation,
    removeAnnotation,
    addDirective,
    removeDirective,
  } = useCanvasStore();
  const addToast = useToastStore((s) => s.addToast);

  const textareaRef = useRef<HTMLTextAreaElement>(null);
  const [selection, setSelection] = useState<{
    text: string;
    start: number;
    end: number;
  } | null>(null);

  // 批注输入
  const [showAnnotationInput, setShowAnnotationInput] = useState(false);
  const [annotationText, setAnnotationText] = useState('');
  const [annotationMode, setAnnotationMode] = useState<'annotation' | 'targeted_refine'>('annotation');

  // 全局指令输入
  const [newDirective, setNewDirective] = useState('');
  const [showDirectives, setShowDirectives] = useState(true);
  const [fullRefineDirective, setFullRefineDirective] = useState('');
  const [isFullRefining, setIsFullRefining] = useState(false);
  const [snapshotVersions, setSnapshotVersions] = useState<Array<{ version_number: number; created_at: string }>>([]);
  const [isLoadingSnapshots, setIsLoadingSnapshots] = useState(false);
  const [lockedRanges, setLockedRanges] = useState<LockedRange[]>([]);
  const [activeLockedId, setActiveLockedId] = useState<string | null>(null);

  // Research Insights
  const [showInsights, setShowInsights] = useState(false);
  const [insights, setInsights] = useState<ResearchInsight[]>([]);
  const activeJobId = typeof window !== 'undefined' ? localStorage.getItem('deep_research_active_job_id') || '' : '';

  useEffect(() => {
    if (!activeJobId) {
      setInsights([]);
      return;
    }
    let cancelled = false;
    const fetchInsights = async () => {
      try {
        const data = await listInsights(activeJobId);
        if (!cancelled) setInsights(data);
      } catch {
        // ignore
      }
    };
    fetchInsights();
    const timer = window.setInterval(fetchInsights, 15000);
    return () => { cancelled = true; window.clearInterval(timer); };
  }, [activeJobId]);

  const handleDeferInsight = async (insightId: number) => {
    if (!activeJobId) return;
    try {
      await updateInsightStatus(activeJobId, insightId, 'deferred');
      setInsights((prev) => prev.map((i) => i.id === insightId ? { ...i, status: 'deferred' as InsightStatus } : i));
      addToast('已标记为暂缓处理', 'success');
    } catch {
      addToast('操作失败', 'error');
    }
  };

  const insightTypeLabel = (t: InsightType): string => {
    switch (t) {
      case 'gap': return '信息缺口';
      case 'conflict': return '矛盾冲突';
      case 'limitation': return '不足/限制';
      case 'future_direction': return '未来方向';
      default: return t;
    }
  };

  const insightTypeColor = (t: InsightType): string => {
    switch (t) {
      case 'gap': return 'text-red-700 bg-red-50 border-red-200';
      case 'conflict': return 'text-amber-700 bg-amber-50 border-amber-200';
      case 'limitation': return 'text-purple-700 bg-purple-50 border-purple-200';
      case 'future_direction': return 'text-blue-700 bg-blue-50 border-blue-200';
      default: return 'text-slate-200 bg-slate-800/80 border-slate-600/60';
    }
  };

  const insightStatusLabel = (s: InsightStatus): string => {
    switch (s) {
      case 'open': return '待处理';
      case 'addressed': return '已处理';
      case 'deferred': return '暂缓';
      default: return s;
    }
  };

  const insightStatusColor = (s: InsightStatus): string => {
    switch (s) {
      case 'open': return 'text-amber-700 bg-amber-50';
      case 'addressed': return 'text-emerald-700 bg-emerald-50';
      case 'deferred': return 'text-slate-300 bg-slate-700/70';
      default: return '';
    }
  };

  // Group insights by type
  const groupedInsights = insights.reduce<Record<InsightType, ResearchInsight[]>>((acc, ins) => {
    const key = ins.insight_type as InsightType;
    if (!acc[key]) acc[key] = [];
    acc[key].push(ins);
    return acc;
  }, {} as Record<InsightType, ResearchInsight[]>);

  const openInsightsCount = insights.filter((i) => i.status === 'open').length;

  const loadSnapshots = useCallback(async () => {
    if (!canvas?.id) return;
    setIsLoadingSnapshots(true);
    try {
      const rows = await listCanvasSnapshots(canvas.id, 30);
      setSnapshotVersions(rows);
    } catch {
      // ignore
    } finally {
      setIsLoadingSnapshots(false);
    }
  }, [canvas?.id]);

  useEffect(() => {
    loadSnapshots();
  }, [loadSnapshots]);

  // 检测选中文本
  const handleSelect = useCallback(() => {
    if (!editMode) return;
    const ta = textareaRef.current;
    if (!ta) return;
    const start = ta.selectionStart;
    const end = ta.selectionEnd;
    if (start === end) {
      setSelection(null);
      setShowAnnotationInput(false);
      return;
    }
    const selectedText = canvasContent.slice(start, end);
    setSelection({ text: selectedText, start, end });
    if (showAnnotationInput) setShowAnnotationInput(false);
  }, [editMode, canvasContent, showAnnotationInput]);

  // 点击非选区隐藏工具栏
  useEffect(() => {
    const handleClickOutside = () => {
      setTimeout(() => {
        const ta = textareaRef.current;
        if (!ta || ta.selectionStart === ta.selectionEnd) {
          setSelection(null);
        }
      }, 200);
    };
    document.addEventListener('mousedown', handleClickOutside);
    return () => document.removeEventListener('mousedown', handleClickOutside);
  }, []);

  const handleToggleEdit = () => {
    if (!editMode) pushVersion();
    setEditMode(!editMode);
  };

  const handleContentChange = (e: React.ChangeEvent<HTMLTextAreaElement>) => {
    setCanvasContent(e.target.value);
  };

  // 提交行内批注
  const handleSubmitAnnotation = () => {
    if (!annotationText.trim() || !selection) return;
    const annotation: Annotation = {
      id: Date.now().toString(36),
      section_id: '',
      target_text: selection.text.slice(0, 200),
      directive: annotationText.trim(),
      status: 'pending',
      created_at: new Date().toISOString(),
    };
    addAnnotation(annotation);
    setAnnotationText('');
    setShowAnnotationInput(false);
    setSelection(null);
    addToast('批注已添加', 'success');
  };

  const handleTargetedRefine = async () => {
    if (!annotationText.trim() || !selection || !canvas) return;
    setIsAIEditing(true);
    pushVersion();
    try {
      const result = await aiEditCanvas(canvas.id, {
        section_text: selection.text,
        action: 'targeted_refine',
        directive: annotationText.trim(),
        context: canvasContent.slice(
          Math.max(0, selection.start - 200),
          Math.min(canvasContent.length, selection.end + 200)
        ),
        search_mode: 'none',
        preserve_citations: true,
      });
      const newContent =
        canvasContent.slice(0, selection.start) +
        result.edited_text +
        canvasContent.slice(selection.end);
      const updatedStart = selection.start;
      const updatedEnd = selection.start + result.edited_text.length;
      setCanvasContent(newContent);
      if (result.citation_guard_triggered) {
        addToast(result.citation_guard_message || '检测到引用丢失，已自动回退该次局部精炼', 'warning');
      } else {
        addToast('已完成局部定向精炼（仅修改选区）', 'success');
      }
      setAnnotationText('');
      setShowAnnotationInput(false);
      setSelection({
        text: result.edited_text,
        start: updatedStart,
        end: updatedEnd,
      });
      setAnnotationMode('annotation');
      // Keep the modified region selected so users can see the changed scope.
      setTimeout(() => {
        const ta = textareaRef.current;
        if (!ta) return;
        ta.focus();
        ta.setSelectionRange(updatedStart, updatedEnd);
      }, 0);
    } catch (err: unknown) {
      const msg = (err as any)?.response?.data?.detail
        || (err as any)?.message
        || (err instanceof Error ? err.message : String(err));
      addToast(`定向精炼失败: ${msg}`, 'error');
    } finally {
      setIsAIEditing(false);
    }
  };

  // 提交全局指令
  const handleAddDirective = () => {
    if (!newDirective.trim()) return;
    addDirective(newDirective.trim());
    setNewDirective('');
    addToast('修改指令已添加', 'success');
  };

  const canUndo = versionHistory.length > 0 && currentVersionIndex > 0;
  const canRedo = currentVersionIndex >= 0 && currentVersionIndex < versionHistory.length - 1;

  const handleLockSelection = () => {
    if (!selection) return;
    if (!selection.text.trim()) {
      addToast('空白选区无法锁定', 'warning');
      return;
    }
    const duplicate = lockedRanges.some((r) => r.start === selection.start && r.end === selection.end);
    if (duplicate) {
      addToast('该选区已锁定', 'info');
      return;
    }
    const lockId = `${selection.start}-${selection.end}-${Date.now()}`;
    setLockedRanges((prev) => [
      ...prev,
      {
        id: lockId,
        start: selection.start,
        end: selection.end,
        text: selection.text,
      },
    ]);
    setActiveLockedId(lockId);
    addToast('已锁定当前选区（全文重整时不会改动）', 'success');
    setSelection(null);
    setShowAnnotationInput(false);
  };

  const handleUnlockRange = (id: string) => {
    setLockedRanges((prev) => prev.filter((r) => r.id !== id));
    if (activeLockedId === id) {
      setActiveLockedId(null);
    }
  };

  const handleClearLocks = () => {
    setLockedRanges([]);
    setActiveLockedId(null);
  };

  const resolveLockedRange = useCallback((lock: LockedRange) => {
    if (
      lock.start >= 0 &&
      lock.end > lock.start &&
      lock.end <= canvasContent.length &&
      canvasContent.slice(lock.start, lock.end) === lock.text
    ) {
      return { start: lock.start, end: lock.end, text: lock.text };
    }
    const idx = canvasContent.indexOf(lock.text);
    if (idx >= 0) {
      return { start: idx, end: idx + lock.text.length, text: lock.text };
    }
    return null;
  }, [canvasContent]);

  const handleFocusLockedRange = (lock: LockedRange) => {
    const located = resolveLockedRange(lock);
    if (!located) {
      addToast('未能在当前文本中定位该锁定片段（可能已被改动）', 'warning');
      return;
    }
    // If the content changed, refresh stored range so next full-refine can still apply lock.
    if (located.start !== lock.start || located.end !== lock.end) {
      setLockedRanges((prev) => prev.map((r) => (
        r.id === lock.id ? { ...r, start: located.start, end: located.end } : r
      )));
    }
    setActiveLockedId(lock.id);
    if (!editMode) setEditMode(true);
    setShowAnnotationInput(false);
    setTimeout(() => {
      const ta = textareaRef.current;
      if (!ta) return;
      ta.focus();
      ta.setSelectionRange(located.start, located.end);
      const line = canvasContent.slice(0, located.start).split('\n').length - 1;
      const lineHeight = 20;
      ta.scrollTop = Math.max(0, line * lineHeight - ta.clientHeight * 0.3);
      setSelection({
        text: located.text,
        start: located.start,
        end: located.end,
      });
    }, 0);
  };

  const handleRefineFull = async () => {
    if (!canvas?.id || !canvasContent.trim()) return;
    setIsFullRefining(true);
    setIsAIEditing(true);
    pushVersion();
    try {
      const extraDirectives = [
        ...(canvas.user_directives || []),
        ...pendingAnnotations
          .filter((a) => a.status === 'pending')
          .map((a) => a.directive.trim())
          .filter(Boolean),
      ];
      if (fullRefineDirective.trim()) extraDirectives.push(fullRefineDirective.trim());

      const validLocks: Array<{ start: number; end: number; text: string }> = [];
      let skippedLocks = 0;
      for (const lock of lockedRanges) {
        if (lock.start < 0 || lock.end <= lock.start || lock.end > canvasContent.length) {
          skippedLocks += 1;
          continue;
        }
        if (canvasContent.slice(lock.start, lock.end) !== lock.text) {
          skippedLocks += 1;
          continue;
        }
        validLocks.push({ start: lock.start, end: lock.end, text: lock.text });
      }

      const result = await refineCanvasFull(canvas.id, {
        content_md: canvasContent,
        directives: extraDirectives,
        save_snapshot_before: true,
        locked_ranges: validLocks,
      });
      setCanvasContent(result.edited_markdown || canvasContent);
      if (result.lock_guard_triggered) {
        addToast(result.lock_guard_message || '锁定保护触发，已回退到重整前文本', 'warning');
      } else if (result.snapshot_version) {
        addToast(`已完成重新精炼（可回退到快照 v${result.snapshot_version}）`, 'success');
      } else {
        addToast('已完成重新精炼', 'success');
      }
      const skippedTotal = skippedLocks + (result.locked_skipped || 0);
      if ((result.locked_applied || 0) > 0) {
        addToast(`锁定保护生效：${result.locked_applied} 个片段`, 'info');
      }
      if (skippedTotal > 0) {
        addToast(`有 ${skippedTotal} 个锁定片段失效并被跳过（通常因文本位置变化）`, 'warning');
      }
      setFullRefineDirective('');
      try {
        const fresh = await getCanvas(canvas.id);
        setCanvas(fresh);
      } catch {
        // ignore
      }
      await loadSnapshots();
    } catch (err: unknown) {
      const msg = (err as any)?.response?.data?.detail
        || (err as any)?.message
        || (err instanceof Error ? err.message : String(err));
      addToast(`重新精炼失败: ${msg}`, 'error');
    } finally {
      setIsFullRefining(false);
      setIsAIEditing(false);
    }
  };

  const handleSaveSnapshot = async () => {
    if (!canvas?.id) return;
    try {
      await updateCanvas(canvas.id, {
        refined_markdown: canvasContent,
        stage: 'refine',
      });
      const ret = await createSnapshot(canvas.id);
      addToast(`已保存快照 v${ret.version_number}`, 'success');
      await loadSnapshots();
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : String(err);
      addToast(`保存快照失败: ${msg}`, 'error');
    }
  };

  const handleRestoreFromSnapshot = async (versionNumber: number) => {
    if (!canvas?.id) return;
    setIsAIEditing(true);
    try {
      await restoreSnapshot(canvas.id, versionNumber);
      const [freshCanvas, md] = await Promise.all([
        getCanvas(canvas.id),
        exportCanvas(canvas.id, 'markdown'),
      ]);
      setCanvas(freshCanvas);
      setCanvasContent(md.content || freshCanvas.refined_markdown || '');
      setLockedRanges([]);
      pushVersion();
      addToast(`已回退到快照 v${versionNumber}`, 'success');
      await loadSnapshots();
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : String(err);
      addToast(`回退失败: ${msg}`, 'error');
    } finally {
      setIsAIEditing(false);
    }
  };

  return (
    <div className="flex flex-col h-full">
      {/* Toolbar */}
      <div className="flex items-center justify-between px-3 py-2 border-b border-[var(--border-subtle)] bg-[var(--bg-surface)]">
        <div className="flex items-center gap-1">
          <button
            onClick={handleToggleEdit}
            className={`p-1.5 rounded text-xs flex items-center gap-1 transition-colors cursor-pointer ${
              editMode
                ? 'bg-blue-100 text-blue-600'
                : 'hover:bg-[var(--bg-muted)] text-[var(--text-tertiary)]'
            }`}
            title={editMode ? '切换预览' : '切换编辑'}
          >
            {editMode ? <Eye size={14} /> : <Pencil size={14} />}
            <span>{editMode ? '预览' : '编辑'}</span>
          </button>

          {editMode && (
            <>
              <button
                onClick={undo}
                disabled={!canUndo}
                className="p-1.5 hover:bg-[var(--bg-muted)] rounded text-[var(--text-tertiary)] disabled:opacity-30 cursor-pointer"
                title="撤销"
              >
                <Undo2 size={14} />
              </button>
              <button
                onClick={redo}
                disabled={!canRedo}
                className="p-1.5 hover:bg-[var(--bg-muted)] rounded text-[var(--text-tertiary)] disabled:opacity-30 cursor-pointer"
                title="重做"
              >
                <Redo2 size={14} />
              </button>
            </>
          )}
        </div>

        <div className="flex items-center gap-1">
          <button
            onClick={handleRefineFull}
            disabled={isFullRefining || isAIEditing || !canvasContent.trim()}
            className="px-2 py-1.5 rounded text-xs bg-violet-100 text-violet-700 hover:bg-violet-200 disabled:opacity-50 cursor-pointer"
            title="基于当前全文进行全局重整（慎用，可能改动范围较大）"
          >
            {isFullRefining ? '重整中...' : '全文重整(慎用)'}
          </button>
          <button
            onClick={handleLockSelection}
            disabled={!editMode || !selection}
            className="px-2 py-1.5 rounded text-xs bg-emerald-100 text-emerald-700 hover:bg-emerald-200 disabled:opacity-50 cursor-pointer"
            title="锁定当前选区，全文重整时保持不变"
          >
            锁定选区
          </button>
          <button
            onClick={() => {
              if (!editMode || !selection) return;
              setAnnotationMode('targeted_refine');
              setShowAnnotationInput(true);
            }}
            disabled={!editMode || !selection}
            className="px-2 py-1.5 rounded text-xs bg-sky-100 text-sky-700 hover:bg-sky-200 disabled:opacity-50 cursor-pointer"
            title="对当前选区执行定向精炼"
          >
            定向精炼选区
          </button>
          <button
            onClick={handleSaveSnapshot}
            disabled={!canvasContent.trim()}
            className="px-2 py-1.5 rounded text-xs bg-emerald-100 text-emerald-700 hover:bg-emerald-200 disabled:opacity-50 cursor-pointer"
            title="保存当前全文快照，便于多次回退"
          >
            保存快照
          </button>
          <button
            onClick={() => setShowDirectives((v) => !v)}
            className={`p-1.5 rounded text-xs flex items-center gap-1 cursor-pointer transition-colors ${
              showDirectives
                ? 'bg-indigo-100 text-indigo-600'
                : 'hover:bg-[var(--bg-muted)] text-[var(--text-tertiary)]'
            }`}
            title="修改指令面板"
          >
            <MessageSquarePlus size={14} />
            <span>指令</span>
            {(canvas.user_directives.length + pendingAnnotations.length) > 0 && (
              <span className="bg-indigo-500 text-white text-[9px] px-1 rounded-full">
                {canvas.user_directives.length + pendingAnnotations.length}
              </span>
            )}
          </button>
        </div>
      </div>

      {editMode && activeLockedId && (
        <div className="px-3 py-1.5 text-[10px] text-emerald-300 bg-emerald-900/20 border-b border-emerald-700/30">
          已高亮一个锁定片段。你可以直接执行“定向精炼”，或先解锁后再改写。
        </div>
      )}
      {editMode && !selection && (
        <div className="px-3 py-1 text-[10px] text-[var(--text-tertiary)] bg-[var(--bg-muted)] border-b border-[var(--border-subtle)]">
          提示：先在正文里选中文字，再点击上方“锁定选区”或“定向精炼选区”。
        </div>
      )}

      {/* Main Content */}
      <div className="flex-1 overflow-y-auto bg-[var(--bg-panel)]">
        {canvasContent ? (
          editMode ? (
            <textarea
              ref={textareaRef}
              value={canvasContent}
              onChange={handleContentChange}
              onSelect={handleSelect}
              onMouseUp={handleSelect}
              onKeyUp={handleSelect}
                className="refine-editor w-full h-full p-6 resize-none font-mono text-sm text-[var(--text-primary)] leading-relaxed focus:outline-none bg-[var(--bg-panel)]"
              placeholder="在此编辑 Markdown..."
              spellCheck={false}
            />
          ) : (
            <div className="p-6">
              <div className="prose prose-sm prose-invert max-w-none prose-headings:font-bold prose-h1:text-xl prose-h2:text-lg prose-p:text-[var(--text-secondary)]">
                <ReactMarkdown remarkPlugins={[remarkGfm]}>
                  {canvasContent}
                </ReactMarkdown>
              </div>
            </div>
          )
        ) : (
          <div className="flex flex-col items-center justify-center h-full text-[var(--text-tertiary)] text-xs">
            <Pencil size={32} className="mb-2 opacity-20" />
            <p>撰写完成后，全文将显示在此处供精炼</p>
          </div>
        )}
      </div>

      {/* Annotation Input (弹出在选中文本时) */}
      {editMode && showAnnotationInput && selection && (
        <div className="border-t border-[var(--border-subtle)] bg-amber-50 p-3">
          <div className="text-xs text-amber-700 mb-1.5 truncate">
            选中: "{selection.text.slice(0, 80)}{selection.text.length > 80 ? '...' : ''}"
          </div>
          <div className="flex gap-2">
            <input
              type="text"
              value={annotationText}
              onChange={(e) => setAnnotationText(e.target.value)}
              placeholder={annotationMode === 'targeted_refine' ? '输入选区定向精炼指令...' : '输入批注意见...'}
              className="flex-1 border border-amber-500/30 rounded-md px-2.5 py-1.5 text-sm focus:ring-2 focus:ring-amber-400 outline-none bg-slate-900/80 text-slate-100"
              onKeyDown={(e) => {
                if (e.key !== 'Enter') return;
                if (annotationMode === 'targeted_refine') {
                  handleTargetedRefine();
                } else {
                  handleSubmitAnnotation();
                }
              }}
              autoFocus
            />
            <button
              onClick={annotationMode === 'targeted_refine' ? handleTargetedRefine : handleSubmitAnnotation}
              disabled={!annotationText.trim()}
              className="px-3 py-1.5 bg-amber-500 text-white rounded-md text-xs font-medium hover:bg-amber-600 disabled:opacity-50 flex items-center gap-1"
            >
              <span>{annotationMode === 'targeted_refine' ? '执行定向精炼' : '添加批注'}</span>
              <Send size={12} />
            </button>
            <button
              onClick={() => { setShowAnnotationInput(false); setAnnotationText(''); setAnnotationMode('annotation'); }}
              className="p-1.5 text-amber-500 hover:bg-amber-100 rounded"
            >
              <X size={14} />
            </button>
          </div>
        </div>
      )}

      {/* Directives & Annotations Panel */}
      {showDirectives && (
        <div className="border-t border-[var(--border-subtle)] bg-[var(--bg-surface)] max-h-64 overflow-y-auto">
          {/* Pending Annotations */}
          {pendingAnnotations.length > 0 && (
            <div className="p-3 border-b border-[var(--border-subtle)]">
              <h5 className="text-xs font-semibold text-[var(--text-primary)] mb-2 flex items-center gap-1.5">
                <MessageSquarePlus size={12} />
                行内批注 ({pendingAnnotations.length})
              </h5>
              <div className="space-y-1.5">
                {pendingAnnotations.map((ann) => (
                  <div
                    key={ann.id}
                    className={`flex items-start gap-2 text-xs p-2 rounded-md border ${
                      ann.status === 'applied'
                        ? 'bg-emerald-50 border-emerald-200'
                        : ann.status === 'rejected'
                          ? 'bg-red-50 border-red-200 opacity-60'
                          : 'bg-[var(--bg-muted)] border-[var(--border-subtle)]'
                    }`}
                  >
                    <div className="flex-1 min-w-0">
                      <div className="text-[var(--text-tertiary)] truncate mb-0.5">
                        "{ann.target_text.slice(0, 60)}{ann.target_text.length > 60 ? '...' : ''}"
                      </div>
                      <div className="text-[var(--text-secondary)]">{ann.directive}</div>
                    </div>
                    <div className="flex items-center gap-0.5 shrink-0">
                      {ann.status === 'pending' && (
                        <>
                          <span className="w-1.5 h-1.5 rounded-full bg-amber-400" title="待处理" />
                          <button
                            onClick={() => removeAnnotation(ann.id)}
                            className="p-0.5 text-[var(--text-tertiary)] hover:text-red-500"
                          >
                            <Trash2 size={11} />
                          </button>
                        </>
                      )}
                      {ann.status === 'applied' && <CheckCircle2 size={12} className="text-emerald-500" />}
                      {ann.status === 'rejected' && <XCircle size={12} className="text-red-400" />}
                    </div>
                  </div>
                ))}
              </div>
            </div>
          )}

          {/* Global Directives */}
          <div className="p-3">
            <h5 className="text-xs font-semibold text-[var(--text-primary)] mb-2">
              全局修改指令
            </h5>
            <div className="text-[10px] text-[var(--text-tertiary)] mb-2">
              建议优先使用“选区定向精炼”；全文重整可能影响未选中内容。
            </div>
            <div className="mb-2 p-2 rounded-md border border-[var(--border-subtle)] bg-[var(--bg-muted)]">
              <div className="flex items-center justify-between mb-1">
                <span className="text-[11px] font-semibold text-[var(--text-primary)]">锁定片段</span>
                <div className="flex items-center gap-2">
                  <span className="text-[10px] text-[var(--text-tertiary)]">{lockedRanges.length} 个</span>
                  <button
                    onClick={handleClearLocks}
                    disabled={lockedRanges.length === 0}
                    className="px-1.5 py-0.5 text-[10px] text-slate-300 bg-slate-700/70 rounded disabled:opacity-50 cursor-pointer"
                  >
                    清空
                  </button>
                </div>
              </div>
              {lockedRanges.length === 0 ? (
                <div className="text-[10px] text-[var(--text-tertiary)]">在编辑区选中文字后，点击顶部“锁定选区”。</div>
              ) : (
                <div className="space-y-1 max-h-20 overflow-y-auto">
                  {lockedRanges.map((r) => (
                    <div
                      key={r.id}
                      className={`flex items-center gap-2 text-[10px] rounded px-1 py-0.5 ${
                        activeLockedId === r.id ? 'bg-emerald-900/25 ring-1 ring-emerald-700/40' : ''
                      }`}
                    >
                      <span className="flex-1 truncate text-[var(--text-secondary)]">
                        {r.text.replace(/\s+/g, ' ').slice(0, 60)}{r.text.length > 60 ? '...' : ''}
                      </span>
                      {activeLockedId === r.id && (
                        <span className="text-[9px] px-1 py-0.5 rounded bg-emerald-800/60 text-emerald-200">高亮中</span>
                      )}
                      <button
                        onClick={() => handleFocusLockedRange(r)}
                        className="px-1.5 py-0.5 text-[10px] text-emerald-200 bg-emerald-900/40 rounded cursor-pointer"
                      >
                        定位
                      </button>
                      <button
                        onClick={() => handleUnlockRange(r.id)}
                        className="px-1.5 py-0.5 text-[10px] text-red-300 bg-red-900/30 rounded cursor-pointer"
                      >
                        解锁
                      </button>
                    </div>
                  ))}
                </div>
              )}
            </div>
            <div className="flex gap-2 mb-2">
              <input
                type="text"
                value={fullRefineDirective}
                onChange={(e) => setFullRefineDirective(e.target.value)}
                placeholder="本轮重新精炼附加指令（可选）..."
                className="flex-1 border border-[var(--border-subtle)] rounded-md px-2.5 py-1.5 text-sm focus:ring-2 focus:ring-[var(--primary)] outline-none"
              />
              <button
                onClick={handleRefineFull}
                disabled={isFullRefining || isAIEditing || !canvasContent.trim()}
                className="px-2.5 py-1.5 bg-violet-500 text-white rounded-md text-xs hover:bg-violet-600 disabled:opacity-50"
              >
                全文重整
              </button>
            </div>
            {canvas.user_directives.length > 0 && (
              <div className="space-y-1 mb-2">
                {canvas.user_directives.map((d, i) => (
                  <div key={i} className="flex items-center gap-2 text-xs bg-[var(--bg-muted)] border border-[var(--border-subtle)] rounded-md px-2.5 py-1.5">
                    <span className="text-indigo-500 shrink-0">{i + 1}.</span>
                    <span className="flex-1 text-[var(--text-secondary)]">{d}</span>
                    <button
                      onClick={() => removeDirective(i)}
                      className="p-0.5 text-[var(--text-tertiary)] hover:text-red-500"
                    >
                      <Trash2 size={11} />
                    </button>
                  </div>
                ))}
              </div>
            )}
            <div className="flex gap-2">
              <input
                type="text"
                value={newDirective}
                onChange={(e) => setNewDirective(e.target.value)}
                placeholder="添加全局修改指令..."
                className="flex-1 border border-[var(--border-subtle)] rounded-md px-2.5 py-1.5 text-sm focus:ring-2 focus:ring-[var(--primary)] outline-none"
                onKeyDown={(e) => e.key === 'Enter' && handleAddDirective()}
              />
              <button
                onClick={handleAddDirective}
                disabled={!newDirective.trim()}
                className="px-2.5 py-1.5 bg-indigo-500 text-white rounded-md text-xs hover:bg-indigo-600 disabled:opacity-50 flex items-center gap-1"
              >
                <Plus size={12} />
                添加
              </button>
            </div>
            <div className="mt-3 border-t border-[var(--border-subtle)] pt-2">
              <h6 className="text-[11px] font-semibold text-[var(--text-primary)] mb-1.5">回退快照</h6>
              {isLoadingSnapshots ? (
                <div className="text-xs text-[var(--text-tertiary)]">加载中...</div>
              ) : snapshotVersions.length === 0 ? (
                <div className="text-xs text-[var(--text-tertiary)]">暂无快照，先点击“保存快照”或执行“重新精炼”。</div>
              ) : (
                <div className="space-y-1.5 max-h-28 overflow-y-auto">
                  {snapshotVersions.map((v) => (
                    <div key={`snap-${v.version_number}`} className="flex items-center justify-between text-xs bg-[var(--bg-muted)] border border-[var(--border-subtle)] rounded-md px-2 py-1.5">
                      <span className="text-[var(--text-secondary)]">
                        v{v.version_number} · {v.created_at.replace('T', ' ').slice(0, 19)}
                      </span>
                      <button
                        onClick={() => handleRestoreFromSnapshot(v.version_number)}
                        className="px-1.5 py-0.5 text-[10px] text-emerald-700 bg-emerald-100 rounded hover:bg-emerald-200 cursor-pointer"
                      >
                        回退
                      </button>
                    </div>
                  ))}
                </div>
              )}
            </div>
          </div>
        </div>
      )}

      {/* Version Indicator */}
      {editMode && versionHistory.length > 0 && (
        <div className="h-6 border-t border-[var(--border-subtle)] bg-[var(--bg-muted)] flex items-center justify-center text-[10px] text-[var(--text-tertiary)]">
          版本 {currentVersionIndex >= 0 ? currentVersionIndex + 1 : versionHistory.length}/{versionHistory.length}
        </div>
      )}

      {/* Research Insights Ledger */}
      {(insights.length > 0 || (canvas.research_insights?.length ?? 0) > 0) && (
        <div className="border-t border-[var(--border-subtle)]">
          <button
            onClick={() => setShowInsights((v) => !v)}
            className="w-full flex items-center justify-between px-3 py-2 bg-[var(--bg-surface)] hover:bg-[var(--bg-muted)] transition-colors cursor-pointer"
          >
            <div className="flex items-center gap-1.5 text-xs font-semibold text-[var(--text-primary)]">
              <Lightbulb size={13} className="text-amber-500" />
              研究洞察
              {openInsightsCount > 0 && (
                <span className="bg-amber-500 text-white text-[9px] px-1.5 rounded-full">
                  {openInsightsCount}
                </span>
              )}
            </div>
            {showInsights ? <ChevronUp size={14} className="text-[var(--text-tertiary)]" /> : <ChevronDown size={14} className="text-[var(--text-tertiary)]" />}
          </button>
          {showInsights && (
            <div className="max-h-64 overflow-y-auto px-3 py-2 space-y-3 bg-[var(--bg-surface)]">
              {(Object.entries(groupedInsights) as [InsightType, ResearchInsight[]][]).map(([type, items]) => (
                <div key={type}>
                  <div className="text-[10px] font-semibold text-[var(--text-tertiary)] uppercase tracking-wider mb-1">
                    {insightTypeLabel(type)} ({items.length})
                  </div>
                  <div className="space-y-1">
                    {items.map((ins) => (
                      <div
                        key={ins.id}
                        className={`flex items-start gap-2 text-xs p-2 rounded-md border ${insightTypeColor(type)} ${ins.status === 'deferred' ? 'opacity-50' : ''}`}
                      >
                        <div className="flex-1 min-w-0">
                          <div className="text-[var(--text-secondary)] leading-relaxed">{ins.text}</div>
                          {ins.section_id && (
                            <div className="text-[10px] text-[var(--text-tertiary)] mt-0.5">
                              章节: {ins.section_id}
                            </div>
                          )}
                        </div>
                        <div className="flex items-center gap-1 shrink-0">
                          <span className={`text-[9px] px-1.5 py-0.5 rounded ${insightStatusColor(ins.status)}`}>
                            {insightStatusLabel(ins.status)}
                          </span>
                          {ins.status === 'open' && (
                            <button
                              onClick={() => handleDeferInsight(ins.id)}
                              className="text-[10px] text-slate-300 hover:text-slate-100 px-1 py-0.5 rounded hover:bg-slate-700/60 cursor-pointer"
                              title="标记为暂缓处理"
                            >
                              暂缓
                            </button>
                          )}
                        </div>
                      </div>
                    ))}
                  </div>
                </div>
              ))}

              {/* Also show canvas-level research_insights if any */}
              {canvas.research_insights && canvas.research_insights.length > 0 && !insights.length && (
                <div>
                  <div className="text-[10px] font-semibold text-[var(--text-tertiary)] mb-1">来自画布</div>
                  {canvas.research_insights.map((text, i) => (
                    <div key={i} className="text-xs text-[var(--text-secondary)] p-1.5 border border-[var(--border-subtle)] rounded-md mb-1">
                      {text}
                    </div>
                  ))}
                </div>
              )}
            </div>
          )}
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/canvas/StageStepper.tsx">
import { Search, ListTree, PenTool, Sparkles, Check } from 'lucide-react';
import { useEffect, useRef, useState } from 'react';
import { useTranslation } from 'react-i18next';
import type { CanvasStage } from '../../types';

interface StageStepperProps {
  currentStage: CanvasStage;
  activeStage: CanvasStage;
  onStageClick: (stage: CanvasStage) => void;
}

const STAGES: {
  id: CanvasStage;
  labelKey: string;
  icon: React.ReactNode;
}[] = [
  { id: 'explore', labelKey: 'workflow.explore', icon: <Search size={14} /> },
  { id: 'outline', labelKey: 'workflow.outline', icon: <ListTree size={14} /> },
  { id: 'drafting', labelKey: 'workflow.drafting', icon: <PenTool size={14} /> },
  { id: 'refine', labelKey: 'workflow.refine', icon: <Sparkles size={14} /> },
];

const STAGE_ORDER: CanvasStage[] = ['explore', 'outline', 'drafting', 'refine'];

function getStageIndex(stage: CanvasStage): number {
  return STAGE_ORDER.indexOf(stage);
}

export function StageStepper({ currentStage, activeStage, onStageClick }: StageStepperProps) {
  const { t } = useTranslation();
  const currentIdx = getStageIndex(currentStage);
  const containerRef = useRef<HTMLDivElement>(null);
  const contentRef = useRef<HTMLDivElement>(null);
  const [compact, setCompact] = useState(false);

  useEffect(() => {
    const measure = () => {
      const container = containerRef.current;
      const content = contentRef.current;
      if (!container || !content) return;

      const shouldCompact = content.scrollWidth > container.clientWidth + 4;
      setCompact((prev) => (prev === shouldCompact ? prev : shouldCompact));
    };

    measure();
    const ro = new ResizeObserver(measure);
    if (containerRef.current) ro.observe(containerRef.current);
    window.addEventListener('resize', measure);

    return () => {
      ro.disconnect();
      window.removeEventListener('resize', measure);
    };
  }, []);

  return (
    <div ref={containerRef} className="px-3 py-2 bg-slate-800/50 border-b border-slate-700/50">
      <div className="overflow-x-auto scrollbar-hide">
        <div ref={contentRef} className="flex items-center gap-1 min-w-max">
          {STAGES.map((stage, idx) => {
            const stageIdx = getStageIndex(stage.id);
            const isCompleted = stageIdx < currentIdx;
            const isCurrent = stage.id === currentStage;
            const isActive = stage.id === activeStage;
            const isClickable = stageIdx <= currentIdx;

            return (
              <div key={stage.id} className="flex items-center">
                {idx > 0 && (
                  <div
                    className={`${compact ? 'w-3' : 'w-6'} h-px mx-0.5 ${
                      stageIdx <= currentIdx ? 'bg-sky-500' : 'bg-slate-700'
                    }`}
                  />
                )}
                <button
                  onClick={() => isClickable && onStageClick(stage.id)}
                  disabled={!isClickable}
                  className={`
                    flex items-center ${compact ? 'justify-center gap-0 px-2 py-1.5' : 'gap-1.5 px-2.5 py-1.5'} rounded-md text-xs font-medium
                    transition-all duration-150 cursor-pointer
                    ${isActive
                      ? 'bg-sky-600 text-white shadow-sm shadow-sky-500/30'
                      : isCompleted
                        ? 'bg-emerald-900/20 text-emerald-400 hover:bg-emerald-900/30'
                        : isCurrent
                          ? 'bg-sky-900/30 text-sky-400 border border-sky-500/40'
                          : 'bg-slate-800/50 text-slate-500'
                    }
                    ${!isClickable ? 'opacity-50 cursor-not-allowed' : ''}
                  `}
                  title={t(stage.labelKey)}
                >
                  {isCompleted && !isActive ? (
                    <Check size={12} className="text-emerald-400" />
                  ) : (
                    stage.icon
                  )}
                  {!compact && <span>{t(stage.labelKey)}</span>}
                </button>
              </div>
            );
          })}
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/chat/ToolTracePanel.tsx">
/**
 * Agent 工具调用轨迹面板。
 * 展示 ReAct 循环中 LLM 调用的每个工具、参数和结果。
 */

import { useState } from 'react';
import type { ToolTraceItem } from '../../types';

interface Props {
  trace: ToolTraceItem[];
}

const toolIcons: Record<string, string> = {
  search_local: '📚',
  search_web: '🌐',
  search_scholar: '🎓',
  explore_graph: '🕸️',
  canvas: '📝',
  get_citations: '📖',
  compare_papers: '⚖️',
  run_code: '🖥️',
};

export function ToolTracePanel({ trace }: Props) {
  const [expanded, setExpanded] = useState(false);

  if (!trace || trace.length === 0) return null;

  return (
    <div className="border rounded-lg bg-white shadow-sm overflow-hidden mb-3">
      <button
        onClick={() => setExpanded(!expanded)}
        className="w-full px-4 py-2.5 flex items-center justify-between bg-gradient-to-r from-purple-50 to-indigo-50 hover:from-purple-100 hover:to-indigo-100 transition-colors"
      >
        <div className="flex items-center gap-2 text-sm font-medium text-gray-700">
          <span>🔧</span>
          <span>Agent 工具调用</span>
          <span className="text-xs bg-indigo-100 text-indigo-600 px-1.5 py-0.5 rounded-full">
            {trace.length} 次
          </span>
        </div>
        <span className="text-gray-400 text-xs">{expanded ? '收起' : '展开'}</span>
      </button>

      {expanded && (
        <div className="px-4 py-2 space-y-2 max-h-80 overflow-y-auto">
          {trace.map((item, idx) => (
            <div
              key={idx}
              className={`rounded-md p-2.5 text-xs border ${
                item.is_error
                  ? 'bg-red-50 border-red-200'
                  : 'bg-gray-50 border-gray-200'
              }`}
            >
              <div className="flex items-center gap-2 mb-1">
                <span>{toolIcons[item.tool] || '🔧'}</span>
                <span className="font-semibold text-gray-700">{item.tool}</span>
                <span className="text-gray-400">#{item.iteration}</span>
                {item.is_error && (
                  <span className="text-red-500 text-[10px] bg-red-100 px-1 rounded">错误</span>
                )}
              </div>
              <div className="text-gray-500 mb-1">
                <span className="text-gray-400">参数: </span>
                <code className="text-[10px] bg-white px-1 py-0.5 rounded border">
                  {JSON.stringify(item.arguments).slice(0, 120)}
                  {JSON.stringify(item.arguments).length > 120 && '...'}
                </code>
              </div>
              <div className="text-gray-600">
                <span className="text-gray-400">结果: </span>
                {item.result.slice(0, 200)}
                {item.result.length > 200 && '...'}
              </div>
            </div>
          ))}
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/compare/CompareView.tsx">
import { useCallback, useEffect, useState } from 'react';
import { Loader2, Plus, X, GitCompareArrows, AlertCircle, BookOpen, Library } from 'lucide-react';
import {
  listAvailablePapers,
  listCompareCandidates,
  comparePapers,
  type PaperSummary,
  type CompareCandidate,
  type CompareResponse,
} from '../../api/compare';
import { useChatStore } from '../../stores/useChatStore';
import { useCompareStore } from '../../stores/useCompareStore';

const PAGE_SIZE = 12;
type Mode = 'citation' | 'library';

export function CompareView() {
  const sessionId = useChatStore((s) => s.sessionId);
  const { comparePreselectedPaperIds, clearComparePreselected } = useCompareStore();

  const [mode, setMode] = useState<Mode>('citation');
  const [candidates, setCandidates] = useState<CompareCandidate[]>([]);
  const [papers, setPapers] = useState<PaperSummary[]>([]);
  const [papersTotal, setPapersTotal] = useState(0);
  const [libraryOffset, setLibraryOffset] = useState(0);
  const [libraryQ, setLibraryQ] = useState('');
  const [librarySearchInput, setLibrarySearchInput] = useState('');

  const [selected, setSelected] = useState<string[]>([]);
  const [loading, setLoading] = useState(false);
  const [loadingCandidates, setLoadingCandidates] = useState(false);
  const [loadingPapers, setLoadingPapers] = useState(false);
  const [result, setResult] = useState<CompareResponse | null>(null);
  const [error, setError] = useState('');

  // Merge preselected into selected when user lands with preselected IDs (e.g. from chat quick-add), then clear
  useEffect(() => {
    if (comparePreselectedPaperIds.length === 0) return;
    setSelected((prev) => {
      const next = new Set([...prev, ...comparePreselectedPaperIds]);
      return [...next].slice(0, 5);
    });
    clearComparePreselected();
  }, [comparePreselectedPaperIds.length, clearComparePreselected]);

  const fetchCandidates = useCallback(() => {
    if (!sessionId) {
      setCandidates([]);
      setLoadingCandidates(false);
      return;
    }
    setLoadingCandidates(true);
    listCompareCandidates(sessionId, { scope: 'session', limit: 200, offset: 0 })
      .then((res) => {
        setCandidates(res.candidates);
      })
      .catch(() => {
        setCandidates([]);
      })
      .finally(() => setLoadingCandidates(false));
  }, [sessionId]);

  useEffect(() => {
    if (mode === 'citation') fetchCandidates();
  }, [mode, fetchCandidates]);

  const fetchPapers = useCallback(() => {
    setLoadingPapers(true);
    listAvailablePapers({ limit: PAGE_SIZE, offset: libraryOffset, q: libraryQ || undefined })
      .then((res) => {
        setPapers(res.papers);
        setPapersTotal(res.total);
      })
      .catch(() => {
        setPapers([]);
        setPapersTotal(0);
      })
      .finally(() => setLoadingPapers(false));
  }, [libraryOffset, libraryQ]);

  useEffect(() => {
    if (mode === 'library') fetchPapers();
  }, [mode, fetchPapers]);

  const togglePaper = (pid: string, selectable: boolean) => {
    if (!selectable) return;
    setSelected((prev) =>
      prev.includes(pid) ? prev.filter((p) => p !== pid) : prev.length < 5 ? [...prev, pid] : prev,
    );
  };

  const runCompare = async () => {
    if (selected.length < 2 || selected.length > 5) return;
    setLoading(true);
    setError('');
    setResult(null);
    try {
      const res = await comparePapers({ paper_ids: selected });
      setResult(res);
    } catch (e: any) {
      setError(e?.response?.data?.detail || '对比失败');
    } finally {
      setLoading(false);
    }
  };

  const applyLibrarySearch = () => {
    setLibraryQ(librarySearchInput.trim());
    setLibraryOffset(0);
  };

  const canAddMore = selected.length < 5;
  const canRun = selected.length >= 2 && selected.length <= 5;

  // Empty state when no session and citation mode, or no papers in library
  if (mode === 'citation' && !sessionId && !loadingCandidates) {
    return (
      <div className="flex flex-col items-center justify-center h-full text-gray-400 gap-3">
        <BookOpen size={40} />
        <p className="text-sm">暂无对话引文，请先在对话中产生引用后再使用「对话引文」</p>
        <button
          type="button"
          onClick={() => setMode('library')}
          className="text-sm text-blue-600 hover:underline"
        >
          切换到本地文库选择论文
        </button>
      </div>
    );
  }

  if (mode === 'library' && !loadingPapers && papers.length === 0 && papersTotal === 0) {
    return (
      <div className="flex flex-col items-center justify-center h-full text-gray-400 gap-3">
        <AlertCircle size={40} />
        <p className="text-sm">无可用论文，请先通过 Ingest 导入并解析论文</p>
        <button
          type="button"
          onClick={() => setMode('citation')}
          className="text-sm text-blue-600 hover:underline"
        >
          切换到对话引文
        </button>
      </div>
    );
  }

  return (
    <div className="flex flex-col h-full min-h-0 overflow-hidden">
      {/* Mode tabs */}
      <div className="flex-shrink-0 border-b border-gray-200 bg-white px-6 pt-3">
        <div className="flex gap-1 mb-3">
          <button
            type="button"
            onClick={() => setMode('citation')}
            className={`px-3 py-1.5 text-sm rounded-t-lg flex items-center gap-1.5 ${
              mode === 'citation'
                ? 'bg-gray-100 text-gray-800 font-medium border border-b-0 border-gray-200'
                : 'text-gray-500 hover:bg-gray-50 border border-transparent'
            }`}
          >
            <BookOpen size={14} />
            对话引文
          </button>
          <button
            type="button"
            onClick={() => setMode('library')}
            className={`px-3 py-1.5 text-sm rounded-t-lg flex items-center gap-1.5 ${
              mode === 'library'
                ? 'bg-gray-100 text-gray-800 font-medium border border-b-0 border-gray-200'
                : 'text-gray-500 hover:bg-gray-50 border border-transparent'
            }`}
          >
            <Library size={14} />
            本地文库
          </button>
        </div>
      </div>

      {/* Top bar: selection + run + card list — 限制高度并内部滚动，保证下方结果区可见 */}
      <div className="flex-shrink-0 border-b border-gray-200 bg-white px-6 py-4 flex flex-col max-h-[45vh] min-h-0">
        <div className="flex items-center justify-between mb-3">
          <div className="flex items-center gap-2 text-gray-700 font-medium">
            <GitCompareArrows size={18} />
            <span>多文档对比</span>
            <span className="text-xs text-gray-400 ml-1">选择 2-5 篇论文</span>
          </div>
          <button
            onClick={runCompare}
            disabled={!canRun || loading}
            className="px-4 py-1.5 bg-blue-600 text-white text-sm rounded-lg hover:bg-blue-700 disabled:opacity-50 disabled:cursor-not-allowed flex items-center gap-2"
          >
            {loading ? <Loader2 size={14} className="animate-spin" /> : <GitCompareArrows size={14} />}
            生成对比
          </button>
        </div>

        {/* Selected chips */}
        <div className="flex flex-wrap gap-2 mb-3">
          {selected.map((pid) => (
            <span
              key={pid}
              className="inline-flex items-center gap-1 px-2 py-1 bg-blue-50 text-blue-700 text-xs rounded-full"
            >
              {pid.length > 30 ? pid.slice(0, 28) + '…' : pid}
              <button type="button" onClick={() => togglePaper(pid, true)} className="hover:text-red-500">
                <X size={12} />
              </button>
            </span>
          ))}
        </div>

        {/* Library: search + pagination */}
        {mode === 'library' && (
          <div className="flex flex-wrap items-center gap-2 mb-3">
            <input
              type="text"
              placeholder="搜索标题或 paper_id"
              value={librarySearchInput}
              onChange={(e) => setLibrarySearchInput(e.target.value)}
              onKeyDown={(e) => e.key === 'Enter' && applyLibrarySearch()}
              className="px-2.5 py-1.5 border border-gray-200 rounded text-sm w-48"
            />
            <button
              type="button"
              onClick={applyLibrarySearch}
              className="px-2.5 py-1.5 bg-gray-100 text-gray-700 text-sm rounded hover:bg-gray-200"
            >
              搜索
            </button>
            <span className="text-xs text-gray-400">
              共 {papersTotal} 篇 · 第 {libraryOffset + 1}-{Math.min(libraryOffset + PAGE_SIZE, papersTotal)} 条
            </span>
            <div className="flex gap-1">
              <button
                type="button"
                onClick={() => setLibraryOffset((o) => Math.max(0, o - PAGE_SIZE))}
                disabled={libraryOffset === 0}
                className="px-2 py-1 text-sm rounded border border-gray-200 disabled:opacity-50 disabled:cursor-not-allowed"
              >
                上一页
              </button>
              <button
                type="button"
                onClick={() => setLibraryOffset((o) => o + PAGE_SIZE)}
                disabled={libraryOffset + PAGE_SIZE >= papersTotal}
                className="px-2 py-1 text-sm rounded border border-gray-200 disabled:opacity-50 disabled:cursor-not-allowed"
              >
                下一页
              </button>
            </div>
          </div>
        )}

        {/* Card list: citation candidates or library papers — 可滚动 */}
        {mode === 'citation' && (
          <>
            {loadingCandidates ? (
              <div className="flex justify-center py-4">
                <Loader2 size={20} className="animate-spin text-gray-400" />
              </div>
            ) : (
              <div className="flex gap-2 flex-wrap pb-1 overflow-y-auto min-h-0">
                {candidates.map((c) => {
                  const isSelected = selected.includes(c.paper_id);
                  const selectable = c.is_local_ready && canAddMore;
                  return (
                    <button
                      key={c.paper_id}
                      type="button"
                      onClick={() => togglePaper(c.paper_id, selectable)}
                      disabled={!selectable && !isSelected}
                      title={!c.is_local_ready ? '未在本地文库，无法加入对比' : undefined}
                      className={`flex-shrink-0 w-48 text-left p-2.5 rounded-lg border text-xs transition-all ${
                        isSelected
                          ? 'border-blue-400 bg-blue-50 ring-1 ring-blue-200'
                          : !c.is_local_ready
                            ? 'border-gray-200 bg-gray-50 opacity-75 cursor-not-allowed'
                            : 'border-gray-200 hover:border-gray-300 hover:bg-gray-50'
                      }`}
                    >
                      <div className="font-medium text-gray-800 truncate">{c.title || c.paper_id}</div>
                      <div className="flex items-center gap-1.5 mt-0.5 text-gray-500">
                        {c.year && <span>{c.year}</span>}
                        <span>引用 {c.citation_count} 次</span>
                      </div>
                      <div className="text-gray-500 mt-1 line-clamp-2">{c.abstract || '无摘要'}</div>
                      {!c.is_local_ready && (
                        <div className="mt-1 text-amber-600">未在本地，无法对比</div>
                      )}
                      {c.is_local_ready && !isSelected && canAddMore && (
                        <div className="mt-1.5 text-blue-500 flex items-center gap-0.5">
                          <Plus size={10} /> 选择
                        </div>
                      )}
                    </button>
                  );
                })}
              </div>
            )}
          </>
        )}

        {mode === 'library' && (
          <>
            {loadingPapers ? (
              <div className="flex justify-center py-4">
                <Loader2 size={20} className="animate-spin text-gray-400" />
              </div>
            ) : (
              <div className="flex gap-2 flex-wrap pb-1 overflow-y-auto min-h-0">
                {papers.map((p) => {
                  const isSelected = selected.includes(p.paper_id);
                  return (
                    <button
                      key={p.paper_id}
                      type="button"
                      onClick={() => togglePaper(p.paper_id, canAddMore)}
                      className={`flex-shrink-0 w-48 text-left p-2.5 rounded-lg border text-xs transition-all ${
                        isSelected
                          ? 'border-blue-400 bg-blue-50 ring-1 ring-blue-200'
                          : 'border-gray-200 hover:border-gray-300 hover:bg-gray-50'
                      }`}
                    >
                      <div className="font-medium text-gray-800 truncate">{p.title || p.paper_id}</div>
                      {p.year && <div className="text-gray-400 mt-0.5">{p.year}</div>}
                      <div className="text-gray-500 mt-1 line-clamp-2">{p.abstract || '无摘要'}</div>
                      {!isSelected && canAddMore && (
                        <div className="mt-1.5 text-blue-500 flex items-center gap-0.5">
                          <Plus size={10} /> 选择
                        </div>
                      )}
                    </button>
                  );
                })}
              </div>
            )}
          </>
        )}
      </div>

      {/* Result area — 始终占据剩余空间并独立滚动 */}
      <div className="flex-1 min-h-0 overflow-y-auto px-6 py-4 bg-gray-50">
        {error && (
          <div className="mb-4 p-3 bg-red-50 text-red-600 text-sm rounded-lg">{error}</div>
        )}

        {result && (
          <>
            {result.narrative && (
              <div className="mb-4 p-4 bg-white rounded-lg border border-gray-200 shadow-sm">
                <h3 className="text-sm font-medium text-gray-700 mb-2">综合分析</h3>
                <p className="text-sm text-gray-600 leading-relaxed">{result.narrative}</p>
              </div>
            )}

            {(result.papers?.length ?? 0) > 0 && (
              <div className="mb-4 p-3 bg-white rounded-lg border border-gray-200 text-sm text-gray-600">
                <span className="font-medium text-gray-700">已对比论文：</span>
                {result.papers!.map((p) => p.title || p.paper_id).join('、')}
              </div>
            )}

            {Object.keys(result.comparison_matrix || {}).length > 0 && (
              <div className="bg-white rounded-lg border border-gray-200 shadow-sm overflow-hidden">
                <table className="w-full text-sm">
                  <thead>
                    <tr className="bg-gray-50 border-b border-gray-200">
                      <th className="text-left px-4 py-2.5 font-medium text-gray-600 w-32">维度</th>
                      {result.papers.map((p) => (
                        <th key={p.paper_id} className="text-left px-4 py-2.5 font-medium text-gray-700">
                          <div className="truncate max-w-[200px]">{p.title || p.paper_id}</div>
                          {p.year && <span className="text-xs text-gray-400 font-normal ml-1">{p.year}</span>}
                        </th>
                      ))}
                    </tr>
                  </thead>
                  <tbody>
                    {Object.entries(result.comparison_matrix).map(([aspect, cells], idx) => (
                      <tr
                        key={aspect}
                        className={idx % 2 === 0 ? 'bg-white' : 'bg-gray-50/50'}
                      >
                        <td className="px-4 py-3 font-medium text-gray-600 align-top capitalize border-r border-gray-100">
                          {aspect.replace(/_/g, ' ')}
                        </td>
                        {result.papers.map((p) => (
                          <td key={p.paper_id} className="px-4 py-3 text-gray-600 align-top">
                            {cells[p.paper_id] || '—'}
                          </td>
                        ))}
                      </tr>
                    ))}
                  </tbody>
                </table>
              </div>
            )}
          </>
        )}

        {!result && !loading && !error && (
          <div className="flex items-center justify-center h-full text-gray-400 text-sm">
            选择 2-5 篇论文后点击「生成对比」查看结果
          </div>
        )}
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/graph/GraphExplorer.tsx">
import { useEffect, useState, useRef, useCallback } from 'react';
import ForceGraph2D from 'react-force-graph-2d';
import { Search, ZoomIn, ZoomOut, Maximize2, Loader2, AlertCircle } from 'lucide-react';
import {
  getGraphStats,
  getEntities,
  getNeighbors,
  getChunkDetail,
  type GraphStats,
  type EntityItem,
  type NeighborGraph,
  type ChunkDetail,
} from '../../api/graph';
import { useConfigStore } from '../../stores';
import { Modal } from '../ui/Modal';

// ── 颜色映射 ──
const TYPE_COLORS: Record<string, string> = {
  SPECIES: '#3b82f6',
  LOCATION: '#10b981',
  PHENOMENON: '#f59e0b',
  METHOD: '#8b5cf6',
  SUBSTANCE: '#ef4444',
  CHUNK: '#94a3b8',
  ENTITY: '#6b7280',
};

interface GraphData {
  nodes: { id: string; type: string; paper_id?: string; is_center?: boolean; val: number }[];
  links: { source: string; target: string; relation: string; weight: number }[];
}

export function GraphExplorer() {
  const fgRef = useRef<any>(null);
  const currentCollection = useConfigStore((s) => s.currentCollection);

  const [stats, setStats] = useState<GraphStats | null>(null);
  const [entities, setEntities] = useState<EntityItem[]>([]);
  const [searchQuery, setSearchQuery] = useState('');
  const [selectedEntity, setSelectedEntity] = useState('');
  const [depth, setDepth] = useState(1);
  const [graphData, setGraphData] = useState<GraphData>({ nodes: [], links: [] });
  const [loading, setLoading] = useState(false);
  const [error, setError] = useState('');
  const [hoveredNode, setHoveredNode] = useState<string | null>(null);
  const [chunkModalOpen, setChunkModalOpen] = useState(false);
  const [chunkDetailLoading, setChunkDetailLoading] = useState(false);
  const [chunkDetailError, setChunkDetailError] = useState('');
  const [selectedChunk, setSelectedChunk] = useState<ChunkDetail | null>(null);

  // 加载统计
  useEffect(() => {
    getGraphStats().then(setStats).catch(() => setStats(null));
  }, []);

  // 搜索实体
  useEffect(() => {
    const timer = setTimeout(() => {
      getEntities({ q: searchQuery || undefined, limit: 50 })
        .then((res) => setEntities(res.entities))
        .catch(() => setEntities([]));
    }, 300);
    return () => clearTimeout(timer);
  }, [searchQuery]);

  // 加载子图
  const loadNeighbors = useCallback(async (name: string, d?: number) => {
    setLoading(true);
    setError('');
    try {
      const graph: NeighborGraph = await getNeighbors(name, d ?? depth);
      const nodes = graph.nodes.map((n) => ({
        id: n.id,
        type: n.type,
        paper_id: n.paper_id,
        is_center: n.is_center,
        val: n.is_center ? 8 : n.type === 'CHUNK' ? 2 : 4,
      }));
      const links = graph.edges.map((e) => ({
        source: e.source,
        target: e.target,
        relation: e.relation,
        weight: e.weight,
      }));
      setGraphData({ nodes, links });
      setSelectedEntity(name);
      setTimeout(() => fgRef.current?.zoomToFit(400, 40), 200);
    } catch (e: any) {
      setError(e?.response?.data?.detail || '加载失败');
    } finally {
      setLoading(false);
    }
  }, [depth]);

  const handleEntityClick = (name: string) => {
    loadNeighbors(name);
  };

  const handleNodeClick = useCallback((node: any) => {
    if (node.type === 'CHUNK') {
      setChunkModalOpen(true);
      setChunkDetailLoading(true);
      setChunkDetailError('');
      setSelectedChunk(null);
      getChunkDetail({
        chunk_id: String(node.id),
        collection: currentCollection || undefined,
        paper_id: node.paper_id ? String(node.paper_id) : undefined,
      })
        .then((detail) => setSelectedChunk(detail))
        .catch((e: any) => setChunkDetailError(e?.response?.data?.detail || '加载 chunk 详情失败'))
        .finally(() => setChunkDetailLoading(false));
      return;
    }
    loadNeighbors(node.id);
  }, [currentCollection, loadNeighbors]);

  // ── 不可用 ──
  if (stats && !stats.available) {
    return (
      <div className="flex flex-col items-center justify-center h-full text-gray-400 gap-3">
        <AlertCircle size={40} />
        <p className="text-sm">知识图谱尚未构建，请先通过 Ingest 导入论文并构建图谱</p>
      </div>
    );
  }

  return (
    <div className="flex h-full overflow-hidden">
      {/* ── 左侧面板 ── */}
      <div className="w-64 border-r border-gray-200 flex flex-col bg-white">
        {/* 统计 */}
        {stats && (
          <div className="px-3 py-2 border-b border-gray-100 text-xs text-gray-500 flex flex-wrap gap-x-3">
            <span>节点 {stats.total_nodes}</span>
            <span>边 {stats.total_edges}</span>
            <span>实体 {stats.entity_count}</span>
          </div>
        )}

        {/* 搜索 */}
        <div className="px-3 py-2 border-b border-gray-100">
          <div className="relative">
            <Search size={14} className="absolute left-2 top-1/2 -translate-y-1/2 text-gray-400" />
            <input
              type="text"
              placeholder="搜索实体..."
              value={searchQuery}
              onChange={(e) => setSearchQuery(e.target.value)}
              className="w-full pl-7 pr-2 py-1.5 text-sm border border-gray-200 rounded-md focus:outline-none focus:border-blue-400"
            />
          </div>
        </div>

        {/* 深度选择 */}
        <div className="px-3 py-2 border-b border-gray-100 flex items-center gap-2 text-xs">
          <span className="text-gray-500">深度</span>
          {[1, 2, 3].map((d) => (
            <button
              key={d}
              onClick={() => {
                setDepth(d);
                if (selectedEntity) loadNeighbors(selectedEntity, d);
              }}
              className={`px-2 py-0.5 rounded ${
                depth === d ? 'bg-blue-600 text-white' : 'bg-gray-100 text-gray-600 hover:bg-gray-200'
              }`}
            >
              {d}
            </button>
          ))}
        </div>

        {/* 实体列表 */}
        <div className="flex-1 overflow-y-auto">
          {entities.map((e) => (
            <button
              key={e.name}
              onClick={() => handleEntityClick(e.name)}
              className={`w-full text-left px-3 py-1.5 text-sm border-b border-gray-50 hover:bg-gray-50 flex items-center gap-2 ${
                selectedEntity === e.name ? 'bg-blue-50 text-blue-700' : 'text-gray-700'
              }`}
            >
              <span
                className="w-2 h-2 rounded-full flex-shrink-0"
                style={{ backgroundColor: TYPE_COLORS[e.type] || '#6b7280' }}
              />
              <span className="truncate flex-1">{e.name}</span>
              <span className="text-[10px] text-gray-400">{e.mention_count}</span>
            </button>
          ))}
        </div>

        {/* 图例 */}
        <div className="px-3 py-2 border-t border-gray-100 text-[10px] text-gray-500">
          <div className="flex flex-wrap gap-x-3 gap-y-1">
            {Object.entries(TYPE_COLORS).filter(([k]) => k !== 'ENTITY').map(([type, color]) => (
              <span key={type} className="flex items-center gap-1">
                <span className="w-2 h-2 rounded-full" style={{ backgroundColor: color }} />
                {type}
              </span>
            ))}
          </div>
        </div>
      </div>

      {/* ── 右侧图谱 ── */}
      <div className="flex-1 relative bg-gray-50">
        {loading && (
          <div className="absolute inset-0 flex items-center justify-center bg-white/60 z-10">
            <Loader2 size={24} className="animate-spin text-blue-500" />
          </div>
        )}

        {error && (
          <div className="absolute top-3 left-1/2 -translate-x-1/2 z-10 bg-red-50 text-red-600 text-sm px-4 py-2 rounded-lg shadow">
            {error}
          </div>
        )}

        {graphData.nodes.length === 0 && !loading ? (
          <div className="flex items-center justify-center h-full text-gray-400 text-sm">
            选择左侧实体以查看知识图谱
          </div>
        ) : (
          <ForceGraph2D
            ref={fgRef}
            graphData={graphData}
            nodeLabel={(node: any) => `${node.id} (${node.type})`}
            nodeColor={(node: any) => TYPE_COLORS[node.type] || '#6b7280'}
            nodeVal={(node: any) => node.val}
            nodeCanvasObjectMode={() => 'after'}
            nodeCanvasObject={(node: any, ctx: CanvasRenderingContext2D, globalScale: number) => {
              if (globalScale < 1.5 && !node.is_center && node.id !== hoveredNode) return;
              const label = node.id.length > 20 ? node.id.slice(0, 18) + '…' : node.id;
              const fontSize = Math.max(10 / globalScale, 2);
              ctx.font = `${node.is_center ? 'bold ' : ''}${fontSize}px sans-serif`;
              ctx.textAlign = 'center';
              ctx.textBaseline = 'middle';
              ctx.fillStyle = node.is_center ? '#1e40af' : '#374151';
              ctx.fillText(label, node.x, node.y + 8 / globalScale);
            }}
            linkColor={() => '#d1d5db'}
            linkWidth={(link: any) => Math.min(link.weight || 1, 4)}
            linkDirectionalArrowLength={4}
            linkDirectionalArrowRelPos={1}
            linkLabel={(link: any) => link.relation}
            onNodeClick={handleNodeClick}
            onNodeHover={(node: any) => setHoveredNode(node?.id || null)}
            cooldownTicks={80}
            warmupTicks={30}
          />
        )}

        {/* 缩放控制 */}
        <div className="absolute bottom-4 right-4 flex flex-col gap-1">
          <button
            onClick={() => fgRef.current?.zoom(fgRef.current.zoom() * 1.3, 300)}
            className="p-1.5 bg-white border border-gray-200 rounded shadow-sm hover:bg-gray-50"
          >
            <ZoomIn size={16} />
          </button>
          <button
            onClick={() => fgRef.current?.zoom(fgRef.current.zoom() / 1.3, 300)}
            className="p-1.5 bg-white border border-gray-200 rounded shadow-sm hover:bg-gray-50"
          >
            <ZoomOut size={16} />
          </button>
          <button
            onClick={() => fgRef.current?.zoomToFit(400, 40)}
            className="p-1.5 bg-white border border-gray-200 rounded shadow-sm hover:bg-gray-50"
          >
            <Maximize2 size={16} />
          </button>
        </div>
      </div>
      <Modal
        open={chunkModalOpen}
        onClose={() => setChunkModalOpen(false)}
        title="Chunk 详情"
        maxWidth="max-w-3xl"
      >
        {chunkDetailLoading ? (
          <div className="py-10 text-center text-gray-500">
            <Loader2 size={20} className="animate-spin mx-auto mb-2" />
            正在加载内容...
          </div>
        ) : chunkDetailError ? (
          <div className="text-sm text-red-600 bg-red-50 border border-red-100 rounded-lg p-3">
            {chunkDetailError}
          </div>
        ) : selectedChunk ? (
          <div className="space-y-4">
            <div className="grid grid-cols-1 md:grid-cols-2 gap-2 text-xs text-gray-600">
              <div><span className="text-gray-400">Collection:</span> {selectedChunk.collection || '-'}</div>
              <div><span className="text-gray-400">Paper ID:</span> {selectedChunk.paper_id || '-'}</div>
              <div><span className="text-gray-400">Page:</span> {selectedChunk.page ?? '-'}</div>
              <div><span className="text-gray-400">Type:</span> {selectedChunk.content_type || selectedChunk.chunk_type || '-'}</div>
              <div className="md:col-span-2 break-all">
                <span className="text-gray-400">Chunk ID:</span> {selectedChunk.chunk_id}
              </div>
              {selectedChunk.section_path && (
                <div className="md:col-span-2 break-all">
                  <span className="text-gray-400">Section:</span> {selectedChunk.section_path}
                </div>
              )}
            </div>
            {selectedChunk.related_entities && selectedChunk.related_entities.length > 0 && (
              <div>
                <div className="text-xs text-gray-400 mb-1">关联实体（Top {selectedChunk.related_entities.length}）</div>
                <div className="flex flex-wrap gap-1.5">
                  {selectedChunk.related_entities.map((name) => (
                    <button
                      key={name}
                      onClick={() => {
                        setChunkModalOpen(false);
                        loadNeighbors(name);
                      }}
                      className="px-2 py-1 text-xs rounded-full bg-blue-50 text-blue-700 hover:bg-blue-100"
                    >
                      {name}
                    </button>
                  ))}
                </div>
              </div>
            )}
            <div>
              <div className="text-xs text-gray-400 mb-1">内容</div>
              <div className="max-h-[45vh] overflow-y-auto bg-gray-50 border border-gray-200 rounded-lg p-3 text-sm text-gray-700 whitespace-pre-wrap leading-relaxed">
                {selectedChunk.content || '(空内容)'}
              </div>
            </div>
          </div>
        ) : (
          <div className="text-sm text-gray-500">未找到 chunk 内容</div>
        )}
      </Modal>
    </div>
  );
}
</file>

<file path="frontend/src/components/layout/Header.tsx">
import {
  MoreHorizontal,
  MessageSquare,
  UploadCloud,
  Users,
  GitBranch,
  MessageSquarePlus,
  History,
  PlugZap,
  PanelRightClose,
  PanelRightOpen,
  Sparkles,
  Network,
  GitCompareArrows,
  Globe,
} from 'lucide-react';
import { useEffect, useRef, useState } from 'react';
import { useTranslation } from 'react-i18next';
import { useAuthStore, useConfigStore, useChatStore, useUIStore, useToastStore } from '../../stores';
import { checkHealth } from '../../api/health';

interface ModelEntry {
  provider: string;
  model: string;
  label: string;
  group: string;
  color: string;
}
type HeaderTabId = 'chat' | 'ingest' | 'users' | 'graph' | 'compare';

interface HeaderTabConfig {
  id: HeaderTabId;
  labelKey: string;
  icon: React.ComponentType<{ size?: number; className?: string }>;
  activeClass: string;
}

const MODEL_LIST: ModelEntry[] = [
  { provider: 'deepseek',          model: 'deepseek-chat',       label: 'deepseek-chat',       group: 'DeepSeek',       color: 'text-sky-400' },
  { provider: 'deepseek-thinking', model: 'deepseek-reasoner',   label: 'deepseek-reasoner (thinking)', group: 'DeepSeek', color: 'text-sky-500' },
  { provider: 'openai',            model: 'gpt-5-mini',          label: 'gpt-5-mini',          group: 'OpenAI',         color: 'text-emerald-400' },
  { provider: 'openai',            model: 'gpt-5.2',             label: 'gpt-5.2',             group: 'OpenAI',         color: 'text-emerald-400' },
  { provider: 'openai-thinking',   model: 'gpt-5.2',             label: 'gpt-5.2 (thinking)',   group: 'OpenAI',         color: 'text-emerald-500' },
  { provider: 'claude',            model: 'claude-sonnet-4-5',   label: 'claude-sonnet-4.5',   group: 'Claude',         color: 'text-amber-400' },
  { provider: 'claude',            model: 'claude-haiku-4-5',    label: 'claude-haiku-4.5',    group: 'Claude',         color: 'text-amber-500' },
  { provider: 'claude',            model: 'claude-opus-4-6',     label: 'claude-opus-4.6',     group: 'Claude',         color: 'text-amber-600' },
  { provider: 'claude-thinking',   model: 'claude-sonnet-4-5',   label: 'claude-sonnet-4.5 (thinking)', group: 'Claude', color: 'text-amber-400' },
  { provider: 'claude-thinking',   model: 'claude-haiku-4-5',    label: 'claude-haiku-4.5 (thinking)',  group: 'Claude', color: 'text-amber-500' },
  { provider: 'claude-thinking',   model: 'claude-opus-4-6',     label: 'claude-opus-4.6 (thinking)',   group: 'Claude', color: 'text-amber-600' },
  { provider: 'gemini',            model: 'gemini-pro-latest',   label: 'gemini-pro',          group: 'Gemini',         color: 'text-purple-400' },
  { provider: 'gemini',            model: 'gemini-flash-latest', label: 'gemini-flash',        group: 'Gemini',         color: 'text-purple-300' },
  { provider: 'gemini-thinking',   model: 'gemini-pro-latest',   label: 'gemini-pro (thinking)',   group: 'Gemini',     color: 'text-purple-500' },
  { provider: 'gemini-thinking',   model: 'gemini-flash-latest', label: 'gemini-flash (thinking)', group: 'Gemini',     color: 'text-purple-500' },
  { provider: 'gemini-vision',     model: 'gemini-2.5-flash',    label: 'gemini-2.5-flash (vision)', group: 'Gemini',   color: 'text-purple-300' },
  { provider: 'kimi',              model: 'kimi-k2.5',           label: 'kimi-k2.5',           group: 'Kimi',           color: 'text-cyan-400' },
  { provider: 'kimi-thinking',     model: 'kimi-k2.5',           label: 'kimi-k2.5 (thinking)', group: 'Kimi',          color: 'text-cyan-500' },
  { provider: 'kimi-vision',       model: 'kimi-k2.5',           label: 'kimi-k2.5 (vision)',   group: 'Kimi',          color: 'text-cyan-300' },
  { provider: 'sonar',             model: 'sonar',               label: 'sonar (search)',      group: 'Perplexity',    color: 'text-teal-400' },
  { provider: 'sonar',             model: 'sonar-pro',           label: 'sonar-pro (search)',  group: 'Perplexity',    color: 'text-teal-400' },
  { provider: 'sonar',             model: 'sonar-reasoning-pro', label: 'sonar-reasoning-pro', group: 'Perplexity',    color: 'text-teal-500' },
];

function encodeModelValue(provider: string, model: string): string {
  return `${provider}::${model}`;
}
function decodeModelValue(val: string): { provider: string; model: string } {
  const [provider, ...rest] = val.split('::');
  return { provider, model: rest.join('::') };
}

function groupedModels(): Map<string, ModelEntry[]> {
  const map = new Map<string, ModelEntry[]>();
  for (const entry of MODEL_LIST) {
    if (!map.has(entry.group)) map.set(entry.group, []);
    map.get(entry.group)!.push(entry);
  }
  return map;
}

export function Header() {
  const { t, i18n } = useTranslation();
  const headerRef = useRef<HTMLElement>(null);
  const moreMenuRef = useRef<HTMLDivElement>(null);
  const [compactHeader, setCompactHeader] = useState(false);
  const [headerWidth, setHeaderWidth] = useState(0);
  const [showMoreTabs, setShowMoreTabs] = useState(false);
  const user = useAuthStore((s) => s.user);
  const { dbStatus, setDbStatus, selectedProvider, setSelectedProvider, selectedModel, setSelectedModel } = useConfigStore();
  const { workflowStep, deepResearchActive, setShowDeepResearchDialog, newChat } = useChatStore();
  const {
    activeTab,
    setActiveTab,
    toggleSidebar,
    isCanvasOpen,
    toggleCanvas,
    isHistoryOpen,
    toggleHistory,
  } = useUIStore();
  const addToast = useToastStore((s) => s.addToast);

  const currentLang = i18n.language?.startsWith('en') ? 'en' : 'zh';

  const toggleLanguage = () => {
    const next = currentLang === 'zh' ? 'en' : 'zh';
    i18n.changeLanguage(next);
  };

  const handleTabChange = (tab: HeaderTabId) => {
    setActiveTab(tab);
    setShowMoreTabs(false);
  };

  const handleConnect = async () => {
    setDbStatus('connecting');
    addToast(t('header.connectService'), 'info');
    try {
      await checkHealth();
      setDbStatus('connected');
      addToast(t('header.serviceConnected'), 'success');
    } catch {
      setDbStatus('disconnected');
      addToast(t('header.connectFailed'), 'error');
    }
  };

  const handleRefresh = async () => {
    if (dbStatus !== 'connected') return;
    addToast(t('header.syncingStatus'), 'info');
    try {
      await checkHealth();
      addToast(t('header.syncOk'), 'success');
    } catch {
      setDbStatus('disconnected');
      addToast(t('header.disconnectedStatus'), 'error');
    }
  };

  const handleNewChat = () => {
    newChat();
    addToast(t('header.newChatCreated'), 'success');
  };

  useEffect(() => {
    const measure = () => {
      const width = headerRef.current?.clientWidth || 0;
      setHeaderWidth(width);
      setCompactHeader(width > 0 && width < 1320);
    };

    measure();
    const ro = new ResizeObserver(measure);
    if (headerRef.current) ro.observe(headerRef.current);
    window.addEventListener('resize', measure);

    return () => {
      ro.disconnect();
      window.removeEventListener('resize', measure);
    };
  }, []);

  useEffect(() => {
    if (!showMoreTabs) return;
    const handleClickOutside = (e: MouseEvent) => {
      if (moreMenuRef.current && !moreMenuRef.current.contains(e.target as Node)) {
        setShowMoreTabs(false);
      }
    };
    document.addEventListener('mousedown', handleClickOutside);
    return () => document.removeEventListener('mousedown', handleClickOutside);
  }, [showMoreTabs]);

  const tabConfigs: HeaderTabConfig[] = [
    {
      id: 'chat',
      labelKey: 'header.chat',
      icon: MessageSquare,
      activeClass: 'border-sky-400 text-sky-400 shadow-[0_4px_12px_-4px_rgba(56,189,248,0.5)]',
    },
    {
      id: 'ingest',
      labelKey: 'header.ingest',
      icon: UploadCloud,
      activeClass: 'border-sky-400 text-sky-400 shadow-[0_4px_12px_-4px_rgba(56,189,248,0.5)]',
    },
    {
      id: 'graph',
      labelKey: 'header.graph',
      icon: Network,
      activeClass: 'border-emerald-400 text-emerald-400 shadow-[0_4px_12px_-4px_rgba(52,211,153,0.5)]',
    },
    {
      id: 'compare',
      labelKey: 'header.compare',
      icon: GitCompareArrows,
      activeClass: 'border-amber-400 text-amber-400 shadow-[0_4px_12px_-4px_rgba(251,191,36,0.5)]',
    },
  ];
  if (user?.role === 'admin') {
    tabConfigs.push({
      id: 'users',
      labelKey: 'header.users',
      icon: Users,
      activeClass: 'border-purple-400 text-purple-400 shadow-[0_4px_12px_-4px_rgba(192,132,252,0.5)]',
    });
  }

  const maxVisibleTabs = headerWidth >= 1320
    ? tabConfigs.length
    : headerWidth >= 1200
      ? Math.min(tabConfigs.length, 4)
      : headerWidth >= 1040
        ? Math.min(tabConfigs.length, 3)
        : Math.min(tabConfigs.length, 2);

  const tabPriority: HeaderTabId[] = ['chat', 'ingest', 'graph', 'compare', 'users'];
  const visibleTabIds: HeaderTabId[] = [];
  const addVisible = (id: HeaderTabId) => {
    if (!tabConfigs.some((tab) => tab.id === id)) return;
    if (!visibleTabIds.includes(id) && visibleTabIds.length < maxVisibleTabs) {
      visibleTabIds.push(id);
    }
  };
  addVisible('chat');
  addVisible(activeTab as HeaderTabId);
  tabPriority.forEach(addVisible);

  const visibleTabs = tabConfigs.filter((tab) => visibleTabIds.includes(tab.id));
  const hiddenTabs = tabConfigs.filter((tab) => !visibleTabIds.includes(tab.id));
  const showCompactRightText = headerWidth >= 1180;
  const onlineLabel = currentLang === 'zh' ? '在线' : 'Online';
  const offlineLabel = currentLang === 'zh' ? '离线' : 'Offline';
  const canvasLabel = currentLang === 'zh' ? '画布' : 'Canvas';

  const renderTabButton = (tab: HeaderTabConfig) => {
    const Icon = tab.icon;
    const isActive = activeTab === tab.id;
    return (
      <button
        key={tab.id}
        onClick={() => handleTabChange(tab.id)}
        className={`h-16 flex items-center ${compactHeader ? 'gap-1 px-1.5' : 'gap-2 px-1'} border-b-2 font-medium text-sm transition-all focus:outline-none whitespace-nowrap ${
          isActive
            ? tab.activeClass
            : 'border-transparent text-slate-400 hover:text-slate-200'
        }`}
        title={t(tab.labelKey)}
      >
        <Icon size={18} />
        <span className="text-[11px]">{t(tab.labelKey)}</span>
      </button>
    );
  };

  return (
    <header ref={headerRef} className="glass-header px-4 h-16 flex items-center justify-between gap-3 flex-shrink-0 z-30 transition-colors duration-300 overflow-visible">
      <div className="flex gap-3 min-w-0 items-center">
        <button
          onClick={toggleSidebar}
          className="text-slate-400 hover:text-sky-400 p-1 transition-colors"
          title={currentLang === 'zh' ? '切换侧边栏' : 'Toggle Sidebar'}
        >
          <MoreHorizontal size={20} />
        </button>
        {!compactHeader && <div className="h-8 w-[1px] bg-slate-700/50"></div>}

        {visibleTabs.map(renderTabButton)}

        {hiddenTabs.length > 0 && (
          <div ref={moreMenuRef} className="relative">
            <button
              onClick={() => setShowMoreTabs((prev) => !prev)}
              className={`h-16 flex items-center gap-1 px-1.5 border-b-2 font-medium text-sm transition-all focus:outline-none whitespace-nowrap ${
                hiddenTabs.some((tab) => tab.id === activeTab)
                  ? 'border-sky-400 text-sky-400'
                  : 'border-transparent text-slate-400 hover:text-slate-200'
              }`}
              title={currentLang === 'zh' ? '更多' : 'More'}
            >
              <MoreHorizontal size={18} />
            </button>
            {showMoreTabs && (
              <div className="absolute top-[56px] left-0 min-w-[170px] bg-slate-900/95 border border-slate-700 rounded-lg shadow-xl py-1.5 z-[60]">
                {hiddenTabs.map((tab) => {
                  const Icon = tab.icon;
                  const isActive = activeTab === tab.id;
                  return (
                    <button
                      key={`more-${tab.id}`}
                      onClick={() => handleTabChange(tab.id)}
                      className={`w-full px-3 py-1.5 text-xs flex items-center gap-2 transition-colors ${
                        isActive
                          ? 'text-sky-300 bg-sky-900/40'
                          : 'text-slate-300 hover:bg-slate-800'
                      }`}
                      title={t(tab.labelKey)}
                    >
                      <Icon size={14} />
                      {t(tab.labelKey)}
                    </button>
                  );
                })}
              </div>
            )}
          </div>
        )}
      </div>

      {/* Center: Model Selector + Workflow */}
      <div className="flex items-center gap-2 shrink-0">
        <div className={`flex items-center gap-1.5 ${compactHeader ? 'px-2 py-1' : 'px-2.5 py-1'} bg-slate-800/60 rounded-lg border border-slate-700/60 backdrop-blur-sm shadow-sm hover:border-sky-500/30 transition-all`}>
          <Sparkles
            size={13}
            className={
              MODEL_LIST.find(
                (m) => m.provider === selectedProvider && m.model === selectedModel
              )?.color || 'text-slate-500'
            }
          />
          <select
            value={encodeModelValue(selectedProvider, selectedModel)}
            onChange={(e) => {
              const { provider, model } = decodeModelValue(e.target.value);
              setSelectedProvider(provider);
              setSelectedModel(model);
            }}
            className={`bg-transparent text-xs font-medium text-slate-200 focus:outline-none cursor-pointer pr-4 appearance-none ${compactHeader ? 'max-w-[140px]' : 'max-w-[220px]'}`}
            title={currentLang === 'zh' ? '选择模型' : 'Select Model'}
            style={{
              backgroundImage: `url("data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='12' height='12' viewBox='0 0 24 24' fill='none' stroke='%239ca3af' stroke-width='2' stroke-linecap='round' stroke-linejoin='round'%3E%3Cpolyline points='6 9 12 15 18 9'%3E%3C/polyline%3E%3C/svg%3E")`,
              backgroundRepeat: 'no-repeat',
              backgroundPosition: 'right 0px center',
            }}
          >
            {Array.from(groupedModels().entries()).map(([group, entries]) => (
              <optgroup key={group} label={group} className="bg-slate-800 text-slate-300">
                {entries.map((entry) => (
                  <option
                    key={encodeModelValue(entry.provider, entry.model)}
                    value={encodeModelValue(entry.provider, entry.model)}
                    className="bg-slate-800 text-slate-300"
                  >
                    {entry.label}
                  </option>
                ))}
              </optgroup>
            ))}
          </select>
        </div>

        {!compactHeader && workflowStep !== 'idle' && (
          <div className="flex items-center gap-2 px-4 py-1.5 bg-sky-900/30 text-sky-400 rounded-full text-xs font-bold border border-sky-500/30 animate-pulse-glow">
            <GitBranch size={14} />
            <span className="uppercase tracking-wide">{t('header.workflow')}: {workflowStep}</span>
          </div>
        )}
        {!compactHeader && deepResearchActive && (
          <button
            onClick={() => setShowDeepResearchDialog(true)}
            className="flex items-center gap-2 px-3 py-1.5 bg-indigo-900/30 text-indigo-400 rounded-full text-xs font-bold border border-indigo-500/30 hover:bg-indigo-900/50 transition-colors"
            title={currentLang === 'zh' ? '打开 Deep Research 面板' : 'Open Deep Research Panel'}
          >
            <div className="w-1.5 h-1.5 bg-indigo-400 rounded-full animate-pulse" />
            Deep Research Running
          </button>
        )}
      </div>

      <div className="flex items-center gap-2 shrink-0">
        {activeTab === 'chat' && dbStatus === 'connected' && (
          <>
            <button
              onClick={handleNewChat}
              className={`flex items-center ${compactHeader ? 'gap-1 px-2 py-1.5' : 'gap-2 px-3 py-1.5'} bg-slate-800/60 hover:bg-slate-700/60 text-slate-300 rounded-lg text-xs font-medium transition-colors border border-slate-700/60 hover:border-sky-500/30 hover:text-sky-300`}
              title={t('header.newChat')}
            >
              <MessageSquarePlus size={14} />
              <span className="text-[11px]">{showCompactRightText || !compactHeader ? t('header.newChat') : (currentLang === 'zh' ? '新建' : 'New')}</span>
            </button>
            <button
              onClick={toggleHistory}
              className={`p-2 rounded-lg transition-colors cursor-pointer ${
                isHistoryOpen
                  ? 'bg-sky-900/30 text-sky-400 border border-sky-500/30'
                  : 'hover:bg-slate-800/60 text-slate-400'
              }`}
              title={t('header.toggleHistory')}
            >
              <History size={18} />
            </button>
          </>
        )}

        {dbStatus === 'connected' ? (
          <button
            onClick={handleRefresh}
            className={`flex items-center ${compactHeader ? 'gap-1 px-2 py-1' : 'gap-2 px-3 py-1'} bg-emerald-900/20 text-emerald-400 rounded-full text-xs font-medium border border-emerald-500/30 shadow-sm cursor-pointer hover:bg-emerald-900/30 transition-colors whitespace-nowrap`}
            title={t('header.systemOnline')}
          >
            <div className="w-2 h-2 bg-emerald-400 rounded-full animate-pulse shadow-[0_0_8px_rgba(52,211,153,0.6)]"></div>
            <span className="text-[11px]">{showCompactRightText || !compactHeader ? t('header.systemOnline') : onlineLabel}</span>
          </button>
        ) : (
          <button
            onClick={handleConnect}
            className={`flex items-center ${compactHeader ? 'gap-1 px-2 py-1' : 'gap-2 px-3 py-1'} bg-red-900/20 text-red-400 hover:bg-red-900/30 rounded-full text-xs font-medium border border-red-500/30 shadow-sm transition-colors cursor-pointer animate-pulse whitespace-nowrap`}
            title={t('header.disconnected')}
          >
            <PlugZap size={14} />
            <span className="text-[11px]">{showCompactRightText || !compactHeader ? t('header.disconnected') : offlineLabel}</span>
          </button>
        )}

        {/* Language Switcher */}
        <button
          onClick={toggleLanguage}
          className="flex items-center gap-1.5 px-2.5 py-1.5 rounded-lg text-xs font-medium transition-all border border-slate-700/60 bg-slate-800/60 text-slate-300 hover:border-sky-500/30 hover:text-sky-300"
          title={currentLang === 'zh' ? 'Switch to English' : '切换到中文'}
        >
          <Globe size={14} />
          <span>{currentLang === 'zh' ? 'EN' : '中文'}</span>
        </button>

        <button
          onClick={toggleCanvas}
          className={`flex items-center gap-2 px-3 py-1.5 rounded-lg text-xs font-medium transition-colors border ${
            isCanvasOpen
              ? 'bg-sky-900/30 border-sky-500/30 text-sky-400 shadow-[0_0_10px_rgba(56,189,248,0.2)]'
              : 'bg-transparent border-slate-700/60 text-slate-400 hover:bg-slate-800/60'
          } whitespace-nowrap`}
          title={currentLang === 'zh' ? '切换画布面板' : 'Toggle Canvas Panel'}
        >
          {isCanvasOpen ? (
            <PanelRightClose size={14} />
          ) : (
            <PanelRightOpen size={14} />
          )}
          <span>{showCompactRightText || !compactHeader ? canvasLabel : 'CV'}</span>
        </button>
      </div>
    </header>
  );
}
</file>

<file path="frontend/src/components/layout/Sidebar.tsx">
import { useState, useEffect, useRef } from 'react';
import { useTranslation } from 'react-i18next';
import {
  Layers,
  Shield,
  Cpu,
  Network,
  Filter,
  Globe,
  Database,
  Server,
  PlugZap,
  Loader2,
  History,
  Clock,
  Trash2,
  Settings,
  LogOut,
  GripVertical,
  DownloadCloud,
  CloudCheck,
  HelpCircle,
} from 'lucide-react';
import { useAuthStore, useConfigStore, useUIStore, useToastStore, useChatStore, useCanvasStore } from '../../stores';
import { checkHealth } from '../../api/health';
import { listSessions, deleteSession } from '../../api/chat';
import { getModelStatus, syncModels } from '../../api/models';
import { exportCanvas, getCanvas } from '../../api/canvas';
import type { SessionListItem } from '../../types';

interface SidebarProps {
  onStartResize: () => void;
}

/** 悬停约 1s 后显示小弹窗说明，移出后短暂延迟关闭 */
function HelpTooltip({
  content,
  delayMs = 1000,
  children,
}: {
  content: string;
  delayMs?: number;
  children: React.ReactNode;
}) {
  const [visible, setVisible] = useState(false);
  const showRef = useRef<number | null>(null);
  const hideRef = useRef<number | null>(null);

  const handleEnter = () => {
    if (hideRef.current) {
      window.clearTimeout(hideRef.current);
      hideRef.current = null;
    }
    showRef.current = window.setTimeout(() => setVisible(true), delayMs);
  };

  const handleLeave = () => {
    if (showRef.current) {
      window.clearTimeout(showRef.current);
      showRef.current = null;
    }
    hideRef.current = window.setTimeout(() => setVisible(false), 150);
  };

  return (
    <span
      className="relative inline-flex items-center text-slate-500 hover:text-sky-400 transition-colors cursor-help shrink-0"
      onMouseEnter={handleEnter}
      onMouseLeave={handleLeave}
    >
      {children}
      {visible && (
        <span
          className="absolute left-1/2 -translate-x-1/2 bottom-full mb-2 px-3 py-2 text-xs text-slate-200 bg-slate-900/95 backdrop-blur-md rounded-lg shadow-[0_4px_20px_rgba(0,0,0,0.5)] border border-slate-700/50 max-w-[260px] whitespace-normal z-[100] pointer-events-none animate-in fade-in zoom-in-95 duration-200"
          role="tooltip"
        >
          {content}
          <div className="absolute left-1/2 -translate-x-1/2 top-full w-2 h-2 bg-slate-900 border-r border-b border-slate-700/50 transform rotate-45 -mt-1"></div>
        </span>
      )}
    </span>
  );
}

export function Sidebar({ onStartResize }: SidebarProps) {
  const { t } = useTranslation();
  const user = useAuthStore((s) => s.user);
  const logout = useAuthStore((s) => s.logout);
  const addToast = useToastStore((s) => s.addToast);

  const {
    dbAddress,
    setDbAddress,
    dbStatus,
    setDbStatus,
    currentCollection,
    collections,
    setCurrentCollection,
    ragConfig,
    updateRagConfig,
    webSearchConfig,
    setWebSearchEnabled,
    toggleWebSource,
    updateWebSourceParam,
    setQueryOptimizer,
    setMaxQueriesPerProvider,
    setContentFetcherEnabled,
    setAgentEnabled,
  } = useConfigStore();

  const {
    sidebarWidth,
    isSidebarOpen,
    setShowSettingsModal,
  } = useUIStore();

  const { loadSession, isLoadingSession, sessionId: currentSessionId } = useChatStore();
  const { setCanvas, setCanvasContent, clearCanvas, setIsLoading: setCanvasLoading } = useCanvasStore();

  const [showQuerySettings, setShowQuerySettings] = useState(false);
  const [chatHistory, setChatHistory] = useState<SessionListItem[]>([]);
  const [isLoadingSessions, setIsLoadingSessions] = useState(false);
  const [isSyncingModels, setIsSyncingModels] = useState(false);
  const [modelStatusSummary, setModelStatusSummary] = useState<string | null>(null);

  // 加载会话历史
  useEffect(() => {
    const loadSessions = async () => {
      if (dbStatus !== 'connected') return;
      setIsLoadingSessions(true);
      try {
        const sessions = await listSessions(50);
        setChatHistory(sessions);
      } catch (error) {
        console.error('Failed to load sessions:', error);
      } finally {
        setIsLoadingSessions(false);
      }
    };
    loadSessions();
  }, [dbStatus]);

  const handleDeleteSession = async (sessionId: string) => {
    try {
      await deleteSession(sessionId);
      setChatHistory((prev) => prev.filter((s) => s.session_id !== sessionId));
      addToast(t('sidebar.sessionDeleted'), 'success');
    } catch (error) {
      addToast(t('sidebar.deleteFailed'), 'error');
    }
  };

  const handleLoadSession = async (sessionId: string) => {
    if (isLoadingSession || sessionId === currentSessionId) return;
    try {
      await loadSession(sessionId);
      addToast(t('sidebar.sessionLoaded'), 'success');
      
      // 加载 Canvas 内容（如果有）
      const loadedCanvasId = useChatStore.getState().canvasId;
      if (loadedCanvasId) {
        clearCanvas();
        setCanvasLoading(true);
        try {
          const [canvasData, exportResp] = await Promise.all([
            getCanvas(loadedCanvasId).catch(() => null),
            exportCanvas(loadedCanvasId, 'markdown').catch(() => null),
          ]);
          if (canvasData) setCanvas(canvasData);
          if (exportResp?.content) setCanvasContent(exportResp.content);
        } catch (err) {
          console.error('[Sidebar] Failed to load canvas:', err);
        } finally {
          setCanvasLoading(false);
        }
      } else {
        clearCanvas();
      }
    } catch (error) {
      addToast(t('sidebar.loadSessionFailed'), 'error');
    }
  };

  const handleConnect = async () => {
    setDbStatus('connecting');
    addToast(t('sidebar.connectingTo', { address: dbAddress }), 'info');
    try {
      await checkHealth();
      setDbStatus('connected');
      addToast(t('sidebar.connected'), 'success');
    } catch {
      setDbStatus('disconnected');
      addToast(t('sidebar.connectFailed'), 'error');
    }
  };

  const handleLogout = () => {
    logout();
    addToast(t('sidebar.loggedOut'), 'info');
  };

  const handleSyncModels = async () => {
    if (isSyncingModels) return;
    setIsSyncingModels(true);
    setModelStatusSummary(null);
    try {
      // 默认仅在发现新版本时升级；已是最新版本则跳过。
      const result = await syncModels({ force_update: false });
      const upgraded = result.items.filter((i) => i.updated).length;
      const skipped = result.items.filter((i) => i.status === 'already_latest').length;
      const failed = result.items.filter((i) => i.status === 'failed').length;
      setModelStatusSummary(t('sidebar.modelSyncComplete', { upgraded, skipped, failed }));
      addToast(
        t('sidebar.modelSyncComplete', { upgraded, skipped, failed }),
        failed > 0 ? 'error' : 'success',
      );
    } catch (error) {
      addToast(t('sidebar.modelSyncFailed'), 'error');
    } finally {
      setIsSyncingModels(false);
    }
  };

  const handleCheckModels = async () => {
    if (isSyncingModels) return;
    setIsSyncingModels(true);
    setModelStatusSummary(null);
    try {
      const result = await getModelStatus();
      const ok = result.items.filter((i) => i.exists).length;
      const failed = result.items.filter((i) => !i.exists).length;
      setModelStatusSummary(t('sidebar.modelStatusReady', { ok, failed }));
      addToast(t('sidebar.modelStatusReady', { ok, failed }), failed > 0 ? 'error' : 'success');
    } catch (error) {
      addToast(t('sidebar.modelStatusFailed'), 'error');
    } finally {
      setIsSyncingModels(false);
    }
  };

  if (!user) return null;

  return (
    <div
      className="glass-sidebar flex flex-col relative flex-shrink-0 z-40 transition-[width] duration-100 ease-linear shadow-[5px_0_30px_rgba(0,0,0,0.3)]"
      style={{ width: isSidebarOpen ? sidebarWidth : 80 }}
    >
      {/* Logo */}
      <div className="p-6 flex items-center gap-3 h-20 overflow-hidden relative">
        <div className="absolute top-0 left-0 w-full h-full bg-gradient-to-b from-sky-500/10 to-transparent pointer-events-none"></div>
        <div className="bg-gradient-to-br from-sky-500 to-blue-600 p-2 rounded-xl text-white flex-shrink-0 shadow-[0_0_15px_rgba(14,165,233,0.5)] border border-sky-400/30 animate-pulse-glow">
          <Layers size={24} />
        </div>
        {isSidebarOpen && (
          <span className="font-bold text-xl tracking-tight whitespace-nowrap text-sky-100 drop-shadow-md">
            RAG Lab
          </span>
        )}
      </div>

      {/* 内容区 */}
      <div className="flex-1 overflow-y-auto p-4 space-y-8 scrollbar-thin">
        {/* 用户信息 */}
        <div
          className={`flex items-center gap-3 p-3 bg-slate-800/40 rounded-xl border border-slate-700/50 hover:border-sky-500/30 transition-colors ${
            !isSidebarOpen && 'justify-center'
          }`}
        >
          <img
            src={user.avatar}
            alt="Avatar"
            className="w-10 h-10 rounded-full bg-slate-900 border border-slate-600 shadow-sm"
          />
          {isSidebarOpen && (
            <div className="flex-1 min-w-0">
              <div className="font-bold text-sm truncate text-slate-200">
                {user.username || user.user_id}
              </div>
              <div className="text-xs text-slate-400 flex items-center gap-1">
                <Shield size={10} className="text-sky-400" />
                <span className="capitalize">{user.role}</span>
              </div>
            </div>
          )}
        </div>

        {/* 检索策略配置 */}
        {isSidebarOpen && (
          <section>
            <div className="flex items-center gap-2 mb-4 text-sky-500/80 font-semibold text-xs uppercase tracking-wider pl-1">
              <Cpu size={14} /> {t('sidebar.retrievalConfig')}
            </div>
            <div className="bg-slate-900/30 rounded-xl p-3 border border-slate-700/50 space-y-3 shadow-inner">
              {/* Local RAG Toggle */}
              <div className="flex items-center justify-between pb-2 border-b border-slate-700/50">
                <div className="flex items-center gap-2">
                  <Database size={14} className="text-sky-500" />
                  <span className="text-sm font-medium text-slate-300">{t('sidebar.localKnowledge')}</span>
                  <HelpTooltip content={t('sidebar.localKnowledgeHelp')}>
                    <HelpCircle size={12} />
                  </HelpTooltip>
                </div>
                <input
                  id="rag-enabled"
                  name="rag-enabled"
                  type="checkbox"
                  checked={ragConfig.enabled ?? true}
                  onChange={(e) =>
                    updateRagConfig({ enabled: e.target.checked })
                  }
                  className="accent-sky-500 w-4 h-4 cursor-pointer"
                />
              </div>

              {/* Top-K Slider - 只在启用时显示 */}
              {ragConfig.enabled && (
              <div>
                <div className="flex items-center justify-between text-[10px] text-slate-500 mb-1">
                  <span>{t('sidebar.queryCollection')}</span>
                </div>
                <select
                  id="query-collection"
                  name="query-collection"
                  value={currentCollection || ''}
                  onChange={(e) => setCurrentCollection(e.target.value)}
                  className="w-full text-xs bg-slate-950 border border-slate-700 text-slate-300 rounded px-2 py-1.5 mb-2 focus:border-sky-500 focus:outline-none"
                >
                  {collections.length === 0 ? (
                    <option value="">{t('sidebar.noCollection')}</option>
                  ) : (
                    collections.map((name) => (
                      <option key={`query-collection-${name}`} value={name}>
                        {name}
                      </option>
                    ))
                  )}
                </select>
              </div>
              )}

              {/* Top-K Slider - 只在启用时显示 */}
              {ragConfig.enabled && <div>
                <div className="flex items-center justify-between text-[10px] text-slate-500 mb-1">
                  <span>Local RAG Top-K</span>
                  <div className="flex items-center gap-2">
                    <span className="font-mono bg-slate-950 px-1.5 py-0.5 rounded border border-slate-700 text-sky-400">
                      {ragConfig.localTopK}
                    </span>
                    <input
                      id="local-topk-num"
                      name="local-topk-num"
                      type="number"
                      min="1"
                      max="200"
                      step="1"
                      value={ragConfig.localTopK}
                      onChange={(e) =>
                        updateRagConfig({ localTopK: Math.max(1, Number(e.target.value)) })
                      }
                      className="w-16 text-[10px] bg-slate-950 border border-slate-700 text-slate-300 rounded px-1 py-0.5 focus:border-sky-500 focus:outline-none"
                      title={t('sidebar.localTopKTitle')}
                    />
                  </div>
                </div>
                <input
                  id="local-topk-range"
                  name="local-topk-range"
                  type="range"
                  min="1"
                  max="60"
                  step="1"
                  value={Math.min(ragConfig.localTopK, 60)}
                  onChange={(e) =>
                    updateRagConfig({ localTopK: Number(e.target.value) })
                  }
                  className="w-full accent-sky-500 h-1.5 bg-slate-700 rounded-lg appearance-none cursor-pointer"
                />
              </div>}

              {/* HippoRAG Toggle - 只在启用时显示 */}
              {ragConfig.enabled && <div className="flex items-center justify-between">
                <div className="flex items-center gap-2">
                  <Network size={14} className="text-purple-400" />
                  <span className="text-sm text-slate-300">{t('sidebar.hippoRAG')}</span>
                  <HelpTooltip content={t('sidebar.hippoRAGHelp')}>
                    <HelpCircle size={12} />
                  </HelpTooltip>
                </div>
                <input
                  id="hippo-rag"
                  name="hippo-rag"
                  type="checkbox"
                  checked={ragConfig.enableHippoRAG}
                  onChange={(e) =>
                    updateRagConfig({ enableHippoRAG: e.target.checked })
                  }
                  className="accent-purple-500 w-4 h-4 cursor-pointer"
                />
              </div>}

              {/* Reranker Toggle - 只在启用时显示 */}
              {ragConfig.enabled && <div className="flex items-center justify-between">
                <div className="flex items-center gap-2">
                  <Filter size={14} className="text-amber-400" />
                  <span className="text-sm text-slate-300">{t('sidebar.colbertReranker')}</span>
                </div>
                <input
                  id="reranker"
                  name="reranker"
                  type="checkbox"
                  checked={ragConfig.enableReranker}
                  onChange={(e) =>
                    updateRagConfig({ enableReranker: e.target.checked })
                  }
                  className="accent-amber-500 w-4 h-4 cursor-pointer"
                />
              </div>}
            </div>
          </section>
        )}

        {/* 合并检索参数（独立于 Local / Web，始终可见） */}
        {isSidebarOpen && (
          <section>
            <div className="flex items-center gap-2 mb-4 text-sky-500/80 font-semibold text-xs uppercase tracking-wider pl-1">
              <Layers size={14} /> {t('sidebar.mergeParams')}
            </div>
            <div className="bg-slate-900/30 rounded-xl p-3 border border-slate-700/50 space-y-3 shadow-inner">
              {/* Final Top-K */}
              <div>
                <div className="flex items-center justify-between text-[10px] text-slate-500 mb-1">
                  <span>{t('sidebar.finalTopK')}</span>
                  <div className="flex items-center gap-2">
                    <span className="font-mono bg-slate-950 px-1.5 py-0.5 rounded border border-slate-700 text-sky-400">
                      {ragConfig.finalTopK ?? 10}
                    </span>
                    <input
                      id="final-topk-num"
                      name="final-topk-num"
                      type="number"
                      min="1"
                      max="200"
                      step="1"
                      value={ragConfig.finalTopK ?? 10}
                      onChange={(e) =>
                        updateRagConfig({ finalTopK: Math.max(1, Number(e.target.value)) })
                      }
                      className="w-16 text-[10px] bg-slate-950 border border-slate-700 text-slate-300 rounded px-1 py-0.5 focus:border-sky-500 focus:outline-none"
                      title={t('sidebar.finalTopKDesc')}
                    />
                  </div>
                </div>
                <input
                  id="final-topk-range"
                  name="final-topk-range"
                  type="range"
                  min="1"
                  max="60"
                  step="1"
                  value={Math.min(ragConfig.finalTopK ?? 10, 60)}
                  onChange={(e) =>
                    updateRagConfig({ finalTopK: Number(e.target.value) })
                  }
                  className="w-full accent-amber-500 h-1.5 bg-slate-700 rounded-lg appearance-none cursor-pointer"
                />
                <div className="text-[9px] text-slate-600 mt-0.5">
                  {t('sidebar.finalTopKDesc')}
                </div>
              </div>

              {/* 相似度阈值 */}
              <div>
                <div className="flex items-center justify-between text-[10px] text-slate-500 mb-1">
                  <span>{t('sidebar.threshold')}</span>
                  <div className="flex items-center gap-2">
                    <span className="font-mono bg-slate-950 px-1.5 py-0.5 rounded border border-slate-700 text-sky-400">
                      {ragConfig.localThreshold?.toFixed(2) ?? '0.50'}
                    </span>
                    <input
                      id="threshold-num"
                      name="threshold-num"
                      type="number"
                      min="0"
                      max="1"
                      step="0.01"
                      value={ragConfig.localThreshold ?? 0.5}
                      onChange={(e) =>
                        updateRagConfig({ localThreshold: Math.min(1, Math.max(0, Number(e.target.value))) })
                      }
                      className="w-16 text-[10px] bg-slate-950 border border-slate-700 text-slate-300 rounded px-1 py-0.5 focus:border-sky-500 focus:outline-none"
                      title={t('sidebar.thresholdTitle')}
                    />
                  </div>
                </div>
                <input
                  id="threshold-range"
                  name="threshold-range"
                  type="range"
                  min="0"
                  max="1"
                  step="0.05"
                  value={ragConfig.localThreshold ?? 0.5}
                  onChange={(e) =>
                    updateRagConfig({ localThreshold: Number(e.target.value) })
                  }
                  className="w-full accent-emerald-500 h-1.5 bg-slate-700 rounded-lg appearance-none cursor-pointer"
                />
                <div className="text-[9px] text-slate-600 mt-0.5">
                  {t('sidebar.thresholdDesc')}
                </div>
              </div>
            </div>
          </section>
        )}

        {/* Web Search Config */}
        {isSidebarOpen && (
          <section>
            <div className="flex items-center gap-2 mb-4 text-sky-500/80 font-semibold text-xs uppercase tracking-wider pl-1">
              <Globe size={14} /> {t('sidebar.webSearch')}
            </div>
            <div className="space-y-3">
              {/* 总开关 */}
              <div className="border border-slate-700/50 rounded-xl p-3 bg-slate-900/30 hover:border-sky-500/30 hover:bg-slate-800/50 transition-all cursor-pointer shadow-sm">
                <label className="flex items-center justify-between cursor-pointer w-full">
                  <div className="flex items-center gap-3">
                    <div
                      className={`p-2 rounded-lg shadow-sm transition-colors ${
                        webSearchConfig.enabled
                          ? 'bg-indigo-900/40 text-indigo-400 shadow-[0_0_10px_rgba(99,102,241,0.2)]'
                          : 'bg-slate-800 text-slate-500'
                      }`}
                    >
                      <Globe size={16} />
                    </div>
                    <div>
                      <div className="flex items-center gap-1.5">
                        <span className="text-sm font-medium text-slate-200">{t('sidebar.webSearchToggle')}</span>
                        <HelpTooltip content={t('sidebar.webSearchHelp')}>
                          <HelpCircle size={12} />
                        </HelpTooltip>
                      </div>
                      <div className="text-[10px] text-slate-500">
                        {t('sidebar.webSearchDesc')}
                      </div>
                    </div>
                  </div>
                  <input
                    id="web-search-enabled"
                    name="web-search-enabled"
                    type="checkbox"
                    checked={webSearchConfig.enabled}
                    onChange={(e) => {
                      setWebSearchEnabled(e.target.checked);
                      addToast(
                        e.target.checked ? t('sidebar.webSearchEnabled') : t('sidebar.webSearchDisabled'),
                        'info'
                      );
                    }}
                    className="accent-indigo-500 w-4 h-4 cursor-pointer"
                  />
                </label>
              </div>

              {webSearchConfig.enabled && (
                <div className="bg-slate-900/30 border border-slate-700/50 rounded-xl overflow-hidden animate-in slide-in-from-top-2 duration-200">
                  <div className="bg-slate-800/50 px-3 py-2 border-b border-slate-700/50 text-[10px] font-bold text-slate-500 uppercase">
                    Search Sources
                  </div>
                  <div className="divide-y divide-slate-700/30">
                    {webSearchConfig.sources.map((source) => (
                      <div
                        key={source.id}
                        className="p-3 hover:bg-slate-800/30 transition-colors"
                      >
                        <div className="flex items-center justify-between mb-2">
                          <span
                            className={`text-sm font-medium ${
                              source.enabled ? 'text-slate-200' : 'text-slate-500'
                            }`}
                          >
                            {source.name}
                          </span>
                          <input
                            id={`web-source-${source.id}`}
                            name={`web-source-${source.id}`}
                            type="checkbox"
                            checked={source.enabled}
                            onChange={() => toggleWebSource(source.id)}
                            className="accent-sky-500 w-4 h-4 cursor-pointer"
                          />
                        </div>
                        {source.enabled && (
                          <div className="space-y-2 animate-in slide-in-from-top-1 duration-200">
                            {/* Top-K 配置 */}
                            <div className="flex items-center justify-between text-[10px] text-slate-500">
                              <span>Top-K</span>
                              <div className="flex items-center gap-2">
                                <span className="font-mono bg-slate-950 px-1.5 py-0.5 rounded border border-slate-700 text-sky-400">
                                  {source.topK}
                                </span>
                                <input
                                  id={`web-source-topk-num-${source.id}`}
                                  name={`web-source-topk-num-${source.id}`}
                                  type="number"
                                  min="1"
                                  max="100"
                                  step="1"
                                  value={source.topK}
                                  onChange={(e) =>
                                    updateWebSourceParam(
                                      source.id,
                                      'topK',
                                      Math.max(1, Number(e.target.value))
                                    )
                                  }
                                  className="w-14 text-[10px] bg-slate-950 border border-slate-700 text-slate-300 rounded px-1 py-0.5 focus:border-sky-500 focus:outline-none"
                                />
                              </div>
                            </div>
                            <input
                              id={`web-source-topk-range-${source.id}`}
                              name={`web-source-topk-range-${source.id}`}
                              type="range"
                              min="1"
                              max="60"
                              step="1"
                              value={Math.min(source.topK, 60)}
                              onChange={(e) =>
                                updateWebSourceParam(
                                  source.id,
                                  'topK',
                                  Number(e.target.value)
                                )
                              }
                              className="w-full accent-sky-500 h-1 bg-slate-700 rounded-lg appearance-none cursor-pointer"
                            />
                            {/* Threshold 配置 */}
                            <div className="flex items-center justify-between text-[10px] text-slate-500 mt-2">
                              <span>Threshold</span>
                              <div className="flex items-center gap-2">
                                <span className="font-mono bg-slate-950 px-1.5 py-0.5 rounded border border-slate-700 text-sky-400">
                                  {source.threshold.toFixed(2)}
                                </span>
                                <input
                                  id={`web-source-threshold-num-${source.id}`}
                                  name={`web-source-threshold-num-${source.id}`}
                                  type="number"
                                  min="0"
                                  max="1"
                                  step="0.05"
                                  value={source.threshold}
                                  onChange={(e) =>
                                    updateWebSourceParam(
                                      source.id,
                                      'threshold',
                                      Math.min(1, Math.max(0, Number(e.target.value)))
                                    )
                                  }
                                  className="w-14 text-[10px] bg-slate-950 border border-slate-700 text-slate-300 rounded px-1 py-0.5 focus:border-sky-500 focus:outline-none"
                                />
                              </div>
                            </div>
                            <input
                              id={`web-source-threshold-range-${source.id}`}
                              name={`web-source-threshold-range-${source.id}`}
                              type="range"
                              min="0"
                              max="1"
                              step="0.05"
                              value={source.threshold}
                              onChange={(e) =>
                                updateWebSourceParam(
                                  source.id,
                                  'threshold',
                                  Number(e.target.value)
                                )
                              }
                              className="w-full accent-emerald-500 h-1 bg-slate-700 rounded-lg appearance-none cursor-pointer"
                            />
                          </div>
                        )}
                      </div>
                    ))}
                  </div>
                </div>
              )}

              {/* 查询增强选项 */}
              {webSearchConfig.enabled && (
                <div className="bg-slate-900/30 border border-slate-700/50 rounded-xl p-3 animate-in slide-in-from-top-2 duration-200 space-y-3">
                  <div className="text-[10px] font-bold text-slate-500 uppercase">
                    Query Enhancement
                  </div>
                  
                  {/* 查询优化器 */}
                  <div className="flex items-center justify-between">
                    <div>
                      <div className="flex items-center gap-1.5">
                        <span className="text-sm font-medium text-slate-300">{t('sidebar.queryOptimizer')}</span>
                        <HelpTooltip content={t('sidebar.queryOptimizerHelp')}>
                          <HelpCircle size={12} />
                        </HelpTooltip>
                      </div>
                      <div className="text-[10px] text-slate-500">
                        {t('sidebar.queryOptimizerDesc')}
                      </div>
                    </div>
                    <div className="flex items-center gap-2">
                      <button
                        type="button"
                        onClick={() => setShowQuerySettings((v) => !v)}
                        className="text-[10px] text-sky-400 px-2 py-1 border border-slate-700/50 rounded-md hover:bg-slate-800 transition-colors"
                        title={t('sidebar.settingsQueryTitle')}
                      >
                        {t('common.settings')}
                      </button>
                      <input
                        id="query-optimizer"
                        name="query-optimizer"
                        type="checkbox"
                        checked={webSearchConfig.queryOptimizer ?? true}
                        onChange={(e) => {
                          setQueryOptimizer(e.target.checked);
                          addToast(
                            e.target.checked
                              ? t('sidebar.queryOptimizerEnabled')
                              : t('sidebar.queryOptimizerDisabled'),
                            'info'
                          );
                        }}
                        className="accent-sky-500 w-4 h-4 cursor-pointer"
                      />
                    </div>
                  </div>

                  {showQuerySettings && (
                    <div className="pt-2 border-t border-slate-700/50">
                      <label className="flex items-center justify-between">
                        <span className="text-xs text-slate-400">{t('sidebar.queriesPerProvider')}</span>
                        <select
                          id="max-queries-per-provider"
                          name="max-queries-per-provider"
                          value={webSearchConfig.maxQueriesPerProvider ?? 3}
                          onChange={(e) => {
                            setMaxQueriesPerProvider(Number(e.target.value));
                            addToast(t('sidebar.queryCountSet', { count: Number(e.target.value) }), 'info');
                          }}
                          className="text-xs border border-slate-700 rounded-md px-2 py-1 bg-slate-950 text-slate-300 focus:outline-none focus:border-sky-500"
                        >
                          {[1, 2, 3, 4, 5].map((n) => (
                            <option key={n} value={n}>
                              {n}
                            </option>
                          ))}
                        </select>
                      </label>
                    </div>
                  )}
                </div>
              )}

              {/* 全文抓取 */}
              {webSearchConfig.enabled && (
                <div className="bg-slate-900/30 border border-slate-700/50 rounded-xl p-3 animate-in slide-in-from-top-2 duration-200">
                  <div className="flex items-center justify-between">
                    <div>
                      <div className="flex items-center gap-1.5">
                        <span className="text-sm font-medium text-slate-300">{t('sidebar.contentFetcher')}</span>
                        <HelpTooltip content={t('sidebar.contentFetcherHelp')}>
                          <HelpCircle size={12} />
                        </HelpTooltip>
                      </div>
                      <div className="text-[10px] text-slate-500">
                        {t('sidebar.contentFetcherDesc')}
                      </div>
                    </div>
                    <input
                      id="content-fetcher"
                      name="content-fetcher"
                      type="checkbox"
                      checked={webSearchConfig.enableContentFetcher ?? false}
                      onChange={(e) => {
                        setContentFetcherEnabled(e.target.checked);
addToast(
                        e.target.checked ? t('sidebar.contentFetcherEnabled') : t('sidebar.contentFetcherDisabled'),
                          'info'
                        );
                      }}
                      className="accent-sky-500 w-4 h-4 cursor-pointer"
                    />
                  </div>
                </div>
              )}
            </div>
          </section>
        )}

        {/* Agent 模式 */}
        {isSidebarOpen && (
          <section>
            <div className="flex items-center gap-2 mb-4 text-sky-500/80 font-semibold text-xs uppercase tracking-wider pl-1">
              <Cpu size={14} /> {t('sidebar.agentMode')}
            </div>
            <div className="bg-slate-900/30 border border-slate-700/50 rounded-xl p-3 shadow-inner">
              <div className="flex items-center justify-between">
                <div>
                  <div className="flex items-center gap-1.5">
                    <span className="text-sm font-medium text-slate-300">{t('sidebar.reactAgent')}</span>
                    <HelpTooltip content={t('sidebar.reactAgentHelp')}>
                      <HelpCircle size={12} />
                    </HelpTooltip>
                  </div>
                  <div className="text-[10px] text-slate-500">
                    {t('sidebar.reactAgentDesc')}
                  </div>
                </div>
                <input
                  id="agent-enabled"
                  name="agent-enabled"
                  type="checkbox"
                  checked={ragConfig.enableAgent ?? true}
                  onChange={(e) => {
                    setAgentEnabled(e.target.checked);
addToast(
                    e.target.checked ? t('sidebar.agentEnabled') : t('sidebar.agentDisabled'),
                      'info'
                    );
                  }}
                  className="accent-indigo-500 w-4 h-4 cursor-pointer"
                />
              </div>
              <div className="mt-2 text-[10px] rounded-md p-2 bg-slate-900/50 text-slate-400 border border-slate-700/50">
                {t('sidebar.agentNote').split('\n').map((line, i, arr) => (
                  <span key={i}>
                    {line}
                    {i < arr.length - 1 && <br />}
                  </span>
                ))}
              </div>
            </div>
          </section>
        )}

        {/* 服务连接 */}
        {isSidebarOpen && (
          <section>
            <div className="flex items-center gap-2 mb-4 text-sky-500/80 font-semibold text-xs uppercase tracking-wider pl-1">
              <Database size={14} /> {t('sidebar.serviceConnection')}
            </div>
            <div className="space-y-3">
              <div>
                <label className="flex items-center gap-1.5 text-xs text-slate-500 mb-1">
                  {t('sidebar.serviceAddress')}
                  <HelpTooltip content={t('sidebar.serviceAddressHelp')}>
                    <HelpCircle size={12} />
                  </HelpTooltip>
                </label>
                <div className="relative">
                  <input
                    id="service-address"
                    name="service-address"
                    type="text"
                    disabled={user.role !== 'admin'}
                    className={`w-full bg-slate-950 border border-slate-700 rounded-md p-2 text-sm pl-8 text-slate-300 focus:border-sky-500 focus:outline-none ${
                      dbStatus === 'connected'
                        ? 'text-emerald-400 border-emerald-500/30 bg-emerald-900/10'
                        : ''
                    } disabled:opacity-60 disabled:cursor-not-allowed`}
                    value={dbAddress}
                    onChange={(e) => setDbAddress(e.target.value)}
                  />
                  <Server
                    size={14}
                    className="absolute left-2.5 top-2.5 text-slate-500"
                  />
                </div>
              </div>
              {dbStatus === 'connected' ? (
                <div className="p-3 bg-emerald-900/10 rounded-lg border border-emerald-500/30">
                  <div className="flex justify-between items-center text-xs text-emerald-400/80">
                    <span>Status:</span>
                    <span className="text-emerald-400 flex items-center gap-1 font-medium">
                      <div className="w-1.5 h-1.5 bg-emerald-400 rounded-full animate-pulse shadow-[0_0_8px_rgba(52,211,153,0.8)]"></div>
                      Online
                    </span>
                  </div>
                </div>
              ) : (
                <button
                  onClick={handleConnect}
                  disabled={dbStatus === 'connecting'}
                  className="w-full text-xs text-sky-400 font-medium py-2 border border-sky-500/30 rounded-md hover:bg-sky-500/10 flex justify-center items-center gap-2 transition-colors cursor-pointer"
                >
                  {dbStatus === 'connecting' ? (
                    <Loader2 size={14} className="animate-spin" />
                  ) : (
                    <PlugZap size={14} />
                  )}
                  {dbStatus === 'connecting' ? t('sidebar.connecting') : t('sidebar.connectService')}
                </button>
              )}
            </div>
          </section>
        )}

        {/* 本地模型管理 */}
        {isSidebarOpen && (
          <section>
            <div className="flex items-center gap-2 mb-4 text-sky-500/80 font-semibold text-xs uppercase tracking-wider pl-1">
              <DownloadCloud size={14} /> {t('sidebar.localModels')}
            </div>
            <div className="bg-slate-900/30 rounded-lg p-3 border border-slate-700/50 space-y-2">
              <button
                onClick={handleSyncModels}
                disabled={isSyncingModels}
                className="w-full text-xs text-sky-400 font-medium py-2 border border-sky-500/30 rounded-md hover:bg-sky-500/10 flex justify-center items-center gap-2 transition-colors cursor-pointer disabled:opacity-60 disabled:cursor-not-allowed"
                title={t('sidebar.syncModels')}
              >
                {isSyncingModels ? (
                  <Loader2 size={14} className="animate-spin" />
                ) : (
                  <DownloadCloud size={14} />
                )}
                {isSyncingModels ? t('sidebar.syncing') : t('sidebar.syncModels')}
              </button>
              <button
                onClick={handleCheckModels}
                disabled={isSyncingModels}
                className="w-full text-xs text-slate-400 font-medium py-2 border border-slate-700/50 rounded-md hover:bg-slate-800 flex justify-center items-center gap-2 transition-colors cursor-pointer disabled:opacity-60 disabled:cursor-not-allowed"
                title={t('sidebar.checkModelStatus')}
              >
                <CloudCheck size={14} />
                {t('sidebar.checkModelStatus')}
              </button>
              {modelStatusSummary && (
                <div className="text-[10px] text-slate-400 text-center">
                  {modelStatusSummary}
                </div>
              )}
              <div className="text-[10px] text-slate-600 text-center">
                {t('sidebar.defaultCacheDir')}: <code className="font-mono text-slate-500">~/Hug</code>
              </div>
            </div>
          </section>
        )}

        {/* 历史项目 */}
        {isSidebarOpen && (
          <section>
            <div className="flex items-center gap-2 mb-4 text-sky-500/80 font-semibold text-xs uppercase tracking-wider pl-1">
              <History size={14} /> {t('sidebar.history')}
              {isLoadingSessions && <Loader2 size={12} className="animate-spin" />}
            </div>
            <div className="space-y-2 max-h-48 overflow-y-auto pr-1 scrollbar-thin">
              {chatHistory.length === 0 && !isLoadingSessions && (
                <div className="text-xs text-slate-600 text-center py-4">{t('sidebar.noHistory')}</div>
              )}
              {chatHistory.map((h) => (
                <div
                  key={h.session_id}
                  className={`group p-2 rounded-lg hover:bg-slate-800/50 cursor-pointer text-sm relative transition-colors border border-transparent ${
                    currentSessionId === h.session_id 
                      ? 'bg-sky-900/20 border-sky-500/30 shadow-[0_0_10px_rgba(56,189,248,0.1)]' 
                      : ''
                  }`}
                  onClick={() => handleLoadSession(h.session_id)}
                >
                  <div className="flex justify-between items-start">
                    <span className={`font-medium truncate flex-1 transition-colors ${
                      currentSessionId === h.session_id ? 'text-sky-400' : 'text-slate-300 group-hover:text-sky-200'
                    }`}>
                      {h.title}
                    </span>
                    {isLoadingSession && currentSessionId !== h.session_id && (
                      <Loader2 size={12} className="animate-spin text-sky-500 flex-shrink-0" />
                    )}
                  </div>
                  <div className="text-[10px] text-slate-500 flex items-center gap-1 mt-1">
                    <Clock size={10} /> {new Date(h.updated_at).toLocaleString('zh-CN', {
                      month: '2-digit',
                      day: '2-digit',
                      hour: '2-digit',
                      minute: '2-digit',
                    })}
                    <span className="ml-2 text-slate-700">|</span>
                    <span className="ml-1">{h.turn_count} {t('sidebar.turns')}</span>
                  </div>

                  {/* Hover Actions */}
                  <div className="absolute right-1 top-1 hidden group-hover:flex gap-1 bg-slate-900 border border-slate-700 p-1 rounded shadow-lg z-10">
                    <button
                      className="p-1 hover:text-red-400 text-slate-500 transition-colors"
                      title={t('common.delete')}
                      onClick={(e) => {
                        e.stopPropagation();
                        if (confirm(t('sidebar.confirmDeleteSession'))) {
                          handleDeleteSession(h.session_id);
                        }
                      }}
                    >
                      <Trash2 size={12} />
                    </button>
                  </div>
                </div>
              ))}
            </div>
          </section>
        )}
      </div>

      {/* Footer */}
      <div className="p-4 border-t border-slate-700/50 bg-slate-900/50 space-y-2">
        {user.role === 'admin' && (
          <button
            onClick={() => setShowSettingsModal(true)}
            className={`w-full flex items-center justify-center gap-2 bg-slate-800 text-slate-200 py-3 rounded-xl text-sm font-medium hover:bg-slate-700 hover:text-white transition-all border border-slate-700/50 ${
              !isSidebarOpen && 'px-0'
            }`}
          >
            <Settings size={18} />
            {isSidebarOpen && t('sidebar.advancedConfig')}
          </button>
        )}
        <button
          onClick={handleLogout}
          className={`w-full flex items-center justify-center gap-2 text-red-400 hover:bg-red-900/20 py-3 rounded-xl text-sm font-medium transition-all ${
            !isSidebarOpen && 'px-0'
          }`}
        >
          <LogOut size={18} />
          {isSidebarOpen && t('sidebar.logout')}
        </button>
      </div>

      {/* 拖拽把手 */}
      {isSidebarOpen && (
        <div
          onMouseDown={(e) => {
            e.preventDefault();
            onStartResize();
          }}
          className="absolute top-0 right-0 w-1 h-full cursor-col-resize hover:bg-sky-500/50 z-50 group transition-colors"
        >
          <div className="absolute top-1/2 -right-3 w-6 h-8 bg-slate-800 border border-slate-600 rounded shadow-sm flex items-center justify-center opacity-0 group-hover:opacity-100 transition-opacity pointer-events-none">
            <GripVertical size={12} className="text-slate-400" />
          </div>
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/research/ResearchProgressPanel.tsx">
/**
 * Deep Research 实时进度面板。
 *
 * 显示 ReCAP Dashboard 的实时状态：
 * - 当前研究阶段 + 各章节进度
 * - 来源数量和置信度
 * - 信息缺口和冲突
 * - 整体进度条
 */

import { useTranslation } from 'react-i18next';
import type { ResearchDashboardData, ResearchSectionStatus } from '../../types';

interface Props {
  dashboard: ResearchDashboardData | null;
  isActive: boolean;
}

const statusConfig: Record<string, { icon: string; label: string; color: string }> = {
  pending: { icon: '⬜', label: 'research.pending', color: 'text-gray-400' },
  researching: { icon: '🔍', label: 'research.researching', color: 'text-blue-500' },
  writing: { icon: '✍️', label: 'research.writing', color: 'text-orange-500' },
  reviewing: { icon: '🔄', label: 'research.reviewing', color: 'text-purple-500' },
  done: { icon: '✅', label: 'research.done', color: 'text-green-500' },
};

const confidenceConfig: Record<string, { label: string; color: string; bg: string }> = {
  low: { label: 'research.confidenceLow', color: 'text-red-600', bg: 'bg-red-100' },
  medium: { label: 'research.confidenceMedium', color: 'text-yellow-600', bg: 'bg-yellow-100' },
  high: { label: 'research.confidenceHigh', color: 'text-green-600', bg: 'bg-green-100' },
};

function SectionRow({ section }: { section: ResearchSectionStatus }) {
  const { t } = useTranslation();
  const cfg = statusConfig[section.status] || statusConfig.pending;
  const coveragePct = Math.round(section.coverage_score * 100);

  return (
    <div className="flex items-center gap-2 py-1.5 px-2 rounded hover:bg-gray-50 text-sm">
      <span className="w-5 text-center">{cfg.icon}</span>
      <span className="flex-1 truncate font-medium text-gray-700">{section.title}</span>
      <span className={`text-xs ${cfg.color}`}>{t(cfg.label)}</span>
      {section.status !== 'pending' && (
        <span className="text-xs text-gray-400 w-10 text-right">{coveragePct}%</span>
      )}
      {section.source_count > 0 && (
        <span className="text-xs text-gray-400 w-12 text-right">{t('research.sourcesCount', { count: section.source_count })}</span>
      )}
    </div>
  );
}

export function ResearchProgressPanel({ dashboard, isActive }: Props) {
  const { t } = useTranslation();

  if (!dashboard) {
    if (!isActive) return null;
    return (
      <div className="border rounded-lg bg-white p-4 text-sm text-gray-400 text-center">
        {t('research.waiting')}
      </div>
    );
  }

  const progressPct = Math.round(dashboard.progress * 100);
  const coveragePct = Math.round(dashboard.coverage * 100);
  const confCfg = confidenceConfig[dashboard.confidence] || confidenceConfig.low;

  return (
    <div className="border rounded-lg bg-white shadow-sm overflow-hidden">
      {/* Header */}
      <div className="px-4 py-3 bg-gradient-to-r from-indigo-50 to-blue-50 border-b">
        <div className="flex items-center justify-between">
          <h3 className="text-sm font-semibold text-gray-800">
            {t('research.progressTitle')}
          </h3>
          <span className={`text-xs px-2 py-0.5 rounded-full ${confCfg.bg} ${confCfg.color} font-medium`}>
            {t('research.confidence')}: {t(confCfg.label)}
          </span>
        </div>
        {dashboard.topic && (
          <p className="text-xs text-gray-500 mt-0.5 truncate">{dashboard.topic}</p>
        )}
      </div>

      {/* Progress Bar */}
      <div className="px-4 pt-3">
        <div className="flex items-center justify-between text-xs text-gray-500 mb-1">
          <span>{t('research.overallProgress')}</span>
          <span>{progressPct}%</span>
        </div>
        <div className="w-full bg-gray-200 rounded-full h-2">
          <div
            className="bg-indigo-500 h-2 rounded-full transition-all duration-500"
            style={{ width: `${progressPct}%` }}
          />
        </div>
      </div>

      {/* Stats */}
      <div className="px-4 py-2 flex gap-4 text-xs text-gray-500">
        <span>{t('research.sources')}: <strong className="text-gray-700">{dashboard.total_sources}</strong></span>
        <span>{t('research.coverage')}: <strong className="text-gray-700">{coveragePct}%</strong></span>
        <span>{t('research.iterations')}: <strong className="text-gray-700">{dashboard.total_iterations}</strong></span>
      </div>

      {/* Sections */}
      {dashboard.sections.length > 0 && (
        <div className="px-2 pb-2">
          <div className="text-xs text-gray-400 px-2 mb-1 font-medium">{t('research.sections')}</div>
          {dashboard.sections.map((s, i) => (
            <SectionRow key={i} section={s} />
          ))}
        </div>
      )}

      {/* Coverage Gaps */}
      {dashboard.coverage_gaps.length > 0 && (
        <div className="px-4 py-2 border-t">
          <div className="text-xs text-gray-400 mb-1 font-medium">{t('research.coverageGaps')}</div>
          {dashboard.coverage_gaps.slice(0, 5).map((g, i) => (
            <div key={i} className="text-xs text-red-500 flex items-start gap-1 py-0.5">
              <span>❗</span>
              <span>{g}</span>
            </div>
          ))}
        </div>
      )}

      {/* Conflicts */}
      {dashboard.conflict_notes.length > 0 && (
        <div className="px-4 py-2 border-t">
          <div className="text-xs text-gray-400 mb-1 font-medium">{t('research.conflicts')}</div>
          {dashboard.conflict_notes.slice(0, 3).map((c, i) => (
            <div key={i} className="text-xs text-amber-600 flex items-start gap-1 py-0.5">
              <span>⚠️</span>
              <span>{c}</span>
            </div>
          ))}
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/components/settings/SettingsModal.tsx">
import { Settings, Brain, FileText, Cpu, ChevronDown, ChevronRight } from 'lucide-react';
import { useState } from 'react';
import { useTranslation } from 'react-i18next';
import { Modal } from '../ui/Modal';
import { useUIStore, useConfigStore, useToastStore } from '../../stores';

type CiteKeyFormat = 'author_date' | 'numeric' | 'hash';
type MergeLevel = 'document' | 'chunk';
type RerankerMode = 'cascade' | 'bge_only' | 'colbert_only';

/**
 * 高级配置 Modal：
 * - 引文格式
 * - 重排序策略
 * - 知识图谱
 */
export function SettingsModal() {
  const { t } = useTranslation();
  const { showSettingsModal, setShowSettingsModal } = useUIStore();
  const { ragConfig, updateRagConfig } = useConfigStore();
  const addToast = useToastStore((s) => s.addToast);

  // 高级设置状态（持久化到 localStorage）
  const [citeKeyFormat, setCiteKeyFormat] = useState<CiteKeyFormat>(
    () => (localStorage.getItem('adv_cite_key_format') as CiteKeyFormat) || 'author_date'
  );
  const [mergeLevel, setMergeLevel] = useState<MergeLevel>(
    () => (localStorage.getItem('adv_merge_level') as MergeLevel) || 'document'
  );
  const [rerankerMode, setRerankerMode] = useState<RerankerMode>(
    () => (localStorage.getItem('adv_reranker_mode') as RerankerMode) || 'cascade'
  );

  // 折叠区域
  const [expandedSections, setExpandedSections] = useState<Set<string>>(
    new Set(['citation', 'reranker', 'graph'])
  );

  const toggleSection = (id: string) => {
    setExpandedSections((prev) => {
      const next = new Set(prev);
      if (next.has(id)) next.delete(id);
      else next.add(id);
      return next;
    });
  };

  const handleSave = () => {
    localStorage.setItem('adv_cite_key_format', citeKeyFormat);
    localStorage.setItem('adv_merge_level', mergeLevel);
    localStorage.setItem('adv_reranker_mode', rerankerMode);
    addToast(t('settings.saved'), 'success');
    setShowSettingsModal(false);
  };

  const SectionHeader = ({
    id,
    icon,
    title,
  }: {
    id: string;
    icon: React.ReactNode;
    title: string;
  }) => (
    <button
      type="button"
      className="w-full flex items-center gap-2 py-2 text-sm font-semibold text-slate-300 hover:text-slate-100"
      onClick={() => toggleSection(id)}
    >
      {expandedSections.has(id) ? (
        <ChevronDown size={14} className="text-slate-500" />
      ) : (
        <ChevronRight size={14} className="text-slate-500" />
      )}
      {icon}
      {title}
    </button>
  );

  return (
    <Modal
      open={showSettingsModal}
      onClose={() => setShowSettingsModal(false)}
      title={t('settings.advancedConfig')}
      icon={<Settings size={20} className="text-gray-700" />}
      maxWidth="max-w-md"
    >
      <div className="space-y-4 max-h-[65vh] overflow-y-auto pr-1">
        {/* ── 引文格式 ── */}
        <div className="border border-slate-700/50 rounded-lg p-3 bg-slate-800/30">
          <SectionHeader
            id="citation"
            icon={<FileText size={14} className="text-amber-600" />}
            title={t('settings.citationFormat')}
          />
          {expandedSections.has('citation') && (
            <div className="space-y-3 pt-2 pl-6">
              {/* cite_key 格式 */}
              <div>
                <label className="block text-xs text-slate-400 mb-1.5">
                  {t('settings.citeKeyFormat')}
                </label>
                <div className="grid grid-cols-3 gap-1.5">
                  {(
                    [
                      { value: 'author_date', label: 'Author-Date', desc: 'Smith2023' },
                      { value: 'numeric', label: 'Numeric', desc: '[1], [2]' },
                      { value: 'hash', label: 'Hash', desc: 'a3f7b2...' },
                    ] as const
                  ).map((opt) => (
                    <button
                      key={opt.value}
                      type="button"
                      onClick={() => setCiteKeyFormat(opt.value)}
                      className={`px-2 py-1.5 rounded text-[11px] border transition-all ${
                        citeKeyFormat === opt.value
                          ? 'bg-amber-900/20 border-amber-500/40 text-amber-300 font-medium'
                          : 'bg-slate-900/50 border-slate-700 text-slate-400 hover:border-slate-600'
                      }`}
                    >
                      <div>{opt.label}</div>
                      <div className="text-[9px] opacity-60 mt-0.5">{opt.desc}</div>
                    </button>
                  ))}
                </div>
              </div>

              {/* 合并级别 */}
              <div>
                <label className="block text-xs text-slate-400 mb-1.5">
                  {t('settings.mergeLevel')}
                </label>
                <div className="grid grid-cols-2 gap-2">
                  {(
                    [
                      {
                        value: 'document',
                        label: t('settings.docLevel'),
                        desc: t('settings.docLevelDesc'),
                      },
                      {
                        value: 'chunk',
                        label: t('settings.chunkLevel'),
                        desc: t('settings.chunkLevelDesc'),
                      },
                    ] as const
                  ).map((opt) => (
                    <button
                      key={opt.value}
                      type="button"
                      onClick={() => setMergeLevel(opt.value)}
                      className={`px-2 py-2 rounded text-[11px] border transition-all text-left ${
                        mergeLevel === opt.value
                          ? 'bg-amber-900/20 border-amber-500/40 text-amber-300 font-medium'
                          : 'bg-slate-900/50 border-slate-700 text-slate-400 hover:border-slate-600'
                      }`}
                    >
                      <div>{opt.label}</div>
                      <div className="text-[9px] opacity-60 mt-0.5">{opt.desc}</div>
                    </button>
                  ))}
                </div>
              </div>
            </div>
          )}
        </div>

        {/* ── 重排序策略 ── */}
        <div className="border border-slate-700/50 rounded-lg p-3 bg-slate-800/30">
          <SectionHeader
            id="reranker"
            icon={<Cpu size={14} className="text-blue-600" />}
            title={t('settings.rerankerStrategy')}
          />
          {expandedSections.has('reranker') && (
            <div className="space-y-3 pt-2 pl-6">
              <div>
                <label className="block text-xs text-slate-400 mb-1.5">
                  {t('settings.rerankerMode')}
                </label>
                <div className="grid grid-cols-3 gap-1.5">
                  {(
                    [
                      {
                        value: 'cascade',
                        label: 'Cascade',
                        desc: 'BGE 粗排 + ColBERT 精排',
                      },
                      {
                        value: 'bge_only',
                        label: 'BGE Only',
                        desc: '仅 BGE-Reranker',
                      },
                      {
                        value: 'colbert_only',
                        label: 'ColBERT Only',
                        desc: '仅 ColBERT (多语言)',
                      },
                    ] as const
                  ).map((opt) => (
                    <button
                      key={opt.value}
                      type="button"
                      onClick={() => setRerankerMode(opt.value)}
                      className={`px-2 py-1.5 rounded text-[11px] border transition-all ${
                        rerankerMode === opt.value
                          ? 'bg-sky-900/20 border-sky-500/40 text-sky-300 font-medium'
                          : 'bg-slate-900/50 border-slate-700 text-slate-400 hover:border-slate-600'
                      }`}
                    >
                      <div>{opt.label}</div>
                      <div className="text-[9px] opacity-60 mt-0.5">{opt.desc}</div>
                    </button>
                  ))}
                </div>
              </div>
              <div className="flex items-center justify-between">
                <span className="text-xs text-slate-300">{t('settings.enableReranker')}</span>
                <button
                  type="button"
                  onClick={() =>
                    updateRagConfig({ enableReranker: !ragConfig.enableReranker })
                  }
                  className={`relative w-9 h-5 rounded-full transition-colors ${
                    ragConfig.enableReranker ? 'bg-sky-500' : 'bg-slate-600'
                  }`}
                >
                  <span
                    className={`absolute top-0.5 w-4 h-4 rounded-full bg-white shadow transition-transform ${
                      ragConfig.enableReranker
                        ? 'translate-x-4'
                        : 'translate-x-0.5'
                    }`}
                  />
                </button>
              </div>
              <div className="text-[10px] text-slate-400 bg-slate-900/50 rounded px-2 py-1.5 border border-slate-700">
                ColBERT: <span className="font-mono">jinaai/jina-colbert-v2</span>{' '}
                (89 语言)
              </div>
            </div>
          )}
        </div>

        {/* ── 知识图谱 ── */}
        <div className="border border-slate-700/50 rounded-lg p-3 bg-slate-800/30">
          <SectionHeader
            id="graph"
            icon={<Brain size={14} className="text-purple-600" />}
            title={t('settings.knowledgeGraph')}
          />
          {expandedSections.has('graph') && (
            <div className="space-y-3 pt-2 pl-6">
              <div className="flex items-center justify-between">
                <div>
                  <span className="text-xs text-slate-300">{t('settings.enableHippoRAG')}</span>
                  <p className="text-[9px] text-slate-500 mt-0.5">
                    {t('settings.hippoRAGDesc')}
                  </p>
                </div>
                <button
                  type="button"
                  onClick={() =>
                    updateRagConfig({ enableHippoRAG: !ragConfig.enableHippoRAG })
                  }
                  className={`relative w-9 h-5 rounded-full transition-colors ${
                    ragConfig.enableHippoRAG ? 'bg-purple-500' : 'bg-slate-600'
                  }`}
                >
                  <span
                    className={`absolute top-0.5 w-4 h-4 rounded-full bg-white shadow transition-transform ${
                      ragConfig.enableHippoRAG
                        ? 'translate-x-4'
                        : 'translate-x-0.5'
                    }`}
                  />
                </button>
              </div>
            </div>
          )}
        </div>
      </div>

      {/* 底部按钮 */}
      <div className="flex justify-end gap-2 mt-5 pt-4 border-t border-slate-700/50">
        <button
          type="button"
          onClick={() => setShowSettingsModal(false)}
          className="px-4 py-2 text-sm text-slate-400 hover:bg-slate-800 rounded-lg"
        >
          {t('common.cancel')}
        </button>
        <button
          type="button"
          onClick={handleSave}
          className="px-4 py-2 text-sm bg-sky-600 text-white rounded-lg hover:bg-sky-500"
        >
          {t('common.save')}
        </button>
      </div>
    </Modal>
  );
}
</file>

<file path="frontend/src/components/ui/Modal.tsx">
import { X } from 'lucide-react';
import type { ReactNode } from 'react';

interface ModalProps {
  open: boolean;
  onClose: () => void;
  title?: string;
  icon?: ReactNode;
  children: ReactNode;
  maxWidth?: string;
}

export function Modal({
  open,
  onClose,
  title,
  icon,
  children,
  maxWidth = 'max-w-sm',
}: ModalProps) {
  if (!open) return null;

  return (
    <div
      className="fixed inset-0 bg-black/50 z-[70] flex items-center justify-center p-4 backdrop-blur-sm animate-in fade-in duration-200"
      onClick={onClose}
    >
      <div
        className={`bg-white rounded-2xl w-full ${maxWidth} p-6 shadow-2xl animate-in zoom-in-95 duration-200 border border-gray-100`}
        onClick={(e) => e.stopPropagation()}
      >
        {title && (
          <div className="flex justify-between items-center mb-6 pb-4 border-b">
            <h3 className="text-lg font-bold text-gray-900 flex items-center gap-2">
              {icon}
              {title}
            </h3>
            <button
              onClick={onClose}
              className="p-2 hover:bg-gray-100 rounded-full"
            >
              <X size={20} />
            </button>
          </div>
        )}
        {children}
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/ui/PdfViewerModal.tsx">
import { useState, useCallback, useRef } from 'react';
import { Document, Page, pdfjs } from 'react-pdf';
import { X, ChevronLeft, ChevronRight, ZoomIn, ZoomOut, Loader2 } from 'lucide-react';
import 'react-pdf/dist/Page/AnnotationLayer.css';
import 'react-pdf/dist/Page/TextLayer.css';

// PDF.js worker — 使用 CDN 加载以兼容 Vite
pdfjs.GlobalWorkerOptions.workerSrc = `//unpkg.com/pdfjs-dist@${pdfjs.version}/build/pdf.worker.min.mjs`;

interface PdfViewerModalProps {
  open: boolean;
  onClose: () => void;
  /** PDF 文件 URL（通常为 /api/graph/pdf/{paper_id}） */
  pdfUrl: string;
  /** 高亮所在页码（1-based） */
  pageNumber?: number;
  /** Docling bbox 坐标 [x0, y0, x1, y1]（PDF 物理坐标） */
  bbox?: number[];
  /** 标题（用于显示） */
  title?: string;
}

export function PdfViewerModal({
  open,
  onClose,
  pdfUrl,
  pageNumber = 1,
  bbox,
  title,
}: PdfViewerModalProps) {
  const [numPages, setNumPages] = useState<number>(0);
  const [currentPage, setCurrentPage] = useState<number>(pageNumber);
  const [scale, setScale] = useState<number>(1.2);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);
  const pageRef = useRef<HTMLDivElement>(null);

  // PDF 原始页面尺寸（由 onRenderSuccess 获取）
  const [pageSize, setPageSize] = useState<{ width: number; height: number } | null>(null);

  const onDocumentLoadSuccess = useCallback(({ numPages: total }: { numPages: number }) => {
    setNumPages(total);
    setCurrentPage(Math.min(pageNumber, total));
    setLoading(false);
    setError(null);
  }, [pageNumber]);

  const onDocumentLoadError = useCallback(() => {
    setLoading(false);
    setError('PDF 加载失败，请确认文件存在。');
  }, []);

  const onPageRenderSuccess = useCallback((page: { originalWidth: number; originalHeight: number }) => {
    setPageSize({ width: page.originalWidth, height: page.originalHeight });
  }, []);

  const goPrev = () => setCurrentPage((p) => Math.max(1, p - 1));
  const goNext = () => setCurrentPage((p) => Math.min(numPages, p + 1));
  const zoomIn = () => setScale((s) => Math.min(3, s + 0.2));
  const zoomOut = () => setScale((s) => Math.max(0.5, s - 0.2));

  if (!open) return null;

  // 计算高亮覆盖层：将 bbox 物理坐标按比例映射到渲染尺寸
  const renderHighlights = () => {
    if (!bbox || bbox.length < 4 || !pageSize) return null;
    // 仅在目标页显示高亮
    if (currentPage !== (pageNumber || 1)) return null;

    const renderedWidth = pageSize.width * scale;
    const renderedHeight = pageSize.height * scale;

    {
      const [x0, y0, x1, y1] = bbox;

      // bbox 是 PDF 物理坐标（单位：pt），页面原始尺寸也是 pt
      const left = (x0 / pageSize.width) * renderedWidth;
      const top = (y0 / pageSize.height) * renderedHeight;
      const width = ((x1 - x0) / pageSize.width) * renderedWidth;
      const height = ((y1 - y0) / pageSize.height) * renderedHeight;

      return (
        <div
          className="absolute bg-yellow-400/40 border border-yellow-500/60 rounded-sm pointer-events-none"
          style={{ left, top, width, height }}
        />
      );
    }
  };

  return (
    <div
      className="fixed inset-0 bg-black/60 z-[80] flex items-center justify-center p-4 backdrop-blur-sm animate-in fade-in duration-200"
      onClick={onClose}
    >
      <div
        className="bg-slate-900 rounded-2xl w-full max-w-4xl max-h-[90vh] flex flex-col shadow-2xl animate-in zoom-in-95 duration-200 border border-slate-700/50"
        onClick={(e) => e.stopPropagation()}
      >
        {/* Header */}
        <div className="flex items-center justify-between px-5 py-3 border-b border-slate-700/50 shrink-0">
          <div className="flex items-center gap-3 min-w-0">
            <span className="text-lg">📄</span>
            <div className="min-w-0">
              <h3 className="text-sm font-semibold text-slate-200 truncate">
                {title || 'PDF 溯源'}
              </h3>
              {numPages > 0 && (
                <p className="text-xs text-slate-500">
                  第 {currentPage} / {numPages} 页
                </p>
              )}
            </div>
          </div>
          <div className="flex items-center gap-1">
            {/* Zoom controls */}
            <button
              onClick={zoomOut}
              className="p-1.5 text-slate-400 hover:text-sky-400 hover:bg-slate-800 rounded-lg transition-colors"
              title="缩小"
            >
              <ZoomOut size={16} />
            </button>
            <span className="text-xs text-slate-500 w-12 text-center">{Math.round(scale * 100)}%</span>
            <button
              onClick={zoomIn}
              className="p-1.5 text-slate-400 hover:text-sky-400 hover:bg-slate-800 rounded-lg transition-colors"
              title="放大"
            >
              <ZoomIn size={16} />
            </button>
            {/* Page navigation */}
            <div className="w-px h-5 bg-slate-700 mx-1" />
            <button
              onClick={goPrev}
              disabled={currentPage <= 1}
              className="p-1.5 text-slate-400 hover:text-sky-400 hover:bg-slate-800 rounded-lg transition-colors disabled:opacity-30"
              title="上一页"
            >
              <ChevronLeft size={16} />
            </button>
            <button
              onClick={goNext}
              disabled={currentPage >= numPages}
              className="p-1.5 text-slate-400 hover:text-sky-400 hover:bg-slate-800 rounded-lg transition-colors disabled:opacity-30"
              title="下一页"
            >
              <ChevronRight size={16} />
            </button>
            <div className="w-px h-5 bg-slate-700 mx-1" />
            <button
              onClick={onClose}
              className="p-1.5 text-slate-400 hover:text-red-400 hover:bg-slate-800 rounded-lg transition-colors"
              title="关闭"
            >
              <X size={16} />
            </button>
          </div>
        </div>

        {/* PDF Content */}
        <div className="flex-1 overflow-auto flex items-start justify-center p-4 bg-slate-950/50">
          {error ? (
            <div className="text-red-400 text-sm mt-12">{error}</div>
          ) : (
            <Document
              file={pdfUrl}
              onLoadSuccess={onDocumentLoadSuccess}
              onLoadError={onDocumentLoadError}
              loading={
                <div className="flex items-center gap-2 text-slate-400 mt-12">
                  <Loader2 size={18} className="animate-spin" />
                  <span className="text-sm">加载 PDF 中...</span>
                </div>
              }
            >
              {!loading && (
                <div ref={pageRef} className="relative inline-block">
                  <Page
                    pageNumber={currentPage}
                    scale={scale}
                    onRenderSuccess={(page: unknown) => {
                      const p = page as { originalWidth: number; originalHeight: number };
                      onPageRenderSuccess(p);
                    }}
                    loading={
                      <div className="flex items-center gap-2 text-slate-400">
                        <Loader2 size={16} className="animate-spin" />
                        <span className="text-sm">渲染中...</span>
                      </div>
                    }
                  />
                  {/* Bbox highlight overlays */}
                  {renderHighlights()}
                </div>
              )}
            </Document>
          )}
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/ui/Toast.tsx">
import { useToastStore } from '../../stores';

export function ToastContainer() {
  const toasts = useToastStore((s) => s.toasts);

  return (
    <div className="fixed bottom-5 right-5 z-50 flex flex-col gap-2 pointer-events-none">
      {toasts.map((t) => (
        <div
          key={t.id}
          className={`px-4 py-3 rounded-lg shadow-lg text-sm font-medium animate-in slide-in-from-right fade-in duration-300 pointer-events-auto ${
            t.type === 'success'
              ? 'bg-green-600 text-white'
              : t.type === 'error'
              ? 'bg-red-600 text-white'
              : t.type === 'warning'
              ? 'bg-amber-600 text-white'
              : 'bg-gray-800 text-white'
          }`}
        >
          {t.msg}
        </div>
      ))}
    </div>
  );
}
</file>

<file path="frontend/src/components/workflow/CommandPalette.tsx">
import { useEffect, useRef } from 'react';
import { Command } from 'lucide-react';
import { useTranslation } from 'react-i18next';
import { useChatStore } from '../../stores';
import { COMMAND_LIST, type CommandDefinition } from '../../types';

interface CommandPaletteProps {
  inputValue: string;
  onSelectCommand: (command: CommandDefinition) => void;
}

export function CommandPalette({ inputValue, onSelectCommand }: CommandPaletteProps) {
  const { t } = useTranslation();
  const { showCommandPalette, setShowCommandPalette } = useChatStore();
  const paletteRef = useRef<HTMLDivElement>(null);

  // 检测是否应该显示命令面板
  const shouldShow = inputValue.startsWith('/');
  const filterText = inputValue.slice(1).toLowerCase();
  
  // 过滤匹配的命令
  const filteredCommands = shouldShow
    ? COMMAND_LIST.filter(cmd => 
        cmd.command.toLowerCase().includes(filterText) ||
        t(cmd.label).toLowerCase().includes(filterText)
      )
    : [];

  // 同步显示状态
  useEffect(() => {
    if (shouldShow && filteredCommands.length > 0) {
      setShowCommandPalette(true);
    } else {
      setShowCommandPalette(false);
    }
  }, [shouldShow, filteredCommands.length, setShowCommandPalette]);

  // 点击外部关闭
  useEffect(() => {
    function handleClickOutside(event: MouseEvent) {
      if (paletteRef.current && !paletteRef.current.contains(event.target as Node)) {
        setShowCommandPalette(false);
      }
    }
    if (showCommandPalette) {
      document.addEventListener('mousedown', handleClickOutside);
      return () => document.removeEventListener('mousedown', handleClickOutside);
    }
  }, [showCommandPalette, setShowCommandPalette]);

  if (!showCommandPalette || filteredCommands.length === 0) {
    return null;
  }

  return (
    <div
      ref={paletteRef}
      className="absolute bottom-full left-0 right-0 mb-2 bg-slate-900/95 backdrop-blur-md rounded-xl shadow-2xl border border-slate-700/50 overflow-hidden z-50 animate-in fade-in slide-in-from-bottom-2 duration-150"
    >
      <div className="px-3 py-2 bg-slate-800/50 border-b border-slate-700/50 flex items-center gap-2">
        <Command size={14} className="text-sky-500" />
        <span className="text-xs font-medium text-slate-400">{t('commands.availableCommands')}</span>
      </div>
      
      <div className="max-h-64 overflow-y-auto scrollbar-thin">
        {filteredCommands.map((cmd) => (
          <button
            key={cmd.command}
            onClick={() => onSelectCommand(cmd)}
            className="w-full flex items-start gap-3 px-4 py-3 text-left hover:bg-sky-900/20 transition-colors cursor-pointer border-b border-slate-800/50 last:border-b-0"
          >
            <code className="px-2 py-0.5 bg-slate-800 text-sky-400 rounded text-sm font-mono border border-slate-700/50">
              {cmd.command}
            </code>
            <div className="flex-1 min-w-0">
              <div className="font-medium text-slate-200 text-sm">{t(cmd.label)}</div>
              <div className="text-xs text-slate-400">{t(cmd.description)}</div>
              {cmd.example && (
                <div className="mt-1 text-xs text-slate-500 font-mono truncate">
                  {t('commands.example')} {cmd.example}
                </div>
              )}
            </div>
          </button>
        ))}
      </div>
      
      <div className="px-3 py-2 bg-slate-800/50 border-t border-slate-700/50">
        <div className="text-xs text-slate-500">
          {t('commands.press')} <kbd className="px-1 py-0.5 bg-slate-700 rounded text-slate-300 border border-slate-600">Tab</kbd> {t('commands.orClickToSelect')}
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/workflow/index.ts">
export { WorkflowStepper } from './WorkflowStepper';
export { CommandPalette } from './CommandPalette';
export { DeepResearchDialog } from './DeepResearchDialog';
export { DeepResearchSettingsPopover } from './DeepResearchSettingsPopover';
// Legacy exports (kept for backward compatibility, can be removed later)
export { IntentModeSelector } from './IntentModeSelector';
export { IntentConfirmPopover } from './IntentConfirmPopover';
</file>

<file path="frontend/src/components/workflow/IntentConfirmPopover.tsx">
/**
 * @deprecated 已被 DeepResearchDialog 替代。保留空壳避免其他文件导入报错。
 */
export function IntentConfirmPopover(_props: { onConfirm?: (v: any) => void; onCancel?: () => void }) {
  return null;
}
</file>

<file path="frontend/src/components/workflow/IntentModeSelector.tsx">
/**
 * @deprecated 已被 Deep Research 按钮替代。保留空壳避免其他文件导入报错。
 */
export function IntentModeSelector() {
  return null;
}
</file>

<file path="frontend/src/components/workflow/WorkflowStepper.tsx">
import { useTranslation } from 'react-i18next';
import { useChatStore } from '../../stores';
import { WORKFLOW_STAGES } from '../../types';

export function WorkflowStepper() {
  const { t } = useTranslation();
  const { workflowStep } = useChatStore();
  
  // idle 状态不显示
  if (workflowStep === 'idle') {
    return null;
  }

  const currentIndex = WORKFLOW_STAGES.findIndex(s => s.id === workflowStep);

  return (
    <div className="flex items-center justify-center gap-2 py-3 px-4 bg-slate-900/60 backdrop-blur-sm border-b border-slate-700/50">
      {WORKFLOW_STAGES.map((stage, index) => {
        const isActive = stage.id === workflowStep;
        const isCompleted = index < currentIndex;

        return (
          <div key={stage.id} className="flex items-center">
            {/* 连接线 */}
            {index > 0 && (
              <div
                className={`w-8 h-0.5 mx-1 transition-colors ${
                  isCompleted ? 'bg-emerald-500' : 'bg-slate-700'
                }`}
              />
            )}
            
            {/* 阶段节点 */}
            <div
              className={`
                flex items-center gap-2 px-3 py-1.5 rounded-full text-sm font-medium transition-all
                ${isActive 
                  ? 'bg-sky-900/40 text-sky-400 ring-2 ring-sky-500/30 ring-offset-1 ring-offset-slate-900' 
                  : isCompleted 
                    ? 'bg-emerald-900/20 text-emerald-400' 
                    : 'bg-slate-800/50 text-slate-500'
                }
              `}
              title={t(stage.description)}
            >
              <span className="text-base">{stage.icon}</span>
              <span className="hidden sm:inline">{t(stage.label)}</span>
              {isActive && (
                <span className="w-1.5 h-1.5 bg-sky-400 rounded-full animate-pulse shadow-[0_0_6px_rgba(56,189,248,0.8)]" />
              )}
            </div>
          </div>
        );
      })}
    </div>
  );
}
</file>

<file path="frontend/src/i18n/index.ts">
import i18n from 'i18next';
import { initReactI18next } from 'react-i18next';
import LanguageDetector from 'i18next-browser-languagedetector';

import zh from './locales/zh.json';
import en from './locales/en.json';

i18n
  .use(LanguageDetector)
  .use(initReactI18next)
  .init({
    resources: {
      zh: { translation: zh },
      en: { translation: en },
    },
    fallbackLng: 'zh',
    interpolation: {
      escapeValue: false, // React already escapes
    },
    detection: {
      order: ['localStorage', 'navigator'],
      lookupLocalStorage: 'rag_language',
      caches: ['localStorage'],
    },
  });

export default i18n;
</file>

<file path="frontend/src/pages/AdminPage.tsx">
import { useState, useEffect } from 'react';
import { useTranslation } from 'react-i18next';
import { Shield, Users, UserPlus, Trash2 } from 'lucide-react';
import { useUIStore, useToastStore } from '../stores';
import { listUsers, createUser } from '../api/auth';
import { Modal } from '../components/ui/Modal';
import type { UserItem } from '../types';

export function AdminPage() {
  const { t } = useTranslation();
  const { showUserModal, setShowUserModal } = useUIStore();
  const addToast = useToastStore((s) => s.addToast);

  const [users, setUsers] = useState<UserItem[]>([]);
  const [, setIsLoading] = useState(false);
  const [newUsername, setNewUsername] = useState('');
  const [newPassword, setNewPassword] = useState('');
  const [newRole, setNewRole] = useState('user');

  const fetchUsers = async () => {
    setIsLoading(true);
    try {
      const data = await listUsers();
      setUsers(data);
    } catch {
      addToast(t('admin.fetchUsersFailed'), 'error');
    } finally {
      setIsLoading(false);
    }
  };

  useEffect(() => {
    fetchUsers();
  }, []);

  const handleCreateUser = async () => {
    if (!newUsername.trim() || !newPassword.trim()) return;

    try {
      await createUser({
        user_id: newUsername,
        password: newPassword,
        role: newRole,
      });
      addToast(t('admin.userCreated', { name: newUsername }), 'success');
      setShowUserModal(false);
      setNewUsername('');
      setNewPassword('');
      setNewRole('user');
      fetchUsers();
    } catch {
      addToast(t('admin.createFailed'), 'error');
    }
  };

  const handleDeleteUser = (userId: string) => {
    if (userId === 'admin') {
      addToast(t('admin.cannotDeleteAdmin'), 'error');
      return;
    }
    // 删除用户 API（需要后端支持）
    addToast(t('admin.deleteInDev'), 'info');
  };

  return (
    <div className="max-w-5xl mx-auto space-y-8 p-8 animate-in fade-in duration-300">
      <div className="flex justify-between items-end">
        <div>
          <h2 className="text-2xl font-bold text-slate-100">{t('admin.title')}</h2>
          <p className="text-slate-400 mt-1">{t('admin.subtitle')}</p>
        </div>
        <button
          onClick={() => setShowUserModal(true)}
          className="flex items-center gap-2 bg-sky-600 text-white px-5 py-2.5 rounded-xl font-medium shadow-lg shadow-sky-500/20 hover:bg-sky-500 transition-all"
        >
          <UserPlus size={18} /> {t('admin.createUser')}
        </button>
      </div>

      <div className="bg-slate-900/60 border border-slate-700/50 rounded-2xl overflow-hidden shadow-sm">
        <table className="w-full text-sm">
          <thead className="bg-slate-800/50">
            <tr className="text-left text-slate-400 border-b border-slate-700/50">
              <th className="px-6 py-4 font-medium">{t('admin.usernameRole')}</th>
              <th className="px-6 py-4 font-medium">{t('common.status')}</th>
              <th className="px-6 py-4 font-medium">{t('admin.createdTime')}</th>
              <th className="px-6 py-4 font-medium text-right">{t('admin.management')}</th>
            </tr>
          </thead>
          <tbody className="divide-y divide-slate-700/50">
            {users.map((user) => (
              <tr key={user.user_id} className="hover:bg-slate-800/30 transition-colors">
                <td className="px-6 py-4">
                  <div className="flex items-center gap-3">
                    <div
                      className={`w-8 h-8 rounded-full flex items-center justify-center ${
                        user.role === 'admin'
                          ? 'bg-purple-900/30 text-purple-400'
                          : 'bg-sky-900/30 text-sky-400'
                      }`}
                    >
                      {user.role === 'admin' ? (
                        <Shield size={14} />
                      ) : (
                        <Users size={14} />
                      )}
                    </div>
                    <div>
                      <div className="font-bold text-slate-200">{user.user_id}</div>
                      <div className="text-xs text-slate-400 capitalize">
                        {user.role}
                      </div>
                    </div>
                  </div>
                </td>
                <td className="px-6 py-4">
                  <span
                    className={`px-2.5 py-1 rounded-full text-[10px] font-bold uppercase tracking-wide ${
                      user.is_active
                        ? 'bg-emerald-900/20 text-emerald-400 border border-emerald-500/30'
                        : 'bg-slate-800 text-slate-500 border border-slate-700'
                    }`}
                  >
                    {user.is_active ? 'Active' : 'Inactive'}
                  </span>
                </td>
                <td className="px-6 py-4 text-slate-400 font-mono text-xs">
                  {user.created_at}
                </td>
                <td className="px-6 py-4 text-right">
                  <button
                    onClick={() => handleDeleteUser(user.user_id)}
                    className="text-slate-500 hover:text-red-400 hover:bg-red-900/20 p-2 rounded-lg transition-colors"
                    title={t('admin.deleteUser')}
                  >
                    <Trash2 size={16} />
                  </button>
                </td>
              </tr>
            ))}
          </tbody>
        </table>
      </div>

      {/* 新建用户 Modal */}
      <Modal
        open={showUserModal}
        onClose={() => setShowUserModal(false)}
        title={t('admin.createUser')}
        icon={<UserPlus size={20} />}
      >
        <form
          onSubmit={(e) => {
            e.preventDefault();
            handleCreateUser();
          }}
          className="space-y-4"
        >
          <div>
            <label className="text-xs font-medium text-slate-400 uppercase">
              {t('admin.username')}
            </label>
            <input
              value={newUsername}
              onChange={(e) => setNewUsername(e.target.value)}
              required
              type="text"
              className="w-full mt-1 bg-slate-900 border border-slate-700 text-slate-200 rounded-md p-2 text-sm focus:ring-1 focus:ring-sky-500 focus:border-sky-500 outline-none"
            />
          </div>
          <div>
            <label className="text-xs font-medium text-slate-400 uppercase">
              {t('admin.initialPassword')}
            </label>
            <input
              value={newPassword}
              onChange={(e) => setNewPassword(e.target.value)}
              required
              type="password"
              className="w-full mt-1 bg-slate-900 border border-slate-700 text-slate-200 rounded-md p-2 text-sm focus:ring-1 focus:ring-sky-500 focus:border-sky-500 outline-none"
            />
          </div>
          <div>
            <label className="text-xs font-medium text-slate-400 uppercase">
              {t('admin.rolePermission')}
            </label>
            <select
              value={newRole}
              onChange={(e) => setNewRole(e.target.value)}
              className="w-full mt-1 bg-slate-900 border border-slate-700 text-slate-200 rounded-md p-2 text-sm"
            >
              <option value="user">{t('admin.normalUser')}</option>
              <option value="admin">{t('admin.adminUser')}</option>
            </select>
          </div>
          <div className="mt-6 flex justify-end gap-2">
            <button
              type="button"
              onClick={() => setShowUserModal(false)}
              className="px-4 py-2 text-slate-400 hover:bg-slate-800 rounded-lg text-sm"
            >
              {t('common.cancel')}
            </button>
            <button
              type="submit"
              className="px-4 py-2 bg-sky-600 text-white rounded-lg hover:bg-sky-500 text-sm"
            >
              {t('admin.createAccount')}
            </button>
          </div>
        </form>
      </Modal>
    </div>
  );
}
</file>

<file path="frontend/src/pages/ChatPage.tsx">
import { useTranslation } from 'react-i18next';
import { Database, Server } from 'lucide-react';
import { useConfigStore, useToastStore } from '../stores';
import { ChatWindow } from '../components/chat/ChatWindow';
import { ChatInput } from '../components/chat/ChatInput';
import { WorkflowStepper } from '../components/workflow';
import { checkHealth } from '../api/health';

export function ChatPage() {
  const { t } = useTranslation();
  const { dbStatus, setDbStatus, dbAddress } = useConfigStore();
  const addToast = useToastStore((s) => s.addToast);

  const handleConnect = async () => {
    setDbStatus('connecting');
    addToast(t('chatPage.connectingTo', { address: dbAddress }), 'info');
    try {
      await checkHealth();
      setDbStatus('connected');
      addToast(t('chatPage.connected'), 'success');
    } catch {
      setDbStatus('disconnected');
      addToast(t('chatPage.connectFailed'), 'error');
    }
  };

  // 未连接状态
  if (dbStatus === 'disconnected') {
    return (
      <div className="absolute inset-0 flex flex-col items-center justify-center p-8 animate-in fade-in zoom-in duration-300 z-10 bg-slate-950/90 backdrop-blur-sm">
        <div className="bg-slate-900/80 backdrop-blur-md p-12 rounded-3xl shadow-2xl max-w-2xl w-full text-center border border-slate-700/50">
          <div className="w-20 h-20 bg-sky-900/30 text-sky-400 rounded-full flex items-center justify-center mx-auto mb-6 shadow-sm">
            <Database size={40} />
          </div>
          <h2 className="text-3xl font-bold text-slate-100 mb-3">{t('chatPage.connectTitle')}</h2>
          <p className="text-slate-400 mb-10 max-w-md mx-auto">
            {t('chatPage.connectDesc')}
          </p>
          <div className="max-w-xs mx-auto">
            <button
              onClick={handleConnect}
              className="w-full group p-4 rounded-xl border-2 border-sky-500 bg-sky-600 text-white hover:bg-sky-500 transition-all active:scale-95 cursor-pointer flex items-center justify-center gap-3 shadow-lg shadow-sky-500/20"
            >
              <Server size={20} />
              <span className="font-bold">{t('chatPage.connectBtn')}</span>
            </button>
          </div>
        </div>
      </div>
    );
  }

  return (
    <div className="flex flex-col h-full">
      {/* 工作流阶段指示器 */}
      <WorkflowStepper />
      
      {/* 聊天窗口 */}
      <main className="flex-1 overflow-y-auto bg-transparent p-8 scrollbar-thin scrollbar-thumb-gray-200">
        <ChatWindow />
      </main>
      
      {/* 输入框 */}
      <ChatInput />
    </div>
  );
}
</file>

<file path="frontend/src/pages/IngestPage.tsx">
import { useCallback, useEffect, useRef, useState } from 'react';
import {
  Database,
  HardDrive,
  UploadCloud,
  Plus,
  FileText,
  Trash2,
  FolderOpen,
  Loader2,
  CheckCircle2,
  AlertCircle,
  RefreshCw,
  ChevronLeft,
  ChevronRight,
} from 'lucide-react';
import { useConfigStore, useUIStore, useToastStore } from '../stores';
import { Modal } from '../components/ui/Modal';
import {
  listCollections,
  createCollection,
  deleteCollection,
  listPapers,
  deletePaper,
  uploadFiles,
  processFiles,
  cancelIngestJob,
  listIngestJobs,
  streamIngestJobEvents,
  listLLMProviders,
  type CollectionInfo,
  type IngestProgressEvent,
  type LLMProviderInfo,
  type PaperInfo,
  type UploadedFile,
  type EnrichmentOptions,
} from '../api/ingest';

// ---- Types ----

type FileStatus = 'pending' | 'uploading' | 'parsing' | 'chunking' | 'embedding' | 'indexing' | 'done' | 'error' | 'skipped';

interface FileItem {
  id: string;
  name: string;
  size: number;
  status: FileStatus;
  message?: string;
  chunks?: number;
  /** 上传后后端返回的路径 */
  serverPath?: string;
}

const STATUS_LABELS: Record<FileStatus, string> = {
  pending: '待处理',
  uploading: '上传中',
  parsing: '解析中',
  chunking: '切块中',
  embedding: '向量化中',
  indexing: '入库中',
  done: '完成',
  error: '失败',
  skipped: '已跳过',
};

const STATUS_COLORS: Record<FileStatus, string> = {
  pending: 'bg-gray-100 text-gray-500',
  uploading: 'bg-blue-50 text-blue-600',
  parsing: 'bg-blue-50 text-blue-600',
  chunking: 'bg-blue-50 text-blue-600',
  embedding: 'bg-purple-50 text-purple-600',
  indexing: 'bg-orange-50 text-orange-600',
  done: 'bg-green-50 text-green-600',
  error: 'bg-red-50 text-red-600',
  skipped: 'bg-gray-100 text-gray-500',
};

// ---- Collection Templates ----

const COLLECTION_TEMPLATES = [
  { name: 'deepsea_global', desc: '通用深海文献库' },
  { name: 'deepsea_life', desc: '深海生物与生态' },
  { name: 'deepsea_ocean', desc: '海洋学与地质' },
  { name: 'deepsea_env', desc: '深海环境与保护' },
];

// ---- Component ----

export function IngestPage() {
  const { dbAddress, setDbStatus, currentCollection, setCurrentCollection, setCollections, addCollection } =
    useConfigStore();
  const { showCreateCollectionModal, setShowCreateCollectionModal } = useUIStore();
  const addToast = useToastStore((s) => s.addToast);

  // Connection
  const [connectError, setConnectError] = useState('');

  // Collections from backend
  const [backendCollections, setBackendCollections] = useState<CollectionInfo[]>([]);
  const [collectionsLoading, setCollectionsLoading] = useState(false);

  // Papers in collection (持久化文件列表)
  const [papers, setPapers] = useState<PaperInfo[]>([]);
  const [papersLoading, setPapersLoading] = useState(false);
  const [papersPageSize, setPapersPageSize] = useState<10 | 20>(10);
  const [papersPage, setPapersPage] = useState(1);

  // Files
  const [files, setFiles] = useState<FileItem[]>([]);
  const [isProcessing, setIsProcessing] = useState(false);
  const [isCancelling, setIsCancelling] = useState(false);
  const [globalProgress, setGlobalProgress] = useState('');
  const [enrichment, setEnrichment] = useState<EnrichmentOptions>({
    enrich_tables: false,
    enrich_figures: false,
    llm_text_provider: null,
    llm_text_model: null,
    llm_text_concurrency: 1,
    llm_vision_provider: null,
    llm_vision_model: null,
    llm_vision_concurrency: 1,
  });
  const [llmProviders, setLlmProviders] = useState<LLMProviderInfo[]>([]);
  const [activeJobId, setActiveJobId] = useState<string | null>(null);
  /** 解析状态：表格/图片 enrich 进度，按文件名聚合，用于解析阶段展示 */
  const [enrichLogs, setEnrichLogs] = useState<Record<string, string[]>>({});
  const [newCollectionName, setNewCollectionName] = useState('');
  /** 上传后若发现已入库重复（按 content_hash），弹窗让用户选 跳过 / 覆盖 */
  const [duplicateModal, setDuplicateModal] = useState<{
    uploaded: UploadedFile[];
    duplicatePairs: { uploadedFile: UploadedFile; existingPaper: PaperInfo }[];
  } | null>(null);
  /** 对本次及之后所有重复项执行相同操作，不弹窗 */
  const [duplicateActionPreference, setDuplicateActionPreference] = useState<'skip' | 'overwrite' | null>(null);
  /** 弹窗内勾选「应用到所有类似」 */
  const [applyToAllSimilar, setApplyToAllSimilar] = useState(false);

  const fileInputRef = useRef<HTMLInputElement>(null);
  const folderInputRef = useRef<HTMLInputElement>(null);
  const abortRef = useRef<AbortController | null>(null);

  // ---- Auto-connect & load collections on mount ----
  const loadCollections = useCallback(async () => {
    setCollectionsLoading(true);
    setConnectError('');
    try {
      const cols = await listCollections();
      setBackendCollections(cols);
      setDbStatus('connected');
      // Sync to config store
      const names = cols.map((c) => c.name);
      setCollections(names);
      // If current collection not in list, select first
      if (names.length > 0 && !names.includes(currentCollection)) {
        setCurrentCollection(names[0]);
      }
    } catch (err) {
      console.error('Failed to load collections:', err);
      setConnectError(err instanceof Error ? err.message : '无法连接后端服务');
    } finally {
      setCollectionsLoading(false);
    }
  }, [currentCollection, setCollections, setCurrentCollection, setDbStatus, addToast]);

  const loadLlmProviderOptions = useCallback(async () => {
    try {
      const data = await listLLMProviders();
      setLlmProviders(data.providers || []);
      const defaults = data.parser_defaults || {};
      setEnrichment((prev) => ({
        ...prev,
        llm_text_provider: defaults.llm_text_provider ?? prev.llm_text_provider,
        llm_text_model: defaults.llm_text_model ?? prev.llm_text_model,
        llm_text_concurrency:
          typeof defaults.llm_text_concurrency === 'number'
            ? defaults.llm_text_concurrency
            : (prev.llm_text_concurrency ?? 1),
        llm_vision_provider: defaults.llm_vision_provider ?? prev.llm_vision_provider,
        llm_vision_model: defaults.llm_vision_model ?? prev.llm_vision_model,
        llm_vision_concurrency:
          typeof defaults.llm_vision_concurrency === 'number'
            ? defaults.llm_vision_concurrency
            : (prev.llm_vision_concurrency ?? 1),
      }));
    } catch (err) {
      console.warn('Failed to load llm providers:', err);
    }
  }, []);

  useEffect(() => {
    loadCollections();
    loadLlmProviderOptions();
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  useEffect(() => {
    let cancelled = false;
    const reconnectCtrl = new AbortController();

    const reconnectRunningJob = async () => {
      try {
        const savedJobId = localStorage.getItem('ingest_active_job_id');
        let targetJobId: string | null = savedJobId;
        if (!targetJobId) {
          const running = await listIngestJobs(1, 'running');
          targetJobId = running[0]?.job_id || null;
        }
        if (!targetJobId || cancelled) return;
        setActiveJobId(targetJobId);
        setIsProcessing(true);
        setGlobalProgress('已恢复后台任务进度...');
        abortRef.current = reconnectCtrl;
        addToast(`已连接后台任务 ${targetJobId.slice(0, 8)}`, 'info');
        for await (const evt of streamIngestJobEvents(targetJobId, reconnectCtrl.signal, 0)) {
          if (cancelled) break;
          applyIngestEvent(evt);
        }
      } catch (e) {
        if ((e as Error).name !== 'AbortError') {
          localStorage.removeItem('ingest_active_job_id');
        }
      } finally {
        if (!cancelled) {
          setIsProcessing(false);
          if (!activeJobId) {
            abortRef.current = null;
          }
        }
      }
    };

    reconnectRunningJob();
    return () => {
      cancelled = true;
      reconnectCtrl.abort();
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, []);

  // ---- Load papers when collection changes ----
  const loadPapers = useCallback(async (): Promise<PaperInfo[]> => {
    if (!currentCollection) { setPapers([]); setPapersPage(1); return []; }
    setPapersLoading(true);
    try {
      const list = await listPapers(currentCollection);
      setPapers(list);
      setPapersPage(1);
      return list;
    } catch {
      setPapers([]);
      return [];
    } finally {
      setPapersLoading(false);
    }
  }, [currentCollection]);

  useEffect(() => {
    loadPapers();
  }, [loadPapers]);

  // ---- Delete paper from collection ----
  const handleDeletePaper = async (paperId: string, filename: string) => {
    if (!window.confirm(`确定删除「${filename || paperId}」？\n将同时删除该文件在集合中的所有向量数据，不可恢复。`)) {
      return;
    }
    addToast(`正在删除 ${filename || paperId}...`, 'info');
    try {
      const res = await deletePaper(currentCollection, paperId);
      addToast(`已删除 ${filename || paperId} (${res.deleted_chunks} chunks)`, 'success');
      loadPapers();
      loadCollections(); // 刷新集合记录数
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : String(err);
      addToast(`删除失败: ${msg}`, 'error');
    }
  };

  // ---- File selection helpers ----

  const addFilesToList = (fileList: FileList | File[]) => {
    const arr = Array.from(fileList);
    const newItems: FileItem[] = arr
      .filter((f) => f.name.toLowerCase().endsWith('.pdf'))
      .map((f) => ({
        id: `${f.name}-${f.size}-${Date.now()}-${Math.random().toString(36).slice(2, 6)}`,
        name: f.name,
        size: f.size,
        status: 'pending' as FileStatus,
      }));
    if (newItems.length === 0) {
      addToast('未找到 PDF 文件', 'info');
      return;
    }
    setFiles((prev) => [...prev, ...newItems]);
    // Store raw File objects in a map for upload
    for (const item of newItems) {
      rawFileMap.current.set(item.id, arr.find((f) => f.name === item.name)!);
    }
  };

  const rawFileMap = useRef<Map<string, File>>(new Map());

  const handleSelectFiles = () => fileInputRef.current?.click();
  const handleSelectFolder = () => folderInputRef.current?.click();

  const handleFileInputChange = (e: React.ChangeEvent<HTMLInputElement>) => {
    if (e.target.files) addFilesToList(e.target.files);
    e.target.value = ''; // reset so same files can be re-selected
  };

  const handleRemoveFile = (id: string) => {
    setFiles((prev) => prev.filter((f) => f.id !== id));
    rawFileMap.current.delete(id);
  };

  const removeFilesByStatuses = (statuses: FileStatus[]) => {
    const target = new Set(statuses);
    setFiles((prev) => {
      const removeIds = prev.filter((f) => target.has(f.status)).map((f) => f.id);
      for (const id of removeIds) {
        rawFileMap.current.delete(id);
      }
      return prev.filter((f) => !target.has(f.status));
    });
  };

  const handleClearDone = () => {
    removeFilesByStatuses(['done', 'skipped']);
  };

  const handleRetryFailed = () => {
    setFiles((prev) =>
      prev.map((f) => (f.status === 'error' ? { ...f, status: 'pending' as FileStatus, message: undefined } : f)),
    );
  };

  const handleClearFailed = () => {
    removeFilesByStatuses(['error']);
  };

  const handleClearPending = () => {
    removeFilesByStatuses(['pending']);
  };

  // ---- Drag & Drop ----

  const [isDragOver, setIsDragOver] = useState(false);

  const handleDrop = (e: React.DragEvent) => {
    e.preventDefault();
    setIsDragOver(false);
    if (e.dataTransfer.files) addFilesToList(e.dataTransfer.files);
  };

  // ---- Upload & Process ----
  const applyIngestEvent = (evt: IngestProgressEvent) => {
    const d = evt.data as Record<string, unknown>;
    switch (evt.event) {
      case 'job_created': {
        const jobId = d.job_id as string | undefined;
        if (jobId) {
          setActiveJobId(jobId);
          localStorage.setItem('ingest_active_job_id', jobId);
        }
        break;
      }
      case 'start':
        setGlobalProgress(`开始处理 ${d.total} 个文件...`);
        setEnrichLogs({});
        break;
      case 'enrich_progress': {
        const file = d.file as string;
        const kind = (d.kind as string) || 'table';
        const index = d.index as number;
        const total = d.total as number;
        const status = d.status as string;
        const message = d.message as string | undefined;
        const kindLabel = kind === 'figure' ? '图片' : '表格';
        const statusLabel =
          status === 'success'
            ? '完成'
            : status === 'skip'
              ? '跳过'
              : status === 'start'
                ? '进行中'
                : '失败';
        const line = message
          ? `${kindLabel} ${index}/${total} ${statusLabel}: ${message}`
          : `${kindLabel} ${index}/${total} ${statusLabel}`;
        setEnrichLogs((prev) => {
          const list = [...(prev[file] || []), line].slice(-30);
          return { ...prev, [file]: list };
        });
        break;
      }
      case 'progress':
      case 'heartbeat': {
        const fname = d.file as string;
        const stage = d.stage as FileStatus;
        const message = d.message as string;
        // 后端 events 流的连接保活 heartbeat 仅包含 {job_id, message}
        // 不应当落入文件状态更新，否则会生成空文件行。
        if (fname && stage) {
          updateFileStatusByName(fname, stage, message);
        }
        if (message) setGlobalProgress(message);
        break;
      }
      case 'file_done': {
        const fname = d.file as string;
        const chunks = (d.chunks as number) || 0;
        updateFileStatusByName(fname, 'done', `完成 (${chunks} chunks)`);
        break;
      }
      case 'file_error': {
        const fname = d.file as string;
        const errMsg = d.error as string;
        updateFileStatusByName(fname, 'error', errMsg);
        break;
      }
      case 'error':
        addToast(`处理错误: ${d.message}`, 'error');
        break;
      case 'done':
        if (d.cancelled) {
          setGlobalProgress('任务已取消');
          addToast('处理任务已取消', 'info');
        } else {
          setGlobalProgress(`完成: ${d.total_upserted} 条记录入库 (${d.total_chunks} chunks)`);
          addToast(`入库完成: ${d.total_upserted} 条记录写入 ${currentCollection}`, 'success');
        }
        loadCollections();
        loadPapers();
        setActiveJobId(null);
        localStorage.removeItem('ingest_active_job_id');
        setIsCancelling(false);
        break;
      case 'cancelled':
        setGlobalProgress('任务已取消');
        setFiles((prev) =>
          prev.map((f) =>
            ['uploading', 'parsing', 'chunking', 'embedding', 'indexing'].includes(f.status)
              ? { ...f, status: 'pending' as FileStatus, message: '已取消，可重新开始' }
              : f,
          ),
        );
        setActiveJobId(null);
        localStorage.removeItem('ingest_active_job_id');
        setIsCancelling(false);
        break;
    }
  };

  const handleStartProcess = async () => {
    console.log('[IngestPage] handleStartProcess triggered');
    const pendingFiles = files.filter((f) => f.status === 'pending');
    console.log('[IngestPage] pendingFiles:', pendingFiles.length, 'rawFileMap keys:', Array.from(rawFileMap.current.keys()));
    if (pendingFiles.length === 0) {
      addToast('没有待处理的文件', 'info');
      return;
    }

    setIsProcessing(true);
    const abortCtrl = new AbortController();
    abortRef.current = abortCtrl;

    try {
      // Step 1: Upload files to backend
      setGlobalProgress('上传文件...');
      for (const pf of pendingFiles) {
        updateFileStatus(pf.id, 'uploading', '上传中...');
      }

      const rawFiles = pendingFiles
        .map((pf) => {
          const f = rawFileMap.current.get(pf.id);
          console.log('[IngestPage] rawFileMap lookup:', pf.id, '->', f ? f.name : 'NOT FOUND');
          return f;
        })
        .filter(Boolean) as File[];

      console.log('[IngestPage] rawFiles count:', rawFiles.length, 'collection:', currentCollection);

      if (rawFiles.length === 0) {
        addToast('无法读取文件，请重新选择', 'error');
        setIsProcessing(false);
        return;
      }

      let uploaded: UploadedFile[];
      try {
        console.log('[IngestPage] calling uploadFiles...');
        uploaded = await uploadFiles(rawFiles, currentCollection);
        console.log('[IngestPage] upload success:', uploaded);
      } catch (err: unknown) {
        console.error('[IngestPage] upload failed:', err);
        const msg = err instanceof Error ? err.message : String(err);
        addToast(`上传失败: ${msg}`, 'error');
        for (const pf of pendingFiles) {
          updateFileStatus(pf.id, 'error', `上传失败: ${msg}`);
        }
        setIsProcessing(false);
        return;
      }

      const pathMap: Record<string, string> = {};
      for (const u of uploaded) {
        pathMap[u.filename] = u.path;
      }
      setFiles((prev) =>
        prev.map((f) => {
          if (pathMap[f.name]) {
            return { ...f, serverPath: pathMap[f.name], status: 'parsing' as FileStatus, message: '等待处理...' };
          }
          return f;
        }),
      );

      // 检测已入库重复（按 content_hash），先刷新列表
      const existingPapers = await loadPapers();
      const existingHashes = new Set(
        existingPapers.map((p) => p.content_hash).filter((h): h is string => Boolean(h)),
      );
      const duplicatePairs = uploaded
        .filter((u) => u.content_hash && existingHashes.has(u.content_hash))
        .map((u) => ({
          uploadedFile: u,
          existingPaper: existingPapers.find((p) => p.content_hash === u.content_hash)!,
        }))
        .filter((d) => d.existingPaper);
      if (duplicatePairs.length > 0) {
        const modalData = { uploaded, duplicatePairs };
        if (duplicateActionPreference) {
          setDuplicateModal(null);
          await runProcessAfterDuplicateChoice(duplicateActionPreference, modalData);
          return;
        }
        setIsProcessing(false);
        setDuplicateModal(modalData);
        return;
      }

      // Step 2: Process (SSE)
      const filePaths = uploaded.map((u) => u.path);
      const contentHashes = uploaded.reduce<Record<string, string>>((acc, u) => {
        if (u.content_hash) acc[u.path] = u.content_hash;
        return acc;
      }, {});
      setGlobalProgress('处理中...');
      for await (const evt of processFiles(filePaths, currentCollection, enrichment, abortCtrl.signal, contentHashes)) {
        console.log('[IngestPage] SSE event:', evt.event, evt.data);
        applyIngestEvent(evt);
      }
    } catch (err: unknown) {
      console.error('[IngestPage] process error:', err);
      if ((err as Error).name !== 'AbortError') {
        const msg = err instanceof Error ? err.message : String(err);
        addToast(`处理失败: ${msg}`, 'error');
        setGlobalProgress(`失败: ${msg}`);
      }
    } finally {
      setIsProcessing(false);
      setIsCancelling(false);
      abortRef.current = null;
    }
  };

  /** 用户选择 跳过/覆盖 后执行；modalData 可由调用方传入（自动应用偏好时） */
  const runProcessAfterDuplicateChoice = async (
    choice: 'skip' | 'overwrite',
    modalData?: { uploaded: UploadedFile[]; duplicatePairs: { uploadedFile: UploadedFile; existingPaper: PaperInfo }[] },
  ) => {
    const data = modalData ?? duplicateModal;
    if (!data) return;
    const { uploaded, duplicatePairs } = data;
    if (!modalData) setDuplicateModal(null);
    if (applyToAllSimilar && !modalData) setDuplicateActionPreference(choice);
    const dupPaths = new Set(duplicatePairs.map((d) => d.uploadedFile.path));
    const contentHashesAll = uploaded.reduce<Record<string, string>>((acc, u) => {
      if (u.content_hash) acc[u.path] = u.content_hash;
      return acc;
    }, {});

    if (choice === 'skip') {
      const toProcess = uploaded.filter((u) => !dupPaths.has(u.path));
      const toProcessPaths = toProcess.map((u) => u.path);
      setFiles((prev) =>
        prev.map((f) => {
          if (f.serverPath && dupPaths.has(f.serverPath)) return { ...f, status: 'skipped' as FileStatus, message: '已存在，已跳过' };
          return f;
        }),
      );
      if (toProcessPaths.length === 0) {
        addToast('全部为重复文件，未入库', 'info');
        return;
      }
      setGlobalProgress('处理中...');
      setIsProcessing(true);
      const abortCtrl = new AbortController();
      abortRef.current = abortCtrl;
      try {
        const contentHashes = toProcess.reduce<Record<string, string>>((acc, u) => {
          if (u.content_hash) acc[u.path] = u.content_hash;
          return acc;
        }, {});
        for await (const evt of processFiles(toProcessPaths, currentCollection, enrichment, abortCtrl.signal, contentHashes)) {
          applyIngestEvent(evt);
        }
      } catch (e) {
        if ((e as Error).name !== 'AbortError') addToast(`处理失败: ${(e as Error).message}`, 'error');
      } finally {
        setIsProcessing(false);
        setIsCancelling(false);
        abortRef.current = null;
      }
      return;
    }

    // choice === 'overwrite': 按已存在 paper_id 删除再全量 process
    setGlobalProgress('正在删除已存在记录...');
    setIsProcessing(true);
    const abortCtrl = new AbortController();
    abortRef.current = abortCtrl;
    try {
      for (const d of duplicatePairs) {
        await deletePaper(currentCollection, d.existingPaper.paper_id);
      }
      loadPapers();
      setGlobalProgress('处理中...');
      const filePaths = uploaded.map((u) => u.path);
      for await (const evt of processFiles(filePaths, currentCollection, enrichment, abortCtrl.signal, contentHashesAll)) {
        applyIngestEvent(evt);
      }
    } catch (e) {
      if ((e as Error).name !== 'AbortError') addToast(`处理失败: ${(e as Error).message}`, 'error');
    } finally {
      setIsProcessing(false);
      setIsCancelling(false);
      abortRef.current = null;
    }
  };


  const handleAbort = async () => {
    if (isCancelling) return;
    setIsCancelling(true);
    try {
      if (activeJobId) {
        await cancelIngestJob(activeJobId);
        setGlobalProgress('取消请求已发送，正在停止后台任务...');
        addToast('正在取消后台任务...', 'info');
      } else {
        abortRef.current?.abort();
        setIsProcessing(false);
        setGlobalProgress('已停止处理');
        addToast('已停止处理', 'info');
        setIsCancelling(false);
      }
    } catch (err) {
      const msg = err instanceof Error ? err.message : String(err);
      addToast(`取消失败: ${msg}`, 'error');
      setIsCancelling(false);
    }
  };

  // ---- Helpers to update file status ----

  const updateFileStatus = (id: string, status: FileStatus, message?: string) => {
    setFiles((prev) =>
      prev.map((f) => (f.id === id ? { ...f, status, message } : f)),
    );
  };

  const updateFileStatusByName = (name: string, status: FileStatus, message?: string) => {
    if (!name || !status) return;
    setFiles((prev) => {
      let found = false;
      const mapped = prev.map((f) => {
        const serverName = f.serverPath ? f.serverPath.split(/[/\\]/).pop() : '';
        // 优先匹配展示名；如果后端为重名文件加了后缀，则匹配 serverPath basename
        if (f.name === name || serverName === name) {
          found = true;
          if (f.status !== 'done' && f.status !== 'error' && f.status !== 'skipped') {
            return { ...f, status, message };
          }
        }
        return f;
      });
      if (!found) {
        mapped.push({
          id: `resume-${name}-${Date.now()}-${Math.random().toString(36).slice(2, 6)}`,
          name,
          size: 0,
          status,
          message,
        });
      }
      return mapped;
    });
  };

  // ---- Create collection ----

  const handleCreateCollection = async () => {
    if (!newCollectionName.trim()) return;
    setShowCreateCollectionModal(false);
    addToast(`正在创建集合: ${newCollectionName}...`, 'info');
    try {
      await createCollection(newCollectionName.trim());
      addCollection(newCollectionName.trim());
      addToast(`集合 ${newCollectionName} 创建成功`, 'success');
      setNewCollectionName('');
      loadCollections();
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : String(err);
      addToast(`创建失败: ${msg}`, 'error');
    }
  };

  // ---- Delete collection ----
  const handleDeleteCollection = async (name: string) => {
    if (!window.confirm(`确定要删除集合「${name}」？此操作不可恢复，集合内所有数据将被永久删除。`)) {
      return;
    }
    addToast(`正在删除集合: ${name}...`, 'info');
    try {
      await deleteCollection(name);
      addToast(`集合 ${name} 已删除`, 'success');
      // If deleted the current selection, clear it
      if (currentCollection === name) {
        setCurrentCollection('');
      }
      loadCollections();
    } catch (err: unknown) {
      const msg = err instanceof Error ? err.message : String(err);
      addToast(`删除失败: ${msg}`, 'error');
    }
  };

  // ---- Stats ----
  const pendingCount = files.filter((f) => f.status === 'pending').length;
  const doneCount = files.filter((f) => f.status === 'done').length;
  const skippedCount = files.filter((f) => f.status === 'skipped').length;
  const errorCount = files.filter((f) => f.status === 'error').length;
  const currentCollectionInfo = backendCollections.find((c) => c.name === currentCollection);
  const hasCollections = backendCollections.length > 0;
  const isConnected = !connectError && !collectionsLoading;
  const canUpload = hasCollections && isConnected;
  const tableProvider =
    llmProviders.find((p) => p.id === enrichment.llm_text_provider) || null;
  const figureProvider =
    llmProviders.find((p) => p.id === enrichment.llm_vision_provider) || null;
  const concurrencyOptions = [1, 2, 3, 4, 6, 8];

  // ---- Connection error: show reconnect panel ----
  if (connectError && !collectionsLoading) {
    return (
      <div className="max-w-2xl mx-auto p-8 animate-in fade-in">
        <div className="bg-white p-12 rounded-3xl shadow-xl text-center border border-gray-100">
          <div className="w-20 h-20 bg-red-50 text-red-500 rounded-full flex items-center justify-center mx-auto mb-6">
            <AlertCircle size={40} />
          </div>
          <h2 className="text-2xl font-bold text-gray-900 mb-2">无法连接后端服务</h2>
          <p className="text-gray-500 mb-2">请确认后端服务已启动，Milvus 数据库可用</p>
          <p className="text-xs text-red-400 mb-6 font-mono">{connectError}</p>
          <div className="text-sm text-gray-400 mb-4">
            当前地址: <span className="font-mono text-gray-600">{dbAddress}</span>
          </div>
          <button
            onClick={loadCollections}
            className="px-6 py-2.5 bg-blue-600 text-white rounded-lg font-medium hover:bg-blue-700 transition-colors"
          >
            重新连接
          </button>
        </div>
      </div>
    );
  }

  return (
    <div className="max-w-5xl mx-auto space-y-8 p-8 animate-in fade-in">
      {/* Hidden file inputs */}
      <input
        ref={fileInputRef}
        id="ingest-file-input"
        name="ingest-file-input"
        type="file"
        accept=".pdf"
        multiple
        className="hidden"
        onChange={handleFileInputChange}
      />
      <input
        ref={folderInputRef}
        id="ingest-folder-input"
        name="ingest-folder-input"
        type="file"
        accept=".pdf"
        multiple
        // @ts-expect-error: webkitdirectory is non-standard but widely supported
        webkitdirectory=""
        className="hidden"
        onChange={handleFileInputChange}
      />

      {/* 状态卡片 */}
      <div className="grid grid-cols-1 md:grid-cols-3 gap-6">
        <div className="col-span-1 bg-gray-900 text-white rounded-2xl p-6 shadow-lg flex flex-col justify-between relative overflow-hidden">
          <div className="absolute top-0 right-0 p-4 opacity-10">
            <Database size={100} />
          </div>
          <div>
            <div className="flex items-center gap-2 text-gray-400 text-xs font-bold uppercase mb-2">
              <HardDrive size={14} /> Milvus
            </div>
            <div className="text-2xl font-bold font-mono tracking-tight">
              {dbAddress}
            </div>
          </div>
          <div className="mt-6 space-y-2 text-sm text-gray-300">
            <div className="flex justify-between">
              <span>状态:</span>
              <span className="text-green-400 flex items-center gap-1">
                <div className="w-1.5 h-1.5 bg-green-500 rounded-full animate-pulse" />
                已连接
              </span>
            </div>
            <div className="flex justify-between">
              <span>集合数:</span>
              <span className="text-white">{backendCollections.length}</span>
            </div>
            {hasCollections && (
              <>
                <div className="flex justify-between">
                  <span>当前集合:</span>
                  <span className="text-green-400">{currentCollection}</span>
                </div>
                {currentCollectionInfo && (
                  <div className="flex justify-between">
                    <span>文档数:</span>
                    <span className="text-white">
                      {currentCollectionInfo.count >= 0 ? currentCollectionInfo.count : '?'}
                    </span>
                  </div>
                )}
              </>
            )}
          </div>
        </div>

        <div className="col-span-2 bg-white border rounded-2xl p-6 shadow-sm flex flex-col justify-center">
          <div className="flex justify-between items-center mb-4">
            <div>
              <h3 className="text-lg font-bold text-gray-900">选择集合 (Collection)</h3>
              <p className="text-xs text-gray-500">将文档上传至指定的知识库分区</p>
            </div>
            <div className="flex gap-2">
              <button
                onClick={loadCollections}
                disabled={collectionsLoading}
                className="p-2 text-gray-400 hover:text-blue-600 hover:bg-blue-50 rounded-lg transition-colors"
                title="刷新集合列表"
              >
                <RefreshCw size={16} className={collectionsLoading ? 'animate-spin' : ''} />
              </button>
              <button
                onClick={() => setShowCreateCollectionModal(true)}
                className="flex items-center gap-2 px-4 py-2 bg-blue-50 text-blue-600 rounded-lg text-sm font-bold hover:bg-blue-100 transition-colors"
              >
                <Plus size={16} /> 新建集合
              </button>
            </div>
          </div>

          {hasCollections ? (
            <div className="space-y-2">
              {backendCollections.map((c) => (
                <div
                  key={c.name}
                  onClick={() => setCurrentCollection(c.name)}
                  className={`flex items-center justify-between p-3 rounded-xl border cursor-pointer transition-colors ${
                    currentCollection === c.name
                      ? 'border-blue-400 bg-blue-50'
                      : 'border-gray-200 bg-gray-50 hover:border-gray-300'
                  }`}
                >
                  <div className="flex items-center gap-3 min-w-0">
                    <Database size={16} className={currentCollection === c.name ? 'text-blue-500' : 'text-gray-400'} />
                    <div className="min-w-0">
                      <div className="font-medium text-sm text-gray-800 truncate">{c.name}</div>
                      <div className="text-xs text-gray-400">
                        {c.count >= 0 ? `${c.count} 条记录` : '加载中...'}
                      </div>
                    </div>
                  </div>
                  <button
                    onClick={(e) => {
                      e.stopPropagation();
                      handleDeleteCollection(c.name);
                    }}
                    className="p-1.5 text-gray-300 hover:text-red-500 hover:bg-red-50 rounded-lg transition-colors"
                    title={`删除集合 ${c.name}`}
                  >
                    <Trash2 size={14} />
                  </button>
                </div>
              ))}
            </div>
          ) : (
            <div className="bg-amber-50 border border-amber-200 rounded-xl p-4 text-center">
              <p className="text-amber-700 text-sm font-medium mb-2">暂无集合</p>
              <p className="text-amber-600 text-xs mb-3">
                请先创建一个向量集合，然后再上传文档进行入库
              </p>
              <button
                onClick={() => setShowCreateCollectionModal(true)}
                className="px-4 py-2 bg-amber-600 text-white rounded-lg text-sm font-bold hover:bg-amber-700 transition-colors"
              >
                <Plus size={14} className="inline mr-1 -mt-0.5" /> 创建第一个集合
              </button>
            </div>
          )}
        </div>
      </div>

      {/* 已入库文件列表 */}
      {hasCollections && currentCollection && (() => {
        const totalPapers = papers.length;
        const totalPages = Math.max(1, Math.ceil(totalPapers / papersPageSize));
        const effectivePage = Math.min(Math.max(1, papersPage), totalPages);
        const displayedPapers = papers.slice(
          (effectivePage - 1) * papersPageSize,
          effectivePage * papersPageSize,
        );
        return (
          <div className="bg-white border rounded-2xl overflow-hidden shadow-sm">
            <div className="px-6 py-4 border-b bg-gray-50 flex justify-between items-center flex-wrap gap-2">
              <div className="flex items-center gap-3 flex-wrap">
                <span className="font-bold text-sm">已入库文件</span>
                <span className="text-xs bg-gray-100 text-gray-500 px-2 py-0.5 rounded-full">
                  {currentCollection}
                </span>
                {totalPapers > 0 && (
                  <span className="text-xs text-gray-400">
                    {totalPapers} 个文件 · {papers.reduce((s, p) => s + p.row_count, 0)} 条记录
                  </span>
                )}
              </div>
              <div className="flex items-center gap-2">
                {totalPapers > 0 && (
                  <label className="flex items-center gap-1.5 text-xs text-gray-500">
                    每页
                    <select
                      value={papersPageSize}
                      onChange={(e) => {
                        setPapersPageSize(Number(e.target.value) as 10 | 20);
                        setPapersPage(1);
                      }}
                      className="border rounded px-2 py-1 text-gray-700 bg-white"
                    >
                      <option value={10}>10</option>
                      <option value={20}>20</option>
                    </select>
                    条
                  </label>
                )}
                <button
                  onClick={loadPapers}
                  disabled={papersLoading}
                  className="p-2 text-gray-400 hover:text-blue-600 hover:bg-blue-50 rounded-lg transition-colors"
                  title="刷新文件列表"
                >
                  <RefreshCw size={14} className={papersLoading ? 'animate-spin' : ''} />
                </button>
              </div>
            </div>
            {papersLoading ? (
              <div className="px-6 py-8 text-center text-gray-400 text-sm">
                <Loader2 size={20} className="animate-spin mx-auto mb-2" />
                加载中...
              </div>
            ) : totalPapers === 0 ? (
              <div className="px-6 py-8 text-center text-gray-400 text-sm">
                该集合暂无已入库的文件
              </div>
            ) : (
              <>
                <table className="w-full text-sm">
                  <thead>
                    <tr className="text-left text-gray-400 border-b">
                      <th className="px-6 py-2.5 font-medium">文件名</th>
                      <th className="px-6 py-2.5 font-medium w-24">大小</th>
                      <th className="px-6 py-2.5 font-medium w-20">Chunks</th>
                      <th className="px-6 py-2.5 font-medium w-40">图表解析</th>
                      <th className="px-6 py-2.5 font-medium w-24">状态</th>
                      <th className="px-6 py-2.5 font-medium w-36">入库时间</th>
                      <th className="px-6 py-2.5 font-medium text-right w-16">操作</th>
                    </tr>
                  </thead>
                  <tbody className="divide-y">
                    {displayedPapers.map((p) => (
                      <tr key={p.paper_id} className="hover:bg-gray-50 transition-colors">
                        <td className="px-6 py-2.5 font-medium flex items-center gap-2">
                          <FileText size={14} className="text-gray-400 flex-shrink-0" />
                          <span className="truncate max-w-[300px]" title={p.filename || p.paper_id}>
                            {p.filename || p.paper_id}
                          </span>
                        </td>
                        <td className="px-6 py-2.5 text-gray-500">{formatSize(p.file_size)}</td>
                        <td className="px-6 py-2.5 text-gray-500">{p.chunk_count}</td>
                        <td className="px-6 py-2.5 text-[11px] text-gray-500">
                          <div className="flex flex-col gap-0.5">
                            <span>
                              表格: {p.enrich_tables_enabled ? `已开 (${p.table_success || 0}/${p.table_count || 0})` : '未开'}
                            </span>
                            <span>
                              图片: {p.enrich_figures_enabled ? `已开 (${p.figure_success || 0}/${p.figure_count || 0})` : '未开'}
                            </span>
                          </div>
                        </td>
                        <td className="px-6 py-2.5">
                          <span className={`inline-flex items-center gap-1 px-2 py-0.5 rounded-full text-[10px] font-bold uppercase ${
                            p.status === 'done'
                              ? 'bg-green-50 text-green-600'
                              : 'bg-red-50 text-red-600'
                          }`}>
                            {p.status === 'done' ? <CheckCircle2 size={10} /> : <AlertCircle size={10} />}
                            {p.status === 'done' ? '已入库' : '失败'}
                          </span>
                        </td>
                        <td className="px-6 py-2.5 text-gray-400 text-xs">
                          {new Date(p.created_at * 1000).toLocaleString('zh-CN')}
                        </td>
                        <td className="px-6 py-2.5 text-right">
                          <button
                            onClick={() => handleDeletePaper(p.paper_id, p.filename)}
                            className="p-1.5 text-gray-300 hover:text-red-500 hover:bg-red-50 rounded-lg transition-colors"
                            title={`删除 ${p.filename || p.paper_id} 及其全部向量数据`}
                          >
                            <Trash2 size={14} />
                          </button>
                        </td>
                      </tr>
                    ))}
                  </tbody>
                </table>
                {totalPages > 1 && (
                  <div className="px-6 py-3 border-t bg-gray-50 flex items-center justify-between text-sm">
                    <span className="text-gray-500">
                      第 {effectivePage} / {totalPages} 页，共 {totalPapers} 条
                    </span>
                    <div className="flex items-center gap-1">
                      <button
                        onClick={() => setPapersPage((p) => Math.max(1, p - 1))}
                        disabled={effectivePage <= 1}
                        className="p-2 rounded-lg border border-gray-200 text-gray-600 hover:bg-gray-100 disabled:opacity-40 disabled:pointer-events-none"
                        title="上一页"
                      >
                        <ChevronLeft size={16} />
                      </button>
                      <button
                        onClick={() => setPapersPage((p) => Math.min(totalPages, p + 1))}
                        disabled={effectivePage >= totalPages}
                        className="p-2 rounded-lg border border-gray-200 text-gray-600 hover:bg-gray-100 disabled:opacity-40 disabled:pointer-events-none"
                        title="下一页"
                      >
                        <ChevronRight size={16} />
                      </button>
                    </div>
                  </div>
                )}
              </>
            )}
          </div>
        );
      })()}

      {/* 上传区域 */}
      <div
        className={`bg-white p-8 rounded-2xl border-2 border-dashed shadow-sm text-center transition-colors ${
          !canUpload
            ? 'border-gray-100 opacity-60 pointer-events-none'
            : isDragOver
              ? 'border-blue-400 bg-blue-50'
              : 'border-gray-200'
        }`}
        onDragOver={(e) => {
          e.preventDefault();
          if (canUpload) setIsDragOver(true);
        }}
        onDragLeave={() => setIsDragOver(false)}
        onDrop={(e) => canUpload && handleDrop(e)}
      >
        <UploadCloud size={48} className="mx-auto text-blue-500 mb-4" />
        <h2 className="text-xl font-bold">上传文档</h2>
        <p className="text-gray-500 mt-2 mb-6">
          {canUpload
            ? '拖放 PDF 文件到此处，或点击下方按钮选择文件/文件夹'
            : '请先选择或创建一个集合'}
        </p>
        <div className="flex justify-center gap-3">
          <button
            onClick={handleSelectFiles}
            disabled={isProcessing || !canUpload}
            className="flex items-center gap-2 bg-gray-900 text-white px-5 py-2.5 rounded-lg text-sm font-medium hover:bg-gray-800 disabled:opacity-50 transition-colors"
          >
            <FileText size={16} /> 选择文件
          </button>
          <button
            onClick={handleSelectFolder}
            disabled={isProcessing || !canUpload}
            className="flex items-center gap-2 bg-white text-gray-700 border border-gray-300 px-5 py-2.5 rounded-lg text-sm font-medium hover:bg-gray-50 disabled:opacity-50 transition-colors"
          >
            <FolderOpen size={16} /> 选择文件夹
          </button>
        </div>

        {/* LLM 增强选项 */}
        <div className="mt-6 max-w-md mx-auto">
          <div className="text-xs font-semibold text-gray-400 uppercase tracking-wider mb-2">
            LLM 增强选项
          </div>
          <div className="flex justify-center items-stretch gap-3">
            <div
              className={`flex-1 px-4 py-3 rounded-xl border transition-all ${
                enrichment.enrich_figures
                  ? 'border-blue-400 bg-blue-50 text-blue-700 shadow-sm'
                  : 'border-gray-200 bg-gray-50 text-gray-500 hover:border-gray-300'
              }`}
            >
              <label className="flex items-center gap-2.5 cursor-pointer">
                <input
                  id="enrich-figures"
                  name="enrich-figures"
                  type="checkbox"
                  checked={enrichment.enrich_figures}
                  onChange={(e) =>
                    setEnrichment((prev) => ({ ...prev, enrich_figures: e.target.checked }))
                  }
                  className="rounded border-gray-300 text-blue-600 focus:ring-blue-500"
                />
                <div className="text-left">
                  <div className="text-sm font-medium">图片描述</div>
                  <div className="text-[10px] text-gray-400">Vision 模型解析图表含义</div>
                </div>
              </label>
              <div className="mt-2 space-y-1">
                <select
                  value={enrichment.llm_vision_provider || ''}
                  onChange={(e) => {
                    const providerId = e.target.value || null;
                    const provider = llmProviders.find((p) => p.id === providerId) || null;
                    setEnrichment((prev) => ({
                      ...prev,
                      llm_vision_provider: providerId,
                      llm_vision_model: provider?.default_model || null,
                    }));
                  }}
                  disabled={isProcessing || llmProviders.length === 0}
                  className="w-full rounded border border-gray-300 bg-white px-2 py-1 text-xs text-gray-700"
                >
                  <option value="">选择 Provider</option>
                  {llmProviders.map((p) => (
                    <option key={`vision-provider-${p.id}`} value={p.id}>
                      {p.id}
                    </option>
                  ))}
                </select>
                <select
                  value={enrichment.llm_vision_model || '__default__'}
                  onChange={(e) =>
                    setEnrichment((prev) => ({
                      ...prev,
                      llm_vision_model: e.target.value === '__default__' ? null : e.target.value,
                    }))
                  }
                  disabled={isProcessing || !figureProvider}
                  className="w-full rounded border border-gray-300 bg-white px-2 py-1 text-xs text-gray-700"
                >
                  <option value="__default__">模型默认（Provider default）</option>
                  {(figureProvider?.models || []).map((m) => (
                    <option key={`vision-model-${m}`} value={m}>
                      {m}
                    </option>
                  ))}
                </select>
                <div className="flex items-center gap-2">
                  <span className="text-[10px] text-gray-500 whitespace-nowrap">并发</span>
                  <select
                    value={enrichment.llm_vision_concurrency ?? 1}
                    onChange={(e) =>
                      setEnrichment((prev) => ({
                        ...prev,
                        llm_vision_concurrency: Number(e.target.value) || 1,
                      }))
                    }
                    disabled={isProcessing}
                    className="w-full rounded border border-gray-300 bg-white px-2 py-1 text-xs text-gray-700"
                  >
                    {concurrencyOptions.map((v) => (
                      <option key={`vision-concurrency-${v}`} value={v}>
                        {v}
                      </option>
                    ))}
                  </select>
                </div>
              </div>
            </div>
            <div
              className={`flex-1 px-4 py-3 rounded-xl border transition-all ${
                enrichment.enrich_tables
                  ? 'border-purple-400 bg-purple-50 text-purple-700 shadow-sm'
                  : 'border-gray-200 bg-gray-50 text-gray-500 hover:border-gray-300'
              }`}
            >
              <label className="flex items-center gap-2.5 cursor-pointer">
                <input
                  id="enrich-tables"
                  name="enrich-tables"
                  type="checkbox"
                  checked={enrichment.enrich_tables}
                  onChange={(e) =>
                    setEnrichment((prev) => ({ ...prev, enrich_tables: e.target.checked }))
                  }
                  className="rounded border-gray-300 text-purple-600 focus:ring-purple-500"
                />
                <div className="text-left">
                  <div className="text-sm font-medium">表格解析</div>
                  <div className="text-[10px] text-gray-400">LLM 生成表格语义摘要</div>
                </div>
              </label>
              <div className="mt-2 space-y-1">
                <select
                  value={enrichment.llm_text_provider || ''}
                  onChange={(e) => {
                    const providerId = e.target.value || null;
                    const provider = llmProviders.find((p) => p.id === providerId) || null;
                    setEnrichment((prev) => ({
                      ...prev,
                      llm_text_provider: providerId,
                      llm_text_model: provider?.default_model || null,
                    }));
                  }}
                  disabled={isProcessing || llmProviders.length === 0}
                  className="w-full rounded border border-gray-300 bg-white px-2 py-1 text-xs text-gray-700"
                >
                  <option value="">选择 Provider</option>
                  {llmProviders.map((p) => (
                    <option key={`text-provider-${p.id}`} value={p.id}>
                      {p.id}
                    </option>
                  ))}
                </select>
                <select
                  value={enrichment.llm_text_model || '__default__'}
                  onChange={(e) =>
                    setEnrichment((prev) => ({
                      ...prev,
                      llm_text_model: e.target.value === '__default__' ? null : e.target.value,
                    }))
                  }
                  disabled={isProcessing || !tableProvider}
                  className="w-full rounded border border-gray-300 bg-white px-2 py-1 text-xs text-gray-700"
                >
                  <option value="__default__">模型默认（Provider default）</option>
                  {(tableProvider?.models || []).map((m) => (
                    <option key={`text-model-${m}`} value={m}>
                      {m}
                    </option>
                  ))}
                </select>
                <div className="flex items-center gap-2">
                  <span className="text-[10px] text-gray-500 whitespace-nowrap">并发</span>
                  <select
                    value={enrichment.llm_text_concurrency ?? 1}
                    onChange={(e) =>
                      setEnrichment((prev) => ({
                        ...prev,
                        llm_text_concurrency: Number(e.target.value) || 1,
                      }))
                    }
                    disabled={isProcessing}
                    className="w-full rounded border border-gray-300 bg-white px-2 py-1 text-xs text-gray-700"
                  >
                    {concurrencyOptions.map((v) => (
                      <option key={`text-concurrency-${v}`} value={v}>
                        {v}
                      </option>
                    ))}
                  </select>
                </div>
              </div>
            </div>
          </div>
          {!enrichment.enrich_figures && !enrichment.enrich_tables && (
            <p className="text-[10px] text-gray-400 mt-2 text-center">
              未选择增强项，将仅执行基础解析（更快）
            </p>
          )}
        </div>
      </div>

      {/* 解析状态：表格/图片 enrich 进度（仅在有解析中文件且有待展示日志时显示） */}
      {isProcessing && Object.keys(enrichLogs).length > 0 && (
        <div className="mb-4 bg-amber-50/80 border border-amber-200 rounded-xl overflow-hidden">
          <div className="px-4 py-2 border-b border-amber-200 bg-amber-100/80 text-amber-800 text-sm font-medium">
            解析状态（表格/图片）
          </div>
          <div className="px-4 py-3 max-h-40 overflow-y-auto space-y-2">
            {Object.entries(enrichLogs).map(([file, lines]) => (
              <div key={file} className="text-xs">
                <div className="font-medium text-gray-700 mb-1 truncate" title={file}>
                  {file}
                </div>
                <ul className="text-gray-600 space-y-0.5 font-mono">
                  {lines.slice(-10).map((line, i) => (
                    <li key={i}>{line}</li>
                  ))}
                </ul>
              </div>
            ))}
          </div>
        </div>
      )}

      {/* 文件列表 + 处理 */}
      {files.length > 0 && (
        <div className="bg-white border rounded-2xl overflow-hidden shadow-sm">
          <div className="px-6 py-4 border-b bg-gray-50 flex justify-between items-center">
            <div className="flex items-center gap-3">
              <span className="font-bold text-sm">
                文件列表 ({files.length} 个)
              </span>
              {pendingCount > 0 && (
                <span className="text-xs bg-blue-50 text-blue-600 px-2 py-0.5 rounded-full">
                  {pendingCount} 待处理
                </span>
              )}
              {doneCount > 0 && (
                <span className="text-xs bg-green-50 text-green-600 px-2 py-0.5 rounded-full">
                  {doneCount} 完成
                </span>
              )}
              {errorCount > 0 && (
                <span className="text-xs bg-red-50 text-red-600 px-2 py-0.5 rounded-full">
                  {errorCount} 失败
                </span>
              )}
            </div>
            <div className="flex items-center gap-2">
              {errorCount > 0 && !isProcessing && (
                <button
                  onClick={handleRetryFailed}
                  className="text-xs text-orange-500 hover:text-orange-700 px-3 py-1 rounded-lg hover:bg-orange-50 font-medium"
                >
                  重试失败 ({errorCount})
                </button>
              )}
              {pendingCount > 0 && !isProcessing && (
                <button
                  onClick={handleClearPending}
                  className="text-xs text-gray-500 hover:text-gray-700 px-3 py-1 rounded-lg hover:bg-gray-100"
                >
                  删除待处理 ({pendingCount})
                </button>
              )}
              {errorCount > 0 && !isProcessing && (
                <button
                  onClick={handleClearFailed}
                  className="text-xs text-red-500 hover:text-red-700 px-3 py-1 rounded-lg hover:bg-red-50"
                >
                  删除失败 ({errorCount})
                </button>
              )}
              {(doneCount > 0 || skippedCount > 0) && !isProcessing && (
                <button
                  onClick={handleClearDone}
                  className="text-xs text-gray-400 hover:text-gray-600 px-3 py-1 rounded-lg hover:bg-gray-100"
                >
                  清除已完成 ({doneCount + skippedCount})
                </button>
              )}
              {isProcessing ? (
                <button
                  onClick={handleAbort}
                  disabled={isCancelling}
                  className="px-4 py-2 bg-red-50 text-red-600 rounded-lg text-sm font-bold hover:bg-red-100 transition-colors"
                >
                  {isCancelling ? '取消中...' : '取消处理'}
                </button>
              ) : (
                <button
                  onClick={handleStartProcess}
                  disabled={pendingCount === 0}
                  className="px-4 py-2 bg-blue-600 text-white rounded-lg text-sm font-bold hover:bg-blue-700 disabled:opacity-50 transition-colors"
                >
                  开始入库 ({pendingCount})
                </button>
              )}
            </div>
          </div>

          {/* Progress bar */}
          {globalProgress && (
            <div className="px-6 py-3 bg-blue-50 border-b text-sm text-blue-700 flex items-center gap-2">
              {isProcessing && <Loader2 size={14} className="animate-spin" />}
              {globalProgress}
            </div>
          )}

          <table className="w-full text-sm">
            <thead>
              <tr className="text-left text-gray-400 border-b">
                <th className="px-6 py-3 font-medium">文件名</th>
                <th className="px-6 py-3 font-medium w-24">大小</th>
                <th className="px-6 py-3 font-medium w-32">状态</th>
                <th className="px-6 py-3 font-medium">信息</th>
                <th className="px-6 py-3 font-medium text-right w-16">操作</th>
              </tr>
            </thead>
            <tbody className="divide-y">
              {files.map((item) => (
                <tr key={item.id} className="hover:bg-gray-50 transition-colors">
                  <td className="px-6 py-3 font-medium flex items-center gap-2">
                    <FileText size={16} className="text-gray-400 flex-shrink-0" />
                    <span className="truncate max-w-[300px]">{item.name}</span>
                  </td>
                  <td className="px-6 py-3 text-gray-500">
                    {formatSize(item.size)}
                  </td>
                  <td className="px-6 py-3">
                    <span
                      className={`inline-flex items-center gap-1 px-2 py-1 rounded-full text-[10px] font-bold uppercase ${STATUS_COLORS[item.status]}`}
                    >
                      {item.status === 'done' && <CheckCircle2 size={10} />}
                      {item.status === 'error' && <AlertCircle size={10} />}
                      {['uploading', 'parsing', 'chunking', 'embedding', 'indexing'].includes(item.status) && (
                        <Loader2 size={10} className="animate-spin" />
                      )}
                      {STATUS_LABELS[item.status]}
                    </span>
                  </td>
                  <td className="px-6 py-3 text-gray-500 text-xs truncate max-w-[200px]">
                    {item.message || '-'}
                  </td>
                  <td className="px-6 py-3 text-right">
                    {item.status === 'pending' && !isProcessing && (
                      <button
                        onClick={() => handleRemoveFile(item.id)}
                        className="text-gray-300 hover:text-red-500"
                      >
                        <Trash2 size={16} />
                      </button>
                    )}
                  </td>
                </tr>
              ))}
            </tbody>
          </table>
        </div>
      )}

      {/* 重复文件确认 Modal（按 content_hash 判定） */}
      <Modal
        open={duplicateModal !== null}
        onClose={() => setDuplicateModal(null)}
        title="以下文件已在本集合中（内容相同）"
        maxWidth="max-w-md"
      >
        {duplicateModal && (
          <>
            <p className="text-sm text-gray-600 mb-3">
              选择「跳过」不重复入库，选择「覆盖」将先删除该文件在本集合中的向量数据再重新入库。
            </p>
            <ul className="mb-4 max-h-48 overflow-y-auto rounded-lg border border-gray-200 bg-gray-50 p-2 text-sm">
              {duplicateModal.duplicatePairs.map((d) => (
                <li key={d.uploadedFile.path} className="py-1 truncate" title={d.uploadedFile.filename}>
                  {d.uploadedFile.filename}
                </li>
              ))}
            </ul>
            <label className="flex items-center gap-2 mb-4 text-sm text-gray-600">
              <input
                type="checkbox"
                checked={applyToAllSimilar}
                onChange={(e) => setApplyToAllSimilar(e.target.checked)}
                className="rounded border-gray-300 text-blue-600 focus:ring-blue-500"
              />
              对本次及之后所有重复项执行相同操作
            </label>
            {duplicateActionPreference && (
              <p className="text-xs text-gray-400 mb-2">
                当前偏好：{duplicateActionPreference === 'skip' ? '跳过已存在' : '覆盖并重新入库'}
                <button
                  type="button"
                  onClick={() => setDuplicateActionPreference(null)}
                  className="ml-2 text-blue-600 hover:underline"
                >
                  清除偏好
                </button>
              </p>
            )}
            <div className="flex justify-end gap-2">
              <button
                onClick={() => setDuplicateModal(null)}
                className="px-4 py-2 text-gray-600 hover:bg-gray-100 rounded-lg text-sm"
              >
                取消
              </button>
              <button
                onClick={() => runProcessAfterDuplicateChoice('skip')}
                className="px-4 py-2 bg-gray-100 text-gray-700 rounded-lg hover:bg-gray-200 text-sm"
              >
                跳过已存在
              </button>
              <button
                onClick={() => runProcessAfterDuplicateChoice('overwrite')}
                className="px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 text-sm"
              >
                覆盖并重新入库
              </button>
            </div>
          </>
        )}
      </Modal>

      {/* 新建集合 Modal */}
      <Modal
        open={showCreateCollectionModal}
        onClose={() => setShowCreateCollectionModal(false)}
        title="新建向量集合"
        maxWidth="max-w-md"
      >
        <div className="space-y-4">
          <div>
            <label className="text-xs font-medium text-gray-500 uppercase">
              集合名称
            </label>
            <input
              id="new-collection-name"
              name="new-collection-name"
              type="text"
              value={newCollectionName}
              onChange={(e) => setNewCollectionName(e.target.value)}
              placeholder="自定义集合名称，例如 my_research_2026"
              className="w-full mt-1 border rounded-md p-2 text-sm focus:ring-2 focus:ring-blue-500 outline-none"
              onKeyDown={(e) => e.key === 'Enter' && handleCreateCollection()}
            />
          </div>

          {/* 推荐模板 */}
          <div>
            <label className="text-xs font-medium text-gray-500 uppercase mb-2 block">
              快速选择模板
            </label>
            <div className="grid grid-cols-2 gap-2">
              {COLLECTION_TEMPLATES.map((t) => (
                <button
                  key={t.name}
                  onClick={() => setNewCollectionName(t.name)}
                  className={`text-left p-2.5 rounded-lg border text-xs transition-colors ${
                    newCollectionName === t.name
                      ? 'border-blue-400 bg-blue-50 text-blue-700'
                      : 'border-gray-200 hover:border-gray-300 text-gray-600'
                  }`}
                >
                  <div className="font-medium">{t.name}</div>
                  <div className="text-gray-400 mt-0.5">{t.desc}</div>
                </button>
              ))}
            </div>
          </div>

          <p className="text-xs text-gray-400">
            使用 v2 schema (chunk_id 主键)，支持 upsert 去重。名称创建后不可修改。
          </p>
        </div>
        <div className="mt-6 flex justify-end gap-2">
          <button
            onClick={() => setShowCreateCollectionModal(false)}
            className="px-4 py-2 text-gray-600 hover:bg-gray-100 rounded-lg text-sm"
          >
            取消
          </button>
          <button
            onClick={handleCreateCollection}
            disabled={!newCollectionName.trim()}
            className="px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 disabled:opacity-50 text-sm"
          >
            创建
          </button>
        </div>
      </Modal>
    </div>
  );
}

function formatSize(bytes: number): string {
  if (bytes < 1024) return `${bytes} B`;
  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(1)} KB`;
  return `${(bytes / (1024 * 1024)).toFixed(1)} MB`;
}
</file>

<file path="frontend/src/pages/LoginPage.tsx">
import { useState, type FormEvent } from 'react';
import { useTranslation } from 'react-i18next';
import { Layers, Users, Lock, AlertCircle, Loader2 } from 'lucide-react';
import { useAuthStore } from '../stores';

export function LoginPage() {
  const { t } = useTranslation();
  const [username, setUsername] = useState('');
  const [password, setPassword] = useState('');
  const { login, isLoading, error, clearError } = useAuthStore();

  const handleSubmit = async (e: FormEvent) => {
    e.preventDefault();
    clearError();
    try {
      await login(username, password);
    } catch {
      // error 已在 store 中处理
    }
  };

  return (
    <div className="min-h-screen bg-gray-900 flex flex-col items-center justify-center p-4 relative overflow-hidden">
      {/* 背景装饰 */}
      <div className="absolute top-0 left-0 w-full h-full overflow-hidden z-0">
        <div className="absolute -top-[20%] -left-[10%] w-[50%] h-[50%] bg-blue-600/20 rounded-full blur-[120px]"></div>
        <div className="absolute top-[40%] -right-[10%] w-[40%] h-[60%] bg-purple-600/20 rounded-full blur-[120px]"></div>
      </div>

      <div className="bg-white/10 backdrop-blur-lg border border-white/20 p-8 rounded-3xl w-full max-w-md shadow-2xl z-10 animate-in fade-in zoom-in duration-500">
        <div className="text-center mb-8">
          <div className="w-16 h-16 bg-blue-600 rounded-2xl flex items-center justify-center mx-auto mb-4 shadow-lg shadow-blue-900/50">
            <Layers size={32} className="text-white" />
          </div>
          <h1 className="text-2xl font-bold text-white tracking-tight">
            {t('login.title')}
          </h1>
          <p className="text-gray-400 text-sm mt-2">
            {t('login.subtitle')}
          </p>
        </div>

        <form onSubmit={handleSubmit} className="space-y-5">
          <div>
            <label className="block text-xs font-medium text-gray-300 uppercase mb-2 ml-1">
              {t('login.account')}
            </label>
            <div className="relative">
              <Users
                size={18}
                className="absolute left-3 top-3.5 text-gray-400"
              />
              <input
                type="text"
                value={username}
                onChange={(e) => setUsername(e.target.value)}
                className="w-full bg-gray-800/50 border border-gray-600 text-white rounded-xl py-3 pl-10 pr-4 focus:ring-2 focus:ring-blue-500 focus:border-transparent outline-none transition-all placeholder-gray-500"
                placeholder={t('login.usernamePlaceholder')}
              />
            </div>
          </div>

          <div>
            <label className="block text-xs font-medium text-gray-300 uppercase mb-2 ml-1">
              {t('login.password')}
            </label>
            <div className="relative">
              <Lock
                size={18}
                className="absolute left-3 top-3.5 text-gray-400"
              />
              <input
                type="password"
                value={password}
                onChange={(e) => setPassword(e.target.value)}
                className="w-full bg-gray-800/50 border border-gray-600 text-white rounded-xl py-3 pl-10 pr-4 focus:ring-2 focus:ring-blue-500 focus:border-transparent outline-none transition-all placeholder-gray-500"
                placeholder="••••••••"
              />
            </div>
          </div>

          {error && (
            <div className="flex items-center gap-2 text-red-400 text-xs bg-red-900/20 p-3 rounded-lg border border-red-900/30">
              <AlertCircle size={14} />
              {error}
            </div>
          )}

          <button
            type="submit"
            disabled={isLoading}
            className="w-full bg-blue-600 hover:bg-blue-500 text-white font-bold py-3.5 rounded-xl transition-all shadow-lg shadow-blue-900/40 active:scale-[0.98] flex items-center justify-center gap-2 disabled:opacity-60"
          >
            {isLoading ? (
              <Loader2 size={20} className="animate-spin" />
            ) : (
              t('login.signIn')
            )}
          </button>
        </form>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/stores/index.ts">
export { useAuthStore } from './useAuthStore';
export { useCompareStore } from './useCompareStore';
export { useConfigStore } from './useConfigStore';
export { useChatStore } from './useChatStore';
export { useCanvasStore } from './useCanvasStore';
export { useProjectsStore } from './useProjectsStore';
export { useToastStore } from './useToastStore';
export { useUIStore } from './useUIStore';
</file>

<file path="frontend/src/stores/useAuthStore.ts">
import { create } from 'zustand';
import { persist } from 'zustand/middleware';
import type { User } from '../types';
import { login as apiLogin } from '../api/auth';

interface AuthState {
  user: User | null;
  token: string | null;
  isLoading: boolean;
  error: string | null;

  login: (userId: string, password: string) => Promise<void>;
  logout: () => void;
  clearError: () => void;
}

export const useAuthStore = create<AuthState>()(
  persist(
    (set) => ({
      user: null,
      token: null,
      isLoading: false,
      error: null,

      login: async (userId: string, password: string) => {
        set({ isLoading: true, error: null });
        try {
          const res = await apiLogin({ user_id: userId, password });
          const user: User = {
            user_id: res.user_id,
            username: res.user_id,
            role: res.role as 'user' | 'admin',
            avatar: `https://api.dicebear.com/7.x/avataaars/svg?seed=${res.user_id}`,
          };
          localStorage.setItem('token', res.token);
          set({ user, token: res.token, isLoading: false });
        } catch (err: unknown) {
          const message =
            (err as { response?: { data?: { detail?: string } } })?.response
              ?.data?.detail || '登录失败';
          set({ error: message, isLoading: false });
          throw err;
        }
      },

      logout: () => {
        localStorage.removeItem('token');
        set({ user: null, token: null });
      },

      clearError: () => set({ error: null }),
    }),
    {
      name: 'auth-storage',
      partialize: (state) => ({ user: state.user, token: state.token }),
    }
  )
);
</file>

<file path="frontend/src/stores/useCanvasStore.ts">
import { create } from 'zustand';
import type { Canvas, CanvasStage, Annotation } from '../types';

interface CanvasState {
  canvas: Canvas | null;
  canvasContent: string; // Markdown 内容（用于 Refine 阶段预览和编辑）
  isLoading: boolean;

  // 阶段控制
  activeStage: CanvasStage;  // 当前用户正在查看的阶段（可独立于 canvas.stage 导航）

  // P1 编辑模式
  editMode: boolean;
  versionHistory: string[];      // 快照列表（Markdown 内容）
  currentVersionIndex: number;   // 当前版本指针（-1 = 当前编辑态）
  isAIEditing: boolean;

  // 批注（Refine 阶段）
  pendingAnnotations: Annotation[];

  setCanvas: (canvas: Canvas | null) => void;
  setCanvasContent: (content: string) => void;
  appendCanvasContent: (delta: string) => void;
  clearCanvas: () => void;
  setIsLoading: (loading: boolean) => void;

  // 阶段导航
  setActiveStage: (stage: CanvasStage) => void;

  // P1 编辑方法
  setEditMode: (mode: boolean) => void;
  pushVersion: () => void;        // 保存当前内容为快照
  undo: () => void;
  redo: () => void;
  setIsAIEditing: (editing: boolean) => void;

  // 批注管理
  addAnnotation: (annotation: Annotation) => void;
  removeAnnotation: (id: string) => void;
  updateAnnotationStatus: (id: string, status: Annotation['status']) => void;
  clearAnnotations: () => void;

  // 全局指令管理
  addDirective: (directive: string) => void;
  removeDirective: (index: number) => void;
}

export const useCanvasStore = create<CanvasState>((set, get) => ({
  canvas: null,
  canvasContent: '',
  isLoading: false,

  activeStage: 'explore',

  editMode: false,
  versionHistory: [],
  currentVersionIndex: -1,
  isAIEditing: false,

  pendingAnnotations: [],

  setCanvas: (canvas) => set({
    canvas,
    activeStage: canvas?.stage || 'explore',
  }),
  setCanvasContent: (content) => set({ canvasContent: content }),
  appendCanvasContent: (delta) =>
    set((state) => ({ canvasContent: state.canvasContent + delta })),
  clearCanvas: () =>
    set({
      canvas: null,
      canvasContent: '',
      activeStage: 'explore',
      editMode: false,
      versionHistory: [],
      currentVersionIndex: -1,
      isAIEditing: false,
      pendingAnnotations: [],
    }),
  setIsLoading: (loading) => set({ isLoading: loading }),

  setActiveStage: (stage) => set({ activeStage: stage }),

  setEditMode: (mode) => set({ editMode: mode }),

  pushVersion: () => {
    const { canvasContent, versionHistory, currentVersionIndex } = get();
    // 如果不在最新版本，截断后续历史
    const base =
      currentVersionIndex >= 0
        ? versionHistory.slice(0, currentVersionIndex + 1)
        : [...versionHistory];
    const next = [...base, canvasContent];
    // 限制历史深度（最多 50 个快照）
    if (next.length > 50) next.shift();
    set({ versionHistory: next, currentVersionIndex: next.length - 1 });
  },

  undo: () => {
    const { versionHistory, currentVersionIndex } = get();
    if (versionHistory.length === 0) return;
    const idx =
      currentVersionIndex < 0
        ? versionHistory.length - 1
        : currentVersionIndex - 1;
    if (idx < 0) return;
    set({
      canvasContent: versionHistory[idx],
      currentVersionIndex: idx,
    });
  },

  redo: () => {
    const { versionHistory, currentVersionIndex } = get();
    if (currentVersionIndex < 0 || currentVersionIndex >= versionHistory.length - 1) return;
    const idx = currentVersionIndex + 1;
    set({
      canvasContent: versionHistory[idx],
      currentVersionIndex: idx,
    });
  },

  setIsAIEditing: (editing) => set({ isAIEditing: editing }),

  // 批注管理
  addAnnotation: (annotation) => set((state) => ({
    pendingAnnotations: [...state.pendingAnnotations, annotation],
  })),
  removeAnnotation: (id) => set((state) => ({
    pendingAnnotations: state.pendingAnnotations.filter((a) => a.id !== id),
  })),
  updateAnnotationStatus: (id, status) => set((state) => ({
    pendingAnnotations: state.pendingAnnotations.map((a) =>
      a.id === id ? { ...a, status } : a
    ),
  })),
  clearAnnotations: () => set({ pendingAnnotations: [] }),

  // 全局指令管理
  addDirective: (directive) => set((state) => {
    if (!state.canvas) return {};
    return {
      canvas: {
        ...state.canvas,
        user_directives: [...state.canvas.user_directives, directive],
      },
    };
  }),
  removeDirective: (index) => set((state) => {
    if (!state.canvas) return {};
    return {
      canvas: {
        ...state.canvas,
        user_directives: state.canvas.user_directives.filter((_, i) => i !== index),
      },
    };
  }),
}));
</file>

<file path="frontend/src/stores/useCompareStore.ts">
import { create } from 'zustand';

interface CompareState {
  comparePreselectedPaperIds: string[];
  addComparePreselected: (id: string | string[]) => void;
  removeComparePreselected: (id: string) => void;
  clearComparePreselected: () => void;
}

export const useCompareStore = create<CompareState>((set) => ({
  comparePreselectedPaperIds: [],
  addComparePreselected: (id) =>
    set((state) => {
      const ids = Array.isArray(id) ? id : [id];
      const next = new Set([...state.comparePreselectedPaperIds, ...ids]);
      return { comparePreselectedPaperIds: [...next] };
    }),
  removeComparePreselected: (id) =>
    set((state) => ({
      comparePreselectedPaperIds: state.comparePreselectedPaperIds.filter((x) => x !== id),
    })),
  clearComparePreselected: () => set({ comparePreselectedPaperIds: [] }),
}));
</file>

<file path="frontend/src/stores/useProjectsStore.ts">
import { create } from 'zustand';
import type { Project } from '../types';
import {
  listProjects as apiListProjects,
  archiveProject as apiArchiveProject,
  unarchiveProject as apiUnarchiveProject,
  deleteProject as apiDeleteProject,
} from '../api/projects';

interface ProjectsState {
  projects: Project[];
  isLoading: boolean;

  fetchProjects: (includeArchived?: boolean) => Promise<void>;
  toggleArchive: (canvasId: string, isArchived: boolean) => Promise<void>;
  deleteProject: (canvasId: string) => Promise<void>;
  clearProjects: () => void;
}

export const useProjectsStore = create<ProjectsState>((set, get) => ({
  projects: [],
  isLoading: false,

  fetchProjects: async (includeArchived = true) => {
    set({ isLoading: true });
    try {
      const projects = await apiListProjects(includeArchived);
      set({ projects, isLoading: false });
    } catch {
      set({ isLoading: false });
    }
  },

  toggleArchive: async (canvasId, isArchived) => {
    try {
      if (isArchived) {
        await apiUnarchiveProject(canvasId);
      } else {
        await apiArchiveProject(canvasId);
      }
      // 更新本地状态
      set((state) => ({
        projects: state.projects.map((p) =>
          p.id === canvasId ? { ...p, archived: !isArchived } : p
        ),
      }));
    } catch {
      // 重新获取
      get().fetchProjects();
    }
  },

  deleteProject: async (canvasId) => {
    try {
      await apiDeleteProject(canvasId);
      set((state) => ({
        projects: state.projects.filter((p) => p.id !== canvasId),
      }));
    } catch {
      get().fetchProjects();
    }
  },

  clearProjects: () => set({ projects: [] }),
}));
</file>

<file path="frontend/src/stores/useToastStore.ts">
import { create } from 'zustand';
import type { Toast } from '../types';

interface ToastState {
  toasts: Toast[];
  addToast: (msg: string, type?: Toast['type']) => void;
  removeToast: (id: number) => void;
}

export const useToastStore = create<ToastState>((set) => ({
  toasts: [],

  addToast: (msg, type = 'info') => {
    const id = Date.now();
    set((state) => ({
      toasts: [...state.toasts, { id, msg, type }],
    }));
    // 3 秒后自动移除
    setTimeout(() => {
      set((state) => ({
        toasts: state.toasts.filter((t) => t.id !== id),
      }));
    }, 3000);
  },

  removeToast: (id) =>
    set((state) => ({
      toasts: state.toasts.filter((t) => t.id !== id),
    })),
}));
</file>

<file path="frontend/src/stores/useUIStore.ts">
import { create } from 'zustand';
import { persist } from 'zustand/middleware';

type ActiveTab = 'chat' | 'ingest' | 'users' | 'graph' | 'compare';

interface UIState {
  // 布局尺寸
  sidebarWidth: number;
  canvasWidth: number;

  // 面板开关
  isSidebarOpen: boolean;
  isCanvasOpen: boolean;
  isHistoryOpen: boolean;

  // 当前 Tab
  activeTab: ActiveTab;

  // Modals
  showSettingsModal: boolean;
  showCreateCollectionModal: boolean;
  showUserModal: boolean;

  // Actions
  setSidebarWidth: (width: number) => void;
  setCanvasWidth: (width: number) => void;
  toggleSidebar: () => void;
  toggleCanvas: () => void;
  setCanvasOpen: (open: boolean) => void;
  toggleHistory: () => void;
  setActiveTab: (tab: ActiveTab) => void;
  setShowSettingsModal: (show: boolean) => void;
  setShowCreateCollectionModal: (show: boolean) => void;
  setShowUserModal: (show: boolean) => void;
}

export const useUIStore = create<UIState>()(
  persist(
    (set) => ({
      sidebarWidth: 340,
      canvasWidth: 500,
      isSidebarOpen: true,
      isCanvasOpen: false,
      isHistoryOpen: false,
      activeTab: 'chat',
      showSettingsModal: false,
      showCreateCollectionModal: false,
      showUserModal: false,

      setSidebarWidth: (width) => set({ sidebarWidth: width }),
      setCanvasWidth: (width) => set({ canvasWidth: width }),
      toggleSidebar: () => set((s) => ({ isSidebarOpen: !s.isSidebarOpen })),
      toggleCanvas: () => set((s) => ({ isCanvasOpen: !s.isCanvasOpen })),
      setCanvasOpen: (open) => set({ isCanvasOpen: open }),
      toggleHistory: () => set((s) => ({ isHistoryOpen: !s.isHistoryOpen })),
      setActiveTab: (tab) => set({ activeTab: tab }),
      setShowSettingsModal: (show) => set({ showSettingsModal: show }),
      setShowCreateCollectionModal: (show) => set({ showCreateCollectionModal: show }),
      setShowUserModal: (show) => set({ showUserModal: show }),
    }),
    {
      name: 'ui-storage',
      partialize: (state) => ({
        sidebarWidth: state.sidebarWidth,
        canvasWidth: state.canvasWidth,
        isSidebarOpen: state.isSidebarOpen,
      }),
    }
  )
);
</file>

<file path="frontend/src/App.tsx">
import { useEffect, useCallback, useRef } from 'react';
import { useAuthStore, useUIStore } from './stores';
import { LoginPage } from './pages/LoginPage';
import { ChatPage } from './pages/ChatPage';
import { IngestPage } from './pages/IngestPage';
import { AdminPage } from './pages/AdminPage';
import { GraphExplorer } from './components/graph/GraphExplorer';
import { CompareView } from './components/compare/CompareView';
import { Sidebar } from './components/layout/Sidebar';
import { Header } from './components/layout/Header';
import { CanvasPanel } from './components/canvas/CanvasPanel';
import { SettingsModal } from './components/settings/SettingsModal';
import { DeepResearchDialog } from './components/workflow/DeepResearchDialog';
import { ToastContainer } from './components/ui/Toast';

function App() {
  const user = useAuthStore((s) => s.user);
  const logout = useAuthStore((s) => s.logout);

  const {
    activeTab,
    isCanvasOpen,
    setSidebarWidth,
    setCanvasWidth,
  } = useUIStore();

  // 监听全局登出事件
  useEffect(() => {
    const handleLogout = () => logout();
    window.addEventListener('auth:logout', handleLogout);
    return () => window.removeEventListener('auth:logout', handleLogout);
  }, [logout]);

  // 拖拽调整面板宽度
  const isResizingRef = useRef<false | 'sidebar' | 'canvas'>(false);

  const stopResizing = useCallback(() => {
    isResizingRef.current = false;
  }, []);

  const resize = useCallback(
    (e: MouseEvent) => {
      if (isResizingRef.current === 'sidebar') {
        const newWidth = e.clientX;
        if (newWidth > 200 && newWidth < 600) {
          setSidebarWidth(newWidth);
        }
      } else if (isResizingRef.current === 'canvas') {
        const newWidth = window.innerWidth - e.clientX;
        if (newWidth > 350 && newWidth < 1200) {
          setCanvasWidth(newWidth);
        }
      }
    },
    [setSidebarWidth, setCanvasWidth]
  );

  const handleStartResize = (type: 'sidebar' | 'canvas') => {
    isResizingRef.current = type;
  };

  useEffect(() => {
    window.addEventListener('mousemove', resize);
    window.addEventListener('mouseup', stopResizing);
    return () => {
      window.removeEventListener('mousemove', resize);
      window.removeEventListener('mouseup', stopResizing);
    };
  }, [resize, stopResizing]);

  // 未登录显示登录页
  if (!user) {
    return (
      <>
        <LoginPage />
        <ToastContainer />
      </>
    );
  }

  // 渲染当前 Tab 页面
  const renderContent = () => {
    switch (activeTab) {
      case 'chat':
        return <ChatPage />;
      case 'ingest':
        return <IngestPage />;
      case 'users':
        return <AdminPage />;
      case 'graph':
        return <GraphExplorer />;
      case 'compare':
        return <CompareView />;
      default:
        return <ChatPage />;
    }
  };

  return (
    <div className="flex h-screen bg-[var(--bg-app)] text-[var(--text-primary)] font-sans overflow-hidden bg-cover bg-fixed">
      {/* Toast */}
      <ToastContainer />

      {/* Sidebar */}
      <Sidebar onStartResize={() => handleStartResize('sidebar')} />

      {/* 主内容区 */}
      <div className="flex-1 flex flex-col h-full overflow-hidden relative bg-transparent">
        <Header />
        <div className="flex-1 flex min-h-0 relative">
          <div className="flex-1 flex flex-col min-h-0 overflow-y-auto scrollbar-thin">
            {renderContent()}
          </div>
        </div>
      </div>

      {/* Canvas Panel */}
      {isCanvasOpen && (
        <CanvasPanel onStartResize={() => handleStartResize('canvas')} />
      )}

      {/* 高级配置 Modal */}
      <SettingsModal />

      {/* Deep Research 澄清对话框 */}
      <DeepResearchDialog />
    </div>
  );
}

export default App;
</file>

<file path="frontend/src/index.css">
@import "tailwindcss";

:root {
  /* --- 深海主题配色 (Deep Sea Theme) --- */

  /* 基础背景 - 深邃海洋 */
  /* 使用渐变色模拟深海光影，从极深的午夜蓝过渡到深蓝 */
  --bg-app: #020617;        /* slate-950/base black */
  --bg-deep-gradient: radial-gradient(circle at top left, #1e293b 0%, #020617 40%, #020617 100%);
  
  /* 侧边栏 - 深海潜航器内壁感 */
  --bg-sidebar: rgba(15, 23, 42, 0.95); /* slate-900 alpha */
  
  /* 面板/卡片 - 玻璃拟态/水下UI */
  --bg-panel: rgba(30, 41, 59, 0.86);    /* slate-800 alpha */
  --bg-surface: rgba(30, 41, 59, 0.72);  /* slate-800 alpha */
  --bg-surface-hover: rgba(51, 65, 85, 0.85); /* slate-700 alpha */
  --bg-muted: rgba(15, 23, 42, 0.72);    /* slate-900 alpha */
  
  /* 聊天气泡 */
  /* 用户: 科技感青蓝渐变 */
  --bg-bubble-user: linear-gradient(135deg, #0ea5e9 0%, #2563eb 100%);
  /* AI: 深海半透明玻璃 */
  --bg-bubble-ai: rgba(30, 41, 59, 0.8);

  /* 文字颜色 - 生物荧光感 */
  --text-primary: #f8fafc;   /* slate-50 - 提高主文本可读性 */
  --text-secondary: #cbd5e1; /* slate-300 - 用于正文，避免过浅 */
  --text-tertiary: #94a3b8;  /* slate-400 - 次级提示 */
  --text-highlight: #38bdf8; /* sky-400 - 高亮荧光蓝 */
  --text-on-primary: #ffffff;

  /* 边框与分割线 - 微弱光晕 */
  --border-subtle: rgba(148, 163, 184, 0.15);  /* 极淡的白线 */
  --border-highlight: rgba(56, 189, 248, 0.4); /* 高亮蓝线 */
  --border-focus: #22d3ee;   /* cyan-400 */

  /* 品牌色 - 海洋之光 */
  --primary: #0ea5e9;        /* sky-500 */
  --primary-hover: #0284c7;  /* sky-600 */
  --primary-glow: rgba(14, 165, 233, 0.5); /* 主色光晕 */
  
  /* 辅助色 */
  --accent-cyan: #06b6d4;    /* cyan-500 */
  --accent-teal: #14b8a6;    /* teal-500 */
  --accent-indigo: #6366f1;  /* indigo-500 */

  /* 功能色 */
  --success: #10b981;        /* emerald-500 */
  --warning: #f59e0b;        /* amber-500 */
  --error: #ef4444;          /* red-500 */
  
  /* 特效 */
  --glass-border: 1px solid rgba(255, 255, 255, 0.08);
  --shadow-neon: 0 0 10px rgba(14, 165, 233, 0.3), 0 0 20px rgba(14, 165, 233, 0.1);
  --backdrop-blur: blur(12px);
}

body {
  background-color: var(--bg-app);
  background-image: 
    radial-gradient(circle at 15% 50%, rgba(14, 165, 233, 0.08) 0%, transparent 25%),
    radial-gradient(circle at 85% 30%, rgba(99, 102, 241, 0.08) 0%, transparent 25%);
  background-attachment: fixed;
  color: var(--text-primary);
}

/* 滚动条 */
.scrollbar-thin::-webkit-scrollbar {
  width: 6px;
  height: 6px;
}
.scrollbar-thin::-webkit-scrollbar-track {
  background: rgba(15, 23, 42, 0.5);
}
.scrollbar-thin::-webkit-scrollbar-thumb {
  background: rgba(71, 85, 105, 0.6);
  border-radius: 3px;
  border: 1px solid rgba(255,255,255,0.05);
}
.scrollbar-thin::-webkit-scrollbar-thumb:hover {
  background: rgba(100, 116, 139, 0.8);
}

.scrollbar-hide::-webkit-scrollbar {
  display: none;
}
.scrollbar-hide {
  -ms-overflow-style: none;
  scrollbar-width: none;
}

/* 动画定义 */
@keyframes float {
  0% { transform: translateY(0px); }
  50% { transform: translateY(-6px); }
  100% { transform: translateY(0px); }
}
@keyframes pulse-glow {
  0%, 100% { box-shadow: 0 0 5px rgba(56, 189, 248, 0.2); opacity: 0.8; }
  50% { box-shadow: 0 0 15px rgba(56, 189, 248, 0.6); opacity: 1; }
}
@keyframes ripple {
  0% { box-shadow: 0 0 0 0 rgba(34, 211, 238, 0.2); }
  100% { box-shadow: 0 0 0 10px rgba(34, 211, 238, 0); }
}

/* 动画类 */
.animate-float { animation: float 6s ease-in-out infinite; }
.animate-pulse-glow { animation: pulse-glow 3s infinite; }
.animate-ripple { animation: ripple 2s infinite; }

.animate-in {
  animation-duration: 300ms;
  animation-timing-function: ease-out;
  animation-fill-mode: both;
}
.fade-in { animation-name: fade-in; }
/* ...保持原有slide动画... */
@keyframes fade-in { from { opacity: 0; } to { opacity: 1; } }
@keyframes slide-in-from-right { from { transform: translateX(100%); } to { transform: translateX(0); } }
@keyframes slide-in-from-bottom { from { transform: translateY(8px); opacity: 0; } to { transform: translateY(0); opacity: 1; } }
@keyframes slide-in-from-top { from { transform: translateY(-4px); opacity: 0; } to { transform: translateY(0); opacity: 1; } }
@keyframes zoom-in { from { transform: scale(0.95); opacity: 0; } to { transform: scale(1); opacity: 1; } }

.slide-in-from-right { animation-name: slide-in-from-right; }
.slide-in-from-bottom-2 { animation-name: slide-in-from-bottom; }
.slide-in-from-top-1, .slide-in-from-top-2 { animation-name: slide-in-from-top; }
.zoom-in, .zoom-in-95 { animation-name: zoom-in; }

.duration-200 { animation-duration: 200ms; }
.duration-300 { animation-duration: 300ms; }

/* 深海科技组件风格 */

/* 玻璃面板 */
.glass-panel {
  background: var(--bg-panel);
  backdrop-filter: var(--backdrop-blur);
  border: var(--glass-border);
  box-shadow: 0 8px 32px 0 rgba(0, 0, 0, 0.36);
}

/* 侧边栏玻璃 */
.glass-sidebar {
  background: rgba(2, 6, 23, 0.85);
  backdrop-filter: blur(8px);
  border-right: 1px solid rgba(255, 255, 255, 0.05);
}

/* 头部玻璃 */
.glass-header {
  background: rgba(2, 6, 23, 0.6);
  backdrop-filter: blur(12px);
  border-bottom: 1px solid rgba(255, 255, 255, 0.05);
}

/* 霓虹输入框 */
.tech-input {
  background-color: rgba(15, 23, 42, 0.6);
  border: 1px solid var(--border-subtle);
  color: var(--text-primary);
  transition: all 0.3s ease;
}
.tech-input:focus {
  border-color: var(--border-highlight);
  box-shadow: 0 0 10px rgba(56, 189, 248, 0.2);
  outline: none;
  background-color: rgba(15, 23, 42, 0.9);
}

/* 科技按钮 */
.tech-button {
  background: rgba(56, 189, 248, 0.1);
  border: 1px solid rgba(56, 189, 248, 0.2);
  color: var(--text-highlight);
  transition: all 0.2s;
}
.tech-button:hover {
  background: rgba(56, 189, 248, 0.2);
  box-shadow: 0 0 8px rgba(56, 189, 248, 0.3);
  border-color: rgba(56, 189, 248, 0.4);
}
.tech-button.active {
  background: rgba(56, 189, 248, 0.25);
  box-shadow: 0 0 12px rgba(56, 189, 248, 0.4);
  border-color: var(--text-highlight);
}

/* Refine editor selection: semi-transparent highlight */
textarea.refine-editor::selection {
  background: rgba(56, 189, 248, 0.28) !important;
  color: inherit;
}

textarea.refine-editor::-moz-selection {
  background: rgba(56, 189, 248, 0.28) !important;
  color: inherit;
}
</file>

<file path="frontend/src/main.tsx">
import { StrictMode } from 'react'
import { createRoot } from 'react-dom/client'
import './i18n'
import './index.css'
import App from './App'

createRoot(document.getElementById('root')!).render(
  <StrictMode>
    <App />
  </StrictMode>,
)
</file>

<file path="frontend/.gitignore">
# Logs
logs
*.log
npm-debug.log*
yarn-debug.log*
yarn-error.log*
pnpm-debug.log*
lerna-debug.log*

node_modules
dist
dist-ssr
*.local

# Editor directories and files
.vscode/*
!.vscode/extensions.json
.idea
.DS_Store
*.suo
*.ntvs*
*.njsproj
*.sln
*.sw?
</file>

<file path="frontend/eslint.config.js">
import js from '@eslint/js'
import globals from 'globals'
import reactHooks from 'eslint-plugin-react-hooks'
import reactRefresh from 'eslint-plugin-react-refresh'
import tseslint from 'typescript-eslint'
import { defineConfig, globalIgnores } from 'eslint/config'

export default defineConfig([
  globalIgnores(['dist']),
  {
    files: ['**/*.{ts,tsx}'],
    extends: [
      js.configs.recommended,
      tseslint.configs.recommended,
      reactHooks.configs.flat.recommended,
      reactRefresh.configs.vite,
    ],
    languageOptions: {
      ecmaVersion: 2020,
      globals: globals.browser,
    },
  },
])
</file>

<file path="frontend/index.html">
<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <link rel="icon" type="image/svg+xml" href="/vite.svg" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>frontend</title>
  </head>
  <body>
    <div id="root"></div>
    <script type="module" src="/src/main.tsx"></script>
  </body>
</html>
</file>

<file path="frontend/README.md">
# Frontend（DeepSea RAG）

前端基于 React + TypeScript + Vite + Zustand + Tailwind CSS，负责聊天交互、画布编辑、配置管理、图谱探索、多文档对比和 Deep Research 工作流。

更新时间：2026-02-19

## 技术栈

| 技术 | 版本 | 用途 |
|---|---|---|
| React | 19.2.0 | UI 框架 |
| TypeScript | ~5.9.3 | 类型系统 |
| Vite | ^7.2.4 | 构建工具 |
| Zustand | ^5.0.11 | 状态管理 |
| Tailwind CSS | ^4.1.18 | 样式框架 |
| React Router DOM | ^7.13.0 | 路由 |
| i18next | — | 国际化（中/英） |
| react-markdown | ^10.1.0 | Markdown 渲染 |
| react-pdf | ^10.3.0 | PDF 预览 |
| react-force-graph-2d | ^1.29.1 | 图谱可视化 |
| Lucide React | — | 图标库 |

## 本地开发

要求：

- Node.js `^20.19.0 || >=22.12.0`
- npm（建议与 Node LTS 配套）

```bash
cd frontend
npm install
npm run dev
```

默认地址：`http://localhost:5173`

## 与后端联调

- 后端默认：`http://127.0.0.1:9999`
- Vite 配置了 `/api` 代理到后端
- 推荐从仓库根目录启动：`bash scripts/start.sh`
- 如需与 lock 文件完全一致，使用 `npm ci` 替代 `npm install`

## 构建

```bash
npm run build     # TypeScript 编译 + Vite 构建
npm run preview   # 预览生产构建
```

## 目录说明

```text
src/
├── main.tsx              # 应用入口
├── App.tsx               # 根组件（路由配置）
├── index.css             # 全局样式
│
├── pages/                # 页面组件
│   ├── ChatPage.tsx      #   主聊天界面
│   ├── IngestPage.tsx    #   文档入库
│   ├── LoginPage.tsx     #   登录认证
│   └── AdminPage.tsx     #   管理后台
│
├── components/           # 业务组件
│   ├── chat/             #   聊天相关
│   │   ├── ChatWindow.tsx
│   │   ├── ChatInput.tsx
│   │   ├── ToolTracePanel.tsx
│   │   └── RetrievalDebugPanel.tsx
│   ├── canvas/           #   画布协作
│   │   ├── CanvasPanel.tsx
│   │   ├── ExploreStage.tsx
│   │   ├── OutlineStage.tsx
│   │   ├── DraftingStage.tsx
│   │   ├── RefineStage.tsx
│   │   ├── StageStepper.tsx
│   │   └── FloatingToolbar.tsx
│   ├── compare/          #   多文档对比
│   │   └── CompareView.tsx
│   ├── graph/            #   图谱可视化
│   │   └── GraphExplorer.tsx
│   ├── workflow/         #   Deep Research 工作流
│   │   ├── DeepResearchDialog.tsx
│   │   ├── DeepResearchSettingsPopover.tsx
│   │   ├── WorkflowStepper.tsx
│   │   ├── CommandPalette.tsx
│   │   ├── IntentModeSelector.tsx
│   │   └── IntentConfirmPopover.tsx
│   ├── research/         #   研究进度
│   │   └── ResearchProgressPanel.tsx
│   ├── settings/         #   设置
│   │   └── SettingsModal.tsx
│   ├── layout/           #   布局
│   │   ├── Header.tsx
│   │   └── Sidebar.tsx
│   └── ui/               #   通用 UI 组件
│       ├── Modal.tsx
│       ├── Toast.tsx
│       └── PdfViewerModal.tsx
│
├── stores/               # Zustand 状态管理
│   ├── index.ts
│   ├── useChatStore.ts       # 聊天状态
│   ├── useCanvasStore.ts     # 画布状态
│   ├── useConfigStore.ts     # 配置（含 Deep Research 设置持久化）
│   ├── useAuthStore.ts       # 认证状态
│   ├── useProjectsStore.ts   # 项目管理
│   ├── useCompareStore.ts    # 对比状态
│   ├── useUIStore.ts         # UI 全局状态
│   └── useToastStore.ts      # 消息提示
│
├── api/                  # 后端接口封装
│   ├── index.ts
│   ├── client.ts             # Axios 实例配置
│   ├── chat.ts               # /chat, /chat/stream
│   ├── canvas.ts             # /canvas/*
│   ├── compare.ts            # /compare/*
│   ├── graph.ts              # /graph/*
│   ├── ingest.ts             # /ingest/*
│   ├── models.ts             # /models/*, /llm/*
│   ├── auth.ts               # /auth/*, /admin/*
│   ├── projects.ts           # /projects/*
│   ├── auto.ts               # /auto-complete
│   └── health.ts             # /health, /health/detailed
│
├── types/                # TypeScript 类型定义
│   └── index.ts
│
└── i18n/                 # 国际化
    ├── index.ts              # i18next 初始化
    └── locales/
        ├── en.json           # English
        └── zh.json           # 中文
```

## 关键交互

### 聊天

- 请求：`POST /chat`、`POST /chat/stream`
- SSE 事件：`meta` → `dashboard` → `tool_trace` → `delta` → `done`
- 工具轨迹面板实时展示 Agent 工具调用过程

### Deep Research

- 启动前设置：`DeepResearchSettingsPopover`（持久化到 localStorage）
- 意图检测：`IntentModeSelector` + `IntentConfirmPopover`
- 研究对话框：`DeepResearchDialog`（澄清 → 大纲确认 → 执行 → 审核）
- 进度面板：`ResearchProgressPanel`（coverage 曲线、成本状态、效率评分）
- 后台任务轮询：`GET /deep-research/jobs/{id}/events`

### 画布协作

- 四阶段流程：Explore → Outline → Drafting → Refine
- 各阶段独立组件，`StageStepper` 导航
- 支持快照、恢复、AI 段落编辑、引用管理

### 多文档对比

- 候选来源：会话引文 + 本地文库搜索
- 结构化对比：`POST /compare`

### 图谱可视化

- `GraphExplorer`：力导向图布局，实体/关系交互

## 开发约定

- 新接口先在 `src/api/` 封装，再接入组件
- 新状态优先进入对应 Zustand store，避免组件内分散状态
- 新增后端字段时同步更新 `src/types/index.ts` 与消费组件
- 新增文案时同步更新 `src/i18n/locales/en.json` 和 `zh.json`
- 提交前至少跑通关键页面：聊天、入库、对比
- CSS 优先使用 Tailwind 工具类，避免自定义 CSS
</file>

<file path="frontend/tsconfig.app.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.app.tsbuildinfo",
    "target": "ES2022",
    "useDefineForClassFields": true,
    "lib": ["ES2022", "DOM", "DOM.Iterable"],
    "module": "ESNext",
    "types": ["vite/client"],
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,
    "jsx": "react-jsx",

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["src"]
}
</file>

<file path="frontend/tsconfig.json">
{
  "files": [],
  "references": [
    { "path": "./tsconfig.app.json" },
    { "path": "./tsconfig.node.json" }
  ]
}
</file>

<file path="frontend/tsconfig.node.json">
{
  "compilerOptions": {
    "tsBuildInfoFile": "./node_modules/.tmp/tsconfig.node.tsbuildinfo",
    "target": "ES2023",
    "lib": ["ES2023"],
    "module": "ESNext",
    "types": ["node"],
    "skipLibCheck": true,

    /* Bundler mode */
    "moduleResolution": "bundler",
    "allowImportingTsExtensions": true,
    "verbatimModuleSyntax": true,
    "moduleDetection": "force",
    "noEmit": true,

    /* Linting */
    "strict": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "erasableSyntaxOnly": true,
    "noFallthroughCasesInSwitch": true,
    "noUncheckedSideEffectImports": true
  },
  "include": ["vite.config.ts"]
}
</file>

<file path="frontend/vite.config.ts">
import { defineConfig } from 'vite'
import react from '@vitejs/plugin-react'
import tailwindcss from '@tailwindcss/vite'

// https://vite.dev/config/
export default defineConfig({
  plugins: [react(), tailwindcss()],
  server: {
    port: 5173,
    proxy: {
      '/api': {
        target: 'http://127.0.0.1:9999',
        changeOrigin: true,
        rewrite: (path) => path.replace(/^\/api/, ''),
      },
    },
  },
})
</file>

<file path="scripts/_test_ncbi_integration.py">
"""
快速集成测试：NCBI + 路由计划
Layer 1 – NCBISearcher 直连 PubMed（真实网络请求，含 efetch 摘要）
Layer 2 – 时效性启发式规则路由（无 LLM）
Layer 3 – LLM 驱动的智能路由计划 (deepseek)
"""
import asyncio
import sys
sys.path.insert(0, ".")

SEP = "─" * 62


# ─────────────────────────────────────────────────────────────────
# Layer 1: NCBI 搜索器（live 网络请求）
# ─────────────────────────────────────────────────────────────────
async def test_ncbi():
    print(f"\n{SEP}")
    print("Layer 1 — NCBISearcher  (live PubMed API)")
    print(SEP)

    from src.retrieval.ncbi_search import get_ncbi_searcher

    searcher = get_ncbi_searcher()
    query = "deep sea cold seep methane oxidizing bacteria"
    print(f"Query : {query!r}")
    hits = await searcher.search(query, limit=3)

    if not hits:
        print("❌  No results returned")
        return False

    has_abstract = 0
    for i, h in enumerate(hits, 1):
        m = h.get("metadata", {})
        abstract = (m.get("abstract") or "")
        abstract_snippet = abstract[:130] + ("…" if len(abstract) > 130 else "")
        if abstract:
            has_abstract += 1
        print(f"\n  [{i}] {m.get('title', '(no title)')}")
        print(f"       Year={m.get('year')}  DOI={m.get('doi') or '—'}")
        print(f"       Score={h.get('score')}  URL={m.get('url', '')}")
        if abstract_snippet:
            print(f"       Abstract: {abstract_snippet}")
        else:
            print(f"       Abstract: (none)")

    print(f"\n  Results: {len(hits)}/3   Abstracts fetched: {has_abstract}/{len(hits)}")
    ok = len(hits) > 0
    print(f"{'✅' if ok else '❌'}  Layer 1 {'PASS' if ok else 'FAIL'}")
    return ok


# ─────────────────────────────────────────────────────────────────
# Layer 2: 启发式路由（纯规则，无 LLM）
# ─────────────────────────────────────────────────────────────────
def test_heuristic_routing():
    print(f"\n{SEP}")
    print("Layer 2 — Freshness heuristic + fallback routing  (no LLM)")
    print(SEP)

    from src.retrieval.smart_query_optimizer import (
        SmartQueryOptimizer,
        _is_fresh_query_heuristic,
    )

    # 2a: 时效性检测
    cases = [
        ("cold seep microbiology",                   False),
        ("latest breakthroughs in mRNA vaccine 2025", True),
        ("最新深海冷泉研究进展",                        True),
        ("CRISPR gene editing mechanism",             False),
        ("today breaking news AI",                    True),
    ]
    all_ok = True
    print("  Freshness heuristic:")
    for q, expect in cases:
        got = _is_fresh_query_heuristic(q)
        ok  = got == expect
        if not ok:
            all_ok = False
        print(f"    {'✅' if ok else '❌'}  expect={expect!s:<5}  got={got!s:<5}  {q!r}")

    # 2b: 回退路由计划
    opt = SmartQueryOptimizer()
    candidates = ["ncbi", "tavily", "scholar", "google"]

    plan_bio = opt._fallback_routing_plan("cold seep microbiology", candidates, is_fresh=False)
    plan_fresh = opt._fallback_routing_plan("latest AI news 2026", candidates, is_fresh=True)

    print("\n  Fallback routing plans:")
    print(f"    bio   → primary={plan_bio.primary}  fallback={plan_bio.fallback}  is_fresh={plan_bio.is_fresh}")
    print(f"    fresh → primary={plan_fresh.primary}  fallback={plan_fresh.fallback}  is_fresh={plan_fresh.is_fresh}")

    bio_ok   = "ncbi" in plan_bio.primary
    fresh_ok = "tavily" in plan_fresh.primary
    if not bio_ok:
        print("  ❌  bio plan should have ncbi in primary")
        all_ok = False
    if not fresh_ok:
        print("  ❌  fresh plan should have tavily in primary")
        all_ok = False

    print(f"\n{'✅' if all_ok else '❌'}  Layer 2 {'PASS' if all_ok else 'FAIL'}")
    return all_ok


# ─────────────────────────────────────────────────────────────────
# Layer 3: LLM 智能路由计划（deepseek）
# ─────────────────────────────────────────────────────────────────
def test_llm_routing():
    print(f"\n{SEP}")
    print("Layer 3 — LLM routing plan  (deepseek)")
    print(SEP)

    from src.retrieval.smart_query_optimizer import get_smart_query_optimizer

    opt = get_smart_query_optimizer()
    candidates = ["ncbi", "tavily", "scholar", "google"]

    test_cases = [
        # (query, must_have_in_primary, must_NOT_have_in_primary)
        ("deep sea cold seep microbial diversity",        "ncbi",   None),
        ("latest ChatGPT model features 2026",            "tavily", None),
        ("BRCA1 breast cancer mutation clinical trial",   "ncbi",   None),
    ]

    all_ok = True
    for q, must, must_not in test_cases:
        print(f"\n  Query: {q!r}")
        try:
            plan = opt.get_routing_plan(q, candidates)
            print(f"    primary  : {plan.primary}")
            print(f"    fallback : {plan.fallback}")
            print(f"    is_fresh : {plan.is_fresh}")
            for eng, qlist in plan.queries.items():
                for qstr in qlist:
                    print(f"    [{eng}] {qstr!r}")

            # 软断言
            if must and (must not in plan.primary):
                print(f"    ⚠️  expected '{must}' in primary (routing heuristic may differ)")
        except Exception as e:
            print(f"    ❌ Error: {e}")
            all_ok = False

    print(f"\n{'✅' if all_ok else '❌'}  Layer 3 {'PASS' if all_ok else 'FAIL'}")
    return all_ok


# ─────────────────────────────────────────────────────────────────
# main
# ─────────────────────────────────────────────────────────────────
async def main():
    results: dict = {}

    try:
        results["ncbi_live"]   = await test_ncbi()
    except Exception as e:
        print(f"\n❌ Layer 1 exception: {e}")
        import traceback; traceback.print_exc()
        results["ncbi_live"] = False

    try:
        results["heuristic"]   = test_heuristic_routing()
    except Exception as e:
        print(f"\n❌ Layer 2 exception: {e}")
        import traceback; traceback.print_exc()
        results["heuristic"] = False

    try:
        results["llm_routing"] = test_llm_routing()
    except Exception as e:
        print(f"\n❌ Layer 3 exception: {e}")
        import traceback; traceback.print_exc()
        results["llm_routing"] = False

    print(f"\n{'═' * 62}")
    print("Test Summary")
    print("═" * 62)
    for k, v in results.items():
        print(f"  {'✅' if v else '❌'}  {k}")
    passed = sum(results.values())
    total  = len(results)
    print(f"\n  Result: {passed}/{total} layers passed")
    sys.exit(0 if passed == total else 1)


if __name__ == "__main__":
    asyncio.run(main())
</file>

<file path="scripts/00_healthcheck_docker.sh">
#!/usr/bin/env bash
# ============================================================
#  Docker 健康检查脚本 - 启动后必须执行
# ============================================================
set -euo pipefail

echo "========== Docker 健康检查 =========="

# 容器状态
echo "[1/4] 检查容器状态..."
docker compose ps

# Milvus 健康
echo ""
echo "[2/4] 检查 Milvus..."
if curl -sf http://localhost:9091/healthz >/dev/null 2>&1; then
  echo "[OK] Milvus healthz OK"
else
  echo "[FAIL] Milvus healthz failed"
  echo "  尝试: docker logs --tail 50 deepsea-milvus"
fi

# MinIO 健康
echo ""
echo "[3/4] 检查 MinIO..."
if curl -sf http://localhost:9000/minio/health/live >/dev/null 2>&1; then
  echo "[OK] MinIO live OK"
else
  echo "[FAIL] MinIO health failed"
  echo "  尝试: docker logs --tail 50 deepsea-minio"
fi

# etcd 健康
echo ""
echo "[4/4] 检查 etcd..."
if docker exec deepsea-etcd etcdctl endpoint health >/dev/null 2>&1; then
  echo "[OK] etcd endpoint health OK"
else
  echo "[FAIL] etcd health failed"
  echo "  尝试: docker logs --tail 50 deepsea-etcd"
fi

echo ""
echo "========== 健康检查完成 =========="
</file>

<file path="scripts/00_preflight_check.sh">
#!/usr/bin/env bash
# ============================================================
#  环境预检脚本 - 启动前必须执行
# ============================================================
set -euo pipefail

echo "========== 环境预检 =========="

# Python
if command -v python >/dev/null 2>&1; then
  echo "[OK] Python: $(python --version 2>&1)"
else
  echo "[FAIL] Python not found"
  exit 1
fi

# Docker
if command -v docker >/dev/null 2>&1; then
  echo "[OK] Docker: $(docker --version)"
else
  echo "[FAIL] Docker not found"
  exit 1
fi

# Docker Compose
if docker compose version >/dev/null 2>&1; then
  echo "[OK] Docker Compose: $(docker compose version --short)"
else
  echo "[FAIL] Docker Compose not found"
  exit 1
fi

# 端口检查
check_port() {
  local port=$1
  if lsof -i:"$port" >/dev/null 2>&1; then
    echo "[WARN] Port $port is in use"
  else
    echo "[OK] Port $port is free"
  fi
}

check_port 19530
check_port 9091
check_port 9000
check_port 2379

# .env 检查
if [[ -f ".env" ]]; then
  echo "[OK] .env exists"
  # 关键变量
  for var in RAG_ENV MILVUS_HOST COMPUTE_DEVICE INDEX_TYPE; do
    val=$(grep "^${var}=" .env 2>/dev/null | cut -d'=' -f2 || true)
    if [[ -n "$val" ]]; then
      echo "     $var=$val"
    else
      echo "[WARN] $var not set in .env"
    fi
  done
else
  echo "[WARN] .env not found, will copy from .env.example on install"
fi

# GPU 检查（仅 prod）
RAG_ENV=$(grep "^RAG_ENV=" .env 2>/dev/null | cut -d'=' -f2 || echo "dev")
if [[ "$RAG_ENV" == "prod" ]]; then
  if command -v nvidia-smi >/dev/null 2>&1; then
    echo "[OK] nvidia-smi available"
    nvidia-smi --query-gpu=name --format=csv,noheader | head -2
  else
    echo "[WARN] nvidia-smi not found (GPU may not work)"
  fi
fi

echo "========== 预检完成 =========="
</file>

<file path="scripts/01_init_env.py">
#!/usr/bin/env python
"""步骤1: 初始化环境"""

import sys
import json
from datetime import datetime

sys.path.insert(0, ".")

from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)
from src.indexing.milvus_ops import milvus
from src.indexing.embedder import embedder


def main():
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    settings.path.ensure_dirs()

    logger.info("=" * 60)
    logger.info("深海科研知识库 - 环境初始化")
    logger.info("=" * 60)

    settings.print_info()

    artifact = {"run_id": run_id, "steps": []}

    # 1. 测试 Milvus
    logger.info("[1/3] 测试 Milvus 连接...")
    try:
        collections = milvus.client.list_collections()
        logger.info(f"Milvus 连接成功: {settings.milvus.uri}")
        artifact["steps"].append({"step": "milvus_connect", "status": "ok"})
    except Exception as e:
        logger.error(f"连接失败: {e}")
        logger.error("请运行: docker compose --profile dev up -d")
        logger.error("然后执行: bash scripts/00_healthcheck_docker.sh")
        artifact["steps"].append({"step": "milvus_connect", "status": "fail", "error": str(e)})
        _save_artifact(artifact, run_id)
        return

    # 2. 创建 Collections
    logger.info("[2/3] 初始化 Collections...")
    milvus.init_all_collections()
    artifact["steps"].append({
        "step": "init_collections",
        "status": "ok",
        "collections": settings.collection.all()
    })

    # 3. 测试 Embedding
    logger.info("[3/3] 测试 Embedding 模型...")
    try:
        result = embedder.encode(["测试文本"])
        dense_dim = len(result["dense"][0])
        sparse_nnz = result["sparse"]._getrow(0).nnz
        logger.info(f"BGE-M3 正常 (dense_dim={dense_dim}, sparse_nnz={sparse_nnz})")

        if dense_dim != 1024:
            logger.warning(f"dense_dim 应为 1024，实际为 {dense_dim}")

        artifact["steps"].append({
            "step": "embedding_test",
            "status": "ok",
            "dense_dim": dense_dim,
            "sparse_nnz": sparse_nnz
        })
    except Exception as e:
        logger.error(f"模型加载失败: {e}")
        artifact["steps"].append({"step": "embedding_test", "status": "fail", "error": str(e)})
        _save_artifact(artifact, run_id)
        return

    _save_artifact(artifact, run_id)

    logger.info("=" * 60)
    logger.info("环境初始化完成！")
    logger.info("=" * 60)


def _save_artifact(data: dict, run_id: str):
    path = settings.path.artifacts / f"01_init_env_{run_id}.json"
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    logger.info(f"产物已保存: {path}")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/02_parse_papers.py">
#!/usr/bin/env python
"""步骤2: 解析 PDF 文件（EnrichedDoc 完整 schema）"""

import sys
import argparse
from pathlib import Path
from datetime import datetime

sys.path.insert(0, ".")

from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)


def main():
    parser = argparse.ArgumentParser(description="解析 PDF 为 EnrichedDoc")
    parser.add_argument("--skip-enrichment", action="store_true", help="跳过 LLM 增强（表格摘要、图表解读）")
    parser.add_argument("--skip-table-enrichment", action="store_true", help="跳过表格 LLM 增强")
    parser.add_argument("--skip-figure-enrichment", action="store_true", help="跳过图像 LLM 增强")
    parser.add_argument("--llm-text-provider", type=str, help="覆盖文本 LLM provider")
    parser.add_argument("--llm-vision-provider", type=str, help="覆盖图像 LLM provider")
    parser.add_argument("--llm-text-model", type=str, help="覆盖文本 LLM model")
    parser.add_argument("--llm-vision-model", type=str, help="覆盖图像 LLM model")
    parser.add_argument("--llm-vision-concurrency", type=int, help="图像 LLM 并发数")
    parser.add_argument("--llm-text-max-tokens", type=int, help="覆盖文本 LLM max_tokens")
    parser.add_argument("--llm-vision-max-tokens", type=int, help="覆盖图像 LLM max_tokens")
    parser.add_argument("--llm-json-repair-max-tokens", type=int, help="覆盖 JSON 修复 max_tokens")
    args = parser.parse_args()

    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    settings.path.ensure_dirs()

    logger.info("=" * 60)
    logger.info("深海科研知识库 - PDF 解析（EnrichedDoc）")
    logger.info("=" * 60)

    input_dir = settings.path.raw_papers
    output_base = settings.path.parsed

    pdf_files = list(input_dir.glob("*.pdf"))
    logger.info(f"输入目录: {input_dir}")
    logger.info(f"找到 PDF: {len(pdf_files)} 个")

    if not pdf_files:
        logger.warning("未找到 PDF 文件")
        logger.warning(f"请将 PDF 放入: {input_dir}")
        return

    # 加载 parser 与 LLM
    try:
        from src.parser.pdf_parser import PDFProcessor, ParserConfig
    except ImportError as e:
        logger.error(f"pdf_parser 导入失败: {e}")
        return

    config_path = Path(__file__).parent.parent / "config" / "rag_config.json"
    if config_path.exists():
        try:
            config = ParserConfig.from_json(config_path)
        except Exception:
            config = ParserConfig()
    else:
        config = ParserConfig()
    if args.llm_text_provider:
        config.llm_text_provider = args.llm_text_provider
    if args.llm_vision_provider:
        config.llm_vision_provider = args.llm_vision_provider
    if args.llm_text_model:
        config.llm_text_model = args.llm_text_model
    if args.llm_vision_model:
        config.llm_vision_model = args.llm_vision_model
    if args.llm_vision_concurrency is not None:
        config.llm_vision_concurrency = max(1, int(args.llm_vision_concurrency))
    if args.llm_text_max_tokens is not None:
        config.llm_text_max_tokens = max(1, int(args.llm_text_max_tokens))
    if args.llm_vision_max_tokens is not None:
        config.llm_vision_max_tokens = max(1, int(args.llm_vision_max_tokens))
    if args.llm_json_repair_max_tokens is not None:
        config.llm_json_repair_max_tokens = max(1, int(args.llm_json_repair_max_tokens))
    if args.skip_table_enrichment:
        config.enrich_tables = False
    if args.skip_figure_enrichment:
        config.enrich_figures = False

    llm_manager = None
    if not args.skip_enrichment:
        try:
            from src.llm import LLMManager
            llm_manager = LLMManager.from_json(str(config_path))
        except Exception as e:
            logger.warning(f"LLM 未加载，将跳过增强: {e}")

    processor = PDFProcessor(config=config, llm_manager=llm_manager)

    artifact = {
        "run_id": run_id,
        "input_count": len(pdf_files),
        "success_count": 0,
        "fail_count": 0,
        "results": [],
        "badcases": [],
    }

    logger.info(f"开始解析... (skip_enrichment={args.skip_enrichment})")
    for i, pdf_path in enumerate(pdf_files, 1):
        logger.info(f"[{i}/{len(pdf_files)}] {pdf_path.name}")

        try:
            output_dir = output_base / pdf_path.stem
            doc = processor.process(
                pdf_path,
                output_dir=output_dir,
                skip_enrichment=args.skip_enrichment,
            )

            n_blocks = len(doc.content_flow)
            n_tables = sum(1 for b in doc.content_flow if b.block_type.value == "table")
            n_figures = sum(1 for b in doc.content_flow if b.block_type.value == "figure")

            status = "ok"
            text_len = sum(len(b.text or "") for b in doc.content_flow)
            if text_len < 2000:
                status = "badcase"
                artifact["badcases"].append({
                    "paper_id": pdf_path.stem,
                    "reason": f"text_chars={text_len} < 2000",
                })

            artifact["results"].append({
                "paper_id": pdf_path.stem,
                "status": status,
                "blocks": n_blocks,
                "tables": n_tables,
                "figures": n_figures,
                "output": str(output_dir / "enriched.json"),
            })
            artifact["success_count"] += 1

            logger.info(f"blocks={n_blocks}, tables={n_tables}, figures={n_figures}")

        except Exception as e:
            logger.error(f"{e}")
            artifact["fail_count"] += 1
            artifact["results"].append({
                "paper_id": pdf_path.stem,
                "status": "fail",
                "error": str(e),
            })

    artifact_path = settings.path.artifacts / f"02_parse_{run_id}.json"
    import json
    with open(artifact_path, "w", encoding="utf-8") as f:
        json.dump(artifact, f, ensure_ascii=False, indent=2)

    logger.info("=" * 60)
    logger.info(f"解析完成: {artifact['success_count']}/{artifact['input_count']} 成功")
    if artifact["badcases"]:
        logger.warning(f"Badcases: {len(artifact['badcases'])} 个")
    logger.info(f"产物已保存: {artifact_path}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/03b_build_graph.py">
#!/usr/bin/env python
"""步骤3b: 构建 HippoRAG 知识图谱"""

import sys
import json
from datetime import datetime

sys.path.insert(0, ".")

from config.settings import settings
from src.log import get_logger
from src.graph.hippo_rag import HippoRAG

logger = get_logger(__name__)


def main():
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    settings.path.ensure_dirs()

    logger.info("=" * 60)
    logger.info("深海科研知识库 - HippoRAG 图谱构建")
    logger.info("=" * 60)

    parsed_dir = settings.path.parsed
    graph_path = settings.path.data / "hippo_graph.json"

    # 检查解析结果
    json_files = list(parsed_dir.rglob("enriched.json"))
    if not json_files:
        logger.warning(f"未找到解析结果: {parsed_dir}")
        logger.warning("请先运行: python scripts/02_parse_papers.py")
        return

    logger.info(f"解析目录: {parsed_dir}")
    logger.info(f"找到文档: {len(json_files)} 个")

    # 构建图谱
    hippo = HippoRAG()
    hippo.build_from_parsed_docs(parsed_dir, use_llm=False)

    # 统计
    stats = hippo.stats()
    logger.info("图谱统计: 总节点=%s, 总边数=%s, 实体数=%s, Chunk数=%s",
                stats['total_nodes'], stats['total_edges'], stats['entity_count'], stats['chunk_count'])

    # 保存
    hippo.save(graph_path)

    # 保存 artifact
    artifact = {
        "run_id": run_id,
        "graph_path": str(graph_path),
        "stats": stats
    }
    artifact_path = settings.path.artifacts / f"03b_graph_{run_id}.json"
    with open(artifact_path, "w", encoding="utf-8") as f:
        json.dump(artifact, f, ensure_ascii=False, indent=2)

    logger.info("=" * 60)
    logger.info("图谱构建完成")
    logger.info(f"图谱文件: {graph_path}")
    logger.info(f"产物已保存: {artifact_path}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/04_test_search.py">
#!/usr/bin/env python
"""步骤4: 检索测试（调试版）"""

import sys
import json
from datetime import datetime

sys.path.insert(0, ".")

from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)
from src.indexing.milvus_ops import milvus
from src.indexing.embedder import embedder
from pymilvus import AnnSearchRequest, RRFRanker


def main():
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    settings.path.ensure_dirs()

    logger.info("=" * 60)
    logger.info("深海科研知识库 - 检索测试")
    logger.info("=" * 60)

    collection_name = settings.collection.global_
    test_queries = [
        "深海热液喷口",
        "海洋生物多样性",
        "深海探测技术",
    ]

    artifact = {
        "run_id": run_id,
        "collection": collection_name,
        "queries": []
    }

    # 检查 collection
    count = milvus.count(collection_name)
    logger.info("Collection: %s, 文档数量: %s", collection_name, count)

    if count == 0:
        logger.warning("Collection 为空")
        logger.warning("请先运行: python scripts/03_index_papers.py")
        return

    for query in test_queries:
        logger.info("Query: %s", query)

        query_result = {"query": query, "levels": {}}

        # Level 1: Embedding 检查
        logger.info("[Level 1] Embedding 输出")
        emb = embedder.encode([query])
        dense_vec = emb["dense"][0]
        sparse_vec = emb["sparse"]._getrow(0)

        dense_dim = len(dense_vec)
        sparse_nnz = sparse_vec.nnz

        logger.info("dense_dim: %s, sparse_nnz: %s", dense_dim, sparse_nnz)

        query_result["levels"]["embedding"] = {
            "dense_dim": dense_dim,
            "sparse_nnz": sparse_nnz,
            "status": "ok" if dense_dim == 1024 and sparse_nnz > 0 else "warn"
        }

        # Level 2: Milvus hybrid_search
        logger.info("[Level 2] Hybrid Search")

        dense_req = AnnSearchRequest(
            data=[dense_vec.tolist()],
            anns_field="dense_vector",
            param={"metric_type": "COSINE", "params": {"nprobe": 16}},
            limit=10,
        )

        # 转换 sparse vector 格式
        sparse_coo = sparse_vec.tocoo()
        sparse_dict = {int(col): float(val) for col, val in zip(sparse_coo.col, sparse_coo.data)}

        sparse_req = AnnSearchRequest(
            data=[sparse_dict],
            anns_field="sparse_vector",
            param={"metric_type": "IP"},
            limit=10,
        )

        results = milvus.hybrid_search(
            collection=collection_name,
            reqs=[dense_req, sparse_req],
            ranker=RRFRanker(k=settings.search.rrf_k),
            limit=5,
            output_fields=["paper_id", "chunk_id", "content", "section_path", "page"]
        )

        hits = results[0] if results else []
        logger.info("返回结果: %s 条", len(hits))

        query_result["levels"]["hybrid_search"] = {
            "hit_count": len(hits),
            "status": "ok" if len(hits) >= 3 else "warn"
        }

        # 显示结果
        hit_details = []
        for j, hit in enumerate(hits[:5], 1):
            paper_id = hit.entity.get("paper_id", "N/A")
            chunk_id = hit.entity.get("chunk_id", "N/A")
            content = hit.entity.get("content", "")[:100]
            score = hit.distance

            print(f"\n  [{j}] score={score:.4f}")
            print(f"      paper_id: {paper_id}")
            print(f"      chunk_id: {chunk_id}")
            print(f"      content: {content}...")

            hit_details.append({
                "rank": j,
                "score": score,
                "paper_id": paper_id,
                "chunk_id": chunk_id
            })

        query_result["hits"] = hit_details

        # Level 3: Rerank（可选）
        logger.info("[Level 3] Rerank")
        if hits:
            docs = [hit.entity.get("content", "") for hit in hits]
            try:
                reranked = embedder.rerank(query, docs, top_k=3)
                logger.info("Rerank 结果: %s 条", len(reranked))

                rerank_details = []
                for r in reranked:
                    logger.info("原排名 %s -> 新分数 %s", r.index + 1, f"{r.score:.4f}")
                    rerank_details.append({
                        "original_rank": r.index + 1,
                        "rerank_score": r.score
                    })

                query_result["levels"]["rerank"] = {
                    "status": "ok",
                    "results": rerank_details
                }
            except Exception as e:
                logger.warning("Rerank 失败: %s", e)
                query_result["levels"]["rerank"] = {"status": "fail", "error": str(e)}
        else:
            query_result["levels"]["rerank"] = {"status": "skip", "reason": "no hits"}

        artifact["queries"].append(query_result)

    # 保存 artifact
    artifact_path = settings.path.artifacts / f"04_search_{run_id}.json"
    with open(artifact_path, "w", encoding="utf-8") as f:
        json.dump(artifact, f, ensure_ascii=False, indent=2)

    logger.info("=" * 60)
    logger.info("检索测试完成")
    logger.info("产物已保存: %s", artifact_path)
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/05_test_rag.py">
#!/usr/bin/env python
"""步骤5: RAG 端到端测试（支持 HippoRAG）"""

import sys
import json
import os
from datetime import datetime

sys.path.insert(0, ".")

from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)
from src.indexing.milvus_ops import milvus
from src.retrieval.hybrid_retriever import retriever, RetrievalConfig
from src.generation.llm_client import call_llm


def retrieve(query: str, collection: str, top_k: int = 5, mode: str = "hybrid"):
    """
    检索相关文档

    Args:
        mode: vector / graph / hybrid
    """
    config = RetrievalConfig(mode=mode, top_k=top_k, rerank=True, graph_weight=0.3)
    hits = retriever.retrieve(query, collection, config)

    # 转换为兼容格式
    converted = []
    for hit in hits:
        converted.append(type("Hit", (), {
            "entity": {
                "paper_id": hit.get("metadata", {}).get("paper_id", ""),
                "chunk_id": hit.get("metadata", {}).get("chunk_id", ""),
                "content": hit.get("content", ""),
                "raw_content": hit.get("raw_content", ""),
                "section_path": hit.get("metadata", {}).get("section_path", ""),
                "page": hit.get("metadata", {}).get("page", 0),
            },
            "distance": hit.get("score", 0),
            "graph_score": hit.get("graph_score", 0),
            "vector_score": hit.get("vector_score", 0),
        })())
    return converted


def build_context(hits) -> str:
    """构建上下文"""
    parts = []
    for i, hit in enumerate(hits, 1):
        content = hit.entity.get("raw_content") or hit.entity.get("content", "")
        section = hit.entity.get("section_path", "")
        paper_id = hit.entity.get("paper_id", "")

        parts.append(f"""
【参考文献 {i}】
论文: {paper_id}
章节: {section}
内容:
{content[:1000]}
""")
    return "\n".join(parts)


def generate_answer(
    query: str,
    context: str,
    dry_run: bool = False,
    llm_provider: str = None,
    model_override: str = None,
) -> str:
    """
    生成回答。provider 与 model 可由调用方通过参数指定。
    """
    system_prompt = """你是深海科学研究助手。基于参考文献回答问题。

规则：
1. 用 [来源X] 标注引用
2. 证据不足时说明"根据现有资料无法确定"
3. 不要编造信息"""

    user_prompt = f"""基于以下参考文献回答问题。

{context}

问题: {query}

请给出带引用标注的回答:"""

    if dry_run:
        return f"[DRY_RUN] 问题: {query}\n上下文长度: {len(context)} 字符\n参考文献数: {context.count('【参考文献')}"

    provider = (llm_provider or settings.llm.default).lower()
    
    if not settings.llm.is_available(provider):
        return f"[ERROR] 未配置 {provider} 的 API Key，请在 config/rag_config.json 或环境变量中设置"

    return call_llm(
        provider=provider,
        system=system_prompt,
        user_prompt=user_prompt,
        model_override=model_override,
        max_tokens=2000,
    )


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--mode", choices=["vector", "graph", "hybrid"], default="hybrid",
                       help="检索模式: vector/graph/hybrid")
    parser.add_argument("--collection", type=str, default=None,
                       help="指定 collection 名称（默认用 settings.collection.global_）")
    parser.add_argument("--llm", type=str, default=None,
                       help="LLM 提供方 (如 openai, deepseek, gemini-thinking)，默认用 config/rag_config.json 中的 default")
    parser.add_argument("--model", type=str, default=None,
                       help="本次调用使用的模型名或别名，覆盖 config 中该 provider 的 default_model/models")
    args = parser.parse_args()

    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    settings.path.ensure_dirs()

    logger.info("=" * 60)
    logger.info("深海科研知识库 - RAG 测试（HippoRAG）")
    logger.info("=" * 60)

    dry_run = settings.llm.dry_run
    retrieval_mode = args.mode
    llm_provider = args.llm or settings.llm.default
    model_override = args.model

    logger.info("检索模式: %s, LLM: %s%s", retrieval_mode, llm_provider,
                f" (model={model_override})" if model_override else "")
    if dry_run:
        logger.info("LLM_DRY_RUN=true，仅测试检索链路")

    collection_name = args.collection or settings.collection.global_
    test_questions = [
        "深海热液喷口的温度范围是多少？",
        "深海生态系统有哪些特点？",
    ]

    artifact = {
        "run_id": run_id,
        "dry_run": dry_run,
        "retrieval_mode": retrieval_mode,
        "llm_provider": llm_provider,
        "model_override": model_override,
        "results": []
    }

    # 检查 collection
    count = milvus.count(collection_name)
    if count == 0:
        logger.warning("Collection 为空")
        logger.warning("请先运行: python scripts/06_ingest_langgraph.py")
        return

    for question in test_questions:
        logger.info("问题: %s", question)

        result = {"question": question}

        # 1. 检索
        logger.info("[1] 检索...")
        hits = retrieve(question, collection_name, top_k=3, mode=retrieval_mode)
        logger.info("命中: %s 条", len(hits))

        result["sources"] = []
        for i, hit in enumerate(hits, 1):
            paper_id = hit.entity.get("paper_id", "N/A")
            chunk_id = hit.entity.get("chunk_id", "N/A")
            graph_score = getattr(hit, "graph_score", 0)
            vector_score = getattr(hit, "vector_score", 0)
            logger.info("[%s] %s - %s", i, paper_id, chunk_id)
            if retrieval_mode == "hybrid":
                logger.info("vector=%s, graph=%s", f"{vector_score:.3f}", f"{graph_score:.3f}")
            result["sources"].append({
                "paper_id": paper_id,
                "chunk_id": chunk_id,
                "graph_score": graph_score,
                "vector_score": vector_score
            })

        # 2. 构建上下文
        context = build_context(hits)

        # 3. 生成回答
        logger.info("[2] 生成回答...")
        answer = generate_answer(
            question, context, dry_run=dry_run,
            llm_provider=llm_provider, model_override=model_override,
        )

        print(f"\n回答:\n{answer}")

        # 4. 检查引用
        has_citation = "[来源" in answer or "[来源1]" in answer or "参考文献" in answer
        result["answer"] = answer
        result["has_citation"] = has_citation
        result["context_length"] = len(context)

        if not dry_run and not has_citation:
            logger.warning("回答中未检测到引用标记")

        artifact["results"].append(result)

    # 保存 artifact
    artifact_path = settings.path.artifacts / f"05_rag_{run_id}.json"
    with open(artifact_path, "w", encoding="utf-8") as f:
        json.dump(artifact, f, ensure_ascii=False, indent=2)

    logger.info("=" * 60)
    logger.info("RAG 测试完成")
    logger.info("产物已保存: %s", artifact_path)
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/06_ingest_langgraph.py">
#!/usr/bin/env python
"""LangGraph 离线入库：解析 PDF → 切块 → 向量化 → Milvus upsert（chunk_id 主键）→ HippoRAG 建图"""

import sys
import argparse
from pathlib import Path
from datetime import datetime

sys.path.insert(0, str(Path(__file__).resolve().parent.parent))

from config.settings import settings
from src.log import get_logger
from src.indexing.milvus_ops import milvus

logger = get_logger(__name__)
from src.indexing.embedder import embedder
from src.pipelines.ingestion_graph import build_ingestion_graph
from langgraph.checkpoint.sqlite import SqliteSaver


def main():
    parser = argparse.ArgumentParser(description="LangGraph 离线入库（upsert + HippoRAG）")
    parser.add_argument("--skip-enrichment", action="store_true", help="跳过 LLM 增强")
    parser.add_argument("--skip-table-enrichment", action="store_true", help="跳过表格 LLM 增强")
    parser.add_argument("--skip-figure-enrichment", action="store_true", help="跳过图像 LLM 增强")
    parser.add_argument("--build-graph", action="store_true", help="入库后构建 HippoRAG 图谱")
    parser.add_argument("--max-docs", type=int, default=None, help="最多处理 PDF 数量")
    parser.add_argument("--recreate-collection", action="store_true", help="删除并重建 Collection（v2 chunk_id 主键）")
    args = parser.parse_args()

    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    settings.path.ensure_dirs()

    logger.info("=" * 60)
    logger.info("深海科研知识库 - LangGraph 离线入库")
    logger.info("=" * 60)

    collection_name = settings.collection.global_
    milvus.create_collection(collection_name, recreate=args.recreate_collection, schema_version="v2")
    raw_papers = settings.path.raw_papers
    parsed_dir = settings.path.parsed
    graph_path = settings.path.data / "hippo_graph.json"
    artifacts_dir = settings.path.artifacts
    checkpoint_dir = settings.path.data / "checkpoints"
    checkpoint_dir.mkdir(parents=True, exist_ok=True)
    checkpoint_path = checkpoint_dir / "ingestion.db"

    config_path = Path(__file__).resolve().parent.parent / "config" / "rag_config.json"
    try:
        from src.parser.pdf_parser import PDFProcessor, ParserConfig
    except ImportError as e:
        logger.error("pdf_parser 导入失败: %s", e)
        return 1

    parser_cfg = ParserConfig.from_json(config_path) if config_path.exists() else ParserConfig()
    if args.skip_table_enrichment:
        parser_cfg.enrich_tables = False
    if args.skip_figure_enrichment:
        parser_cfg.enrich_figures = False

    llm_manager = None
    if not args.skip_enrichment:
        try:
            from src.llm import LLMManager
            llm_manager = LLMManager.from_json(str(config_path))
        except Exception as e:
            logger.warning("LLM 未加载，将跳过增强: %s", e)
    processor = PDFProcessor(config=parser_cfg, llm_manager=llm_manager)

    chunk_config = {
        "target_chars": settings.chunk.target_chars,
        "min_chars": settings.chunk.min_chars,
        "max_chars": settings.chunk.max_chars,
        "overlap_sentences": settings.chunk.overlap_sentences,
        "table_rows_per_chunk": settings.chunk.table_rows_per_chunk,
    }

    configurable = {
        "thread_id": run_id,
        "processor": processor,
        "embedder": embedder,
        "milvus": milvus,
        "chunk_config": chunk_config,
        "collection_name": collection_name,
        "skip_enrichment": args.skip_enrichment,
        "parsed_dir": str(parsed_dir),
        "raw_papers_path": str(raw_papers),
        "artifacts_dir": str(artifacts_dir),
        "run_id": run_id,
        "graph_output_path": str(graph_path),
        "max_docs": args.max_docs,
    }

    initial_state = {
        "run_id": run_id,
        "build_graph": args.build_graph,
    }

    with SqliteSaver.from_conn_string(str(checkpoint_path)) as checkpointer:
        graph = build_ingestion_graph(checkpointer=checkpointer)
        result = graph.invoke(initial_state, config={"configurable": configurable})

    logger.info("=" * 60)
    logger.info("入库完成: %s 条 upsert, 总 chunks: %s",
                result.get('total_upserted', 0), result.get('total_chunks', 0))
    if result.get("errors"):
        logger.warning("错误: %s 条", len(result['errors']))
    if result.get("artifact_path"):
        logger.info("产物: %s", result['artifact_path'])
    if args.build_graph:
        logger.info("图谱: %s", graph_path)
    logger.info("=" * 60)
    return 0


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="scripts/07_test_web_search.py">
#!/usr/bin/env python
"""步骤7: Tavily 网络搜索测试（与 RAG 检索格式兼容）"""

import sys
import argparse

sys.path.insert(0, ".")

from src.log import get_logger
from src.retrieval.web_search import web_searcher

logger = get_logger(__name__)


def main():
    parser = argparse.ArgumentParser(description="Test Tavily web search")
    parser.add_argument("query", nargs="?", default="deep sea hydrothermal vent microbes", help="Search query")
    parser.add_argument("--expand", action="store_true", help="Use LLM query expansion")
    parser.add_argument("--async", dest="async_", action="store_true", help="Run async_search")
    args = parser.parse_args()

    if not web_searcher.enabled:
        logger.warning("Web search is disabled or api_key not set.")
        logger.warning("Set config/rag_config.json -> web_search.api_key (or env RAG_LLM__* for LLM).")
        return 0

    query = args.query
    logger.info("Query: %s, Expand: %s", query, args.expand)
    if args.async_:
        import asyncio
        results = asyncio.run(web_searcher.async_search(query, use_query_expansion=args.expand))
    else:
        results = web_searcher.search(query, use_query_expansion=args.expand)

    logger.info("Results: %s", len(results))
    for i, hit in enumerate(results[:5], 1):
        meta = hit.get("metadata", {})
        print(f"  [{i}] score={hit.get('score', 0):.3f} | {meta.get('title', '')[:50]} | {meta.get('url', '')[:60]}")
        print(f"      content: {(hit.get('content') or '')[:120]}...")

    if results:
        from src.generation.context_packer import pack_qa_context
        ctx = pack_qa_context(results, top_n=3)
        print("\n--- pack_qa_context (top 3) ---")
        print(ctx[:800] + "..." if len(ctx) > 800 else ctx)  # 结果数据保留 print

    return 0


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="scripts/08_run_api.py">
#!/usr/bin/env python3
"""
启动多轮对话 API 服务

用法（需在 conda 环境 deepsea-rag 下）:
  conda run -n deepsea-rag python scripts/08_run_api.py
  conda run -n deepsea-rag python scripts/08_run_api.py --port 8001 --host 0.0.0.0

或先激活环境再执行:
  conda activate deepsea-rag
  python scripts/08_run_api.py
"""

import argparse
import sys
from pathlib import Path

# 项目根目录加入 path
ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main() -> None:
    from config.settings import settings
    parser = argparse.ArgumentParser(description="Run DeepSea RAG Chat API")
    parser.add_argument("--host", default=settings.api.host, help="Bind host")
    parser.add_argument("--port", type=int, default=settings.api.port, help="Bind port")
    parser.add_argument("--reload", action="store_true", help="Enable reload (dev)")
    args = parser.parse_args()

    import uvicorn
    from src.api.server import app

    uvicorn.run(
        app,
        host=args.host,
        port=args.port,
        reload=args.reload,
    )


if __name__ == "__main__":
    main()
</file>

<file path="scripts/08_test_logging.py">
#!/usr/bin/env python
"""测试统一日志管理：分级输出、按运行实例命名、清理策略。"""

import sys

sys.path.insert(0, ".")

from src.log import get_logger, init_logging, cleanup_logs


def main():
    logger = get_logger(__name__)
    logger.debug("debug message")
    logger.info("info message")
    logger.warning("warn message")
    logger.error("error message")

    report = cleanup_logs()
    print("Cleanup report:", report)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/09_test_google_search.py">
#!/usr/bin/env python3
"""
Google Scholar / Google 搜索测试脚本

测试 GoogleSearcher 和 UnifiedWebSearcher 功能。

使用方法:
    python scripts/09_test_google_search.py "deep learning"
    python scripts/09_test_google_search.py "machine learning" --provider scholar
    python scripts/09_test_google_search.py "neural networks" --provider unified --limit 10
"""

import argparse
import asyncio
import json
import sys
from pathlib import Path

# 添加项目根目录到 sys.path
sys.path.insert(0, str(Path(__file__).resolve().parent.parent))


async def test_scholar_search(query: str, limit: int = 5):
    """测试 Google Scholar 搜索"""
    from src.retrieval.google_search import google_searcher
    
    print(f"\n{'='*60}")
    print(f"测试 Google Scholar 搜索")
    print(f"查询: {query}")
    print(f"限制: {limit} 条")
    print(f"{'='*60}\n")
    
    if not google_searcher.enabled or not google_searcher.scholar_enabled:
        print("⚠️  Google Scholar 搜索未启用，请检查配置")
        return []
    
    results = await google_searcher.search_scholar(query, limit=limit)
    
    print(f"✅ 返回 {len(results)} 条结果\n")
    
    for i, r in enumerate(results, 1):
        meta = r.get("metadata", {})
        print(f"[{i}] {meta.get('title', '无标题')}")
        print(f"    来源: {meta.get('source', 'unknown')}")
        print(f"    URL: {meta.get('url', '无')}")
        if meta.get('year'):
            print(f"    年份: {meta.get('year')}")
        if meta.get('cited_by'):
            print(f"    引用: {meta.get('cited_by')}")
        print(f"    摘要: {r.get('content', '')[:100]}...")
        print()
    
    return results


async def test_google_search(query: str, limit: int = 5):
    """测试 Google 搜索"""
    from src.retrieval.google_search import google_searcher
    
    print(f"\n{'='*60}")
    print(f"测试 Google 搜索")
    print(f"查询: {query}")
    print(f"限制: {limit} 条")
    print(f"{'='*60}\n")
    
    if not google_searcher.enabled or not google_searcher.google_enabled:
        print("⚠️  Google 搜索未启用，请检查配置")
        return []
    
    results = await google_searcher.search_google(query, limit=limit)
    
    print(f"✅ 返回 {len(results)} 条结果\n")
    
    for i, r in enumerate(results, 1):
        meta = r.get("metadata", {})
        print(f"[{i}] {meta.get('title', '无标题')}")
        print(f"    来源: {meta.get('source', 'unknown')}")
        print(f"    URL: {meta.get('url', '无')}")
        print(f"    摘要: {r.get('content', '')[:100]}...")
        print()
    
    return results


async def test_unified_search(query: str, limit: int = 5, providers: list = None):
    """测试统一搜索"""
    from src.retrieval.unified_web_search import unified_web_searcher
    
    print(f"\n{'='*60}")
    print(f"测试统一网络搜索")
    print(f"查询: {query}")
    print(f"来源: {providers or '自动检测'}")
    print(f"每来源限制: {limit} 条")
    print(f"{'='*60}\n")
    
    results = await unified_web_searcher.search(
        query, 
        providers=providers,
        max_results_per_provider=limit
    )
    
    print(f"✅ 合并去重后返回 {len(results)} 条结果\n")
    
    # 按来源分组统计
    source_counts = {}
    for r in results:
        source = r.get("metadata", {}).get("source", "unknown")
        source_counts[source] = source_counts.get(source, 0) + 1
    
    print("来源分布:")
    for source, count in sorted(source_counts.items(), key=lambda x: -x[1]):
        print(f"  - {source}: {count} 条")
    print()
    
    for i, r in enumerate(results, 1):
        meta = r.get("metadata", {})
        print(f"[{i}] [{meta.get('source', '?')}] {meta.get('title', '无标题')}")
        print(f"    URL: {meta.get('url', '无')}")
        print(f"    摘要: {r.get('content', '')[:80]}...")
        print()
    
    return results


def main():
    parser = argparse.ArgumentParser(description="测试 Google Scholar / Google 搜索")
    parser.add_argument("query", help="搜索查询")
    parser.add_argument("--provider", choices=["scholar", "google", "unified"], 
                       default="scholar", help="搜索提供者")
    parser.add_argument("--limit", type=int, default=5, help="最大结果数")
    parser.add_argument("--output", "-o", help="输出 JSON 文件")
    
    args = parser.parse_args()
    
    if args.provider == "scholar":
        results = asyncio.run(test_scholar_search(args.query, args.limit))
    elif args.provider == "google":
        results = asyncio.run(test_google_search(args.query, args.limit))
    else:  # unified
        results = asyncio.run(test_unified_search(args.query, args.limit))
    
    if args.output and results:
        with open(args.output, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\n结果已保存到: {args.output}")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/10_test_multiturn.py">
#!/usr/bin/env python3
"""
多轮对话与意图分流验证：调用 POST /chat，校验检索分支与无检索分支。

前置：先启动 API
  python scripts/08_run_api.py

然后执行（项目根目录）:
  python scripts/10_test_multiturn.py
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

def main():
    try:
        import requests
    except ImportError:
        print("pip install requests")
        sys.exit(1)

    from config.settings import settings
    base = f"http://{settings.api.host}:{settings.api.port}"

    # 1) 显式非检索指令 /status：不走检索（不依赖 BGE/Milvus），验证意图分流
    #    需配置有效 LLM API Key（config/rag_config.json 或环境变量）
    r1 = requests.post(
        f"{base}/chat",
        json={"message": "/status"},
        timeout=60,
    )
    r1.raise_for_status()
    d1 = r1.json()
    session_id = d1.get("session_id")
    total_chunks_1 = (d1.get("evidence_summary") or {}).get("total_chunks", -1)
    print(f"[1] /status (no retrieval) -> session_id={session_id}, total_chunks={total_chunks_1}, response_len={len(d1.get('response',''))}")

    if not session_id:
        print("no session_id in response")
        sys.exit(1)

    # 2) 自然语言（非检索意图）：应不走检索
    r2 = requests.post(
        f"{base}/chat",
        json={"session_id": session_id, "message": "你好，当前进度如何？"},
        timeout=60,
    )
    r2.raise_for_status()
    d2 = r2.json()
    total_chunks_2 = (d2.get("evidence_summary") or {}).get("total_chunks", -1)
    print(f"[2] 你好，当前进度如何？ -> total_chunks={total_chunks_2}, response_len={len(d2.get('response',''))}")

    # 3) GET session 校验历史
    r3 = requests.get(f"{base}/sessions/{session_id}", timeout=10)
    r3.raise_for_status()
    sess = r3.json()
    print(f"[3] GET session -> turn_count={sess.get('turn_count', 0)}")

    print("OK: multiturn + intent routing exercised.")

if __name__ == "__main__":
    main()
</file>

<file path="scripts/11_test_llm_providers.py">
#!/usr/bin/env python3
"""
批量测试所有 LLM provider 的最小可用性。

用法（建议在 conda 环境 deepsea-rag 下）:
  conda run -n deepsea-rag python scripts/11_test_llm_providers.py
"""

import sys
import time
from multiprocessing import Process, Queue
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def _run_single(provider: str, out: Queue) -> None:
    from src.llm import LLMManager

    try:
        manager = LLMManager.from_json(ROOT / "config" / "rag_config.json")
        client = manager.get_client(provider)
        overrides = {"max_tokens": 16}
        if provider.startswith("openai"):
            overrides = {"max_completion_tokens": 16}
        if provider == "claude-thinking":
            overrides = {
                "max_tokens": 16000,
                "thinking": {"type": "enabled", "budget_tokens": 8000},
            }
        resp = client.chat(
            [
                {"role": "system", "content": "你是一个简洁助手。"},
                {"role": "user", "content": "请回复 OK"},
            ],
            **overrides,
        )
        text = (resp.get("final_text") or "").strip()
        out.put(("ok", text))
    except Exception as exc:
        detail = str(exc)
        resp = getattr(exc, "response", None)
        if resp is not None:
            try:
                detail = f"{detail} | body: {resp.text[:500]}"
            except Exception:
                pass
        out.put(("fail", detail))


def main() -> None:
    from src.llm import LLMManager

    manager = LLMManager.from_json(ROOT / "config" / "rag_config.json")
    providers = manager.get_provider_names()
    if not providers:
        print("No providers found in config.")
        return

    print("LLM providers:", ", ".join(providers), flush=True)
    print("-" * 60, flush=True)

    timeout_s = 10
    for name in providers:
        t0 = time.time()
        q: Queue = Queue()
        p = Process(target=_run_single, args=(name, q))
        p.start()
        per_timeout = 25 if name == "claude-thinking" else timeout_s
        p.join(timeout=per_timeout)
        elapsed = int((time.time() - t0) * 1000)
        if p.is_alive():
            p.terminate()
            p.join(timeout=3)
            print(f"[TIMEOUT] {name} ({elapsed}ms) -> >{per_timeout}s", flush=True)
            continue
        if q.empty():
            print(f"[FAIL] {name} ({elapsed}ms) -> no result", flush=True)
            continue
        status, msg = q.get()
        if status == "ok":
            print(f"[OK] {name} ({elapsed}ms) -> {msg[:50]}", flush=True)
        else:
            print(f"[FAIL] {name} ({elapsed}ms) -> {msg}", flush=True)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/12_test_workflow_stage.py">
#!/usr/bin/env python3
"""
工作流阶段切换验证：/outline -> outline, /draft -> drafting, /edit -> refine。

前置：先启动 API
  python scripts/08_run_api.py

然后执行（项目根目录）:
  python scripts/12_test_workflow_stage.py
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main():
    try:
        import requests
    except ImportError:
        print("pip install requests")
        sys.exit(1)

    from config.settings import settings
    base = f"http://{settings.api.host}:{settings.api.port}"

    session_id = None

    # 1) 新会话 /outline -> stage 应为 outline
    r1 = requests.post(
        f"{base}/chat",
        json={"message": "/outline"},
        timeout=60,
    )
    r1.raise_for_status()
    session_id = r1.json().get("session_id")
    if not session_id:
        print("no session_id")
        sys.exit(1)
    s1 = requests.get(f"{base}/sessions/{session_id}", timeout=10)
    s1.raise_for_status()
    stage1 = s1.json().get("stage", "")
    print(f"[1] /outline -> stage={stage1}")
    assert stage1 == "outline", f"expected outline got {stage1}"

    # 2) 同会话 /draft -> stage 应为 drafting
    r2 = requests.post(
        f"{base}/chat",
        json={"session_id": session_id, "message": "/draft 第一节"},
        timeout=60,
    )
    r2.raise_for_status()
    s2 = requests.get(f"{base}/sessions/{session_id}", timeout=10)
    s2.raise_for_status()
    stage2 = s2.json().get("stage", "")
    print(f"[2] /draft -> stage={stage2}")
    assert stage2 == "drafting", f"expected drafting got {stage2}"

    # 3) 同会话 /edit -> stage 应为 refine
    r3 = requests.post(
        f"{base}/chat",
        json={"session_id": session_id, "message": "/edit 润色第一段"},
        timeout=60,
    )
    r3.raise_for_status()
    s3 = requests.get(f"{base}/sessions/{session_id}", timeout=10)
    s3.raise_for_status()
    stage3 = s3.json().get("stage", "")
    print(f"[3] /edit -> stage={stage3}")
    assert stage3 == "refine", f"expected refine got {stage3}"

    print("OK: workflow stage transitions verified.")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/13_test_canvas_api.py">
#!/usr/bin/env python3
"""
Canvas API 联调：创建 -> 大纲 -> 草稿 -> 快照 -> 导出。
前置：先启动 API（python scripts/08_run_api.py）
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main():
    import requests
    from config.settings import settings

    base = f"http://{settings.api.host}:{settings.api.port}"

    # 1) POST /canvas 创建
    r1 = requests.post(
        f"{base}/canvas",
        json={"session_id": "", "topic": "深海冷泉综述"},
        timeout=10,
    )
    r1.raise_for_status()
    d1 = r1.json()
    canvas_id = d1["id"]
    print(f"[1] POST /canvas -> canvas_id={canvas_id}")

    # 2) PATCH 更新
    r2 = requests.patch(
        f"{base}/canvas/{canvas_id}",
        json={"working_title": "深海冷泉生态系统研究进展"},
        timeout=10,
    )
    r2.raise_for_status()
    print("[2] PATCH /canvas/{id} -> ok")

    # 3) POST outline
    r3 = requests.post(
        f"{base}/canvas/{canvas_id}/outline",
        json={
            "sections": [
                {"id": "s1", "title": "引言", "level": 1, "order": 0, "status": "done"},
                {"id": "s2", "title": "冷泉分布与形成", "level": 1, "order": 1, "status": "drafting"},
            ]
        },
        timeout=10,
    )
    r3.raise_for_status()
    print("[3] POST /canvas/{id}/outline -> ok")

    # 4) POST draft
    r4 = requests.post(
        f"{base}/canvas/{canvas_id}/drafts",
        json={
            "block": {
                "section_id": "s1",
                "content_md": "深海冷泉是海底渗出的流体系统。",
                "version": 1,
            }
        },
        timeout=10,
    )
    r4.raise_for_status()
    print("[4] POST /canvas/{id}/drafts -> ok")

    # 5) POST snapshot
    r5 = requests.post(f"{base}/canvas/{canvas_id}/snapshot", timeout=10)
    r5.raise_for_status()
    ver = r5.json()["version_number"]
    print(f"[5] POST /canvas/{canvas_id}/snapshot -> version_number={ver}")

    # 6) GET export
    r6 = requests.get(f"{base}/canvas/{canvas_id}/export", timeout=10)
    r6.raise_for_status()
    export = r6.json()
    assert "outline" in export and "drafts" in export
    assert len(export["outline"]) == 2
    assert "s1" in export["drafts"]
    print(f"[6] GET /canvas/{canvas_id}/export -> outline={len(export['outline'])}, drafts keys={list(export['drafts'].keys())}")

    # 7) GET canvas
    r7 = requests.get(f"{base}/canvas/{canvas_id}", timeout=10)
    r7.raise_for_status()
    print(f"[7] GET /canvas/{canvas_id} -> ok")

    print("OK: canvas CRUD + outline + drafts + snapshot + export verified.")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/14_test_memory.py">
#!/usr/bin/env python3
"""
记忆扩展联调：Working Memory（Canvas 摘要缓存）与 Persistent Store（user_id 偏好）。
前置：先启动 API（python scripts/08_run_api.py）
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main():
    import requests
    from config.settings import settings
    from src.collaboration.memory.working_memory import get_working_memory
    from src.collaboration.memory.persistent_store import get_user_profile, upsert_user_profile

    base = f"http://{settings.api.host}:{settings.api.port}"

    # 1) 创建 Canvas 并写入 outline
    r1 = requests.post(
        f"{base}/canvas",
        json={"session_id": "", "topic": "深海冷泉综述"},
        timeout=10,
    )
    r1.raise_for_status()
    canvas_id = r1.json()["id"]
    requests.post(
        f"{base}/canvas/{canvas_id}/outline",
        json={"sections": [{"id": "s1", "title": "引言", "level": 1, "order": 0}]},
        timeout=10,
    ).raise_for_status()
    print(f"[1] Canvas created: {canvas_id}")

    # 2) POST /chat 绑定该 canvas（新 session），触发生成 Working Memory
    r2 = requests.post(
        f"{base}/chat",
        json={"canvas_id": canvas_id, "message": "当前进度如何？"},
        timeout=60,
    )
    r2.raise_for_status()
    session_id = r2.json()["session_id"]
    print(f"[2] Chat with canvas_id -> session_id={session_id}")

    # 3) 检查 Working Memory 缓存
    wm = get_working_memory(canvas_id)
    assert wm is not None and wm.get("summary"), "working memory should be cached"
    print(f"[3] Working memory cached: {wm['summary'][:80]}...")

    # 4) Persistent Store: 写入用户偏好
    user_id = "test_user_14"
    upsert_user_profile(user_id, {"llm_provider": "deepseek", "citation_style": "apa"})
    profile = get_user_profile(user_id)
    assert profile is not None and profile.get("preferences", {}).get("llm_provider") == "deepseek"
    print("[4] User profile upserted and read back ok")

    # 5) /chat 带 user_id，确认不报错（偏好会注入系统提示）
    r5 = requests.post(
        f"{base}/chat",
        json={"session_id": session_id, "user_id": user_id, "message": "谢谢"},
        timeout=60,
    )
    r5.raise_for_status()
    print("[5] POST /chat with user_id -> ok")

    print("OK: working memory + persistent store exercised.")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/15_test_citations.py">
#!/usr/bin/env python3
"""
引用管理联调：检索 -> citation_pool 入库 -> 导出 BibTeX + 参考文献段落。
前置：先启动 API（python scripts/08_run_api.py）
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main():
    import requests
    from config.settings import settings

    base = f"http://{settings.api.host}:{settings.api.port}"

    # 1) 创建 Canvas
    r1 = requests.post(
        f"{base}/canvas",
        json={"session_id": "", "topic": "深海冷泉综述"},
        timeout=10,
    )
    r1.raise_for_status()
    canvas_id = r1.json()["id"]
    print(f"[1] Canvas created: {canvas_id}")

    # 2) POST /chat 绑定 canvas，发送触发检索的消息（如 /search 或直接问检索类问题）
    r2 = requests.post(
        f"{base}/chat",
        json={"canvas_id": canvas_id, "message": "/search 冷泉 生态系统", "search_mode": "local"},
        timeout=60,
    )
    r2.raise_for_status()
    data = r2.json()
    session_id = data["session_id"]
    citations = data.get("citations", [])
    print(f"[2] Chat (retrieval) -> session_id={session_id}, citations={len(citations)} cite_keys: {citations[:5]}")

    # 3) GET /canvas/{id}/citations?format=bibtex
    r3 = requests.get(f"{base}/canvas/{canvas_id}/citations", params={"format": "bibtex"}, timeout=10)
    r3.raise_for_status()
    bibtex = r3.json().get("content", "")
    print("[3] GET citations (bibtex):")
    print(bibtex[:500] + "..." if len(bibtex) > 500 else bibtex)

    # 4) GET /canvas/{id}/citations?format=text
    r4 = requests.get(f"{base}/canvas/{canvas_id}/citations", params={"format": "text"}, timeout=10)
    r4.raise_for_status()
    ref_list = r4.json().get("content", "")
    print("[4] GET citations (reference list):")
    print(ref_list[:500] + "..." if len(ref_list) > 500 else ref_list)

    # 5) GET format=both 确认结构
    r5 = requests.get(f"{base}/canvas/{canvas_id}/citations", params={"format": "both"}, timeout=10)
    r5.raise_for_status()
    both = r5.json()
    assert "bibtex" in both and "reference_list" in both and "citations" in both
    print(f"[5] GET format=both -> {len(both.get('citations', []))} citation(s)")

    print("OK: retrieval -> citation_pool -> bibtex + reference list exercised.")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/16_test_export.py">
#!/usr/bin/env python3
"""
导出联调：创建画布 -> 写入大纲与草稿 -> 导出 Markdown。

支持命令行参数：
  --cite-format: 引用键格式 (numeric | hash | author_date)

前置：先启动 API（python scripts/08_run_api.py）
"""

import argparse
import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main():
    parser = argparse.ArgumentParser(description="导出测试：Canvas -> Markdown")
    parser.add_argument(
        "--cite-format",
        choices=["numeric", "hash", "author_date"],
        default=None,
        help="引用键格式（默认从配置读取）",
    )
    args = parser.parse_args()

    import requests
    from config.settings import settings

    base = f"http://{settings.api.host}:{settings.api.port}"

    # 1) 创建 Canvas
    r1 = requests.post(
        f"{base}/canvas",
        json={"session_id": "", "topic": "深海冷泉综述"},
        timeout=10,
    )
    r1.raise_for_status()
    canvas_id = r1.json()["id"]
    print(f"[1] Canvas created: {canvas_id}")

    # 2) 写入大纲
    requests.post(
        f"{base}/canvas/{canvas_id}/outline",
        json={
            "sections": [
                {"id": "s1", "title": "引言", "level": 1, "order": 0},
                {"id": "s2", "title": "研究进展", "level": 1, "order": 1},
                {"id": "s2-1", "title": "生态系统", "level": 2, "order": 2, "parent_id": "s2"},
            ]
        },
        timeout=10,
    ).raise_for_status()
    print("[2] Outline upserted")

    # 3) 写入草稿
    requests.post(
        f"{base}/canvas/{canvas_id}/drafts",
        json={
            "block": {
                "section_id": "s1",
                "content_md": "这是引言部分的草稿内容。深海冷泉是一种重要的海底地质现象 [Smith2023]。",
                "version": 1,
                "used_fragment_ids": [],
                "used_citation_ids": [],
            }
        },
        timeout=10,
    ).raise_for_status()
    print("[3] Draft upserted")

    # 4) 添加测试引用
    requests.post(
        f"{base}/canvas/{canvas_id}/citations",
        json={
            "citations": [
                {
                    "cite_key": "Smith2023",
                    "title": "Deep-sea cold seep ecosystems",
                    "authors": ["Smith, John", "Wang, Lei"],
                    "year": 2023,
                    "doi": "10.1234/example1",
                },
                {
                    "cite_key": "Jones2022",
                    "title": "Methane seepage in the South China Sea",
                    "authors": ["Jones, Mary", "Chen, Wei"],
                    "year": 2022,
                    "url": "https://example.com/paper2",
                },
                {
                    "cite_key": "Smith2023b",
                    "title": "Microbial communities at cold seeps",
                    "authors": ["Smith, John"],
                    "year": 2023,
                    "doi": "10.1234/example3",
                },
            ]
        },
        timeout=10,
    ).raise_for_status()
    print("[4] Citations added")

    # 5) 导出 Markdown（可指定引用格式）
    export_params = {"canvas_id": canvas_id, "format": "markdown"}
    if args.cite_format:
        export_params["cite_key_format"] = args.cite_format
        print(f"[5] Exporting with cite_key_format: {args.cite_format}")
    else:
        print("[5] Exporting with default cite_key_format from config")

    r5 = requests.post(
        f"{base}/export",
        json=export_params,
        timeout=10,
    )
    r5.raise_for_status()
    md = r5.json().get("content", "")
    print("\n" + "=" * 60)
    print("EXPORTED MARKDOWN:")
    print("=" * 60)
    print(md[:800] + "..." if len(md) > 800 else md)
    print("=" * 60)

    print("\nOK: export markdown exercised.")
    print(f"\nTip: 使用 --cite-format 参数切换引用格式：")
    print(f"  python {Path(__file__).name} --cite-format numeric")
    print(f"  python {Path(__file__).name} --cite-format hash")
    print(f"  python {Path(__file__).name} --cite-format author_date")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/17_test_chat_stream.py">
#!/usr/bin/env python3
"""
流式对话联调：POST /chat/stream，打印 SSE 事件。
前置：先启动 API（python scripts/08_run_api.py）
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main():
    import requests
    from config.settings import settings

    base = f"http://{settings.api.host}:{settings.api.port}"

    with requests.post(
        f"{base}/chat/stream",
        json={"message": "请简要解释深海冷泉的生态意义。"},
        stream=True,
        timeout=120,
    ) as resp:
        resp.raise_for_status()
        print("[stream] status ok, start receiving events...")
        for line in resp.iter_lines(decode_unicode=True):
            if not line:
                continue
            print(line)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/18_eval_rag.py">
#!/usr/bin/env python
"""RAG 量化评测：检索召回 / 生成相关性 / 引用准确性"""

import sys
import json
from datetime import datetime
from pathlib import Path

sys.path.insert(0, ".")

from config.settings import settings
from src.evaluation.dataset import load_dataset
from src.evaluation.runner import evaluate_dataset
from src.llm.llm_manager import get_manager
from src.log import get_logger
from src.retrieval.service import get_retrieval_service

logger = get_logger(__name__)


def main():
    import argparse

    parser = argparse.ArgumentParser()
    parser.add_argument("--dataset", default="data/eval_mini.json", help="评测数据集路径")
    parser.add_argument("--mode", default=None, help="覆盖检索模式：local/web/hybrid")
    parser.add_argument("--top-k", type=int, default=10, help="检索返回条数")
    parser.add_argument("--max-context-chunks", type=int, default=8, help="用于生成的证据条数")
    parser.add_argument("--max-context-chars", type=int, default=800, help="每条证据最大字符数")
    parser.add_argument("--llm", type=str, default=None, help="LLM provider（默认 config 中 default）")
    parser.add_argument("--model", type=str, default=None, help="覆盖模型名或别名")
    parser.add_argument("--no-generate", action="store_true", help="仅评测检索")
    parser.add_argument("--max-cases", type=int, default=None, help="限制评测条数")
    parser.add_argument("--config", type=str, default="config/rag_config.json", help="配置文件路径")
    parser.add_argument("--output", type=str, default=None, help="输出 artifact 路径")
    args = parser.parse_args()

    settings.path.ensure_dirs()
    cases, meta = load_dataset(args.dataset)
    if args.max_cases:
        cases = cases[: args.max_cases]

    retrieval = get_retrieval_service(top_k=args.top_k)

    llm_client = None
    if not args.no_generate:
        if settings.llm.dry_run:
            logger.warning("LLM_DRY_RUN=true，跳过生成评测")
        else:
            manager = get_manager(args.config)
            llm_client = manager.get_client(args.llm)

    logger.info("评测样本数: %s", len(cases))
    logger.info("检索模式: %s", args.mode or "dataset/default")
    logger.info("top_k=%s, max_context_chunks=%s", args.top_k, args.max_context_chunks)

    report = evaluate_dataset(
        cases=cases,
        retrieval=retrieval,
        llm_client=llm_client,
        mode_override=args.mode,
        top_k=args.top_k,
        max_context_chunks=args.max_context_chunks,
        max_context_chars=args.max_context_chars,
        model_override=args.model,
    )

    artifact = {
        "run_id": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "dataset": str(args.dataset),
        "meta": meta,
        "config_path": args.config,
        "params": {
            "mode_override": args.mode,
            "top_k": args.top_k,
            "max_context_chunks": args.max_context_chunks,
            "max_context_chars": args.max_context_chars,
            "llm": args.llm,
            "model": args.model,
            "no_generate": args.no_generate or llm_client is None,
        },
        "summary": report["summary"],
        "results": report["results"],
    }

    output_path = args.output
    if not output_path:
        output_path = settings.path.artifacts / f"eval_{artifact['run_id']}.json"
    else:
        output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    output_path.write_text(json.dumps(artifact, ensure_ascii=False, indent=2), encoding="utf-8")

    logger.info("评测完成，结果写入: %s", output_path)
    logger.info("Summary: %s", artifact["summary"])


if __name__ == "__main__":
    main()
</file>

<file path="scripts/19_cleanup_storage.py">
#!/usr/bin/env python3
"""
存储清理脚本：按生命周期和大小限制清理过期数据。

用法：
    python scripts/19_cleanup_storage.py                 # 使用配置默认值
    python scripts/19_cleanup_storage.py --max-age 7     # 清理 7 天前的数据
    python scripts/19_cleanup_storage.py --max-size 2    # 限制总大小 2GB
    python scripts/19_cleanup_storage.py --vacuum        # 清理后执行 VACUUM
    python scripts/19_cleanup_storage.py --stats         # 仅显示统计，不清理
"""

import argparse
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from config.settings import settings
from src.utils.storage_cleaner import (
    run_cleanup,
    vacuum_databases,
    get_storage_stats,
)


def main():
    parser = argparse.ArgumentParser(description="持久化存储清理工具")
    parser.add_argument(
        "--max-age",
        type=int,
        default=None,
        help=f"数据保留天数（默认 {settings.storage.max_age_days}）",
    )
    parser.add_argument(
        "--max-size",
        type=float,
        default=None,
        help=f"总大小上限 GB（默认 {settings.storage.max_size_gb}）",
    )
    parser.add_argument(
        "--vacuum",
        action="store_true",
        help="清理后执行 VACUUM 回收空间",
    )
    parser.add_argument(
        "--stats",
        action="store_true",
        help="仅显示统计信息，不执行清理",
    )
    args = parser.parse_args()

    # 显示当前状态
    stats_before = get_storage_stats()
    print(f"当前存储状态: {stats_before['total_size_mb']:.2f} MB")
    for db_name, size_mb in stats_before["databases"].items():
        print(f"  - {db_name}: {size_mb:.2f} MB")

    if args.stats:
        return

    # 执行清理
    max_age = args.max_age if args.max_age is not None else settings.storage.max_age_days
    max_size = args.max_size if args.max_size is not None else settings.storage.max_size_gb

    print(f"\n执行清理: max_age={max_age}天, max_size={max_size}GB")
    result = run_cleanup(
        max_age_days=max_age,
        max_size_gb=max_size,
        batch_size=settings.storage.cleanup_batch_size,
    )

    print(f"\n清理结果:")
    print(f"  - 按时间清理: canvas={result['age_cleanup']['canvas']}, "
          f"session={result['age_cleanup']['session']}, "
          f"project={result['age_cleanup']['project']}")
    print(f"  - 按大小清理: {result['size_cleanup']} 条记录")
    print(f"  - 当前大小: {result['final_size_mb']:.2f} MB")

    # VACUUM
    if args.vacuum:
        print("\n执行 VACUUM...")
        vacuum_databases()
        stats_after = get_storage_stats()
        print(f"VACUUM 后大小: {stats_after['total_size_mb']:.2f} MB")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/20_bootstrap_admin.py">
#!/usr/bin/env python3
"""
创建首个管理员账号（无现有用户时使用 config.auth 中的 admin_username / admin_default_password）。

用法：
    python scripts/20_bootstrap_admin.py
    python scripts/20_bootstrap_admin.py --username admin --password mypass
"""

import argparse
import sys
from pathlib import Path

sys.path.insert(0, str(Path(__file__).resolve().parents[1]))

from config.settings import settings
from src.collaboration.memory.persistent_store import list_users, create_user


def main():
    parser = argparse.ArgumentParser(description="Bootstrap first admin user")
    parser.add_argument("--username", default=None, help="Admin username (default: from config)")
    parser.add_argument("--password", default=None, help="Admin password (default: from config)")
    args = parser.parse_args()

    users = list_users()
    if users:
        print(f"Users already exist ({len(users)}). Use admin token to create users via POST /admin/users.")
        return

    username = args.username or settings.auth.admin_username
    password = args.password or settings.auth.admin_default_password
    if not username or not password:
        print("Error: username and password required (set in config or --username/--password)")
        sys.exit(1)

    try:
        create_user(user_id=username, password=password, role="admin")
        print(f"Created admin user: {username}")
        print("Login: POST /auth/login with body {\"user_id\": \"%s\", \"password\": \"...\"}" % username)
    except ValueError as e:
        print(f"Error: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/21_test_intent_override.py">
#!/usr/bin/env python3
"""
测试意图覆盖与显式命令优先级。
前置：先启动 API（python scripts/08_run_api.py）
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def _get_base_url():
    from config.settings import settings

    return f"http://{settings.api.host}:{settings.api.port}"


def _read_meta_event(resp):
    """读取 SSE 的 meta 事件数据并返回解析后的 dict。"""
    current_event = None
    for line in resp.iter_lines(decode_unicode=True):
        if not line:
            continue
        if line.startswith("event: "):
            current_event = line.replace("event: ", "").strip()
            continue
        if line.startswith("data: ") and current_event == "meta":
            import json

            return json.loads(line.replace("data: ", "", 1))
    return None


def test_intent_detect():
    import requests

    base = _get_base_url()
    resp = requests.post(
        f"{base}/intent/detect",
        json={"message": "请帮我继续上一段内容。", "current_stage": "drafting"},
        timeout=30,
    )
    resp.raise_for_status()
    data = resp.json()
    print("[intent_detect]", data)
    assert "intent_type" in data
    assert "needs_retrieval" in data


def test_intent_override_chat():
    import requests

    base = _get_base_url()
    with requests.post(
        f"{base}/chat/stream",
        json={
            "message": "随便聊聊",
            "search_mode": "none",
            "intent_override": "chitchat",
        },
        stream=True,
        timeout=120,
    ) as resp:
        resp.raise_for_status()
        meta = _read_meta_event(resp)
        print("[override_chat meta]", meta)
        assert meta and meta.get("intent", {}).get("intent_type") == "chitchat"


def test_command_priority_over_override():
    import requests

    base = _get_base_url()
    with requests.post(
        f"{base}/chat/stream",
        json={
            "message": "/search deep sea cold seep",
            "search_mode": "local",
            "intent_override": "chitchat",
        },
        stream=True,
        timeout=120,
    ) as resp:
        resp.raise_for_status()
        meta = _read_meta_event(resp)
        print("[command_priority meta]", meta)
        assert meta and meta.get("intent", {}).get("intent_type") == "search_targeted"


def main():
    test_intent_detect()
    test_intent_override_chat()
    test_command_priority_over_override()
    print("All intent override tests passed.")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/22_test_offline_models.py">
#!/usr/bin/env python3
"""
离线模型加载测试：
1) 设置 HF_LOCAL_FILES_ONLY=true
2) 尝试加载 BGE-M3 / BGE-Reranker / ColBERT
前置：模型已下载到本地缓存
"""

import os
import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main():
    os.environ["HF_LOCAL_FILES_ONLY"] = os.getenv("HF_LOCAL_FILES_ONLY", "true")

    print("[offline] HF_LOCAL_FILES_ONLY =", os.environ["HF_LOCAL_FILES_ONLY"])

    from src.indexing.embedder import embedder
    from src.retrieval.colbert_reranker import colbert_reranker

    # BGE-M3
    try:
        _ = embedder.ef
        print("[OK] BGE-M3 loaded")
    except Exception as e:
        print("[FAIL] BGE-M3 load failed:", e)

    # BGE-Reranker
    try:
        _ = embedder.reranker
        print("[OK] BGE-Reranker loaded")
    except Exception as e:
        print("[FAIL] BGE-Reranker load failed:", e)

    # ColBERT
    try:
        _ = colbert_reranker.model
        print("[OK] ColBERT loaded")
    except Exception as e:
        print("[FAIL] ColBERT load failed:", e)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/23_sync_local_models.py">
#!/usr/bin/env python3
"""
同步/检查本地模型缓存。
前置：可通过环境变量控制缓存路径与离线模式
- MODEL_CACHE_ROOT=~/Hug
- EMBEDDING_CACHE_DIR=...
- RERANKER_CACHE_DIR=...
- COLBERT_CACHE_DIR=...
- HF_LOCAL_FILES_ONLY=true
- HF_ENDPOINT=https://hf-mirror.com
- 或 RAG_HF_ENDPOINTS=https://hf-mirror.com,https://huggingface.co
"""

import argparse
import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--force", action="store_true", help="强制更新模型缓存")
    parser.add_argument("--offline", action="store_true", help="仅检查本地缓存（不联网）")
    args = parser.parse_args()

    from src.utils.model_sync import check_models, sync_models

    if args.offline:
        items = check_models(local_files_only=True)
        print("[model_status]")
        for i in items:
            print(f"- {i.name}: exists={i.exists} cache_dir={i.cache_dir}")
        return

    items = sync_models(force_update=args.force, local_files_only=False)
    print("[model_sync]")
    for i in items:
        print(
            f"- {i.name}: status={i.status} updated={i.updated} message={i.message} cache_dir={i.cache_dir}"
        )


if __name__ == "__main__":
    main()
</file>

<file path="scripts/23_test_deep_research_e2e.py">
#!/usr/bin/env python3
"""
Deep Research E2E validation:
- revise priority queue
- supplement load/consume
- review gate behavior
- insights ledger lifecycle

Run (repo root):
  conda run --no-capture-output -n deepsea-rag python scripts/23_test_deep_research_e2e.py
"""

from __future__ import annotations

import sys
import tempfile
from pathlib import Path
from typing import Dict, Optional

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))

from src.collaboration.research import job_store
from src.collaboration.research.agent import (
    _consume_revision_queue,
    _load_section_supplements,
    _mark_section_supplements_consumed,
    _scan_fresh_revise_signals,
    review_gate_node,
    synthesize_node,
)
from src.collaboration.research.dashboard import ResearchDashboard


class FakeClient:
    def chat(self, messages, model=None, max_tokens=None):
        last = ""
        if messages and isinstance(messages[-1], dict):
            last = messages[-1].get("content", "")
        if "Limitations and Future Directions" in last or "不足与未来方向" in last:
            return {
                "final_text": "Limitations are acknowledged. Future work should collect broader longitudinal evidence."
            }
        return {"final_text": "This is a concise abstract."}


def _waiter(job_id: str):
    def _inner(section_id: str) -> Optional[Dict[str, object]]:
        return job_store.get_pending_review(job_id, section_id)

    return _inner


def main() -> None:
    temp_dir = tempfile.TemporaryDirectory()
    job_store._DB_PATH = Path(temp_dir.name) / "deep_research_jobs_test.db"

    job_id = "job-e2e-001"
    section_a = "Section A"
    section_b = "Section B"

    dashboard = ResearchDashboard()
    dashboard.add_section(section_a)
    dashboard.add_section(section_b)

    # 1) revise priority queue
    job_store.submit_review(job_id, section_a, action="revise", feedback="Need more evidence A")
    state = {
        "job_id": job_id,
        "dashboard": dashboard,
        "skip_draft_review": False,
        "review_waiter": _waiter(job_id),
        "review_seen_at": {},
        "revision_queue": [],
    }
    _scan_fresh_revise_signals(state)
    assert state["revision_queue"] == [section_a]
    _scan_fresh_revise_signals(state)  # no duplicate enqueue
    assert state["revision_queue"] == [section_a]
    assert _consume_revision_queue(state) == section_a
    assert state["revision_queue"] == []
    print("[PASS] revise priority queue")

    # 2) supplement load + consume
    job_store.submit_gap_supplement(
        job_id,
        section_a,
        "Missing baseline dataset",
        "direct_info",
        {"text": "Baseline dataset 2019 includes 120 samples."},
    )
    job_store.submit_gap_supplement(
        job_id,
        section_a,
        "Need method details",
        "material",
        {"text": "Method appendix URL and extraction protocol."},
    )
    ctx = _load_section_supplements(state, section_a)
    assert "Section-scoped user supplements" in ctx
    assert "Baseline dataset 2019" in ctx
    assert len(job_store.list_gap_supplements(job_id, section_id=section_a, status="pending")) == 2
    _mark_section_supplements_consumed(state, section_a)
    assert len(job_store.list_gap_supplements(job_id, section_id=section_a, status="pending")) == 0
    assert len(job_store.list_gap_supplements(job_id, section_id=section_a, status="consumed")) == 2
    print("[PASS] supplement load+consume")

    # 3) review gate requeue + limitation insight
    job_store.submit_review(job_id, section_a, action="approve", feedback="")
    job_store.submit_review(
        job_id, section_b, action="revise", feedback="Section B lacks contradiction analysis"
    )
    state_gate = {
        "job_id": job_id,
        "dashboard": dashboard,
        "skip_draft_review": False,
        "review_waiter": _waiter(job_id),
        "review_handled_at": {},
    }
    state_gate = review_gate_node(state_gate)
    assert state_gate.get("review_gate_next") == "research"
    assert state_gate.get("current_section") == section_b
    lim_insights = job_store.list_insights(job_id, insight_type="limitation", status="open")
    assert any("lacks contradiction analysis" in (x.get("text") or "") for x in lim_insights)
    print("[PASS] review gate requeue + limitation insight")

    # 4) review gate all approve -> synthesize
    job_store.submit_review(job_id, section_b, action="approve", feedback="ok now")
    state_gate2 = {
        "job_id": job_id,
        "dashboard": dashboard,
        "skip_draft_review": False,
        "review_waiter": _waiter(job_id),
        "review_handled_at": state_gate.get("review_handled_at", {}),
    }
    state_gate2 = review_gate_node(state_gate2)
    assert state_gate2.get("review_gate_next") == "synthesize"
    print("[PASS] review gate all approve -> synthesize")

    # 5) synthesize consumes open insights -> addressed
    job_store.append_insight(
        job_id, "gap", "Need larger multilingual corpus", section_id=section_b, source_context="test"
    )
    job_store.append_insight(
        job_id, "conflict", "Two studies disagree on causality", section_id=section_b, source_context="test"
    )
    state_syn = {
        "job_id": job_id,
        "llm_client": FakeClient(),
        "model_override": None,
        "output_language": "en",
        "markdown_parts": ["# Demo Review", "## Section A\ntext", "## Section B\ntext"],
        "canvas_id": "",
        "dashboard": dashboard,
    }
    state_syn = synthesize_node(state_syn)
    joined = "\n".join(state_syn.get("markdown_parts", []))
    assert "Limitations and Future Directions" in joined
    assert len(job_store.list_insights(job_id, status="open")) == 0
    assert len(job_store.list_insights(job_id, status="addressed")) >= 3
    print("[PASS] synthesize consumes insights and marks addressed")

    print("E2E validation finished: ALL CHECKS PASSED")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/24_generate_eval_dataset.py">
#!/usr/bin/env python
"""
LLM 辅助生成评估数据集

读取所有 data/parsed/*/enriched.json，从每篇论文的 Abstract + 前几个 content blocks
提取要点，调用 LLM 生成 3-5 个 QA pairs / 论文，输出标准 eval_mini.json 格式。

使用方法:
---------
# 预览（不写入文件）
python scripts/24_generate_eval_dataset.py --dry-run

# 生成并写入
python scripts/24_generate_eval_dataset.py --output data/eval_generated.json

# 指定 LLM provider / model
python scripts/24_generate_eval_dataset.py --llm deepseek --model deepseek-chat

# 追加到已有数据集
python scripts/24_generate_eval_dataset.py --append-to data/eval_mini.json

# 限制论文数
python scripts/24_generate_eval_dataset.py --max-papers 5

# 每篇论文的 QA 数量
python scripts/24_generate_eval_dataset.py --qa-per-paper 5
"""

import json
import re
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

sys.path.insert(0, ".")

from config.settings import settings
from src.llm.llm_manager import get_manager
from src.log import get_logger

logger = get_logger(__name__)

# 每篇论文提取的最大 content block 数
MAX_BLOCKS_PER_PAPER = 15
# 每个 block 的最大字符数
MAX_BLOCK_CHARS = 500


def _load_enriched(path: Path) -> Optional[Dict[str, Any]]:
    """加载 enriched.json"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"加载失败 {path}: {e}")
        return None


def _extract_paper_summary(doc: Dict[str, Any]) -> str:
    """从 enriched.json 提取论文摘要文本（Abstract + 前几个 content blocks）"""
    doc_id = doc.get("doc_id", "unknown")
    blocks = doc.get("content_flow", [])

    text_parts = [f"Paper ID: {doc_id}"]

    count = 0
    for block in blocks:
        if count >= MAX_BLOCKS_PER_PAPER:
            break

        block_type = block.get("type", "")
        content = ""

        if block_type == "heading":
            content = block.get("text", "").strip()
            if content:
                text_parts.append(f"\n## {content}")
                count += 1
        elif block_type == "paragraph":
            content = block.get("text", "").strip()
            if content:
                if len(content) > MAX_BLOCK_CHARS:
                    content = content[:MAX_BLOCK_CHARS] + "..."
                text_parts.append(content)
                count += 1
        elif block_type == "table":
            desc = block.get("llm_description", "")
            if desc:
                if len(desc) > MAX_BLOCK_CHARS:
                    desc = desc[:MAX_BLOCK_CHARS] + "..."
                text_parts.append(f"[Table] {desc}")
                count += 1

    return "\n\n".join(text_parts)


def _build_generation_prompt(paper_summary: str, doc_id: str, qa_count: int = 3) -> str:
    """构建 LLM 生成 QA 对的 prompt"""
    return f"""Based on the following scientific paper content, generate {qa_count} high-quality question-answer pairs for evaluating a retrieval-augmented generation (RAG) system.

Requirements for the QA pairs:
1. Questions should be answerable from the paper content provided
2. Include a mix of:
   - Factual questions (specific findings, data, measurements)
   - Methodology questions (experimental design, techniques used)
   - Data questions (specific numbers, statistics, quantities)
3. Answers should be concise but complete (2-4 sentences)
4. Each question should be standalone (understandable without seeing other questions)

Paper content:
{paper_summary}

Return a JSON array (no other text):
[
  {{
    "query": "The question text",
    "tags": ["factual" or "methodology" or "data_query"],
    "reference_answer": "The concise answer based on the paper"
  }}
]

IMPORTANT: Return ONLY the JSON array, no markdown fences or other text."""


def _parse_llm_response(text: str) -> List[Dict[str, Any]]:
    """解析 LLM 返回的 JSON"""
    # 去掉 markdown code fences
    text = text.strip()
    text = re.sub(r"^```(?:json)?\s*", "", text)
    text = re.sub(r"\s*```\s*$", "", text)

    try:
        data = json.loads(text)
        if isinstance(data, list):
            return data
        if isinstance(data, dict) and "cases" in data:
            return data["cases"]
        return []
    except json.JSONDecodeError as e:
        logger.warning(f"JSON 解析失败: {e}")
        # 尝试修复常见问题
        try:
            # 寻找第一个 [ 和最后一个 ]
            start = text.index("[")
            end = text.rindex("]") + 1
            return json.loads(text[start:end])
        except Exception:
            return []


def generate_for_paper(
    doc: Dict[str, Any],
    llm_client: Any,
    qa_count: int = 3,
    model_override: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """为单篇论文生成 QA 对"""
    doc_id = doc.get("doc_id", "unknown")
    summary = _extract_paper_summary(doc)

    if len(summary) < 100:
        logger.warning(f"论文内容过短，跳过: {doc_id}")
        return []

    prompt = _build_generation_prompt(summary, doc_id, qa_count)

    try:
        resp = llm_client.chat(
            [
                {"role": "system", "content": "You are a scientific QA pair generator. Return only valid JSON."},
                {"role": "user", "content": prompt},
            ],
            model=model_override,
            max_tokens=2000,
        )
        text = (resp.get("final_text") or "").strip()
        raw_cases = _parse_llm_response(text)
    except Exception as e:
        logger.error(f"LLM 调用失败 ({doc_id}): {e}")
        return []

    # 标准化输出
    cases = []
    for i, raw in enumerate(raw_cases):
        query = (raw.get("query") or "").strip()
        answer = (raw.get("reference_answer") or "").strip()
        tags = raw.get("tags", ["factual"])
        if isinstance(tags, str):
            tags = [tags]

        if not query or not answer:
            continue

        cases.append({
            "query": query,
            "tags": tags,
            "expected_doc_ids": [doc_id],
            "expected_citations": [],
            "reference_answer": answer,
        })

    logger.info(f"  {doc_id}: 生成 {len(cases)}/{qa_count} 条 QA")
    return cases


def main():
    import argparse

    parser = argparse.ArgumentParser(description="LLM 辅助生成评估 QA 数据集")
    parser.add_argument("--output", type=str, default="data/eval_generated.json", help="输出文件路径")
    parser.add_argument("--append-to", type=str, default=None, help="追加到已有数据集（合并 cases）")
    parser.add_argument("--parsed-dir", type=str, default="data/parsed", help="enriched.json 所在目录")
    parser.add_argument("--qa-per-paper", type=int, default=3, help="每篇论文生成的 QA 数量")
    parser.add_argument("--max-papers", type=int, default=None, help="最多处理的论文数")
    parser.add_argument("--llm", type=str, default=None, help="LLM provider")
    parser.add_argument("--model", type=str, default=None, help="模型覆盖")
    parser.add_argument("--config", type=str, default="config/rag_config.json", help="配置文件路径")
    parser.add_argument("--dry-run", action="store_true", help="仅预览，不写入文件")
    args = parser.parse_args()

    # 扫描所有 enriched.json
    parsed_dir = Path(args.parsed_dir)
    enriched_files = sorted(parsed_dir.glob("*/enriched.json"))

    if not enriched_files:
        logger.error(f"未找到 enriched.json 文件: {parsed_dir}")
        sys.exit(1)

    if args.max_papers:
        enriched_files = enriched_files[: args.max_papers]

    logger.info(f"找到 {len(enriched_files)} 篇论文")

    # 初始化 LLM
    if settings.llm.dry_run:
        logger.warning("LLM_DRY_RUN=true，无法生成 QA 对")
        sys.exit(1)

    manager = get_manager(args.config)
    llm_client = manager.get_client(args.llm)

    # 生成 QA 对
    all_cases: List[Dict[str, Any]] = []
    id_counter = 1

    for enriched_path in enriched_files:
        doc = _load_enriched(enriched_path)
        if not doc:
            continue

        doc_id = doc.get("doc_id", enriched_path.parent.name)
        logger.info(f"处理: {doc_id}")

        cases = generate_for_paper(
            doc=doc,
            llm_client=llm_client,
            qa_count=args.qa_per_paper,
            model_override=args.model,
        )

        for case in cases:
            case["id"] = f"gen_{id_counter:03d}"
            case["mode"] = "local"
            id_counter += 1
            all_cases.append(case)

    logger.info(f"总计生成 {len(all_cases)} 条 QA 对")

    if args.dry_run:
        print(json.dumps(all_cases[:5], ensure_ascii=False, indent=2))
        logger.info("(dry-run 模式，不写入文件)")
        return

    # 构建输出
    output_data = {
        "name": "deepsea_eval_generated",
        "description": f"LLM-generated evaluation dataset ({len(all_cases)} cases from {len(enriched_files)} papers)",
        "version": "1.0.0",
        "cases": all_cases,
    }

    # 追加模式
    if args.append_to:
        append_path = Path(args.append_to)
        if append_path.exists():
            with open(append_path, "r", encoding="utf-8") as f:
                existing = json.load(f)
            existing_cases = existing.get("cases", [])
            existing_ids = {c["id"] for c in existing_cases}

            # 去重并追加
            new_cases = [c for c in all_cases if c["id"] not in existing_ids]
            existing_cases.extend(new_cases)
            existing["cases"] = existing_cases
            existing["description"] = (
                existing.get("description", "") +
                f" + {len(new_cases)} generated cases"
            )

            output_path = append_path
            output_data = existing
            logger.info(f"追加 {len(new_cases)} 条到 {append_path}")
        else:
            output_path = append_path
    else:
        output_path = Path(args.output)

    output_path.parent.mkdir(parents=True, exist_ok=True)
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output_data, f, ensure_ascii=False, indent=2)

    logger.info(f"写入: {output_path} ({len(output_data['cases'])} 条)")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/25_extract_claims.py">
#!/usr/bin/env python
"""
批量 Claim 提取脚本

对已解析的论文（data/parsed/*/enriched.json）批量提取核心 claims，
结果写回 enriched.json 的 `claims` 字段（不需要重新解析 PDF）。

使用方法:
---------
# 提取所有论文的 claims
python scripts/25_extract_claims.py

# 仅处理指定论文
python scripts/25_extract_claims.py --paper-id "2026_Botté_et_al_Artificial_Light"

# 指定 LLM provider
python scripts/25_extract_claims.py --llm deepseek

# 跳过已有 claims 的论文
python scripts/25_extract_claims.py --skip-existing

# 预览（不写入文件）
python scripts/25_extract_claims.py --dry-run

# 限制论文数
python scripts/25_extract_claims.py --max-papers 5
"""

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

sys.path.insert(0, ".")

from config.settings import settings
from src.llm.llm_manager import get_manager
from src.log import get_logger
from src.parser.claim_extractor import ClaimExtractor

logger = get_logger(__name__)

_CONFIG_PATH = Path("config/rag_config.json")


def _find_enriched_files(data_dir: Optional[Path] = None) -> List[Path]:
    """查找所有 enriched.json 文件"""
    base = data_dir or settings.path.data / "parsed"
    if not base.exists():
        logger.warning(f"目录不存在: {base}")
        return []
    return sorted(base.glob("*/enriched.json"))


def _load_enriched(path: Path) -> Optional[Dict[str, Any]]:
    """加载 enriched.json"""
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        logger.warning(f"加载失败 {path}: {e}")
        return None


def _save_enriched(path: Path, data: Dict[str, Any]) -> None:
    """保存 enriched.json"""
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)


class _SimpleDoc:
    """轻量 EnrichedDoc 代理，用于 ClaimExtractor"""

    def __init__(self, doc_id: str, content_flow: List[Any]):
        self.doc_id = doc_id
        self.content_flow = content_flow


class _SimpleBlock:
    """轻量 ContentBlock 代理"""

    def __init__(self, block_id: str, heading_path: List[str], text: str):
        self.block_id = block_id
        self.heading_path = heading_path
        self.text = text


def _build_doc_proxy(data: Dict[str, Any]) -> _SimpleDoc:
    """从 enriched.json 的 dict 构建轻量 doc 对象"""
    doc_id = data.get("doc_id", "unknown")
    blocks = []
    for block in data.get("content_flow", []):
        blocks.append(_SimpleBlock(
            block_id=block.get("block_id", ""),
            heading_path=block.get("heading_path", []),
            text=block.get("text", ""),
        ))
    return _SimpleDoc(doc_id, blocks)


def main():
    parser = argparse.ArgumentParser(description="批量提取论文核心 Claims")
    parser.add_argument("--paper-id", type=str, default=None, help="仅处理指定 paper_id")
    parser.add_argument("--llm", type=str, default=None, help="LLM provider (如 deepseek)")
    parser.add_argument("--model", type=str, default=None, help="覆盖默认模型")
    parser.add_argument("--skip-existing", action="store_true", help="跳过已有 claims 的论文")
    parser.add_argument("--dry-run", action="store_true", help="预览模式，不写入文件")
    parser.add_argument("--max-papers", type=int, default=None, help="最多处理的论文数")
    parser.add_argument("--data-dir", type=str, default=None, help="数据目录（默认 data/parsed）")
    args = parser.parse_args()

    # 初始化 LLM
    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client(args.llm or None)
    if args.model:
        # 存储 model override
        _original_chat = client.chat

        def _chat_with_model(**kwargs):
            kwargs.setdefault("model", args.model)
            return _original_chat(**kwargs)

        client.chat = _chat_with_model

    extractor = ClaimExtractor()

    # 查找文件
    data_dir = Path(args.data_dir) if args.data_dir else None
    files = _find_enriched_files(data_dir)
    logger.info(f"找到 {len(files)} 个 enriched.json 文件")

    if args.paper_id:
        files = [f for f in files if args.paper_id in f.parent.name]
        logger.info(f"过滤后: {len(files)} 个文件 (paper_id={args.paper_id})")

    if args.max_papers:
        files = files[:args.max_papers]

    # 处理
    total_claims = 0
    processed = 0
    errors = 0

    for fpath in files:
        data = _load_enriched(fpath)
        if data is None:
            errors += 1
            continue

        doc_id = data.get("doc_id", fpath.parent.name)

        # 跳过已有 claims
        if args.skip_existing and data.get("claims"):
            logger.info(f"[{doc_id}] 已有 {len(data['claims'])} claims，跳过")
            continue

        logger.info(f"[{doc_id}] 提取 claims...")

        try:
            doc_proxy = _build_doc_proxy(data)
            claims = extractor.extract(doc_proxy, client)
            claims_dicts = [c.to_dict() for c in claims]

            total_claims += len(claims_dicts)
            processed += 1

            if args.dry_run:
                print(f"\n{'='*60}")
                print(f"Paper: {doc_id}")
                print(f"Claims ({len(claims_dicts)}):")
                for c in claims_dicts:
                    print(f"  [{c['confidence']}] {c['text'][:100]}...")
            else:
                data["claims"] = claims_dicts
                _save_enriched(fpath, data)
                logger.info(f"[{doc_id}] 写入 {len(claims_dicts)} claims")

        except Exception as e:
            logger.error(f"[{doc_id}] 提取失败: {e}")
            errors += 1

    # 汇总
    print(f"\n{'='*60}")
    print(f"完成: 处理 {processed} 篇论文, 提取 {total_claims} 个 claims, {errors} 个错误")
    if args.dry_run:
        print("（预览模式，未写入文件）")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/26_backfill_doi.py">
#!/usr/bin/env python
"""
步骤26: 为已入库论文回填 DOI 和完整标题

基于 Docling 已有 parse 产物（data/parsed/*/enriched.json），
而非重新打开 PDF，保持与已有框架一致。

策略：
1. 从 enriched.json 的 content_flow 前 N 块正则提取 DOI（零成本）
2. 若提取到 DOI → CrossRef /works/{doi} 获取完整元数据
3. 若未提取到 → CrossRef /works?query.bibliographic= 按文件名搜索

持久化: data/paper_metadata.db（SQLite, 增量式, 可重跑）

用法:
    python scripts/26_backfill_doi.py                  # 正常运行
    python scripts/26_backfill_doi.py --dry-run        # 仅扫描，不调用 CrossRef
    python scripts/26_backfill_doi.py --force           # 忽略已有结果，全量重跑
"""

import argparse
import json
import re
import sys
import time
from pathlib import Path
from typing import Any, Dict, List, Optional

sys.path.insert(0, ".")

import requests

from config.settings import settings
from src.log import get_logger
from src.indexing.paper_metadata_store import PaperMetadataStore

logger = get_logger(__name__)

# ── 常量 ──────────────────────────────────────────────

DOI_RE = re.compile(
    r"(?:doi[:\s]*|(?:https?://)?(?:dx\.)?doi\.org/)"
    r"?(10\.\d{4,9}/[^\s,;)\]\"'<>]+[^\s,;)\]\"'<>.:])",
    re.IGNORECASE,
)

CROSSREF_BASE = "https://api.crossref.org"
CROSSREF_MAILTO = ""
CROSSREF_TIMEOUT = 20
RATE_LIMIT_SLEEP = 0.3
SCAN_BLOCKS = 30  # enriched.json 前多少个 block 内搜索 DOI


# ── 从 enriched.json 提取 ─────────────────────────────

def extract_doi_from_enriched(content_flow: List[Dict], max_blocks: int = SCAN_BLOCKS) -> Optional[str]:
    """从 content_flow 前 N 个 block 的 text 中正则提取 DOI"""
    for block in content_flow[:max_blocks]:
        text = block.get("text")
        if not text or not isinstance(text, str):
            continue
        m = DOI_RE.search(text)
        if m:
            return m.group(1).rstrip(".:")
    return None


def extract_title_from_enriched(content_flow: List[Dict]) -> Optional[str]:
    """
    从 content_flow 头部提取论文标题。

    Pass 1: heading 块中找长度 10-300 且非分类标签的第一个。
    Pass 2: 若 heading 没命中，退回到 page_index==0 的 text 块中
            找第一个看起来像标题的（15-300 字符，排除 URL / 作者行等噪声）。
    """
    skip_labels = {
        "original article", "research article", "review", "letter",
        "communication", "open", "open access", "article", "brief report",
        "research paper", "full paper", "short communication",
        "abstract", "abstract:", "data note", "introduction",
        "keywords", "highlights", "graphical abstract", "contents",
    }

    # Pass 1: heading blocks
    for block in content_flow[:15]:
        bt = (block.get("block_type") or "").lower()
        text = (block.get("text") or "").strip()
        if bt != "heading" or not text:
            continue
        if text.lower() in skip_labels:
            continue
        if 10 <= len(text) <= 300:
            return text

    # Pass 2: text blocks on page 0 — find first title-like block
    for block in content_flow[:15]:
        bt = (block.get("block_type") or "").lower()
        if bt not in ("text", ""):
            continue
        text = (block.get("text") or "").strip()
        page = block.get("page_index", 0)
        if page != 0 or not text:
            continue
        if not (15 <= len(text) <= 300):
            continue
        tl = text.lower()
        if tl in skip_labels:
            continue
        # skip URLs, copyright lines, author lines with affiliations
        if tl.startswith(("http", "www.", "©", "copyright")):
            continue
        if re.search(r"\d{4}\s*(international|society|elsevier|springer|wiley|nature)", tl):
            continue
        # author lines typically have many superscript numbers/commas
        digit_ratio = sum(c.isdigit() for c in text) / max(len(text), 1)
        if digit_ratio > 0.08:
            continue
        return text

    return None


# ── CrossRef API ──────────────────────────────────────

def _crossref_headers() -> Dict[str, str]:
    ua = "RAG-DOI-Backfill/1.0"
    if CROSSREF_MAILTO:
        ua += f" (mailto:{CROSSREF_MAILTO})"
    return {"User-Agent": ua}


def _crossref_get(url: str, params: Optional[Dict] = None) -> Optional[Dict]:
    params = params or {}
    if CROSSREF_MAILTO:
        params["mailto"] = CROSSREF_MAILTO
    for attempt in range(3):
        try:
            resp = requests.get(
                url, params=params, headers=_crossref_headers(), timeout=CROSSREF_TIMEOUT,
            )
            if resp.status_code == 200:
                return resp.json()
            if resp.status_code == 429:
                wait = 2 ** (attempt + 1)
                logger.warning("  CrossRef 429, retry in %ds", wait)
                time.sleep(wait)
                continue
            logger.warning("  CrossRef HTTP %d for %s", resp.status_code, url[:120])
            return None
        except requests.RequestException as e:
            logger.warning("  CrossRef error (attempt %d): %s", attempt + 1, e)
            time.sleep(1)
    return None


def _parse_crossref_item(item: Dict) -> Dict[str, Any]:
    titles = item.get("title", [])
    title = titles[0] if titles else None
    doi = item.get("DOI")

    authors: List[str] = []
    for a in item.get("author", []):
        name = f"{a.get('given', '')} {a.get('family', '')}".strip()
        if name:
            authors.append(name)

    year = None
    for date_field in ("published-print", "published-online", "created"):
        dp = (item.get(date_field) or {}).get("date-parts", [[]])
        if dp and dp[0] and dp[0][0]:
            year = dp[0][0]
            break

    return {"doi": doi, "title": title, "authors": authors or None, "year": year}


def crossref_by_doi(doi: str) -> Optional[Dict[str, Any]]:
    encoded = requests.utils.quote(doi, safe="")
    data = _crossref_get(f"{CROSSREF_BASE}/works/{encoded}")
    if data:
        return _parse_crossref_item(data.get("message", {}))
    return None


def crossref_by_title(query: str, rows: int = 5) -> Optional[Dict[str, Any]]:
    data = _crossref_get(
        f"{CROSSREF_BASE}/works",
        params={"query.bibliographic": query, "rows": rows},
    )
    if not data:
        return None
    items = data.get("message", {}).get("items", [])
    if not items:
        return None

    query_words = _word_set(query)
    best, best_score = None, -1.0
    for item in items:
        parsed = _parse_crossref_item(item)
        score = _title_overlap(query_words, parsed.get("title") or "")
        if score > best_score:
            best_score = score
            best = parsed

    if best_score < 0.35:
        logger.info("  CrossRef best score %.2f < 0.35, rejected", best_score)
        return None
    return best


# ── 相似度 ────────────────────────────────────────────

def _word_set(text: str) -> set:
    return {w for w in re.sub(r"[^a-z0-9\s]", "", text.lower()).split() if len(w) >= 3}


def _title_overlap(query_words: set, crossref_title: str) -> float:
    if not query_words:
        return 0.0
    cr_words = _word_set(crossref_title)
    if not cr_words:
        return 0.0
    return len(query_words & cr_words) / len(query_words)


# ── 文件名 → 查询 ────────────────────────────────────

def query_from_filename(paper_id: str) -> str:
    q = paper_id.replace("_", " ").replace("-", " ")
    q = re.sub(r"\s+", " ", q).strip()
    return q


# ── 核心 ──────────────────────────────────────────────

def process_paper(
    paper_id: str,
    content_flow: List[Dict],
    existing: Optional[Dict[str, Any]] = None,
    dry_run: bool = False,
) -> Dict[str, Any]:
    result: Dict[str, Any] = {
        "doi": None, "title": None, "authors": None, "year": None, "source": None,
    }

    # ① 正则提取 DOI
    doi = extract_doi_from_enriched(content_flow)
    title = extract_title_from_enriched(content_flow)

    if doi:
        logger.info("  [enriched] DOI = %s", doi)
        result["doi"] = doi
        result["title"] = title
        result["source"] = "enriched_regex"

        # 仅当本地无标题且无历史记录时才调 CrossRef
        if not title and not dry_run:
            meta = crossref_by_doi(doi)
            if meta and meta.get("title"):
                result["title"] = meta["title"]
                result["authors"] = meta.get("authors")
                result["year"] = meta.get("year")
                result["source"] = "crossref_by_doi"
        return result

    # ② 本地无 DOI → 先看历史记录里有没有
    if existing and existing.get("doi"):
        logger.info("  [cached] reuse previous DOI = %s", existing["doi"])
        result["doi"] = existing["doi"]
        result["title"] = title or existing.get("title")
        result["authors"] = existing.get("authors")
        result["year"] = existing.get("year")
        result["source"] = "cached"
        return result

    # ③ 真正的新纸、本地也提不到 → CrossRef
    if dry_run:
        result["source"] = "need_crossref"
        result["title"] = title
        return result

    query = title or query_from_filename(paper_id)
    logger.info("  [CrossRef] query = '%s'", query[:80])
    meta = crossref_by_title(query)

    if meta and meta.get("doi"):
        result.update(meta)
        result["source"] = "crossref_by_title"
    else:
        if title:
            result["title"] = title
            result["source"] = "enriched_title_only"
        logger.warning("  [MISS] no DOI for %s", paper_id[:60])

    return result


# ── main ──────────────────────────────────────────────

def main():
    parser = argparse.ArgumentParser(description="Backfill DOI & title from enriched.json + CrossRef")
    parser.add_argument("--dry-run", action="store_true", help="Only scan enriched.json, skip CrossRef")
    parser.add_argument("--force", action="store_true", help="Re-process all (ignore existing metadata)")
    args = parser.parse_args()

    settings.path.ensure_dirs()
    parsed_dir = settings.path.parsed
    store = PaperMetadataStore()

    logger.info("=" * 60)
    logger.info("DOI & Title Backfill  (dry_run=%s, force=%s)", args.dry_run, args.force)
    logger.info("=" * 60)
    logger.info("Parsed dir: %s | DB entries: %d", parsed_dir, store.count())

    enriched_files = sorted(parsed_dir.rglob("enriched.json"))
    logger.info("enriched.json count: %d", len(enriched_files))

    if not enriched_files:
        logger.warning("No enriched.json found. Run ingestion first.")
        return

    stats: Dict[str, int] = {}

    for i, json_path in enumerate(enriched_files, 1):
        paper_id = json_path.parent.name

        existing = store.get(paper_id)
        if not args.force and existing and existing.get("doi") and existing.get("title"):
            stats["skipped"] = stats.get("skipped", 0) + 1
            continue

        logger.info("[%d/%d] %s", i, len(enriched_files), paper_id[:70])

        try:
            with open(json_path, "r", encoding="utf-8") as f:
                doc = json.load(f)
        except Exception as e:
            logger.warning("  Failed to read %s: %s", json_path, e)
            stats["error"] = stats.get("error", 0) + 1
            continue

        content_flow = doc.get("content_flow", [])

        # DOI already known but title missing → try enriched.json locally first
        if existing and existing.get("doi") and not existing.get("title"):
            title = extract_title_from_enriched(content_flow)
            if title:
                store.upsert(paper_id, title=title)
                stats["title_from_enriched"] = stats.get("title_from_enriched", 0) + 1
                continue
            if not args.dry_run:
                doi = existing["doi"]
                logger.info("  [CrossRef] title missing locally, fetching by DOI %s", doi)
                meta = crossref_by_doi(doi)
                if meta and meta.get("title"):
                    store.upsert(
                        paper_id,
                        title=meta["title"],
                        authors=meta.get("authors"),
                        year=meta.get("year"),
                        source="crossref_by_doi",
                    )
                stats["title_from_crossref"] = stats.get("title_from_crossref", 0) + 1
                time.sleep(RATE_LIMIT_SLEEP)
            continue

        result = process_paper(paper_id, content_flow, existing=existing, dry_run=args.dry_run)
        store.upsert(
            paper_id,
            doi=result.get("doi"),
            title=result.get("title"),
            authors=result.get("authors"),
            year=result.get("year"),
            source=result.get("source"),
        )

        src = result.get("source") or "miss"
        stats[src] = stats.get(src, 0) + 1

        if not args.dry_run and src in ("crossref_by_doi", "crossref_by_title"):
            time.sleep(RATE_LIMIT_SLEEP)

    total = store.count()
    doi_n = len(store.all_dois())
    title_n = len(store.all_normalized_titles())

    logger.info("=" * 60)
    logger.info("DB     : %s (%d entries)", store._db_path, total)
    logger.info("Stats  : %s", {k: v for k, v in stats.items() if v})
    logger.info("DOI    : %d / %d  (%.1f%%)", doi_n, total, 100 * doi_n / max(total, 1))
    logger.info("Title  : %d / %d  (%.1f%%)", title_n, total, 100 * title_n / max(total, 1))
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/27_test_year_window_and_citation_style.py">
#!/usr/bin/env python3
"""
API 回归脚本：年份窗口硬过滤 + 引文格式（APA/IEEE）。

覆盖点：
1) /chat 透传并回显 diagnostics.year_window
2) /deep-research/submit 接受 year_start/year_end
3) （可选）/deep-research/start 接受 year_start/year_end
4) format_reference_list 支持 style="apa" / "ieee"

运行示例：
  conda run --no-capture-output -n deepsea-rag python scripts/27_test_year_window_and_citation_style.py
  conda run --no-capture-output -n deepsea-rag python scripts/27_test_year_window_and_citation_style.py --with-start
"""

from __future__ import annotations

import argparse
import json
import sys
from pathlib import Path
from typing import Any, Dict, Optional, Tuple

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def _assert(cond: bool, msg: str) -> None:
    if not cond:
        raise AssertionError(msg)


def _post_json(base: str, path: str, payload: Dict[str, Any], timeout: int) -> Dict[str, Any]:
    import requests

    url = f"{base}{path}"
    resp = requests.post(url, json=payload, timeout=timeout)
    if resp.status_code >= 400:
        detail = resp.text[:1200]
        raise RuntimeError(f"{path} failed ({resp.status_code}): {detail}")
    try:
        return resp.json()
    except Exception as e:
        raise RuntimeError(f"{path} did not return JSON: {e}") from e


def _check_chat_year_window(
    base: str,
    timeout: int,
    year_start: int,
    year_end: int,
    expect_window: Tuple[int, int],
    session_id: Optional[str] = None,
) -> str:
    """
    发送 chat 请求并校验 diagnostics.year_window。
    返回 session_id 供下一轮复用。
    """
    payload: Dict[str, Any] = {
        "session_id": session_id,
        "message": "请基于文献总结深海冷泉研究进展，并给出关键证据。",
        "search_mode": "local",
        "year_start": year_start,
        "year_end": year_end,
        "local_top_k": 6,
        "final_top_k": 6,
    }
    data = _post_json(base, "/chat", payload, timeout=timeout)
    sid = data.get("session_id") or ""
    _assert(bool(sid), "chat response missing session_id")
    ev = data.get("evidence_summary") or {}
    diag = ev.get("diagnostics") or {}
    yw = diag.get("year_window") or {}

    got_start = yw.get("year_start")
    got_end = yw.get("year_end")
    exp_start, exp_end = expect_window
    _assert(
        got_start == exp_start and got_end == exp_end,
        (
            "year_window mismatch: "
            f"expect=({exp_start}, {exp_end}), got=({got_start}, {got_end}), "
            f"full_diag={json.dumps(diag, ensure_ascii=False)}"
        ),
    )
    return sid


def _check_submit_year_window(base: str, timeout: int, year_start: int, year_end: int) -> str:
    payload: Dict[str, Any] = {
        "topic": "深海冷泉微生物研究进展",
        "search_mode": "local",
        "confirmed_outline": ["背景与定义", "研究进展", "挑战与展望"],
        "year_start": year_start,
        "year_end": year_end,
    }
    data = _post_json(base, "/deep-research/submit", payload, timeout=timeout)
    job_id = data.get("job_id") or ""
    _assert(bool(job_id), "deep-research/submit response missing job_id")
    return job_id


def _check_start_year_window(base: str, timeout: int, year_start: int, year_end: int) -> None:
    payload: Dict[str, Any] = {
        "topic": "深海冷泉甲烷循环机制",
        "search_mode": "local",
        "year_start": year_start,
        "year_end": year_end,
        "local_top_k": 8,
        "final_top_k": 12,
    }
    data = _post_json(base, "/deep-research/start", payload, timeout=timeout)
    outline = data.get("outline") or []
    _assert(isinstance(outline, list), "deep-research/start response outline must be list")


def _cancel_job(base: str, timeout: int, job_id: str) -> None:
    import requests

    url = f"{base}/deep-research/jobs/{job_id}/cancel"
    resp = requests.post(url, timeout=timeout)
    if resp.status_code >= 400:
        raise RuntimeError(f"/deep-research/jobs/{job_id}/cancel failed: {resp.status_code} {resp.text[:500]}")


def _check_formatter_styles() -> None:
    from src.collaboration.canvas.models import Citation
    from src.collaboration.citation.formatter import format_reference_list

    sample = Citation(
        title="Deep Sea Cold Seep Ecosystems",
        authors=["John Smith", "Alice Brown"],
        year=2023,
        doi="10.1000/example-doi",
    )
    apa = format_reference_list([sample], style="apa")
    ieee = format_reference_list([sample], style="ieee")

    _assert("Smith" in apa and "(2023)." in apa, f"APA output unexpected: {apa}")
    _assert(ieee.strip().startswith("[1]"), f"IEEE output unexpected: {ieee}")


def main() -> None:
    parser = argparse.ArgumentParser(description="回归：年份窗口硬过滤 + APA/IEEE")
    parser.add_argument("--base", default="", help="API base URL, e.g. http://127.0.0.1:8000")
    parser.add_argument("--timeout", type=int, default=180, help="HTTP timeout seconds")
    parser.add_argument(
        "--with-start",
        action="store_true",
        help="额外执行 /deep-research/start（较慢，依赖 LLM 可用）",
    )
    args = parser.parse_args()

    if args.base:
        base = args.base.rstrip("/")
    else:
        from config.settings import settings

        base = f"http://{settings.api.host}:{settings.api.port}"

    print(f"[info] base={base}")

    # 1) Chat: 正常年份窗口
    sid = _check_chat_year_window(
        base=base,
        timeout=args.timeout,
        year_start=2020,
        year_end=2024,
        expect_window=(2020, 2024),
    )
    print("[1] /chat year_window passthrough -> PASS")

    # 2) Chat: 反向输入（校验后端自动归一化）
    _check_chat_year_window(
        base=base,
        timeout=args.timeout,
        year_start=2026,
        year_end=2022,
        expect_window=(2022, 2026),
        session_id=sid,
    )
    print("[2] /chat reversed year_window normalize -> PASS")

    # 3) Deep Research submit: year 字段可被 API 接受
    job_id = _check_submit_year_window(base=base, timeout=args.timeout, year_start=2021, year_end=2025)
    print(f"[3] /deep-research/submit year_window accepted -> PASS (job_id={job_id})")

    # 避免任务继续占资源，提交后立即取消（若 worker 尚未执行则会直接 cancelled）
    _cancel_job(base=base, timeout=args.timeout, job_id=job_id)
    print("[4] /deep-research/jobs/{job_id}/cancel -> PASS")

    # 4) 可选：Deep Research start 合同验证
    if args.with_start:
        _check_start_year_window(base=base, timeout=max(args.timeout, 240), year_start=2019, year_end=2024)
        print("[5] /deep-research/start year_window accepted -> PASS")

    # 5) Formatter regression
    _check_formatter_styles()
    print("[6] formatter style=apa/ieee -> PASS")

    print("ALL CHECKS PASSED")


if __name__ == "__main__":
    main()
</file>

<file path="scripts/check_sonar_health.py">
#!/usr/bin/env python3
"""
Sonar provider health check for DeepSea-RAG.

Checks:
1) Config merge and provider presence
2) Client creation via LLMManager
3) Optional live chat request
4) Optional raw network probe

Usage:
  conda run -n deepsea-rag python scripts/check_sonar_health.py
  conda run -n deepsea-rag python scripts/check_sonar_health.py --skip-chat
"""

from __future__ import annotations

import argparse
import json
import socket
import ssl
import sys
import traceback
from pathlib import Path
from typing import Any, Dict, List, Tuple
from urllib.parse import urlparse


REPO_ROOT = Path(__file__).resolve().parents[1]
CONFIG_PATH = REPO_ROOT / "config" / "rag_config.json"
if str(REPO_ROOT) not in sys.path:
    sys.path.insert(0, str(REPO_ROOT))


def _ok(msg: str) -> None:
    print(f"[PASS] {msg}")


def _warn(msg: str) -> None:
    print(f"[WARN] {msg}")


def _fail(msg: str) -> None:
    print(f"[FAIL] {msg}")


def deep_merge(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    out = dict(base)
    for k, v in override.items():
        if isinstance(out.get(k), dict) and isinstance(v, dict):
            out[k] = deep_merge(out[k], v)
        else:
            out[k] = v
    return out


def load_merged_config() -> Dict[str, Any]:
    base = json.loads(CONFIG_PATH.read_text(encoding="utf-8"))
    local_path = CONFIG_PATH.with_name("rag_config.local.json")
    if local_path.exists():
        local = json.loads(local_path.read_text(encoding="utf-8"))
        return deep_merge(base, local)
    return base


def check_network_tls(base_url: str, timeout_sec: int = 8) -> Tuple[bool, str]:
    try:
        u = urlparse(base_url)
        host = u.hostname
        port = u.port or 443
        if not host:
            return False, f"Invalid URL host: {base_url}"
        ip = socket.gethostbyname(host)
        ctx = ssl.create_default_context()
        with socket.create_connection((host, port), timeout=timeout_sec) as sock:
            with ctx.wrap_socket(sock, server_hostname=host):
                pass
        return True, f"DNS+TLS OK ({host} -> {ip}:{port})"
    except Exception as e:
        return False, f"DNS/TLS failed for {base_url}: {e!r}"


def main() -> int:
    parser = argparse.ArgumentParser(description="Check sonar provider health")
    parser.add_argument("--skip-chat", action="store_true", help="Skip live chat request")
    args = parser.parse_args()

    print("=== Sonar Health Check ===")
    print(f"Config: {CONFIG_PATH}")

    # 1) Merged config sanity
    merged = load_merged_config()
    llm = (merged.get("llm") or {})
    providers = (llm.get("providers") or {})
    sonar = providers.get("sonar")
    if not sonar:
        _fail("Provider 'sonar' missing in merged config")
        return 1
    _ok("Provider 'sonar' exists in merged config")

    base_url = str(sonar.get("base_url") or "")
    default_model = str(sonar.get("default_model") or "")
    models = sonar.get("models") or {}
    api_key = str(sonar.get("api_key") or "")

    if base_url:
        _ok(f"base_url: {base_url}")
    else:
        _fail("base_url is empty")
        return 1

    if default_model:
        _ok(f"default_model: {default_model}")
    else:
        _warn("default_model is empty")

    if isinstance(models, dict) and models:
        _ok(f"models: {sorted(models.keys())}")
    else:
        _warn("models map is empty")

    if api_key.strip():
        _ok("api_key is set (non-empty)")
    else:
        _fail("api_key is empty; please set llm.providers.sonar.api_key in rag_config.local.json")
        return 1

    # 2) TLS reachability probe
    tls_ok, tls_msg = check_network_tls(base_url)
    if tls_ok:
        _ok(tls_msg)
    else:
        _warn(tls_msg)

    # 3) LLMManager client creation + optional live request
    try:
        from src.llm.llm_manager import LLMManager

        mgr = LLMManager.from_json(CONFIG_PATH)
        if "sonar" not in mgr.get_provider_names():
            _fail(f"'sonar' not in manager providers: {mgr.get_provider_names()}")
            return 1
        _ok("LLMManager loads provider list with sonar")

        client = mgr.get_client("sonar")
        _ok(f"get_client('sonar') -> {type(client).__name__}")
    except Exception as e:
        _fail(f"Client creation failed: {e!r}")
        traceback.print_exc()
        return 1

    if args.skip_chat:
        print("Skipping live chat request (--skip-chat).")
        return 0

    try:
        resp = client.chat(
            messages=[
                {"role": "system", "content": "You are a concise assistant."},
                {"role": "user", "content": "Reply exactly with SONAR_OK"},
            ],
            model="sonar",
            max_tokens=20,
        )
        final_text = (resp.get("final_text") or "").strip()
        provider = resp.get("provider")
        model = resp.get("model")
        latency = (resp.get("meta") or {}).get("latency_ms")

        _ok(f"Live chat succeeded provider={provider} model={model} latency_ms={latency}")
        print(f"final_text: {final_text}")
        if "SONAR_OK" in final_text:
            _ok("Response content check passed")
        else:
            _warn("Response did not exactly contain SONAR_OK (API path still works)")
        return 0
    except Exception as e:
        _fail(f"Live chat failed: {e!r}")
        traceback.print_exc()
        return 2


if __name__ == "__main__":
    sys.exit(main())
</file>

<file path="scripts/debug_chat.py">
#!/usr/bin/env python3
"""本地调用 chat 逻辑，复现 500 并打印 traceback。"""
import sys
from pathlib import Path
ROOT = Path(__file__).resolve().parents[1]
sys.path.insert(0, str(ROOT))

def main():
    from src.api.routes_chat import chat_post
    from src.api.schemas import ChatRequest
    body = ChatRequest(message="/search 深海冷泉", search_mode="local")
    try:
        out = chat_post(body)
        print("OK", out.session_id, out.evidence_summary.total_chunks)
    except Exception as e:
        import traceback
        traceback.print_exc()
        raise

if __name__ == "__main__":
    main()
</file>

<file path="scripts/demo_llm_manager.py">
#!/usr/bin/env python
"""
LLM Manager 演示脚本

演示功能：
1. 加载配置
2. 列出可用 providers
3. dry_run 模式测试
4. 实际调用测试（可选）
5. 日志清理
"""

import sys
import argparse

sys.path.insert(0, ".")

from src.log import get_logger
from src.llm import LLMManager, RawLogStore

logger = get_logger(__name__)


def main():
    parser = argparse.ArgumentParser(description="LLM Manager 演示")
    parser.add_argument("--config", default="config/rag_config.json", help="配置文件路径")
    parser.add_argument("--provider", type=str, help="指定 provider（默认使用 config.default）")
    parser.add_argument("--model", type=str, help="指定模型（覆盖默认）")
    parser.add_argument("--real", action="store_true", help="实际调用 API（否则仅 dry_run）")
    parser.add_argument("--cleanup", action="store_true", help="执行日志清理")
    args = parser.parse_args()

    logger.info("=" * 60)
    logger.info("LLM Manager 演示")
    logger.info("=" * 60)

    # 1. 加载配置
    logger.info("[1] 加载配置...")
    manager = LLMManager.from_json(args.config)
    logger.info("配置文件: %s, 默认 provider: %s, dry_run: %s",
                args.config, manager.config.default, manager.config.dry_run)

    # 2. 列出 providers
    logger.info("[2] 可用 providers:")
    for name in manager.get_provider_names():
        available = manager.is_available(name)
        pcfg = manager.config.providers[name]
        status = "✓" if available else "✗"
        logger.info("%s %s, default_model: %s, models: %s",
                    status, name, pcfg.default_model, list(pcfg.models.keys()))
        if pcfg.params:
            logger.info("  params: %s", list(pcfg.params.keys()))

    # 3. 模型解析演示
    provider_name = args.provider or manager.config.default
    logger.info("[3] 模型解析演示 (provider=%s)", provider_name)

    resolved_default = manager.resolve_model(provider_name)
    logger.info("默认模型: %s", resolved_default)

    if args.model:
        resolved_custom = manager.resolve_model(provider_name, args.model)
        logger.info("指定模型 '%s' -> %s", args.model, resolved_custom)

    # 4. 测试调用
    logger.info("[4] 测试调用 (provider=%s)", provider_name)

    # 强制 dry_run 除非指定 --real
    if not args.real:
        manager.config.dry_run = True
        logger.info("(dry_run 模式)")

    try:
        client = manager.get_client(provider_name)
        messages = [
            {"role": "system", "content": "你是一个简洁的助手。"},
            {"role": "user", "content": "用一句话介绍深海热液喷口。"},
        ]
        
        resp = client.chat(messages, model=args.model)
        
        print(f"\n  provider: {resp['provider']}")
        print(f"  model: {resp['model']}")
        print(f"  final_text: {resp['final_text'][:200]}...")
        print(f"  reasoning_text: {resp['reasoning_text'][:100] if resp['reasoning_text'] else None}")
        print(f"  latency_ms: {resp['meta']['latency_ms']}")
        print(f"  usage: {resp['meta']['usage']}")
        
    except ValueError as e:
        logger.warning("[SKIP] %s", e)
    except Exception as e:
        logger.error("[ERROR] %s", e)

    # 5. 日志清理
    if args.cleanup:
        logger.info("[5] 日志清理")
        report = manager.cleanup_logs()
        logger.info("按时间删除: %s, 按大小删除: %s, 剩余大小: %.2f MB",
                    report['deleted_by_age'], report['deleted_by_size'], report['remaining_mb'])

    logger.info("=" * 60)
    logger.info("演示完成")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/start.sh">
#!/usr/bin/env bash
# ============================================================
# 一键启动 DeepSea RAG 全栈（后端 API + 前端 Dev Server）
#
# 用法:
#   bash scripts/start.sh              # 默认端口（后端 9999，前端 5173）
#   bash scripts/start.sh --backend-only   # 仅启动后端
#   bash scripts/start.sh --frontend-only  # 仅启动前端
#   API_PORT=8000 bash scripts/start.sh    # 自定义后端端口
#
# 退出: Ctrl+C 会同时关闭后端和前端进程
# ============================================================

set -e

ROOT="$(cd "$(dirname "$0")/.." && pwd)"
cd "$ROOT"

# ── 运行时命令解析（优先 conda 环境） ──
# 目标：迁移时尽量复用 conda 管理，不依赖系统级 python/npm 绝对路径
CONDA_ENV_NAME="${CONDA_ENV_NAME:-deepsea-rag}"
PY_CMD=(python)
NPM_CMD=(npm)

if command -v conda >/dev/null 2>&1; then
  if [ "${CONDA_DEFAULT_ENV:-}" != "$CONDA_ENV_NAME" ]; then
    if conda run -n "$CONDA_ENV_NAME" python --version >/dev/null 2>&1; then
      PY_CMD=(conda run -n "$CONDA_ENV_NAME" python)
    fi
    if conda run -n "$CONDA_ENV_NAME" npm -v >/dev/null 2>&1; then
      NPM_CMD=(conda run -n "$CONDA_ENV_NAME" npm)
    fi
  fi
fi

# ── HuggingFace 下载源（中国网络默认镜像 + 官方回退） ──
# 可通过外部环境变量覆盖：
# - RAG_HF_ENDPOINTS="https://hf-mirror.com,https://huggingface.co"
# - HF_ENDPOINT="https://hf-mirror.com"
if [ -z "${RAG_HF_ENDPOINTS:-}" ] && [ -z "${HF_ENDPOINT:-}" ] && [ -z "${HF_MIRROR:-}" ]; then
  export RAG_HF_ENDPOINTS="https://hf-mirror.com,https://huggingface.co"
fi

# ── 颜色 ──
GREEN='\033[0;32m'
CYAN='\033[0;36m'
YELLOW='\033[1;33m'
NC='\033[0m'

# ── 参数解析 ──
RUN_BACKEND=true
RUN_FRONTEND=true
for arg in "$@"; do
  case "$arg" in
    --backend-only)  RUN_FRONTEND=false ;;
    --frontend-only) RUN_BACKEND=false ;;
  esac
done

# ── 清理函数 ──
PIDS=()
cleanup() {
  echo ""
  echo -e "${YELLOW}[start.sh] 正在关闭服务...${NC}"
  for pid in "${PIDS[@]}"; do
    kill "$pid" 2>/dev/null || true
  done
  wait 2>/dev/null
  echo -e "${GREEN}[start.sh] 已退出${NC}"
}
trap cleanup EXIT INT TERM

# ── 启动后端 ──
if [ "$RUN_BACKEND" = true ]; then
  BACKEND_PORT="${API_PORT:-9999}"
  BACKEND_HOST="${API_HOST:-127.0.0.1}"
  echo -e "${CYAN}[start.sh] 启动后端 API → http://${BACKEND_HOST}:${BACKEND_PORT}${NC}"
  echo -e "${CYAN}[start.sh] HF endpoint candidates: ${RAG_HF_ENDPOINTS:-${HF_ENDPOINT:-${HF_MIRROR:-https://huggingface.co}}}${NC}"
  "${PY_CMD[@]}" -m uvicorn src.api.server:app \
    --host "$BACKEND_HOST" \
    --port "$BACKEND_PORT" \
    --reload &
  PIDS+=($!)
  # 等待后端就绪
  echo -e "${CYAN}[start.sh] 等待后端就绪...${NC}"
  for i in $(seq 1 30); do
    if curl -s "http://${BACKEND_HOST}:${BACKEND_PORT}/health" > /dev/null 2>&1; then
      echo -e "${GREEN}[start.sh] 后端已就绪 ✓${NC}"
      break
    fi
    sleep 1
  done
fi

# ── 启动前端 ──
if [ "$RUN_FRONTEND" = true ]; then
  if [ ! -d "$ROOT/frontend/node_modules" ]; then
    echo -e "${CYAN}[start.sh] 首次运行，安装前端依赖...${NC}"
    (cd "$ROOT/frontend" && "${NPM_CMD[@]}" install)
  fi
  echo -e "${CYAN}[start.sh] 启动前端 Dev Server → http://localhost:5173${NC}"
  (cd "$ROOT/frontend" && "${NPM_CMD[@]}" run dev) &
  PIDS+=($!)
fi

# ── 打印摘要 ──
echo ""
echo -e "${GREEN}════════════════════════════════════════${NC}"
if [ "$RUN_BACKEND" = true ]; then
  echo -e "${GREEN}  后端 API:    http://${BACKEND_HOST:-127.0.0.1}:${BACKEND_PORT:-9999}/docs${NC}"
fi
if [ "$RUN_FRONTEND" = true ]; then
  echo -e "${GREEN}  前端页面:    http://localhost:5173${NC}"
fi
echo -e "${GREEN}  退出:        Ctrl+C${NC}"
echo -e "${GREEN}════════════════════════════════════════${NC}"
echo ""

# ── 等待子进程 ──
wait
</file>

<file path="scripts/test_chat_hybrid_optimizer.py">
#!/usr/bin/env python3
"""
测试 /chat/stream 在 hybrid + 查询优化器 + Scholar 时的请求与响应。
用于验证前端传参与后端是否一致、是否触发 Scholar 有头浏览器。

前置：先启动 API（python scripts/08_run_api.py）
"""

import sys
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
if str(ROOT) not in sys.path:
    sys.path.insert(0, str(ROOT))


def main():
    import requests
    from config.settings import settings

    base = f"http://{settings.api.host}:{settings.api.port}"
    url = f"{base}/chat/stream"

    # 与前端开启「查询优化」+ 勾选 Scholar 时发送的 body 一致
    body = {
        "message": "深海冷泉的生态意义是什么？",
        "search_mode": "hybrid",
        "web_providers": ["scholar"],
        "use_query_optimizer": True,
        "query_optimizer_max_queries": 3,
    }
    print("POST", url)
    print("Body:", body)
    print()

    with requests.post(url, json=body, stream=True, timeout=120) as resp:
        resp.raise_for_status()
        print("[stream] status ok, receiving events...")
        for line in resp.iter_lines(decode_unicode=True):
            if not line:
                continue
            print(line)


if __name__ == "__main__":
    main()
</file>

<file path="scripts/verify_dependencies.sh">
#!/usr/bin/env bash
# ============================================================
# 依赖核对脚本（Python + Frontend）
# - Python: 强制在 conda 环境 deepsea-rag 中检查
# - Frontend: 检查 Node/NPM 与 package-lock
# ============================================================

set -u

ENV_NAME="deepsea-rag"
ROOT_DIR="$(cd "$(dirname "$0")/.." && pwd)"
FRONTEND_DIR="$ROOT_DIR/frontend"

PASS_COUNT=0
FAIL_COUNT=0
WARN_COUNT=0

ok() {
  echo "[OK] $1"
  PASS_COUNT=$((PASS_COUNT + 1))
}

fail() {
  echo "[FAIL] $1"
  FAIL_COUNT=$((FAIL_COUNT + 1))
}

warn() {
  echo "[WARN] $1"
  WARN_COUNT=$((WARN_COUNT + 1))
}

echo "========== 依赖核对开始 =========="
echo "项目目录: $ROOT_DIR"
echo "Conda 环境: $ENV_NAME"
echo ""

if ! command -v conda >/dev/null 2>&1; then
  fail "未找到 conda，无法执行 Python 依赖核对"
  echo ""
  echo "========== 核对结束 =========="
  echo "PASS=$PASS_COUNT FAIL=$FAIL_COUNT WARN=$WARN_COUNT"
  exit 1
fi

# ---------- Python 依赖核对 ----------
echo "---- Python 依赖核对（conda run -n ${ENV_NAME}）----"
if conda run -n "$ENV_NAME" python --version >/dev/null 2>&1; then
  PY_VERSION="$(conda run -n "$ENV_NAME" python --version 2>/dev/null | tr -d '\r')"
  ok "$PY_VERSION"
else
  fail "conda 环境 '$ENV_NAME' 不可用"
fi

TMP_PY_SCRIPT="$(mktemp)"
cat > "$TMP_PY_SCRIPT" <<'PY'
from importlib.metadata import version, PackageNotFoundError

try:
    from packaging.specifiers import SpecifierSet
except Exception:
    SpecifierSet = None

pkgs = [
    "pymilvus",
    "docling",
    "pymupdf",
    "pillow",
    "transformers",
    "huggingface-hub",
    "sentence-transformers",
    "FlagEmbedding",
    "datasets",
    "ragatouille",
    "anthropic",
    "openai",
    "fastapi",
    "uvicorn",
    "pandas",
    "tqdm",
    "tavily-python",
    "trafilatura",
    "aiohttp",
    "playwright",
    "playwright-stealth",
    "beautifulsoup4",
    "langgraph",
    "langgraph-checkpoint-sqlite",
    "networkx",
    "pydantic",
    "requests",
    "bcrypt",
    "opentelemetry-api",
    "opentelemetry-sdk",
    "opentelemetry-instrumentation-fastapi",
    "opentelemetry-exporter-prometheus",
    "prometheus-client",
    "mcp",
    "pytest",
]

constraints = {
    "transformers": ">=4.42.0,<5.0.0",
    "huggingface-hub": "==0.36.0",
    "fastapi": ">=0.100.0,<1.0.0",
    "pydantic": ">=2.0.0,<3.0.0",
    "openai": ">=1.0.0,<3.0.0",
    "mcp": ">=1.26.0",
}

missing = []
incompatible = []
for p in pkgs:
    try:
        v = version(p)
        print(f"{p}=={v}")
        if p in constraints and SpecifierSet is not None:
            spec = SpecifierSet(constraints[p])
            if v not in spec:
                incompatible.append(f"{p}=={v} (need {constraints[p]})")
    except PackageNotFoundError:
        print(f"{p}==MISSING")
        missing.append(p)

if missing:
    print("MISSING_COUNT=" + str(len(missing)))
    print("MISSING_LIST=" + ",".join(missing))
else:
    print("MISSING_COUNT=0")

if SpecifierSet is None:
    print("CONSTRAINT_CHECK=SKIPPED (packaging not available)")
elif incompatible:
    print("INCOMPATIBLE_COUNT=" + str(len(incompatible)))
    print("INCOMPATIBLE_LIST=" + "|".join(incompatible))
else:
    print("INCOMPATIBLE_COUNT=0")
PY

PY_AUDIT_OUTPUT="$(conda run -n "$ENV_NAME" python "$TMP_PY_SCRIPT" 2>/dev/null || true)"
rm -f "$TMP_PY_SCRIPT"

if [ -z "$PY_AUDIT_OUTPUT" ]; then
  warn "Python 包核对输出为空，请手动执行 conda run -n ${ENV_NAME} python -m pip list"
fi

while IFS= read -r line; do
  case "$line" in
    *"==MISSING")
      fail "Python 包缺失: $line"
      ;;
    MISSING_COUNT=0)
      ok "Python 依赖核对通过（无缺失）"
      ;;
    MISSING_COUNT=*)
      # handled by specific missing lines
      :
      ;;
    MISSING_LIST=*)
      warn "缺失列表: ${line#MISSING_LIST=}"
      ;;
    CONSTRAINT_CHECK=*)
      warn "${line#CONSTRAINT_CHECK=}"
      ;;
    INCOMPATIBLE_COUNT=0)
      ok "关键版本约束检查通过"
      ;;
    INCOMPATIBLE_COUNT=*)
      # handled by incompatible list
      :
      ;;
    INCOMPATIBLE_LIST=*)
      IFS='|' read -r -a bad_versions <<< "${line#INCOMPATIBLE_LIST=}"
      for item in "${bad_versions[@]}"; do
        fail "Python 包版本不满足约束: $item"
      done
      ;;
    *=*)
      echo "  $line"
      ;;
  esac
done <<< "$PY_AUDIT_OUTPUT"

# ---------- Frontend 依赖核对 ----------
echo ""
echo "---- Frontend 依赖核对 ----"

if [ ! -f "$FRONTEND_DIR/package.json" ]; then
  fail "未找到 frontend/package.json"
else
  ok "存在 frontend/package.json"
fi

if [ ! -f "$FRONTEND_DIR/package-lock.json" ]; then
  fail "未找到 frontend/package-lock.json"
else
  ok "存在 frontend/package-lock.json"
fi

NODE_CMD=""
NPM_CMD=""
if command -v node >/dev/null 2>&1; then
  NODE_CMD="$(command -v node)"
elif command -v conda >/dev/null 2>&1 && conda run -n "$ENV_NAME" node -v >/dev/null 2>&1; then
  NODE_CMD="conda-run"
fi

if command -v npm >/dev/null 2>&1; then
  NPM_CMD="$(command -v npm)"
elif command -v conda >/dev/null 2>&1 && conda run -n "$ENV_NAME" npm -v >/dev/null 2>&1; then
  NPM_CMD="conda-run"
fi

if [ -n "$NODE_CMD" ]; then
  if [ "$NODE_CMD" = "conda-run" ]; then
    NODE_VER_RAW="$(conda run -n "$ENV_NAME" node -v 2>/dev/null | tr -d '\r')"
    NODE_FROM="conda env: $ENV_NAME"
  else
    NODE_VER_RAW="$("$NODE_CMD" -v | tr -d '\r')"
    NODE_FROM="$NODE_CMD"
  fi
  NODE_VER="${NODE_VER_RAW#v}"
  ok "Node 版本: $NODE_VER_RAW (from $NODE_FROM)"

  # 要求: ^20.19.0 || >=22.12.0
  NODE_MAJOR="$(echo "$NODE_VER" | cut -d'.' -f1)"
  NODE_MINOR="$(echo "$NODE_VER" | cut -d'.' -f2)"
  NODE_PATCH="$(echo "$NODE_VER" | cut -d'.' -f3)"

  # 默认兜底
  NODE_MAJOR="${NODE_MAJOR:-0}"
  NODE_MINOR="${NODE_MINOR:-0}"
  NODE_PATCH="${NODE_PATCH:-0}"

  NODE_OK=0
  if [ "$NODE_MAJOR" -gt 22 ]; then
    NODE_OK=1
  elif [ "$NODE_MAJOR" -eq 22 ]; then
    if [ "$NODE_MINOR" -gt 12 ] || { [ "$NODE_MINOR" -eq 12 ] && [ "$NODE_PATCH" -ge 0 ]; }; then
      NODE_OK=1
    fi
  elif [ "$NODE_MAJOR" -eq 20 ]; then
    if [ "$NODE_MINOR" -gt 19 ] || { [ "$NODE_MINOR" -eq 19 ] && [ "$NODE_PATCH" -ge 0 ]; }; then
      NODE_OK=1
    fi
  fi

  if [ "$NODE_OK" -eq 1 ]; then
    ok "Node 版本满足要求 (^20.19.0 || >=22.12.0)"
  else
    fail "Node 版本不满足要求: $NODE_VER_RAW（需要 ^20.19.0 || >=22.12.0）"
  fi
else
  fail "未找到 node（PATH 或 conda env: $ENV_NAME），请安装 Node.js ^20.19.0 或 >=22.12.0"
fi

if [ -n "$NPM_CMD" ]; then
  if [ "$NPM_CMD" = "conda-run" ]; then
    NPM_VER="$(conda run -n "$ENV_NAME" npm -v 2>/dev/null | tr -d '\r')"
    ok "npm 版本: $NPM_VER (from conda env: $ENV_NAME)"
  else
    ok "npm 版本: $("$NPM_CMD" -v | tr -d '\r') (from $NPM_CMD)"
  fi
else
  fail "未找到 npm（PATH 或 conda env: $ENV_NAME）"
fi

echo ""
echo "========== 核对结束 =========="
echo "PASS=$PASS_COUNT FAIL=$FAIL_COUNT WARN=$WARN_COUNT"

if [ "$FAIL_COUNT" -gt 0 ]; then
  exit 1
fi

exit 0
</file>

<file path="src/api/__init__.py">
# API layer - FastAPI chat and canvas
</file>

<file path="src/api/routes_auth.py">
"""
认证 API：登录、管理员创建/列出用户。
"""

from fastapi import APIRouter, Depends, HTTPException, Header

from config.settings import settings
from src.api.schemas import LoginRequest, LoginResponse, CreateUserRequest, UserItem
from src.auth.session import create_token, verify_token
from src.collaboration.memory.persistent_store import (
    create_user as store_create_user,
    verify_password as store_verify_password,
    list_users as store_list_users,
    get_user_profile,
)

router = APIRouter(prefix="/auth", tags=["auth"])
admin_router = APIRouter(prefix="/admin", tags=["admin"])


def _get_token_from_header(authorization: str | None = Header(None)) -> str | None:
    if not authorization or not authorization.startswith("Bearer "):
        return None
    return authorization[7:].strip() or None


def get_current_user_id(authorization: str | None = Header(None)) -> str:
    """Dependency: require valid token, return user_id."""
    token = _get_token_from_header(authorization)
    user_id = verify_token(token) if token else None
    if not user_id:
        raise HTTPException(status_code=401, detail="Invalid or missing token")
    return user_id


def get_current_admin_id(authorization: str | None = Header(None)) -> str:
    """Dependency: require valid token and role=admin."""
    user_id = get_current_user_id(authorization)
    profile = get_user_profile(user_id)
    if not profile or profile.get("role") != "admin":
        raise HTTPException(status_code=403, detail="Admin required")
    return user_id


def get_optional_user_id(authorization: str | None = Header(None)) -> str | None:
    """Dependency: return user_id if valid token present, else None (no 401)."""
    token = _get_token_from_header(authorization)
    return verify_token(token) if token else None


@router.post("/login", response_model=LoginResponse)
def login(body: LoginRequest) -> LoginResponse:
    """用户名+密码登录，返回 token。"""
    if not store_verify_password(body.user_id, body.password):
        raise HTTPException(status_code=401, detail="Invalid user_id or password")
    profile = get_user_profile(body.user_id)
    if not profile or not profile.get("is_active", True):
        raise HTTPException(status_code=401, detail="Account disabled")
    token = create_token(body.user_id, expire_hours=settings.auth.token_expire_hours)
    return LoginResponse(
        token=token,
        user_id=body.user_id,
        role=profile.get("role") or "user",
    )


@admin_router.post("/users")
def admin_create_user(
    body: CreateUserRequest,
    _admin_id: str = Depends(get_current_admin_id),
) -> dict:
    """管理员创建用户。"""
    try:
        store_create_user(user_id=body.user_id, password=body.password, role=body.role)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))
    return {"user_id": body.user_id, "role": body.role}


@admin_router.get("/users", response_model=list[UserItem])
def admin_list_users(
    _admin_id: str = Depends(get_current_admin_id),
) -> list[UserItem]:
    """管理员列出所有用户。"""
    users = store_list_users()
    return [
        UserItem(
            user_id=u["user_id"],
            role=u["role"],
            is_active=u["is_active"],
            created_at=u["created_at"],
            updated_at=u["updated_at"],
        )
        for u in users
    ]
</file>

<file path="src/api/routes_auto.py">
"""
自动完成综述 API：POST /auto-complete
"""

from pathlib import Path

from fastapi import APIRouter

from src.api.schemas import AutoCompleteRequest, AutoCompleteResponse
from src.collaboration.auto_complete import AutoCompleteService
from src.llm.llm_manager import get_manager
from src.retrieval.service import get_retrieval_service

router = APIRouter(tags=["auto-complete"])

_CONFIG_PATH = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"


@router.post("/auto-complete", response_model=AutoCompleteResponse)
def auto_complete(body: AutoCompleteRequest) -> AutoCompleteResponse:
    """
    自动完成综述：根据主题检索 -> 生成大纲 -> 逐章写作 -> 返回完整 Markdown。
    """
    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client()
    retrieval = get_retrieval_service(top_k=15)

    svc = AutoCompleteService(
        llm_client=client,
        retrieval_service=retrieval,
        max_sections=body.max_sections,
        include_abstract=True,
    )
    result = svc.complete(
        topic=body.topic,
        canvas_id=body.canvas_id,
        session_id=body.session_id or "",
        search_mode=body.search_mode,
    )
    return AutoCompleteResponse(
        session_id=result.session_id,
        canvas_id=result.canvas_id,
        markdown=result.markdown,
        outline=result.outline,
        citations=result.citations,
        total_time_ms=result.total_time_ms,
    )
</file>

<file path="src/api/routes_models.py">
"""
本地模型管理 API：
- GET /models/status
- POST /models/sync
- GET /llm/providers
"""

import json
from dataclasses import asdict
from pathlib import Path

from fastapi import APIRouter

from src.api.schemas import ModelStatusResponse, ModelSyncRequest, ModelSyncResponse
from src.utils.model_sync import check_models, sync_models

router = APIRouter(tags=["models"])

_CONFIG_PATH = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"


@router.get("/models/status", response_model=ModelStatusResponse)
def get_model_status() -> dict:
    items = check_models()
    return {"items": [asdict(i) for i in items]}


@router.post("/models/sync", response_model=ModelSyncResponse)
def sync_local_models(body: ModelSyncRequest) -> dict:
    items = sync_models(force_update=body.force_update, local_files_only=body.local_files_only)
    return {"items": [asdict(i) for i in items]}


@router.get("/llm/providers")
def list_llm_providers() -> dict:
    """列出可用的 LLM 提供商及其默认模型"""
    from src.llm.llm_manager import get_manager
    manager = get_manager(str(_CONFIG_PATH))
    providers = []
    for name in manager.get_provider_names():
        cfg = manager.config.providers.get(name)
        if cfg:
            providers.append({
                "id": name,
                "default_model": cfg.default_model,
                "models": list(cfg.models.keys()),
            })
    parser_defaults = {}
    try:
        with open(_CONFIG_PATH, "r", encoding="utf-8") as f:
            raw = json.load(f)
        parser_cfg = raw.get("parser", {}) if isinstance(raw, dict) else {}
        parser_defaults = {
            "llm_text_provider": parser_cfg.get("llm_text_provider", "deepseek"),
            "llm_text_model": parser_cfg.get("llm_text_model"),
            "llm_text_concurrency": int(parser_cfg.get("llm_text_concurrency", 1) or 1),
            "llm_vision_provider": parser_cfg.get("llm_vision_provider", "gemini-vision"),
            "llm_vision_model": parser_cfg.get("llm_vision_model"),
            "llm_vision_concurrency": int(parser_cfg.get("llm_vision_concurrency", 1) or 1),
        }
    except Exception:
        parser_defaults = {
            "llm_text_provider": "deepseek",
            "llm_text_model": None,
            "llm_text_concurrency": 1,
            "llm_vision_provider": "gemini-vision",
            "llm_vision_model": None,
            "llm_vision_concurrency": 1,
        }

    return {
        "default": manager.config.default,
        "providers": providers,
        "parser_defaults": parser_defaults,
    }
</file>

<file path="src/api/routes_project.py">
"""
项目管理 API：列出当前用户项目、存档/取消存档、删除。
"""

from fastapi import APIRouter, Depends, HTTPException

from src.api.routes_auth import get_current_user_id
from src.collaboration.canvas.canvas_manager import get_canvas_store

router = APIRouter(prefix="/projects", tags=["projects"])


@router.get("")
def list_projects(
    include_archived: bool = False,
    user_id: str = Depends(get_current_user_id),
) -> list[dict]:
    """列出当前用户的项目（含存档状态）。"""
    store = get_canvas_store()
    items = store.list_by_user(user_id, include_archived=include_archived)
    return items


@router.post("/{canvas_id}/archive")
def archive_project(
    canvas_id: str,
    user_id: str = Depends(get_current_user_id),
) -> dict:
    """将项目标记为已存档（不受生命周期清理）。"""
    store = get_canvas_store()
    owner = store.get_canvas_owner(canvas_id)
    if owner is None:
        raise HTTPException(status_code=404, detail="project not found")
    if owner != user_id:
        raise HTTPException(status_code=403, detail="not your project")
    if not store.archive(canvas_id):
        raise HTTPException(status_code=404, detail="project not found")
    return {"canvas_id": canvas_id, "archived": True}


@router.post("/{canvas_id}/unarchive")
def unarchive_project(
    canvas_id: str,
    user_id: str = Depends(get_current_user_id),
) -> dict:
    """取消存档。"""
    store = get_canvas_store()
    owner = store.get_canvas_owner(canvas_id)
    if owner is None:
        raise HTTPException(status_code=404, detail="project not found")
    if owner != user_id:
        raise HTTPException(status_code=403, detail="not your project")
    if not store.unarchive(canvas_id):
        raise HTTPException(status_code=404, detail="project not found")
    return {"canvas_id": canvas_id, "archived": False}


@router.delete("/{canvas_id}")
def delete_project(
    canvas_id: str,
    user_id: str = Depends(get_current_user_id),
) -> dict:
    """删除项目。已存档的项目需先取消存档再删除。"""
    store = get_canvas_store()
    owner = store.get_canvas_owner(canvas_id)
    if owner is None:
        raise HTTPException(status_code=404, detail="project not found")
    if owner != user_id:
        raise HTTPException(status_code=403, detail="not your project")
    # Check if archived - require unarchive first
    items = store.list_by_user(user_id, include_archived=True)
    current = next((x for x in items if x["id"] == canvas_id), None)
    if current and current.get("archived"):
        raise HTTPException(
            status_code=400,
            detail="archived project must be unarchived before deletion",
        )
    if not store.delete(canvas_id):
        raise HTTPException(status_code=404, detail="project not found")
    return {"canvas_id": canvas_id, "deleted": True}
</file>

<file path="src/auth/__init__.py">
# Auth: password hashing, token/session management
from src.auth.password import hash_password, verify_password
from src.auth.session import create_token, verify_token, get_current_user_id

__all__ = [
    "hash_password",
    "verify_password",
    "create_token",
    "verify_token",
    "get_current_user_id",
]
</file>

<file path="src/auth/password.py">
"""Password hashing and verification using bcrypt."""

import bcrypt


def hash_password(plain: str) -> str:
    """Hash a plain password. Returns bcrypt hash string."""
    if not plain:
        raise ValueError("password cannot be empty")
    return bcrypt.hashpw(plain.encode("utf-8"), bcrypt.gensalt()).decode("utf-8")


def verify_password(plain: str, hashed: str) -> bool:
    """Verify plain password against stored hash."""
    if not plain or not hashed:
        return False
    try:
        return bcrypt.checkpw(plain.encode("utf-8"), hashed.encode("utf-8"))
    except Exception:
        return False
</file>

<file path="src/auth/session.py">
"""Simple token generation and verification. Tokens stored in memory (or SQLite via persistent_store)."""

import secrets
import time
from typing import Optional

# In-memory token store: token -> (user_id, expiry_timestamp)
_tokens: dict[str, tuple[str, float]] = {}


def create_token(user_id: str, expire_hours: float = 24.0) -> str:
    """Generate a new token for user_id. Returns token string."""
    token = secrets.token_urlsafe(32)
    expiry = time.time() + expire_hours * 3600
    _tokens[token] = (user_id, expiry)
    return token


def verify_token(token: str) -> Optional[str]:
    """Verify token and return user_id if valid, else None."""
    if not token:
        return None
    entry = _tokens.get(token)
    if not entry:
        return None
    user_id, expiry = entry
    if time.time() > expiry:
        del _tokens[token]
        return None
    return user_id


def revoke_token(token: str) -> bool:
    """Remove token so it can no longer be used."""
    if token in _tokens:
        del _tokens[token]
        return True
    return False


def get_current_user_id(token: str) -> Optional[str]:
    """Alias for verify_token for use in dependencies."""
    return verify_token(token)
</file>

<file path="src/chunking/__init__.py">
# 分块模块

from .chunker import Chunk, ChunkConfig, chunk_blocks

__all__ = ["Chunk", "ChunkConfig", "chunk_blocks"]
</file>

<file path="src/chunking/chunker.py">
"""
结构化切块模块
基于 block + section_path + 长度约束
"""

from __future__ import annotations

import hashlib
import re
from dataclasses import dataclass, field
from typing import Any, Iterator, Optional

# Block 可以是 dict（JSON）或带 heading_path/text/table_data/figure_data 的对象


@dataclass
class ChunkConfig:
    target_chars: int = 1000
    min_chars: int = 200
    max_chars: int = 1800
    overlap_sentences: int = 2
    table_rows_per_chunk: int = 10


@dataclass
class Chunk:
    chunk_id: str
    text: str
    content_type: str  # text | table | image_caption
    meta: dict = field(default_factory=dict)


def _normalize_text(t: Optional[str]) -> str:
    if t is None:
        return ""
    s = str(t).strip()
    return s


def _is_blank(text: str) -> bool:
    return not text or not text.strip()


def _section_path(block: dict | Any) -> str:
    hp = getattr(block, "heading_path", None) or block.get("heading_path", []) or []
    return " > ".join(str(h) for h in hp)


def _block_text(block: dict | Any) -> str:
    return _normalize_text(getattr(block, "text", None) or block.get("text"))


def _block_type(block: dict | Any) -> str:
    bt = getattr(block, "block_type", None)
    if bt is not None:
        return getattr(bt, "value", str(bt)) if hasattr(bt, "value") else str(bt)
    return str(block.get("block_type", "text"))


def _page_index(block: dict | Any) -> int:
    return getattr(block, "page_index", None) or block.get("page_index", 0)


def _block_id(block: dict | Any) -> str:
    return getattr(block, "block_id", None) or block.get("block_id", "")


def _block_bbox(block: dict | Any) -> Optional[list]:
    bbox = getattr(block, "bbox", None) or block.get("bbox")
    if bbox is not None:
        return list(bbox) if isinstance(bbox, (list, tuple)) else None
    return None


def _sentence_tokenize(text: str) -> list[str]:
    """简单句子切分：按 . ! ? 。！？；; 分割"""
    if not text.strip():
        return []
    pattern = r'(?<=[.!?。！？；;])\s+'
    parts = re.split(pattern, text)
    result = []
    for p in parts:
        p = p.strip()
        if p:
            result.append(p)
    if not result:
        result = [text.strip()] if text.strip() else []
    return result


def _generate_stable_id(doc_id: str, section_path: str, page_start: int, content_prefix: str, chunk_index: int = 0) -> str:
    raw = f"{doc_id}:{section_path}:{page_start}:{content_prefix[:24]}:{chunk_index}"
    return hashlib.md5(raw.encode()).hexdigest()[:16]


def _merge_blocks_text(blocks: list) -> str:
    parts = []
    for b in blocks:
        t = _block_text(b)
        if t:
            parts.append(t)
    return "\n\n".join(parts)


def _merge_metadata(blocks: list, doc_id: str, content_type: str = "text") -> dict:
    if not blocks:
        return {"doc_id": doc_id, "page_range": [0, 0], "section_path": "", "block_types": [], "source_uri": ""}
    pages = [_page_index(b) for b in blocks]
    section = _section_path(blocks[0])
    block_types = list(dict.fromkeys(_block_type(b) for b in blocks))
    bboxes = []
    for b in blocks:
        bb = _block_bbox(b)
        if bb:
            bboxes.append(bb)
    meta = {
        "doc_id": doc_id,
        "page_range": [min(pages), max(pages)],
        "section_path": section,
        "block_types": block_types,
        "source_uri": f"{doc_id}#{_block_id(blocks[0])}",
    }
    if bboxes:
        meta["bbox"] = bboxes
    return meta


def _split_long_block_by_sentences(
    text: str, meta: dict, doc_id: str, target: int, overlap_sent: int
) -> list[Chunk]:
    sentences = _sentence_tokenize(text)
    if not sentences:
        if text.strip():
            return [Chunk(chunk_id=_generate_stable_id(doc_id, meta.get("section_path", ""), meta.get("page_range", [0, 0])[0], text[:32]), text=text.strip(), content_type="text", meta=meta)]
        return []
    chunks = []
    i = 0
    chunk_idx = 0
    while i < len(sentences):
        chunk_sents = []
        chunk_len = 0
        while i < len(sentences) and chunk_len + len(sentences[i]) <= target:
            chunk_sents.append(sentences[i])
            chunk_len += len(sentences[i])
            i += 1
        if not chunk_sents:
            chunk_sents = [sentences[i]]
            chunk_len = len(sentences[i])
            i += 1
        chunk_text_val = " ".join(chunk_sents)
        m = meta.copy()
        cid = _generate_stable_id(doc_id, m.get("section_path", ""), m.get("page_range", [0, 0])[0], chunk_text_val[:32], chunk_idx)
        chunks.append(Chunk(chunk_id=cid, text=chunk_text_val, content_type="text", meta=m))
        chunk_idx += 1
        i = max(i - overlap_sent, i - len(chunk_sents) + 1)
    return chunks


def _finalize_buffer(
    buffer: list, doc_id: str, target: int, max_c: int, overlap_sent: int
) -> list[Chunk]:
    text = _merge_blocks_text(buffer)
    if _is_blank(text):
        return []
    meta = _merge_metadata(buffer, doc_id, "text")
    if len(text) > max_c:
        return _split_long_block_by_sentences(text, meta, doc_id, target, overlap_sent)
    cid = _generate_stable_id(doc_id, meta.get("section_path", ""), meta.get("page_range", [0, 0])[0], text[:32])
    return [Chunk(chunk_id=cid, text=text, content_type="text", meta=meta)]


def chunk_table_block(block: dict | Any, doc_id: str, max_chars: int, rows_per_chunk: int = 10) -> list[Chunk]:
    td = getattr(block, "table_data", None) or block.get("table_data")
    if not td:
        return []
    md = td.get("markdown", "") if isinstance(td, dict) else getattr(td, "markdown", "") or ""
    if not md:
        structured = td.get("structured", []) if isinstance(td, dict) else getattr(td, "structured", [])
        if structured:
            header = structured[0] if structured else []
            rows = structured[1:]
            lines = ["| " + " | ".join(str(c) for c in header) + " |", "|" + "---|" * len(header)]
            for r in rows[:50]:
                lines.append("| " + " | ".join(str(c) for c in r[: len(header)]) + " |")
            md = "\n".join(lines)
    if not md.strip():
        return []
    section_path = _section_path(block)
    page = _page_index(block)
    meta = {"doc_id": doc_id, "page_range": [page, page], "section_path": section_path, "block_types": ["table"], "source_uri": f"{doc_id}#{_block_id(block)}"}
    if len(md) <= max_chars:
        cid = _generate_stable_id(doc_id, section_path, page, md[:32])
        return [Chunk(chunk_id=cid, text=md, content_type="table", meta={**meta, "table_id": _block_id(block)})]
    lines = md.split("\n")
    header_lines = lines[:2] if len(lines) >= 2 else lines
    data_lines = lines[2:] if len(lines) > 2 else []
    chunks = []
    for start in range(0, len(data_lines), rows_per_chunk):
        chunk_lines = header_lines + data_lines[start : start + rows_per_chunk]
        chunk_text_val = "\n".join(chunk_lines)
        cid = _generate_stable_id(doc_id, section_path, page, chunk_text_val[:32], start // rows_per_chunk)
        chunks.append(Chunk(chunk_id=cid, text=chunk_text_val, content_type="table", meta={**meta, "table_id": _block_id(block)}))
    return chunks


def chunk_image_block(block: dict | Any, doc_id: str) -> list[Chunk]:
    fd = getattr(block, "figure_data", None) or block.get("figure_data")
    enrichment = getattr(block, "enrichment", None) or block.get("enrichment") or {}
    caption = None
    if fd:
        caption = fd.get("caption") if isinstance(fd, dict) else getattr(fd, "caption", None)
    if not caption:
        interp = enrichment.get("interpretation") if isinstance(enrichment, dict) else getattr(enrichment, "interpretation", None)
        if interp:
            caption = interp.get("description") if isinstance(interp, dict) else getattr(interp, "description", None)
    if not caption or not str(caption).strip():
        return []
    section_path = _section_path(block)
    page = _page_index(block)
    meta = {"doc_id": doc_id, "page_range": [page, page], "section_path": section_path, "block_types": ["figure"], "source_uri": f"{doc_id}#{_block_id(block)}", "figure_id": _block_id(block)}
    cid = _generate_stable_id(doc_id, section_path, page, str(caption)[:32])
    return [Chunk(chunk_id=cid, text=str(caption).strip(), content_type="image_caption", meta=meta)]


def _match_claims_to_chunk(chunk: Chunk, claims: list[dict]) -> list[str]:
    """
    将文档级 claims 匹配到 chunk，返回匹配的 claim_id 列表。

    匹配规则：claim 的 source_block_ids 与 chunk 的 source_uri 有交集。
    """
    if not claims:
        return []
    # 从 chunk meta 提取 block_id（source_uri 格式: doc_id#block_id）
    source_uri = chunk.meta.get("source_uri", "")
    chunk_block_id = source_uri.split("#", 1)[1] if "#" in source_uri else ""
    if not chunk_block_id:
        return []

    matched = []
    for claim in claims:
        block_ids = claim.get("source_block_ids", [])
        if chunk_block_id in block_ids:
            matched.append(claim.get("claim_id", ""))
    return [m for m in matched if m]


def chunk_blocks(
    blocks: list,
    doc_id: str,
    config: Optional[ChunkConfig] = None,
    claims: Optional[list[dict]] = None,
) -> list[Chunk]:
    cfg = config or ChunkConfig()
    target, min_c, max_c, overlap_sent = cfg.target_chars, cfg.min_chars, cfg.max_chars, cfg.overlap_sentences
    chunks = []
    buffer = []
    buffer_len = 0
    current_section = None

    for block in blocks:
        bt = _block_type(block)

        if bt == "table":
            if buffer_len > 0:
                chunks += _finalize_buffer(buffer, doc_id, target, max_c, overlap_sent)
                buffer, buffer_len, current_section = [], 0, None
            chunks += chunk_table_block(block, doc_id, max_c, cfg.table_rows_per_chunk)
            continue

        if bt == "figure":
            if buffer_len > 0:
                chunks += _finalize_buffer(buffer, doc_id, target, max_c, overlap_sent)
                buffer, buffer_len, current_section = [], 0, None
            chunks += chunk_image_block(block, doc_id)
            continue

        if bt in ("caption", "footnote", "formula"):
            continue

        text = _block_text(block)
        if _is_blank(text):
            continue

        section = _section_path(block)

        if section != current_section and buffer_len >= min_c:
            chunks += _finalize_buffer(buffer, doc_id, target, max_c, overlap_sent)
            buffer, buffer_len = [], 0

        current_section = section

        if buffer_len + len(text) <= max_c:
            buffer.append(block)
            buffer_len += len(text)
            if buffer_len >= target:
                chunks += _finalize_buffer(buffer, doc_id, target, max_c, overlap_sent)
                buffer, buffer_len, current_section = [], 0, None
        else:
            if buffer_len > 0:
                chunks += _finalize_buffer(buffer, doc_id, target, max_c, overlap_sent)
                buffer, buffer_len = [], 0
            meta = _merge_metadata([block], doc_id)
            chunks += _split_long_block_by_sentences(text, meta, doc_id, target, overlap_sent)
            current_section = None

    if buffer_len > 0:
        chunks += _finalize_buffer(buffer, doc_id, target, max_c, overlap_sent)

    # 注入 claims 到 chunk.meta
    if claims:
        for chunk in chunks:
            matched = _match_claims_to_chunk(chunk, claims)
            if matched:
                chunk.meta["claims"] = matched

    return chunks
</file>

<file path="src/collaboration/canvas/__init__.py">
from src.collaboration.canvas.models import (
    Citation,
    DraftBlock,
    KnowledgeFragment,
    OutlineSection,
    SurveyCanvas,
)

__all__ = [
    "Citation",
    "DraftBlock",
    "KnowledgeFragment",
    "OutlineSection",
    "SurveyCanvas",
]
</file>

<file path="src/collaboration/canvas/canvas_manager.py">
"""
Canvas 管理服务：CRUD、版本快照、导出；与 session 绑定。
"""

import json
from typing import Any, Dict, List, Optional

from src.collaboration.canvas.canvas_store import CanvasStore
from src.collaboration.canvas.models import Annotation, Citation, DraftBlock, OutlineSection, ResearchBrief, SurveyCanvas


def get_canvas_store() -> CanvasStore:
    return CanvasStore()


def create_canvas(session_id: str = "", topic: str = "", user_id: str = "") -> SurveyCanvas:
    store = get_canvas_store()
    return store.create(session_id=session_id, topic=topic, user_id=user_id)


def get_canvas(canvas_id: str) -> Optional[SurveyCanvas]:
    return get_canvas_store().get(canvas_id)


def update_canvas(canvas_id: str, **fields: Any) -> bool:
    return get_canvas_store().update(canvas_id, **fields)


def delete_canvas(canvas_id: str) -> bool:
    return get_canvas_store().delete(canvas_id)


def list_canvases(user_id: str = "", limit: int = 50) -> List[SurveyCanvas]:
    """列出所有画布（可选按用户过滤）。"""
    import sqlite3
    store = get_canvas_store()
    with sqlite3.connect(store.db_path) as conn:
        conn.row_factory = sqlite3.Row
        if user_id:
            rows = conn.execute(
                "SELECT id FROM canvases WHERE user_id = ? ORDER BY updated_at DESC LIMIT ?",
                (user_id, limit),
            ).fetchall()
        else:
            rows = conn.execute(
                "SELECT id FROM canvases ORDER BY updated_at DESC LIMIT ?",
                (limit,),
            ).fetchall()
    canvases = []
    for r in rows:
        canvas = store.get(r["id"])
        if canvas:
            canvases.append(canvas)
    return canvases


def upsert_outline(canvas_id: str, sections: List[OutlineSection]) -> None:
    get_canvas_store().upsert_outline(canvas_id, sections)


def upsert_draft(canvas_id: str, block: DraftBlock) -> None:
    get_canvas_store().upsert_draft(canvas_id, block)


def create_snapshot(canvas_id: str) -> int:
    return get_canvas_store().snapshot(canvas_id)


def restore_snapshot(canvas_id: str, version_number: int) -> bool:
    return get_canvas_store().restore(canvas_id, version_number)


def list_snapshots(canvas_id: str, limit: int = 50) -> List[Dict[str, Any]]:
    return get_canvas_store().list_versions(canvas_id, limit=limit)


def get_canvas_citations(canvas_id: str) -> List[Citation]:
    """返回画布引用列表（Citation）。"""
    return get_canvas_store().get_citations(canvas_id)


def delete_canvas_citation(canvas_id: str, cite_key: str) -> bool:
    """删除画布中指定 cite_key 的引用。"""
    return get_canvas_store().delete_citation(canvas_id, cite_key)


def filter_canvas_citations(canvas_id: str, keep_keys: List[str]) -> int:
    """筛选引用池，仅保留 keep_keys 中的引用，其余删除。返回删除的数量。"""
    return get_canvas_store().filter_citations(canvas_id, keep_keys)


def export_canvas(canvas_id: str) -> Dict[str, Any]:
    """导出画布为可序列化 JSON 结构。"""
    c = get_canvas(canvas_id)
    if c is None:
        raise ValueError(f"canvas not found: {canvas_id}")
    citations = [
        {
            "id": cit.id,
            "cite_key": cit.cite_key or cit.id,
            "title": cit.title,
            "authors": cit.authors or [],
            "year": cit.year,
            "doi": cit.doi,
            "url": cit.url,
            "bibtex": cit.bibtex,
            "created_at": cit.created_at.isoformat(),
        }
        for cit in c.citation_pool.values()
    ]
    # 序列化 annotations
    annotations_out = []
    for ann in (c.annotations or []):
        annotations_out.append({
            "id": ann.id,
            "section_id": ann.section_id,
            "target_text": ann.target_text,
            "directive": ann.directive,
            "status": ann.status,
            "created_at": ann.created_at.isoformat(),
        })

    # 序列化 research_brief
    brief_out = None
    if c.research_brief:
        rb = c.research_brief
        brief_out = {
            "scope": rb.scope,
            "success_criteria": rb.success_criteria,
            "key_questions": rb.key_questions,
            "exclusions": rb.exclusions,
            "time_range": rb.time_range,
            "source_priority": rb.source_priority,
            "action_plan": rb.action_plan,
        }

    return {
        "id": c.id,
        "session_id": c.session_id,
        "topic": c.topic,
        "working_title": c.working_title,
        "abstract": c.abstract,
        "keywords": c.keywords,
        "stage": c.stage,
        "refined_markdown": c.refined_markdown,
        "outline": [
            {
                "id": s.id,
                "title": s.title,
                "level": s.level,
                "order": s.order,
                "parent_id": s.parent_id,
                "status": s.status,
                "guidance": s.guidance,
            }
            for s in c.outline
        ],
        "drafts": {
            sid: {
                "section_id": b.section_id,
                "content_md": b.content_md,
                "version": b.version,
                "used_fragment_ids": b.used_fragment_ids,
                "used_citation_ids": b.used_citation_ids,
                "updated_at": b.updated_at.isoformat(),
            }
            for sid, b in c.drafts.items()
        },
        "citation_pool": citations,
        "identified_gaps": c.identified_gaps,
        "user_directives": c.user_directives,
        "annotations": annotations_out,
        "research_brief": brief_out,
        "research_insights": c.research_insights or [],
        "skip_draft_review": c.skip_draft_review,
        "skip_refine_review": c.skip_refine_review,
        "version": c.version,
        "created_at": c.created_at.isoformat(),
        "updated_at": c.updated_at.isoformat(),
    }
</file>

<file path="src/collaboration/canvas/canvas_store.py">
"""
Canvas SQLite 持久化：canvases / outline_sections / draft_blocks / canvas_versions。
"""

import json
import sqlite3
import uuid
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from src.collaboration.canvas.models import (
    Annotation,
    Citation,
    DraftBlock,
    OutlineSection,
    ResearchBrief,
    SurveyCanvas,
)


def _default_db_path() -> Path:
    return Path(__file__).resolve().parents[2] / "data" / "canvas.db"


def _ensure_canvas_archive_columns(conn: sqlite3.Connection) -> None:
    """Add archived and user_id to canvases if missing (migration)."""
    cur = conn.execute("PRAGMA table_info(canvases)")
    columns = {row[1] for row in cur.fetchall()}
    if "archived" not in columns:
        conn.execute("ALTER TABLE canvases ADD COLUMN archived INTEGER NOT NULL DEFAULT 0")
    if "user_id" not in columns:
        conn.execute("ALTER TABLE canvases ADD COLUMN user_id TEXT NOT NULL DEFAULT ''")
    if "research_brief" not in columns:
        conn.execute("ALTER TABLE canvases ADD COLUMN research_brief TEXT NOT NULL DEFAULT '{}'")
    if "skip_draft_review" not in columns:
        conn.execute("ALTER TABLE canvases ADD COLUMN skip_draft_review INTEGER NOT NULL DEFAULT 0")
    if "skip_refine_review" not in columns:
        conn.execute("ALTER TABLE canvases ADD COLUMN skip_refine_review INTEGER NOT NULL DEFAULT 0")
    if "research_insights" not in columns:
        conn.execute("ALTER TABLE canvases ADD COLUMN research_insights TEXT NOT NULL DEFAULT '[]'")
    if "refined_markdown" not in columns:
        conn.execute("ALTER TABLE canvases ADD COLUMN refined_markdown TEXT NOT NULL DEFAULT ''")


class CanvasStore:
    def __init__(self, db_path: Optional[Path] = None):
        self.db_path = db_path or _default_db_path()
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_schema()

    def _init_schema(self) -> None:
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS canvases (
                    id TEXT PRIMARY KEY,
                    session_id TEXT NOT NULL DEFAULT '',
                    topic TEXT NOT NULL DEFAULT '',
                    working_title TEXT NOT NULL DEFAULT '',
                    abstract TEXT NOT NULL DEFAULT '',
                    keywords TEXT NOT NULL DEFAULT '[]',
                    stage TEXT NOT NULL DEFAULT 'explore',
                    refined_markdown TEXT NOT NULL DEFAULT '',
                    identified_gaps TEXT NOT NULL DEFAULT '[]',
                    user_directives TEXT NOT NULL DEFAULT '[]',
                    research_brief TEXT NOT NULL DEFAULT '{}',
                    skip_draft_review INTEGER NOT NULL DEFAULT 0,
                    skip_refine_review INTEGER NOT NULL DEFAULT 0,
                    version INTEGER NOT NULL DEFAULT 1,
                    created_at TEXT NOT NULL,
                    updated_at TEXT NOT NULL
                )
                """
            )
            _ensure_canvas_archive_columns(conn)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS outline_sections (
                    canvas_id TEXT NOT NULL,
                    id TEXT NOT NULL,
                    title TEXT NOT NULL DEFAULT '',
                    level INTEGER NOT NULL DEFAULT 1,
                    "order" INTEGER NOT NULL DEFAULT 0,
                    parent_id TEXT,
                    status TEXT NOT NULL DEFAULT 'todo',
                    guidance TEXT,
                    PRIMARY KEY (canvas_id, id),
                    FOREIGN KEY (canvas_id) REFERENCES canvases(id)
                )
                """
            )
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS draft_blocks (
                    canvas_id TEXT NOT NULL,
                    section_id TEXT NOT NULL,
                    content_md TEXT NOT NULL DEFAULT '',
                    version INTEGER NOT NULL DEFAULT 1,
                    used_fragment_ids TEXT NOT NULL DEFAULT '[]',
                    used_citation_ids TEXT NOT NULL DEFAULT '[]',
                    updated_at TEXT NOT NULL,
                    PRIMARY KEY (canvas_id, section_id),
                    FOREIGN KEY (canvas_id) REFERENCES canvases(id)
                )
                """
            )
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS canvas_versions (
                    canvas_id TEXT NOT NULL,
                    version_number INTEGER NOT NULL,
                    snapshot_json TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    PRIMARY KEY (canvas_id, version_number),
                    FOREIGN KEY (canvas_id) REFERENCES canvases(id)
                )
                """
            )
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS canvas_citations (
                    canvas_id TEXT NOT NULL,
                    citation_id TEXT NOT NULL,
                    cite_key TEXT NOT NULL,
                    title TEXT NOT NULL DEFAULT '',
                    authors_json TEXT NOT NULL DEFAULT '[]',
                    year INTEGER,
                    doi TEXT,
                    url TEXT,
                    bibtex TEXT,
                    created_at TEXT NOT NULL,
                    PRIMARY KEY (canvas_id, cite_key),
                    FOREIGN KEY (canvas_id) REFERENCES canvases(id)
                )
                """
            )
            conn.commit()

    def create(self, session_id: str = "", topic: str = "", user_id: str = "") -> SurveyCanvas:
        cid = str(uuid.uuid4())
        now = datetime.now().isoformat()
        with sqlite3.connect(self.db_path) as conn:
            _ensure_canvas_archive_columns(conn)
            conn.execute(
                """INSERT INTO canvases (id, session_id, topic, created_at, updated_at, archived, user_id)
                   VALUES (?, ?, ?, ?, ?, 0, ?)""",
                (cid, session_id, topic, now, now, user_id or ""),
            )
            conn.commit()
        return self.get(cid) or SurveyCanvas(id=cid, session_id=session_id, topic=topic)

    def get(self, canvas_id: str) -> Optional[SurveyCanvas]:
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            _ensure_canvas_archive_columns(conn)
            row = conn.execute(
                "SELECT id, session_id, topic, working_title, abstract, keywords, stage, refined_markdown, "
                "identified_gaps, user_directives, research_brief, research_insights, "
                "skip_draft_review, skip_refine_review, "
                "version, created_at, updated_at FROM canvases WHERE id = ?",
                (canvas_id,),
            ).fetchone()
        if row is None:
            return None
        kw = json.loads(row["keywords"] or "[]")
        gaps = json.loads(row["identified_gaps"] or "[]")
        directives = json.loads(row["user_directives"] or "[]")
        brief_raw = json.loads(row["research_brief"] or "{}")
        insights = json.loads(row["research_insights"] or "[]") if "research_insights" in row.keys() else []
        brief_obj = None
        if isinstance(brief_raw, dict) and brief_raw:
            brief_obj = ResearchBrief(
                scope=brief_raw.get("scope", ""),
                success_criteria=brief_raw.get("success_criteria", []),
                key_questions=brief_raw.get("key_questions", []),
                exclusions=brief_raw.get("exclusions", []),
                time_range=brief_raw.get("time_range", ""),
                source_priority=brief_raw.get("source_priority", []),
                action_plan=brief_raw.get("action_plan", ""),
            )
        outline = self._get_outline(canvas_id)
        drafts = self._get_drafts(canvas_id)
        citation_pool = {c.cite_key or c.id: c for c in self.get_citations(canvas_id)}
        return SurveyCanvas(
            id=row["id"],
            session_id=row["session_id"] or "",
            topic=row["topic"] or "",
            working_title=row["working_title"] or "",
            abstract=row["abstract"] or "",
            keywords=kw,
            stage=row["stage"] or "explore",
            refined_markdown=row["refined_markdown"] or "",
            outline=outline,
            drafts=drafts,
            citation_pool=citation_pool,
            knowledge_pool={},
            identified_gaps=gaps,
            user_directives=directives,
            annotations=[],
            research_brief=brief_obj,
            research_insights=insights,
            skip_draft_review=bool(row["skip_draft_review"]),
            skip_refine_review=bool(row["skip_refine_review"]),
            version=int(row["version"] or 1),
            created_at=datetime.fromisoformat(row["created_at"]),
            updated_at=datetime.fromisoformat(row["updated_at"]),
        )

    def _get_outline(self, canvas_id: str) -> List[OutlineSection]:
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            rows = conn.execute(
                'SELECT id, title, level, "order", parent_id, status, guidance FROM outline_sections WHERE canvas_id = ? ORDER BY "order", level',
                (canvas_id,),
            ).fetchall()
        return [
            OutlineSection(
                id=r["id"],
                title=r["title"] or "",
                level=int(r["level"] or 1),
                order=int(r["order"] or 0),
                parent_id=r["parent_id"],
                status=r["status"] or "todo",
                guidance=r["guidance"],
            )
            for r in rows
        ]

    def _get_drafts(self, canvas_id: str) -> Dict[str, DraftBlock]:
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            rows = conn.execute(
                "SELECT section_id, content_md, version, used_fragment_ids, used_citation_ids, updated_at "
                "FROM draft_blocks WHERE canvas_id = ?",
                (canvas_id,),
            ).fetchall()
        out = {}
        for r in rows:
            sid = r["section_id"]
            out[sid] = DraftBlock(
                section_id=sid,
                content_md=r["content_md"] or "",
                version=int(r["version"] or 1),
                used_fragment_ids=json.loads(r["used_fragment_ids"] or "[]"),
                used_citation_ids=json.loads(r["used_citation_ids"] or "[]"),
                updated_at=datetime.fromisoformat(r["updated_at"]),
            )
        return out

    def update(self, canvas_id: str, **fields: Any) -> bool:
        allowed = {
            "session_id",
            "topic",
            "working_title",
            "abstract",
            "keywords",
            "stage",
            "identified_gaps",
            "user_directives",
            "research_brief",
            "research_insights",
            "refined_markdown",
            "skip_draft_review",
            "skip_refine_review",
            "version",
        }
        updates = []
        params = []
        for k, v in fields.items():
            if k not in allowed:
                continue
            if k in ("keywords", "identified_gaps", "user_directives", "research_insights") and isinstance(v, list):
                v = json.dumps(v, ensure_ascii=False)
            if k == "research_brief" and isinstance(v, dict):
                v = json.dumps(v, ensure_ascii=False)
            if k in ("skip_draft_review", "skip_refine_review"):
                v = 1 if bool(v) else 0
            updates.append(f"{k} = ?")
            params.append(v)
        if not updates:
            return True
        params.append(datetime.now().isoformat())
        params.append(canvas_id)
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                f"UPDATE canvases SET {', '.join(updates)}, updated_at = ? WHERE id = ?",
                params,
            )
            conn.commit()
        return True

    def get_citations(self, canvas_id: str) -> List[Citation]:
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            rows = conn.execute(
                "SELECT citation_id, cite_key, title, authors_json, year, doi, url, bibtex, created_at "
                "FROM canvas_citations WHERE canvas_id = ?",
                (canvas_id,),
            ).fetchall()
        return [
            Citation(
                id=r["citation_id"],
                cite_key=r["cite_key"],
                title=r["title"] or "",
                authors=json.loads(r["authors_json"] or "[]"),
                year=int(r["year"]) if r["year"] is not None else None,
                doi=r["doi"],
                url=r["url"],
                bibtex=r["bibtex"],
                created_at=datetime.fromisoformat(r["created_at"]),
            )
            for r in rows
        ]

    def upsert_citations(self, canvas_id: str, citations: List[Citation]) -> None:
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("DELETE FROM canvas_citations WHERE canvas_id = ?", (canvas_id,))
            for c in citations:
                key = c.cite_key or c.id
                conn.execute(
                    """INSERT INTO canvas_citations (canvas_id, citation_id, cite_key, title, authors_json, year, doi, url, bibtex, created_at)
                       VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                    (
                        canvas_id,
                        c.id,
                        key,
                        c.title or "",
                        json.dumps(c.authors or [], ensure_ascii=False),
                        c.year,
                        c.doi,
                        c.url,
                        c.bibtex,
                        (c.created_at if c.created_at else datetime.now()).isoformat(),
                    ),
                )
            conn.execute("UPDATE canvases SET updated_at = ? WHERE id = ?", (datetime.now().isoformat(), canvas_id))
            conn.commit()

    def delete_citation(self, canvas_id: str, cite_key: str) -> bool:
        """删除指定 cite_key 的引用。"""
        with sqlite3.connect(self.db_path) as conn:
            cur = conn.execute(
                "DELETE FROM canvas_citations WHERE canvas_id = ? AND cite_key = ?",
                (canvas_id, cite_key),
            )
            if cur.rowcount > 0:
                conn.execute(
                    "UPDATE canvases SET updated_at = ? WHERE id = ?",
                    (datetime.now().isoformat(), canvas_id),
                )
            conn.commit()
        return cur.rowcount > 0

    def filter_citations(self, canvas_id: str, keep_keys: List[str]) -> int:
        """
        筛选引用池，仅保留 keep_keys 中的引用，其余删除。
        返回删除的数量。
        """
        existing = self.get_citations(canvas_id)
        keep_set = set(keep_keys)
        to_remove = [c.cite_key or c.id for c in existing if (c.cite_key or c.id) not in keep_set]
        removed = 0
        for key in to_remove:
            if self.delete_citation(canvas_id, key):
                removed += 1
        return removed

    def delete(self, canvas_id: str) -> bool:
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("DELETE FROM canvas_versions WHERE canvas_id = ?", (canvas_id,))
            conn.execute("DELETE FROM canvas_citations WHERE canvas_id = ?", (canvas_id,))
            conn.execute("DELETE FROM draft_blocks WHERE canvas_id = ?", (canvas_id,))
            conn.execute("DELETE FROM outline_sections WHERE canvas_id = ?", (canvas_id,))
            cur = conn.execute("DELETE FROM canvases WHERE id = ?", (canvas_id,))
            conn.commit()
        return cur.rowcount > 0

    def archive(self, canvas_id: str) -> bool:
        """Mark canvas as archived (excluded from lifecycle cleanup)."""
        with sqlite3.connect(self.db_path) as conn:
            cur = conn.execute("UPDATE canvases SET archived = 1, updated_at = ? WHERE id = ?", (datetime.now().isoformat(), canvas_id))
            conn.commit()
        return cur.rowcount > 0

    def unarchive(self, canvas_id: str) -> bool:
        """Clear archived flag."""
        with sqlite3.connect(self.db_path) as conn:
            cur = conn.execute("UPDATE canvases SET archived = 0, updated_at = ? WHERE id = ?", (datetime.now().isoformat(), canvas_id))
            conn.commit()
        return cur.rowcount > 0

    def get_canvas_owner(self, canvas_id: str) -> Optional[str]:
        """Return user_id that owns the canvas, or None if not found."""
        with sqlite3.connect(self.db_path) as conn:
            _ensure_canvas_archive_columns(conn)
            row = conn.execute("SELECT user_id FROM canvases WHERE id = ?", (canvas_id,)).fetchone()
        if not row:
            return None
        return (row[0] or "").strip() or None

    def list_by_user(self, user_id: str, include_archived: bool = False) -> List[Dict[str, Any]]:
        """List canvases for user. Returns list of {id, title, topic, working_title, stage, archived, created_at, updated_at, session_id}."""
        with sqlite3.connect(self.db_path) as conn:
            _ensure_canvas_archive_columns(conn)
            conn.row_factory = sqlite3.Row
            if include_archived:
                rows = conn.execute(
                    "SELECT id, topic, working_title, stage, archived, session_id, created_at, updated_at FROM canvases WHERE user_id = ? ORDER BY updated_at DESC",
                    (user_id,),
                ).fetchall()
            else:
                rows = conn.execute(
                    "SELECT id, topic, working_title, stage, archived, session_id, created_at, updated_at FROM canvases WHERE user_id = ? AND (archived = 0 OR archived IS NULL) ORDER BY updated_at DESC",
                    (user_id,),
                ).fetchall()
        return [
            {
                "id": r["id"],
                "title": r["working_title"] or r["topic"] or "",  # 前端期望 title 字段
                "topic": r["topic"] or "",
                "working_title": r["working_title"] or "",
                "stage": r["stage"] or "explore",
                "archived": bool(r["archived"]) if r["archived"] is not None else False,
                "session_id": r["session_id"] or "",
                "created_at": r["created_at"] or "",
                "updated_at": r["updated_at"] or "",
            }
            for r in rows
        ]

    def upsert_outline(self, canvas_id: str, sections: List[OutlineSection]) -> None:
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("DELETE FROM outline_sections WHERE canvas_id = ?", (canvas_id,))
            for s in sections:
                conn.execute(
                    """INSERT INTO outline_sections (canvas_id, id, title, level, "order", parent_id, status, guidance)
                       VALUES (?, ?, ?, ?, ?, ?, ?, ?)""",
                    (canvas_id, s.id, s.title, s.level, s.order, s.parent_id, s.status, s.guidance),
                )
            # Outline changed; existing full-refine markdown may be stale.
            conn.execute(
                "UPDATE canvases SET refined_markdown = '', updated_at = ? WHERE id = ?",
                (datetime.now().isoformat(), canvas_id),
            )
            conn.commit()

    def upsert_draft(self, canvas_id: str, block: DraftBlock) -> None:
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                """INSERT OR REPLACE INTO draft_blocks (canvas_id, section_id, content_md, version, used_fragment_ids, used_citation_ids, updated_at)
                   VALUES (?, ?, ?, ?, ?, ?, ?)""",
                (
                    canvas_id,
                    block.section_id,
                    block.content_md,
                    block.version,
                    json.dumps(block.used_fragment_ids, ensure_ascii=False),
                    json.dumps(block.used_citation_ids, ensure_ascii=False),
                    block.updated_at.isoformat(),
                ),
            )
            # Draft changed; existing full-refine markdown may be stale.
            conn.execute(
                "UPDATE canvases SET refined_markdown = '', updated_at = ? WHERE id = ?",
                (datetime.now().isoformat(), canvas_id),
            )
            conn.commit()

    def snapshot(self, canvas_id: str) -> int:
        c = self.get(canvas_id)
        if c is None:
            raise ValueError(f"canvas not found: {canvas_id}")
        snap = _canvas_to_snapshot_dict(c)
        with sqlite3.connect(self.db_path) as conn:
            cur = conn.execute(
                "SELECT COALESCE(MAX(version_number), 0) + 1 FROM canvas_versions WHERE canvas_id = ?",
                (canvas_id,),
            )
            ver = cur.fetchone()[0]
            conn.execute(
                "INSERT INTO canvas_versions (canvas_id, version_number, snapshot_json, created_at) VALUES (?, ?, ?, ?)",
                (canvas_id, ver, json.dumps(snap, ensure_ascii=False), datetime.now().isoformat()),
            )
            conn.commit()
        return ver

    def restore(self, canvas_id: str, version_number: int) -> bool:
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            row = conn.execute(
                "SELECT snapshot_json FROM canvas_versions WHERE canvas_id = ? AND version_number = ?",
                (canvas_id, version_number),
            ).fetchone()
        if row is None:
            return False
        snap = json.loads(row["snapshot_json"])
        _apply_snapshot(self, canvas_id, snap)
        return True

    def list_versions(self, canvas_id: str, limit: int = 50) -> List[Dict[str, Any]]:
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            rows = conn.execute(
                "SELECT version_number, created_at FROM canvas_versions WHERE canvas_id = ? ORDER BY version_number DESC LIMIT ?",
                (canvas_id, max(1, int(limit))),
            ).fetchall()
        return [
            {
                "version_number": int(r["version_number"]),
                "created_at": str(r["created_at"] or ""),
            }
            for r in rows
        ]


def _citation_to_dict(c: Citation) -> Dict[str, Any]:
    return {
        "id": c.id,
        "cite_key": c.cite_key or c.id,
        "title": c.title,
        "authors": c.authors or [],
        "year": c.year,
        "doi": c.doi,
        "url": c.url,
        "bibtex": c.bibtex,
        "created_at": c.created_at.isoformat(),
    }


def _canvas_to_snapshot_dict(c: SurveyCanvas) -> Dict[str, Any]:
    return {
        "id": c.id,
        "session_id": c.session_id,
        "topic": c.topic,
        "working_title": c.working_title,
        "abstract": c.abstract,
        "keywords": c.keywords,
        "stage": c.stage,
        "refined_markdown": c.refined_markdown,
        "outline": [
            {"id": s.id, "title": s.title, "level": s.level, "order": s.order, "parent_id": s.parent_id, "status": s.status, "guidance": s.guidance}
            for s in c.outline
        ],
        "drafts": {
            sid: {
                "section_id": b.section_id,
                "content_md": b.content_md,
                "version": b.version,
                "used_fragment_ids": b.used_fragment_ids,
                "used_citation_ids": b.used_citation_ids,
                "updated_at": b.updated_at.isoformat(),
            }
            for sid, b in c.drafts.items()
        },
        "citation_pool": [_citation_to_dict(cit) for cit in c.citation_pool.values()],
        "identified_gaps": c.identified_gaps,
        "user_directives": c.user_directives,
        "research_brief": {
            "scope": c.research_brief.scope,
            "success_criteria": c.research_brief.success_criteria,
            "key_questions": c.research_brief.key_questions,
            "exclusions": c.research_brief.exclusions,
            "time_range": c.research_brief.time_range,
            "source_priority": c.research_brief.source_priority,
            "action_plan": c.research_brief.action_plan,
        } if c.research_brief else {},
        "skip_draft_review": c.skip_draft_review,
        "skip_refine_review": c.skip_refine_review,
        "version": c.version,
    }


def _apply_snapshot(store: CanvasStore, canvas_id: str, snap: Dict[str, Any]) -> None:
    store.update(
        canvas_id,
        topic=snap.get("topic", ""),
        working_title=snap.get("working_title", ""),
        abstract=snap.get("abstract", ""),
        keywords=snap.get("keywords", []),
        stage=snap.get("stage", "explore"),
        refined_markdown=snap.get("refined_markdown", ""),
        identified_gaps=snap.get("identified_gaps", []),
        user_directives=snap.get("user_directives", []),
        research_brief=snap.get("research_brief", {}),
        skip_draft_review=bool(snap.get("skip_draft_review", False)),
        skip_refine_review=bool(snap.get("skip_refine_review", False)),
        version=snap.get("version", 1),
    )
    outline = [
        OutlineSection(
            id=s["id"],
            title=s.get("title", ""),
            level=int(s.get("level", 1)),
            order=int(s.get("order", 0)),
            parent_id=s.get("parent_id"),
            status=s.get("status", "todo"),
            guidance=s.get("guidance"),
        )
        for s in snap.get("outline", [])
    ]
    store.upsert_outline(canvas_id, outline)
    for sid, d in snap.get("drafts", {}).items():
        store.upsert_draft(
            canvas_id,
            DraftBlock(
                section_id=sid,
                content_md=d.get("content_md", ""),
                version=int(d.get("version", 1)),
                used_fragment_ids=d.get("used_fragment_ids", []),
                used_citation_ids=d.get("used_citation_ids", []),
                updated_at=datetime.fromisoformat(d.get("updated_at", datetime.now().isoformat())),
            ),
        )
    if snap.get("citation_pool"):
        all_cits = [
            Citation(
                id=d.get("id", ""),
                cite_key=d.get("cite_key") or d.get("id"),
                title=d.get("title", ""),
                authors=d.get("authors", []),
                year=d.get("year"),
                doi=d.get("doi"),
                url=d.get("url"),
                bibtex=d.get("bibtex"),
                created_at=datetime.fromisoformat(d.get("created_at", datetime.now().isoformat())),
            )
            for d in snap["citation_pool"]
        ]
        store.upsert_citations(canvas_id, all_cits)
</file>

<file path="src/collaboration/citation/__init__.py">
"""引用管理：生成 cite_key、同步到 Canvas、格式化输出。"""

from src.collaboration.citation.manager import (
    CiteKeyGenerator,
    resolve_response_citations,
    sync_evidence_to_canvas,
    chunk_to_citation,
)
from src.collaboration.citation.formatter import (
    format_bibtex,
    format_reference_list,
)

__all__ = [
    "CiteKeyGenerator",
    "resolve_response_citations",
    "sync_evidence_to_canvas",
    "chunk_to_citation",
    "format_bibtex",
    "format_reference_list",
]
</file>

<file path="src/collaboration/export/__init__.py">

</file>

<file path="src/collaboration/export/formatter.py">
"""
Canvas 导出：Markdown 生成。

支持通过 cite_key_format 参数控制引用格式：
- numeric: [1], [2], [3]
- hash: [a3f7b2c91e04]
- author_date: [Smith2023]
"""

from typing import List, Literal, Optional

from src.collaboration.canvas.models import Citation, OutlineSection, SurveyCanvas
from src.collaboration.citation.formatter import (
    format_bibtex,
    format_reference_list,
    citations_to_markdown_list,
)
from src.collaboration.citation.manager import CiteKeyGenerator


def _sorted_outline(sections: List[OutlineSection]) -> List[OutlineSection]:
    return sorted(sections, key=lambda s: (s.order, s.level, s.title))


def _format_outline_markdown(sections: List[OutlineSection]) -> List[str]:
    lines: List[str] = []
    for s in _sorted_outline(sections):
        indent = "  " * max(s.level - 1, 0)
        title = s.title or "未命名章节"
        lines.append(f"{indent}- {title}")
    return lines


def _section_header(level: int, title: str) -> str:
    lvl = min(max(level, 1), 4)
    return "#" * (lvl + 1) + f" {title}"


def _regenerate_cite_keys(
    citations: List[Citation],
    format: Literal["numeric", "hash", "author_date"],
) -> List[Citation]:
    """重新生成所有 citations 的 cite_key（用于导出时切换格式）。"""
    gen = CiteKeyGenerator(format=format)
    result = []
    for c in citations:
        new_key = gen.generate(c)
        # 创建新的 Citation 对象，保留原有字段但更新 cite_key
        new_citation = Citation(
            id=c.id,
            title=c.title,
            authors=c.authors,
            year=c.year,
            doc_id=c.doc_id,
            url=c.url,
            doi=c.doi,
            bibtex=c.bibtex,
            cite_key=new_key,
            created_at=c.created_at,
        )
        result.append(new_citation)
    return result


def export_canvas_markdown(
    canvas: SurveyCanvas,
    cite_key_format: Optional[str] = None,
    include_bibtex: bool = False,
) -> str:
    """
    导出 Canvas 为 Markdown 格式。

    Args:
        canvas: SurveyCanvas 对象
        cite_key_format: 引用键格式（numeric/hash/author_date），None 则使用原有 cite_key
        include_bibtex: 是否在参考文献后附加 BibTeX

    Returns:
        Markdown 文本
    """
    title = canvas.working_title or canvas.topic or "未命名综述"
    lines: List[str] = [f"# {title}", ""]

    if canvas.abstract:
        lines.extend(["## 摘要", canvas.abstract.strip(), ""])

    if canvas.keywords:
        lines.extend(["## 关键词", ", ".join(canvas.keywords), ""])

    if canvas.outline:
        lines.append("## 大纲")
        lines.extend(_format_outline_markdown(canvas.outline))
        lines.append("")

    if canvas.drafts:
        lines.append("## 正文")
        for s in _sorted_outline(canvas.outline):
            block = canvas.drafts.get(s.id)
            if not block or not block.content_md.strip():
                continue
            lines.append(_section_header(s.level, s.title or "未命名章节"))
            lines.append(block.content_md.strip())
            lines.append("")

    citations = list(canvas.citation_pool.values())
    if citations:
        # 如果指定了格式，重新生成 cite_key
        if cite_key_format and cite_key_format in ("numeric", "hash", "author_date"):
            citations = _regenerate_cite_keys(citations, cite_key_format)

        lines.append("## 参考文献")
        lines.append(format_reference_list(citations, use_cite_key=True).strip())
        lines.append("")

        if include_bibtex:
            lines.append("## BibTeX")
            lines.append("```bibtex")
            lines.append(format_bibtex(citations).strip())
            lines.append("```")
            lines.append("")

    return "\n".join(lines).strip() + "\n"
</file>

<file path="src/collaboration/intent/__init__.py">
"""
意图识别：/ 命令 + LLM 二分类（Chat vs Deep Research）。
"""

from .commands import build_search_query_from_context, get_search_query_from_intent
from .parser import (
    COMMAND_PATTERNS,
    IntentParser,
    IntentType,
    ParsedIntent,
    is_deep_research,
    is_retrieval_intent,  # 兼容旧调用
)

__all__ = [
    "COMMAND_PATTERNS",
    "IntentParser",
    "IntentType",
    "ParsedIntent",
    "is_deep_research",
    "is_retrieval_intent",
    "get_search_query_from_intent",
    "build_search_query_from_context",
]
</file>

<file path="src/collaboration/intent/commands.py">
"""
显式指令定义与参数解析。
"""

import re
from typing import Any, Iterable, List, Optional, Set

from .parser import ParsedIntent


def get_search_query_from_intent(parsed: ParsedIntent, fallback: str) -> str:
    """
    从解析结果中取出用于检索的 query。
    显式指令时用 params["args"]，否则用 fallback（原始用户输入）。
    """
    if parsed.from_command and parsed.params:
        args = (parsed.params.get("args") or "").strip()
        if args:
            return args
    params_query = (parsed.params.get("query") if parsed.params else None) or ""
    if isinstance(params_query, str) and params_query.strip():
        return params_query.strip()
    return fallback


def build_search_query_from_context(
    parsed: ParsedIntent,
    fallback: str,
    history: Optional[Iterable[Any]] = None,
    max_user_turns: int = 3,
    max_len: int = 256,
    llm_client: Optional[Any] = None,
    enforce_english_if_input_english: bool = True,
    rolling_summary: str = "",
) -> str:
    """
    Build retrieval query with current-focus enforcement:
    - explicit command first
    - only use context when current message has unresolved references
    - validate generated query still overlaps current message semantics
    """
    base = get_search_query_from_intent(parsed, fallback)
    if parsed.from_command:
        return base

    # Self-contained input should not be polluted by history.
    needs_context = _has_unresolved_references(base)
    if not needs_context or not llm_client:
        return _truncate_query(base, max_len=max_len)

    recent_user_turns = _extract_recent_user_inputs(history, max_user_turns=max_user_turns)
    query = _generate_focused_query(
        llm_client=llm_client,
        current_msg=base,
        rolling_summary=rolling_summary,
        recent_user_turns=recent_user_turns[-2:],
        output_english=bool(enforce_english_if_input_english and base and not _is_chinese(base)),
        max_len=max_len,
    )

    if not query:
        return _truncate_query(base, max_len=max_len)
    if not _validates_current_focus(base, query):
        return _truncate_query(base, max_len=max_len)
    if enforce_english_if_input_english and base and not _is_chinese(base) and _is_chinese(query):
        return _truncate_query(base, max_len=max_len)
    return _truncate_query(query, max_len=max_len)


def _extract_recent_user_inputs(
    history: Optional[Iterable[Any]],
    max_user_turns: int = 3,
) -> List[str]:
    if not history:
        return []
    user_inputs: List[str] = []
    for t in history:
        role = getattr(t, "role", None)
        content = getattr(t, "content", None)
        if role == "user" and isinstance(content, str):
            user_inputs.append(content)
    if max_user_turns > 0:
        user_inputs = user_inputs[-max_user_turns:]
    return user_inputs


_REFERENCE_PATTERNS = re.compile(
    r"\b(it|this|that|these|those|them|its|the above|above|former|latter)\b|前面|上面|之前|这个|那个|它|它们",
    re.IGNORECASE,
)


def _has_unresolved_references(text: str) -> bool:
    """Detect pronouns/references that require session context."""
    return bool(_REFERENCE_PATTERNS.search(text or ""))


def _extract_content_words(text: str) -> List[str]:
    tokens = re.findall(r"[a-zA-Z0-9_]+|[\u4e00-\u9fff]{2,}", text or "")
    if not tokens:
        return []
    stopwords: Set[str] = {
        "the", "a", "an", "is", "are", "was", "were", "be", "to", "of", "in", "on", "for",
        "and", "or", "with", "about", "how", "what", "why", "when", "where", "which", "who",
        "do", "does", "did", "can", "could", "would", "should", "please", "compare",
    }
    zh_stopwords: Set[str] = {"这个", "那个", "一下", "什么", "怎么", "如何", "以及", "还有", "是否", "可以", "请问", "请"}
    out: List[str] = []
    for t in tokens:
        low = t.lower()
        if low in stopwords or t in zh_stopwords:
            continue
        if len(low) <= 1 and re.match(r"[a-zA-Z]", low):
            continue
        out.append(low)
    return out


def _validates_current_focus(current_msg: str, generated_query: str) -> bool:
    """Require overlap with current message to avoid topic drift."""
    current_tokens = set(_extract_content_words(current_msg.lower()))
    query_tokens = set(_extract_content_words(generated_query.lower()))
    if not current_tokens:
        return True
    overlap = current_tokens & query_tokens
    return (len(overlap) / max(1, len(current_tokens))) >= 0.3


def _truncate_query(text: str, max_len: int = 256) -> str:
    s = (text or "").strip()
    if max_len and len(s) > max_len:
        return s[:max_len].rstrip()
    return s


def _is_chinese(text: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", text or ""))


def _generate_focused_query(
    llm_client: Any,
    current_msg: str,
    rolling_summary: str,
    recent_user_turns: List[str],
    output_english: bool,
    max_len: int = 256,
) -> str:
    recent_block = "\n".join(
        f"- {c.strip()}" for c in recent_user_turns if isinstance(c, str) and c.strip()
    )
    prompt = f"""You are a search query generator.
Generate ONE concise search query for academic literature retrieval.

Conversation background (ONLY for resolving references like "it", "this", "the above"):
{rolling_summary or "(first message in session)"}

Recent user turns (optional, for disambiguation only):
{recent_block or "(none)"}

Current user question:
{current_msg}

CRITICAL RULES:
- The query MUST target the CURRENT question, not background topics.
- Use background only to replace unresolved references.
- Output 3-8 precise academic keywords.
- Avoid generic suffixes: review, survey, overview.
- Use {"English" if output_english else "the same language as current user question"}.
- Output ONLY the query string."""
    try:
        resp = llm_client.chat(
            messages=[
                {
                    "role": "system",
                    "content": (
                        "Output only a single English search query."
                        if output_english
                        else "Output only a single search query."
                    ),
                },
                {"role": "user", "content": prompt},
            ],
            max_tokens=64,
        )
        text = (resp.get("final_text") or "").strip()
    except Exception:
        return ""
    text = _truncate_query(text.strip().strip('"').strip("'"), max_len=max_len)
    if not text:
        return ""
    return text
</file>

<file path="src/collaboration/intent/parser.py">
"""
简化意图解析：Chat vs Deep Research 二分类。

- 检索由前端 search_mode 决定，不再由意图控制
- 意图只决定执行路径：普通对话 or 多步综述流水线
- 保留 /auto 命令触发 Deep Research，其余 / 命令均为 Chat 内 prompt hints
"""

import json
import re
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, Iterable, Optional

# ---------------------------------------------------------------------------
# 意图类型（简化为 3 种）
# ---------------------------------------------------------------------------

class IntentType(Enum):
    CHAT = "chat"                    # 普通对话（含搜索、写作、编辑、闲聊等）
    DEEP_RESEARCH = "deep_research"  # 多步综述流水线（原 auto_complete）
    UNCLEAR = "unclear"              # 无法判断，视为 CHAT

    # ---- 旧值兼容别名（前端/日志可能还存有旧值，反序列化时不报错）----
    @classmethod
    def _missing_(cls, value: object):
        """将旧 intent 值映射到新类型"""
        _LEGACY_MAP = {
            "search_exploratory": cls.CHAT,
            "search_targeted": cls.CHAT,
            "outline_generate": cls.CHAT,
            "outline_modify": cls.CHAT,
            "draft_section": cls.CHAT,
            "edit_text": cls.CHAT,
            "query_state": cls.CHAT,
            "set_directive": cls.CHAT,
            "export_document": cls.CHAT,
            "auto_complete": cls.DEEP_RESEARCH,
            "chitchat": cls.CHAT,
        }
        if isinstance(value, str):
            mapped = _LEGACY_MAP.get(value.strip().lower())
            if mapped is not None:
                return mapped
        return None


# 显式指令映射 —— /auto 触发 Deep Research，其余均为 Chat
COMMAND_PATTERNS = {
    "/auto": IntentType.DEEP_RESEARCH,
    "/search": IntentType.CHAT,
    "/explore": IntentType.CHAT,
    "/outline": IntentType.CHAT,
    "/outline add": IntentType.CHAT,
    "/outline delete": IntentType.CHAT,
    "/outline move": IntentType.CHAT,
    "/draft": IntentType.CHAT,
    "/edit": IntentType.CHAT,
    "/status": IntentType.CHAT,
    "/export": IntentType.CHAT,
    "/set": IntentType.CHAT,
}


@dataclass
class ParsedIntent:
    intent_type: IntentType
    confidence: float
    params: Dict[str, Any] = field(default_factory=dict)
    raw_input: str = ""
    from_command: bool = False


def is_deep_research(parsed: ParsedIntent) -> bool:
    """判断是否走 Deep Research 路径"""
    return parsed.intent_type == IntentType.DEEP_RESEARCH


# 保留旧函数名作为兼容（但逻辑已改为：retrieval 由 search_mode 决定）
def is_retrieval_intent(parsed: ParsedIntent) -> bool:
    """兼容旧调用：Deep Research 一定需要检索"""
    return is_deep_research(parsed)


class IntentParser:
    """简化版意图解析：/ 命令优先，否则 LLM 二分类。"""

    def __init__(self, llm_client: Any):
        self.llm = llm_client

    def parse(
        self,
        user_input: str,
        current_stage: str = "explore",
        history: Optional[Iterable[Any]] = None,
    ) -> ParsedIntent:
        user_input = (user_input or "").strip()
        if user_input.startswith("/"):
            return self._parse_command(user_input)
        return self._parse_natural_language(user_input, current_stage, history=history)

    def _parse_command(self, user_input: str) -> ParsedIntent:
        parts = user_input.split(maxsplit=2)
        cmd = parts[0].lower()
        if len(parts) >= 2:
            compound = f"{parts[0]} {parts[1]}".lower()
            if compound in COMMAND_PATTERNS:
                return ParsedIntent(
                    intent_type=COMMAND_PATTERNS[compound],
                    confidence=1.0,
                    params={"args": (parts[2] if len(parts) > 2 else "").strip()},
                    raw_input=user_input,
                    from_command=True,
                )
        if cmd in COMMAND_PATTERNS:
            return ParsedIntent(
                intent_type=COMMAND_PATTERNS[cmd],
                confidence=1.0,
                params={"args": (parts[1] if len(parts) > 1 else "").strip()},
                raw_input=user_input,
                from_command=True,
            )
        # 未知 / 命令
        return ParsedIntent(
            intent_type=IntentType.CHAT,
            confidence=0.5,
            raw_input=user_input,
            from_command=True,
        )

    def _parse_natural_language(
        self,
        user_input: str,
        current_stage: str,
        history: Optional[Iterable[Any]] = None,
    ) -> ParsedIntent:
        history_block = _format_history(history)
        prompt = f"""判断用户意图属于以下哪一类。当前工作阶段: {current_stage}

对话上下文:
{history_block or "（无）"}

用户输入: "{user_input}"

只有两种意图：
- chat: 普通对话、提问、搜索、写作、编辑、闲聊 —— 绝大多数情况都是 chat
- deep_research: 用户明确要求生成一篇完整的多章节综述/报告（如"帮我写一篇完整综述"、"自动完成一篇关于XXX的报告"）

判断原则：
1. 如果用户只是提问、搜索文献、写某个段落、编辑文本、闲聊 → chat
2. 只有用户明确表达要"完整综述"、"全文生成"、"一键综述"时 → deep_research
3. 不确定时默认 chat

请只返回一行 JSON，不要其他文字:
{{"intent": "chat 或 deep_research", "confidence": 0.0-1.0, "params": {{}}}}"""
        try:
            resp = self.llm.chat(
                [
                    {"role": "system", "content": "你是一个意图分析助手，只返回 JSON。"},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=128,
            )
            text = (resp.get("final_text") or "").strip()
        except Exception:
            return ParsedIntent(
                intent_type=IntentType.CHAT,
                confidence=0.5,
                raw_input=user_input,
            )
        text = re.sub(r"^```(?:json)?\s*", "", text)
        text = re.sub(r"\s*```\s*$", "", text)
        try:
            data = json.loads(text)
            intent_str = (data.get("intent") or "chat").strip().lower()
            intent_type = IntentType(intent_str)
        except (ValueError, KeyError, json.JSONDecodeError):
            return ParsedIntent(
                intent_type=IntentType.CHAT,
                confidence=0.5,
                raw_input=user_input,
            )
        return ParsedIntent(
            intent_type=intent_type,
            confidence=float(data.get("confidence", 0.8)),
            params=dict(data.get("params") or {}),
            raw_input=user_input,
            from_command=False,
        )


def _format_history(history: Optional[Iterable[Any]], max_turns: int = 6, max_len: int = 200) -> str:
    if not history:
        return ""
    turns = list(history)
    if max_turns > 0:
        turns = turns[-max_turns:]
    lines = []
    for t in turns:
        role = getattr(t, "role", "") or ""
        content = getattr(t, "content", "") or ""
        if not isinstance(content, str):
            continue
        text = content.strip().replace("\n", " ")
        if max_len and len(text) > max_len:
            text = text[:max_len].rstrip() + "..."
        role_label = "用户" if role == "user" else "助手"
        lines.append(f"{role_label}: {text}")
    return "\n".join(lines)
</file>

<file path="src/collaboration/memory/__init__.py">
from src.collaboration.memory.session_memory import (
    ConversationTurn,
    SessionMemory,
    get_session_store,
    load_session_memory,
)
from src.collaboration.memory.persistent_store import (
    get_user_profile,
    get_user_projects,
    upsert_user_profile,
)
from src.collaboration.memory.working_memory import (
    get_or_generate_working_memory,
    get_working_memory,
    update_working_memory,
)

__all__ = [
    "ConversationTurn",
    "SessionMemory",
    "get_session_store",
    "load_session_memory",
    "get_working_memory",
    "get_or_generate_working_memory",
    "update_working_memory",
    "get_user_profile",
    "upsert_user_profile",
    "get_user_projects",
]
</file>

<file path="src/collaboration/memory/persistent_store.py">
"""
Persistent Store：用户级持久化（偏好、历史项目、认证）。
"""

import json
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional


def _db_path() -> Path:
    return Path(__file__).resolve().parents[2] / "data" / "persistent.db"


def _init_schema(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS user_profiles (
            user_id TEXT PRIMARY KEY,
            preferences_json TEXT NOT NULL DEFAULT '{}',
            created_at TEXT NOT NULL,
            updated_at TEXT NOT NULL
        )
        """
    )
    _ensure_user_profile_auth_columns(conn)
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS user_projects (
            user_id TEXT NOT NULL,
            canvas_id TEXT NOT NULL,
            title TEXT NOT NULL DEFAULT '',
            updated_at TEXT NOT NULL,
            PRIMARY KEY (user_id, canvas_id)
        )
        """
    )
    conn.commit()


def _ensure_user_profile_auth_columns(conn: sqlite3.Connection) -> None:
    """Add auth columns to user_profiles if missing (migration)."""
    cur = conn.execute("PRAGMA table_info(user_profiles)")
    columns = {row[1] for row in cur.fetchall()}
    if "password_hash" not in columns:
        conn.execute("ALTER TABLE user_profiles ADD COLUMN password_hash TEXT NOT NULL DEFAULT ''")
    if "role" not in columns:
        conn.execute("ALTER TABLE user_profiles ADD COLUMN role TEXT NOT NULL DEFAULT 'user'")
    if "is_active" not in columns:
        conn.execute("ALTER TABLE user_profiles ADD COLUMN is_active INTEGER NOT NULL DEFAULT 1")


def get_user_profile(user_id: str) -> Optional[Dict[str, Any]]:
    """返回用户偏好（含 role/is_active），无则返回 None。"""
    if not user_id:
        return None
    path = _db_path()
    path.parent.mkdir(parents=True, exist_ok=True)
    with sqlite3.connect(path) as conn:
        _init_schema(conn)
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT user_id, preferences_json, created_at, updated_at, password_hash, role, is_active FROM user_profiles WHERE user_id = ?",
            (user_id,),
        ).fetchone()
    if row is None:
        return None
    prefs = {}
    if row["preferences_json"]:
        try:
            prefs = json.loads(row["preferences_json"])
        except Exception:
            pass
    return {
        "user_id": row["user_id"],
        "preferences": prefs,
        "created_at": row["created_at"],
        "updated_at": row["updated_at"],
        "password_hash": row["password_hash"] or "",
        "role": row["role"] or "user",
        "is_active": bool(row["is_active"]) if row["is_active"] is not None else True,
    }


def upsert_user_profile(user_id: str, preferences: Dict[str, Any]) -> None:
    """写入或更新用户偏好。"""
    if not user_id:
        return
    path = _db_path()
    path.parent.mkdir(parents=True, exist_ok=True)
    now = datetime.now().isoformat()
    prefs_json = json.dumps(preferences, ensure_ascii=False)
    with sqlite3.connect(path) as conn:
        _init_schema(conn)
        conn.execute(
            """INSERT INTO user_profiles (user_id, preferences_json, created_at, updated_at)
               VALUES (?, ?, ?, ?)
               ON CONFLICT(user_id) DO UPDATE SET preferences_json = ?, updated_at = ?""",
            (user_id, prefs_json, now, now, prefs_json, now),
        )
        conn.commit()


def add_user_project(user_id: str, canvas_id: str, title: str = "") -> None:
    """记录用户关联的画布项目。"""
    if not user_id or not canvas_id:
        return
    path = _db_path()
    path.parent.mkdir(parents=True, exist_ok=True)
    now = datetime.now().isoformat()
    with sqlite3.connect(path) as conn:
        _init_schema(conn)
        conn.execute(
            """INSERT INTO user_projects (user_id, canvas_id, title, updated_at)
               VALUES (?, ?, ?, ?)
               ON CONFLICT(user_id, canvas_id) DO UPDATE SET title = ?, updated_at = ?""",
            (user_id, canvas_id, title or canvas_id, now, title or canvas_id, now),
        )
        conn.commit()


def get_user_projects(user_id: str, limit: int = 50) -> List[Dict[str, Any]]:
    """返回用户最近项目列表。"""
    if not user_id:
        return []
    path = _db_path()
    path.parent.mkdir(parents=True, exist_ok=True)
    with sqlite3.connect(path) as conn:
        _init_schema(conn)
        conn.row_factory = sqlite3.Row
        rows = conn.execute(
            "SELECT user_id, canvas_id, title, updated_at FROM user_projects WHERE user_id = ? ORDER BY updated_at DESC LIMIT ?",
            (user_id, limit),
        ).fetchall()
    return [
        {"user_id": r["user_id"], "canvas_id": r["canvas_id"], "title": r["title"], "updated_at": r["updated_at"]}
        for r in rows
    ]


def create_user(
    user_id: str,
    password: str,
    role: str = "user",
) -> None:
    """创建用户（管理员调用）。password 将被哈希存储。"""
    from src.auth.password import hash_password

    if not user_id or not password:
        raise ValueError("user_id and password are required")
    if role not in ("admin", "user"):
        raise ValueError("role must be admin or user")
    path = _db_path()
    path.parent.mkdir(parents=True, exist_ok=True)
    now = datetime.now().isoformat()
    password_hash = hash_password(password)
    with sqlite3.connect(path) as conn:
        _init_schema(conn)
        conn.execute(
            """INSERT INTO user_profiles (user_id, preferences_json, created_at, updated_at, password_hash, role, is_active)
               VALUES (?, '{}', ?, ?, ?, ?, 1)""",
            (user_id, now, now, password_hash, role),
        )
        conn.commit()


def verify_password(user_id: str, plain_password: str) -> bool:
    """校验用户密码。"""
    from src.auth.password import verify_password as _verify

    profile = get_user_profile(user_id)
    if not profile or not profile.get("is_active", True):
        return False
    return _verify(plain_password, profile.get("password_hash") or "")


def list_users() -> List[Dict[str, Any]]:
    """列出所有用户（管理员用）。不含 password_hash。"""
    path = _db_path()
    path.parent.mkdir(parents=True, exist_ok=True)
    with sqlite3.connect(path) as conn:
        _init_schema(conn)
        conn.row_factory = sqlite3.Row
        rows = conn.execute(
            "SELECT user_id, role, is_active, created_at, updated_at FROM user_profiles ORDER BY created_at"
        ).fetchall()
    return [
        {
            "user_id": r["user_id"],
            "role": r["role"] or "user",
            "is_active": bool(r["is_active"]),
            "created_at": r["created_at"],
            "updated_at": r["updated_at"],
        }
        for r in rows
    ]
</file>

<file path="src/collaboration/memory/session_memory.py">
"""
会话记忆 - 多轮对话滑动窗口与 SQLite 持久化
"""

import json
import sqlite3
import uuid
from dataclasses import dataclass, field
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional

from src.log import get_logger

logger = get_logger(__name__)


@dataclass
class ConversationTurn:
    """单轮对话"""

    role: Literal["user", "assistant"]
    content: str
    intent: Optional[str] = None
    evidence_pack_id: Optional[str] = None
    canvas_patch: Optional[Dict[str, Any]] = None
    citations: List[Dict[str, Any]] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)


def _default_db_path() -> Path:
    return Path(__file__).resolve().parents[2] / "data" / "sessions.db"


def _ensure_stage_column(conn: sqlite3.Connection) -> None:
    """兼容旧库：若 sessions 表无 stage 列则添加。"""
    cur = conn.execute("PRAGMA table_info(sessions)")
    columns = [row[1] for row in cur.fetchall()]
    if "stage" not in columns:
        conn.execute("ALTER TABLE sessions ADD COLUMN stage TEXT NOT NULL DEFAULT 'explore'")


def _ensure_citations_column(conn: sqlite3.Connection) -> None:
    """兼容旧库：若 turns 表无 citations_json 列则添加。"""
    cur = conn.execute("PRAGMA table_info(turns)")
    columns = [row[1] for row in cur.fetchall()]
    if "citations_json" not in columns:
        conn.execute("ALTER TABLE turns ADD COLUMN citations_json TEXT")


def _ensure_summary_columns(conn: sqlite3.Connection) -> None:
    """兼容旧库：若 sessions 表无滚动总结字段则添加。"""
    cur = conn.execute("PRAGMA table_info(sessions)")
    columns = [row[1] for row in cur.fetchall()]
    if "rolling_summary" not in columns:
        conn.execute("ALTER TABLE sessions ADD COLUMN rolling_summary TEXT NOT NULL DEFAULT ''")
    if "summary_at_turn" not in columns:
        conn.execute("ALTER TABLE sessions ADD COLUMN summary_at_turn INTEGER NOT NULL DEFAULT 0")


class SessionStore:
    """SQLite 持久化：会话与轮次"""

    def __init__(self, db_path: Optional[Path] = None):
        self.db_path = db_path or _default_db_path()
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_schema()

    def _init_schema(self) -> None:
        with sqlite3.connect(self.db_path) as conn:
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS sessions (
                    session_id TEXT PRIMARY KEY,
                    canvas_id TEXT NOT NULL DEFAULT '',
                    stage TEXT NOT NULL DEFAULT 'explore',
                    rolling_summary TEXT NOT NULL DEFAULT '',
                    summary_at_turn INTEGER NOT NULL DEFAULT 0,
                    created_at TEXT NOT NULL,
                    updated_at TEXT NOT NULL
                )
                """
            )
            _ensure_stage_column(conn)
            _ensure_summary_columns(conn)
            conn.execute(
                """
                CREATE TABLE IF NOT EXISTS turns (
                    session_id TEXT NOT NULL,
                    turn_index INTEGER NOT NULL,
                    role TEXT NOT NULL,
                    content TEXT NOT NULL,
                    intent TEXT,
                    evidence_pack_id TEXT,
                    canvas_patch TEXT,
                    citations_json TEXT,
                    timestamp TEXT NOT NULL,
                    PRIMARY KEY (session_id, turn_index)
                )
                """
            )
            _ensure_citations_column(conn)
            conn.commit()

    def create_session(self, canvas_id: str = "", stage: str = "explore") -> str:
        session_id = str(uuid.uuid4())
        now = datetime.now().isoformat()
        with sqlite3.connect(self.db_path) as conn:
            _ensure_stage_column(conn)
            _ensure_summary_columns(conn)
            conn.execute(
                """
                INSERT INTO sessions (
                    session_id, canvas_id, stage, rolling_summary, summary_at_turn, created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                (session_id, canvas_id, stage, "", 0, now, now),
            )
            conn.commit()
        return session_id

    def get_session_meta(self, session_id: str) -> Optional[Dict[str, Any]]:
        with sqlite3.connect(self.db_path) as conn:
            _ensure_stage_column(conn)
            _ensure_summary_columns(conn)
            conn.row_factory = sqlite3.Row
            row = conn.execute(
                """
                SELECT session_id, canvas_id, stage, rolling_summary, summary_at_turn, created_at, updated_at
                FROM sessions WHERE session_id = ?
                """,
                (session_id,),
            ).fetchone()
        if row is None:
            return None
        meta = dict(row)
        if meta.get("stage") is None:
            meta["stage"] = "explore"
        return meta

    def get_session_stage(self, session_id: str) -> str:
        meta = self.get_session_meta(session_id)
        if meta is None:
            return "explore"
        return meta.get("stage") or "explore"

    def update_session_stage(self, session_id: str, stage: str) -> None:
        with sqlite3.connect(self.db_path) as conn:
            _ensure_stage_column(conn)
            conn.execute(
                "UPDATE sessions SET stage = ?, updated_at = ? WHERE session_id = ?",
                (stage, datetime.now().isoformat(), session_id),
            )
            conn.commit()

    def update_session_meta(self, session_id: str, meta: Dict[str, Any]) -> None:
        """更新会话元数据，如 canvas_id"""
        updates = []
        values: List[Any] = []
        if isinstance(meta.get("canvas_id"), str):
            updates.append("canvas_id = ?")
            values.append(meta["canvas_id"])
        if isinstance(meta.get("rolling_summary"), str):
            updates.append("rolling_summary = ?")
            values.append(meta["rolling_summary"])
        summary_at_turn = meta.get("summary_at_turn")
        if isinstance(summary_at_turn, int):
            updates.append("summary_at_turn = ?")
            values.append(summary_at_turn)
        if not updates:
            return
        updates.append("updated_at = ?")
        values.append(datetime.now().isoformat())
        values.append(session_id)
        sql = f"UPDATE sessions SET {', '.join(updates)} WHERE session_id = ?"
        with sqlite3.connect(self.db_path) as conn:
            _ensure_summary_columns(conn)
            conn.execute(sql, tuple(values))
            conn.commit()

    def get_turns(
        self,
        session_id: str,
        limit: Optional[int] = None,
        order_desc: bool = False,
    ) -> List[ConversationTurn]:
        with sqlite3.connect(self.db_path) as conn:
            _ensure_citations_column(conn)
            conn.row_factory = sqlite3.Row
            order = "DESC" if order_desc else "ASC"
            sql = (
                "SELECT role, content, intent, evidence_pack_id, canvas_patch, citations_json, timestamp "
                "FROM turns WHERE session_id = ? ORDER BY turn_index "
                + order
            )
            if limit is not None:
                sql += f" LIMIT {int(limit)}"
            rows = conn.execute(sql, (session_id,)).fetchall()
        turns = []
        for r in rows:
            patch = None
            if r["canvas_patch"]:
                try:
                    patch = json.loads(r["canvas_patch"])
                except Exception:
                    pass
            citations = []
            if r["citations_json"]:
                try:
                    citations = json.loads(r["citations_json"])
                except Exception:
                    pass
            ts = datetime.fromisoformat(r["timestamp"]) if r["timestamp"] else datetime.now()
            turns.append(
                ConversationTurn(
                    role=r["role"],
                    content=r["content"] or "",
                    intent=r["intent"],
                    evidence_pack_id=r["evidence_pack_id"],
                    canvas_patch=patch,
                    citations=citations,
                    timestamp=ts,
                )
            )
        if order_desc:
            turns.reverse()
        return turns

    def append_turn(
        self,
        session_id: str,
        role: str,
        content: str,
        intent: Optional[str] = None,
        evidence_pack_id: Optional[str] = None,
        canvas_patch: Optional[Dict[str, Any]] = None,
        citations: Optional[List[Dict[str, Any]]] = None,
    ) -> None:
        meta = self.get_session_meta(session_id)
        if meta is None:
            raise ValueError(f"Session not found: {session_id}")
        with sqlite3.connect(self.db_path) as conn:
            _ensure_citations_column(conn)
            cur = conn.execute(
                "SELECT COALESCE(MAX(turn_index), -1) + 1 FROM turns WHERE session_id = ?",
                (session_id,),
            )
            idx = cur.fetchone()[0]
            conn.execute(
                """INSERT INTO turns (session_id, turn_index, role, content, intent, evidence_pack_id, canvas_patch, citations_json, timestamp)
                   VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                (
                    session_id,
                    idx,
                    role,
                    content,
                    intent,
                    evidence_pack_id,
                    json.dumps(canvas_patch, ensure_ascii=False) if canvas_patch else None,
                    json.dumps(citations, ensure_ascii=False) if citations else None,
                    datetime.now().isoformat(),
                ),
            )
            conn.execute(
                "UPDATE sessions SET updated_at = ? WHERE session_id = ?",
                (datetime.now().isoformat(), session_id),
            )
            conn.commit()

    def delete_session(self, session_id: str) -> bool:
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("DELETE FROM turns WHERE session_id = ?", (session_id,))
            cur = conn.execute("DELETE FROM sessions WHERE session_id = ?", (session_id,))
            conn.commit()
        return cur.rowcount > 0

    def list_all_sessions(self, limit: int = 100) -> List[Dict[str, Any]]:
        """列出所有会话，按更新时间倒序"""
        with sqlite3.connect(self.db_path) as conn:
            _ensure_stage_column(conn)
            _ensure_summary_columns(conn)
            conn.row_factory = sqlite3.Row
            rows = conn.execute(
                """
                SELECT session_id, canvas_id, stage, rolling_summary, summary_at_turn, created_at, updated_at
                FROM sessions ORDER BY updated_at DESC LIMIT ?
                """,
                (limit,),
            ).fetchall()
        sessions = []
        for row in rows:
            meta = dict(row)
            if meta.get("stage") is None:
                meta["stage"] = "explore"
            # 获取第一轮对话作为标题
            turns = self.get_turns(meta["session_id"], limit=1)
            meta["title"] = turns[0].content[:50] + "..." if turns and turns[0].content else "未命名对话"
            meta["turn_count"] = self._count_turns(meta["session_id"])
            sessions.append(meta)
        return sessions

    def _count_turns(self, session_id: str) -> int:
        """统计会话的轮次数"""
        with sqlite3.connect(self.db_path) as conn:
            cur = conn.execute("SELECT COUNT(*) FROM turns WHERE session_id = ?", (session_id,))
            return cur.fetchone()[0]


_store: Optional[SessionStore] = None


def get_session_store(db_path: Optional[Path] = None) -> SessionStore:
    global _store
    if _store is None:
        _store = SessionStore(db_path=db_path)
    return _store


@dataclass
class SessionMemory:
    """会话级短期记忆（滑动窗口，持久化由 SessionStore 负责）"""

    session_id: str
    canvas_id: str
    turns: List[ConversationTurn] = field(default_factory=list)
    max_turns: int = 20
    rolling_summary: str = ""
    summary_at_turn: int = 0

    def add_turn(self, role: str, content: str, **kwargs: Any) -> None:
        turn = ConversationTurn(role=role, content=content, **kwargs)
        self.turns.append(turn)
        store = get_session_store()
        store.append_turn(
            self.session_id,
            role=role,
            content=content,
            intent=kwargs.get("intent"),
            evidence_pack_id=kwargs.get("evidence_pack_id"),
            canvas_patch=kwargs.get("canvas_patch"),
            citations=kwargs.get("citations"),
        )
        if len(self.turns) > self.max_turns:
            self.turns = self.turns[-self.max_turns :]

    def get_context_window(self, n: int = 10) -> List[ConversationTurn]:
        return self.turns[-n:]

    def to_messages(self) -> List[Dict[str, str]]:
        return [{"role": t.role, "content": t.content} for t in self.turns]

    def update_rolling_summary(self, llm_client: Any, interval: int = 4) -> None:
        """Update rolling summary every `interval` turns."""
        current_count = len(self.turns)
        if current_count - self.summary_at_turn < max(1, interval):
            return

        recent_turns = self.turns[self.summary_at_turn:current_count]
        turns_text = "\n".join(
            f"{'User' if t.role == 'user' else 'Assistant'}: {(t.content or '')[:200]}"
            for t in recent_turns
        )
        prompt = f"""Summarize the following conversation segment in 2-3 sentences.
Focus on the main topic, key concepts discussed, and the user's current direction.

Previous summary:
{self.rolling_summary or "(start of conversation)"}

New conversation segment:
{turns_text}

Output only the updated summary."""
        try:
            resp = llm_client.chat(
                messages=[
                    {"role": "system", "content": "Output a concise conversation summary."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=200,
            )
            text = (resp.get("final_text") or "").strip()
            if text:
                self.rolling_summary = text
                self.summary_at_turn = current_count
                get_session_store().update_session_meta(
                    self.session_id,
                    {
                        "rolling_summary": self.rolling_summary,
                        "summary_at_turn": self.summary_at_turn,
                    },
                )
        except Exception as e:
            logger.debug("rolling summary update failed: %s", e)


def load_session_memory(session_id: str, max_turns: int = 20) -> Optional[SessionMemory]:
    """从持久化加载会话记忆"""
    store = get_session_store()
    meta = store.get_session_meta(session_id)
    if meta is None:
        return None
    turns = store.get_turns(session_id, limit=max_turns * 2, order_desc=True)
    return SessionMemory(
        session_id=meta["session_id"],
        canvas_id=meta["canvas_id"] or "",
        turns=turns[-max_turns:] if len(turns) > max_turns else turns,
        max_turns=max_turns,
        rolling_summary=meta.get("rolling_summary") or "",
        summary_at_turn=int(meta.get("summary_at_turn") or 0),
    )
</file>

<file path="src/collaboration/memory/working_memory.py">
"""
Working Memory：与 Canvas 绑定的状态摘要，由 LLM 生成并 SQLite 缓存。
"""

import json
import sqlite3
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, Optional

from src.collaboration.canvas.models import SurveyCanvas
from src.collaboration.canvas.canvas_manager import get_canvas
from src.llm.llm_manager import get_manager


def _db_path() -> Path:
    return Path(__file__).resolve().parents[2] / "data" / "working_memory.db"


def _init_schema(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS working_memory (
            canvas_id TEXT PRIMARY KEY,
            summary TEXT NOT NULL DEFAULT '',
            meta_json TEXT NOT NULL DEFAULT '{}',
            updated_at TEXT NOT NULL
        )
        """
    )
    conn.commit()


def get_working_memory(canvas_id: str) -> Optional[Dict[str, Any]]:
    """返回缓存的 working memory，无则返回 None。"""
    if not canvas_id:
        return None
    path = _db_path()
    path.parent.mkdir(parents=True, exist_ok=True)
    with sqlite3.connect(path) as conn:
        _init_schema(conn)
        conn.row_factory = sqlite3.Row
        row = conn.execute(
            "SELECT canvas_id, summary, meta_json, updated_at FROM working_memory WHERE canvas_id = ?",
            (canvas_id,),
        ).fetchone()
    if row is None:
        return None
    meta = {}
    if row["meta_json"]:
        try:
            meta = json.loads(row["meta_json"])
        except Exception:
            pass
    return {
        "canvas_id": row["canvas_id"],
        "summary": row["summary"] or "",
        "meta": meta,
        "updated_at": row["updated_at"],
    }


def update_working_memory(canvas_id: str, summary: str, meta: Optional[Dict[str, Any]] = None) -> None:
    """写入或更新 working memory 缓存。"""
    if not canvas_id:
        return
    path = _db_path()
    path.parent.mkdir(parents=True, exist_ok=True)
    now = datetime.now().isoformat()
    meta_json = json.dumps(meta or {}, ensure_ascii=False)
    with sqlite3.connect(path) as conn:
        _init_schema(conn)
        conn.execute(
            """INSERT INTO working_memory (canvas_id, summary, meta_json, updated_at)
               VALUES (?, ?, ?, ?)
               ON CONFLICT(canvas_id) DO UPDATE SET summary = ?, meta_json = ?, updated_at = ?""",
            (canvas_id, summary, meta_json, now, summary, meta_json, now),
        )
        conn.commit()


def _canvas_to_context_string(canvas: SurveyCanvas) -> str:
    """把 Canvas 关键信息拼成供 LLM 用的文本。"""
    parts = [
        f"主题: {canvas.topic or '(未设)'}",
        f"阶段: {canvas.stage}",
        f"工作标题: {canvas.working_title or '(未设)'}",
    ]
    if canvas.outline:
        parts.append("大纲章节:")
        for s in canvas.outline:
            parts.append(f"  - [{s.id}] {s.title} (level={s.level}, status={s.status})")
    if canvas.drafts:
        parts.append("草稿概况:")
        for sid, b in canvas.drafts.items():
            parts.append(f"  - 章节 {sid}: {len(b.content_md)} 字, v{b.version}")
    if canvas.identified_gaps:
        parts.append("已识别缺口: " + "; ".join(canvas.identified_gaps[:5]))
    if canvas.research_insights:
        parts.append("研究洞察:")
        for ins in canvas.research_insights[:10]:
            parts.append(f"  - {ins}")
    return "\n".join(parts)


def generate_working_memory_summary(canvas_id: str, config_path: Optional[Path] = None) -> str:
    """
    根据 Canvas 用 LLM 生成摘要并写入缓存，返回 summary 文本。
    若 canvas 不存在则返回空字符串。
    """
    canvas = get_canvas(canvas_id)
    if canvas is None:
        return ""
    context = _canvas_to_context_string(canvas)
    prompt = f"""你是一个学术写作助手。请根据以下综述画布状态，用 2-4 句话概括当前进度与下一步建议。只输出概括内容，不要其他解释。

画布状态：
{context}
"""
    try:
        manager = get_manager(str(config_path) if config_path else None)
        client = manager.get_client()
        resp = client.chat(
            [
                {"role": "system", "content": "你只输出简短概括，不要 markdown 或标题。"},
                {"role": "user", "content": prompt},
            ],
            max_tokens=256,
        )
        summary = (resp.get("final_text") or "").strip()
    except Exception:
        summary = f"主题: {canvas.topic}，阶段: {canvas.stage}，共 {len(canvas.outline)} 个大纲章节，{len(canvas.drafts)} 个草稿。"
    meta: Dict[str, Any] = {
        "stage": canvas.stage,
        "outline_count": len(canvas.outline),
        "draft_count": len(canvas.drafts),
    }
    # Include open research insights in meta for cross-session reuse
    if canvas.research_insights:
        meta["research_insights"] = canvas.research_insights[:20]
    update_working_memory(canvas_id, summary, meta)
    return summary


def get_or_generate_working_memory(canvas_id: str, config_path: Optional[Path] = None) -> Optional[Dict[str, Any]]:
    """
    若已有缓存则返回，否则生成并缓存后返回。
    """
    if not canvas_id:
        return None
    cached = get_working_memory(canvas_id)
    if cached and cached.get("summary"):
        return cached
    summary = generate_working_memory_summary(canvas_id, config_path)
    if not summary:
        return None
    return get_working_memory(canvas_id)
</file>

<file path="src/collaboration/research/__init__.py">
"""
递归研究引擎：RE-TRAC 轨迹压缩 + ReCAP 研究仪表盘 + 验证循环。
"""
</file>

<file path="src/collaboration/research/dashboard.py">
"""
ReCAP (Recursive Context-Aware Planning) Research Dashboard。

始终保持"已知/未知/已排除"的全局视图，钉在 system prompt 顶部防止 context drift。
每轮 Agent 迭代时，Dashboard 注入 system prompt 开头，确保 LLM 始终看到全局目标。
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional


@dataclass
class ResearchBrief:
    """研究简报：Phase 1 Scoping 的输出"""
    topic: str = ""
    scope: str = ""                          # 明确研究边界
    success_criteria: List[str] = field(default_factory=list)  # 完成标准
    key_questions: List[str] = field(default_factory=list)     # 核心问题
    exclusions: List[str] = field(default_factory=list)        # 明确排除的内容
    time_range: str = ""                     # 文献时间范围
    source_priority: List[str] = field(default_factory=list)   # 优先来源类型


@dataclass
class SectionStatus:
    """章节状态"""
    title: str
    status: str = "pending"  # pending | researching | writing | reviewing | done
    coverage_score: float = 0.0  # 0-1，信息充分度
    source_count: int = 0
    gaps: List[str] = field(default_factory=list)  # 该章节的信息缺口
    research_rounds: int = 0  # 该章节已执行的研究轮次（用于 per-section 限制）
    evidence_scarce: bool = False  # 检索证据不足（用于写作降级与最终限制说明）


@dataclass
class ResearchDashboard:
    """
    研究仪表盘 — ReCAP 的核心状态。

    每轮迭代注入 system prompt 顶部，确保 LLM 始终看到：
    - 研究范围和目标
    - 各章节进度
    - 信息缺口
    - 整体置信度
    """
    brief: ResearchBrief = field(default_factory=ResearchBrief)
    sections: List[SectionStatus] = field(default_factory=list)
    overall_confidence: str = "low"  # low | medium | high
    total_sources: int = 0
    total_iterations: int = 0
    coverage_gaps: List[str] = field(default_factory=list)  # 全局信息缺口
    conflict_notes: List[str] = field(default_factory=list)  # 来源冲突记录

    def add_section(self, title: str) -> SectionStatus:
        s = SectionStatus(title=title)
        self.sections.append(s)
        return s

    def get_section(self, title: str) -> Optional[SectionStatus]:
        for s in self.sections:
            if s.title == title:
                return s
        return None

    def update_section(self, title: str, **kwargs) -> None:
        s = self.get_section(title)
        if s:
            for k, v in kwargs.items():
                if hasattr(s, k):
                    setattr(s, k, v)

    def compute_overall_progress(self) -> float:
        """计算整体进度 0-1"""
        if not self.sections:
            return 0.0
        done = sum(1 for s in self.sections if s.status == "done")
        return done / len(self.sections)

    def compute_coverage(self) -> float:
        """计算整体信息覆盖度 0-1"""
        if not self.sections:
            return 0.0
        return sum(s.coverage_score for s in self.sections) / len(self.sections)

    def update_confidence(self) -> None:
        """根据覆盖度自动更新置信度"""
        coverage = self.compute_coverage()
        if coverage >= 0.8 and not self.coverage_gaps:
            self.overall_confidence = "high"
        elif coverage >= 0.5:
            self.overall_confidence = "medium"
        else:
            self.overall_confidence = "low"

    def get_next_section(self) -> Optional[SectionStatus]:
        """获取下一个需要处理的章节"""
        for s in self.sections:
            if s.status in ("pending", "researching"):
                return s
        # 检查是否有 reviewing 的章节需要补充
        for s in self.sections:
            if s.status == "reviewing" and s.coverage_score < 0.6:
                return s
        return None

    def all_done(self) -> bool:
        return all(s.status == "done" for s in self.sections) if self.sections else False

    def to_system_prompt(self) -> str:
        """生成注入 system prompt 顶部的仪表盘文本"""
        lines = ["═══ RESEARCH DASHBOARD ═══"]

        # 研究简报
        b = self.brief
        lines.append(f"Topic: {b.topic}")
        if b.scope:
            lines.append(f"Scope: {b.scope}")
        if b.success_criteria:
            lines.append("Success Criteria: " + "; ".join(b.success_criteria))
        if b.exclusions:
            lines.append("Exclusions: " + "; ".join(b.exclusions))

        # 进度
        progress = self.compute_overall_progress()
        lines.append(f"\nProgress: {progress:.0%} | Sources: {self.total_sources} | Confidence: {self.overall_confidence}")

        # 各章节状态
        lines.append("\nSection Status:")
        for s in self.sections:
            icon = {"pending": "⬜", "researching": "🔍", "writing": "✍️",
                    "reviewing": "🔄", "done": "✅"}.get(s.status, "⬜")
            cov = f"{s.coverage_score:.0%}" if s.coverage_score > 0 else "—"
            gaps_str = f" [Gaps: {', '.join(s.gaps[:2])}]" if s.gaps else ""
            lines.append(f"  {icon} {s.title} (Coverage:{cov}, Sources:{s.source_count}){gaps_str}")

        # 全局缺口
        if self.coverage_gaps:
            lines.append("\nGlobal Information Gaps:")
            for g in self.coverage_gaps[-5:]:
                lines.append(f"  ❗ {g}")

        # 冲突
        if self.conflict_notes:
            lines.append(f"\nSource Conflicts ({len(self.conflict_notes)}):")
            for c in self.conflict_notes[-3:]:
                lines.append(f"  ⚠️ {c}")

        lines.append("═══════════════════════")
        return "\n".join(lines)

    def to_dict(self) -> Dict[str, Any]:
        """序列化为 dict（用于 SSE/API 传输）"""
        return {
            "topic": self.brief.topic,
            "scope": self.brief.scope,
            "progress": self.compute_overall_progress(),
            "coverage": self.compute_coverage(),
            "confidence": self.overall_confidence,
            "total_sources": self.total_sources,
            "total_iterations": self.total_iterations,
            "sections": [
                {
                    "title": s.title,
                    "status": s.status,
                    "coverage_score": s.coverage_score,
                    "source_count": s.source_count,
                    "gaps": s.gaps,
                    "research_rounds": s.research_rounds,
                    "evidence_scarce": s.evidence_scarce,
                }
                for s in self.sections
            ],
            "coverage_gaps": self.coverage_gaps,
            "conflict_notes": self.conflict_notes,
        }
</file>

<file path="src/collaboration/research/trajectory.py">
"""
RE-TRAC (Recursive Trajectory Compression)

解决长研究链的 context drift 问题：
- 当 context 接近上限时，将详细轨迹压缩为高层摘要
- 保留战略发现、未探索分支、关键引用
- 丢弃重复内容、中间推理步骤
"""

from __future__ import annotations

import json
import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from src.log import get_logger

logger = get_logger(__name__)


@dataclass
class SearchAction:
    """一次搜索动作的记录"""
    query: str
    tool: str  # search_local / search_web / search_scholar / explore_graph
    result_summary: str  # 压缩后的结果摘要
    source_count: int = 0
    timestamp: float = field(default_factory=time.time)


@dataclass
class ResearchBranch:
    """一条研究分支"""
    id: str
    title: str
    status: str = "pending"  # pending | in_progress | done | discarded
    key_findings: List[str] = field(default_factory=list)
    search_actions: List[SearchAction] = field(default_factory=list)
    sub_questions: List[str] = field(default_factory=list)


@dataclass
class ResearchTrajectory:
    """
    研究轨迹：记录整个研究过程的状态。

    RE-TRAC 的核心数据结构，维护"已知/未知/已排除"的全局视图。
    """
    topic: str
    branches: List[ResearchBranch] = field(default_factory=list)
    compressed_summaries: List[str] = field(default_factory=list)
    known_facts: List[str] = field(default_factory=list)
    open_questions: List[str] = field(default_factory=list)
    discarded: List[str] = field(default_factory=list)
    total_sources: int = 0
    compression_count: int = 0

    def add_branch(self, branch_id: str, title: str) -> ResearchBranch:
        branch = ResearchBranch(id=branch_id, title=title)
        self.branches.append(branch)
        return branch

    def get_branch(self, branch_id: str) -> Optional[ResearchBranch]:
        for b in self.branches:
            if b.id == branch_id:
                return b
        return None

    def add_search_action(self, branch_id: str, action: SearchAction) -> None:
        branch = self.get_branch(branch_id)
        if branch:
            branch.search_actions.append(action)
            self.total_sources += action.source_count

    def add_finding(self, branch_id: str, finding: str) -> None:
        branch = self.get_branch(branch_id)
        if branch:
            branch.key_findings.append(finding)
            self.known_facts.append(finding)

    def estimate_token_count(self) -> int:
        """粗估当前轨迹占用的 token 数（1 char ≈ 0.5 token for CJK, 0.25 for EN）"""
        total_chars = len(self.topic)
        for b in self.branches:
            total_chars += len(b.title) + sum(len(f) for f in b.key_findings)
            for a in b.search_actions:
                total_chars += len(a.query) + len(a.result_summary)
        for s in self.compressed_summaries:
            total_chars += len(s)
        total_chars += sum(len(f) for f in self.known_facts)
        total_chars += sum(len(q) for q in self.open_questions)
        return int(total_chars * 0.4)  # 粗估

    def needs_compression(self, max_tokens: int = 30000) -> bool:
        return self.estimate_token_count() > max_tokens

    def to_context_string(self) -> str:
        """将轨迹序列化为可注入 system prompt 的字符串"""
        parts = [f"## 研究轨迹: {self.topic}"]
        parts.append(f"已处理来源: {self.total_sources} | 压缩次数: {self.compression_count}")

        if self.compressed_summaries:
            parts.append("\n### 已压缩的研究摘要")
            for i, s in enumerate(self.compressed_summaries):
                parts.append(f"[摘要 {i+1}] {s}")

        active_branches = [b for b in self.branches if b.status != "discarded"]
        if active_branches:
            parts.append("\n### 研究分支")
            for b in active_branches:
                status_icon = {"pending": "⏳", "in_progress": "🔍", "done": "✅"}.get(b.status, "")
                parts.append(f"\n**{b.title}** ({status_icon}{b.status})")
                if b.key_findings:
                    for f in b.key_findings[-5:]:  # 只保留最近 5 个
                        parts.append(f"  - {f}")

        if self.open_questions:
            parts.append("\n### 待回答的问题")
            for q in self.open_questions[-10:]:
                parts.append(f"  - {q}")

        if self.discarded:
            parts.append(f"\n### 已排除方向 ({len(self.discarded)})")
            for d in self.discarded[-5:]:
                parts.append(f"  - {d}")

        return "\n".join(parts)


# ────────────────────────────────────────────────
# RE-TRAC 压缩
# ────────────────────────────────────────────────

_COMPRESS_PROMPT = """你是一个研究轨迹压缩助手。请将以下详细的研究轨迹压缩为一段高质量摘要。

**保留**:
- 战略性发现（关键结论、重要数据点）
- 未探索的分支和待回答的问题
- 关键来源的引用信息
- 研究方向的判断和决策

**丢弃**:
- 重复的搜索结果和中间推理步骤
- 冗余的背景描述
- 已被后续发现否定的信息

研究轨迹:
{trajectory}

请输出压缩后的摘要（控制在 500-800 字以内），格式为：
1. **核心发现**: 已确认的关键事实
2. **未决问题**: 仍需探索的方向
3. **来源概要**: 已使用的重要来源"""


def compress_trajectory(
    trajectory: ResearchTrajectory,
    llm_client: Any,
    model: Optional[str] = None,
) -> str:
    """
    当 context 接近上限时，执行 RE-TRAC 压缩。

    将所有分支的详细搜索记录压缩为高层摘要，释放 context 空间。
    """
    # 构建要压缩的详细内容
    detail_parts = []
    for branch in trajectory.branches:
        if branch.status == "discarded":
            continue
        detail_parts.append(f"\n分支: {branch.title} ({branch.status})")
        for action in branch.search_actions:
            detail_parts.append(f"  搜索 [{action.tool}]: {action.query}")
            detail_parts.append(f"  结果: {action.result_summary[:300]}")
        for finding in branch.key_findings:
            detail_parts.append(f"  发现: {finding}")

    trajectory_text = "\n".join(detail_parts)

    prompt = _COMPRESS_PROMPT.format(trajectory=trajectory_text)

    try:
        resp = llm_client.chat(
            messages=[
                {"role": "system", "content": "你是研究轨迹压缩专家。"},
                {"role": "user", "content": prompt},
            ],
            model=model,
            max_tokens=1500,
        )
        summary = (resp.get("final_text") or "").strip()
    except Exception as e:
        logger.warning(f"RE-TRAC compression failed: {e}")
        # 降级：手动拼接关键发现
        summary = "压缩失败，保留关键发现:\n" + "\n".join(trajectory.known_facts[-20:])

    # 执行压缩：清除详细记录，保留摘要
    trajectory.compressed_summaries.append(summary)
    trajectory.compression_count += 1

    # 清除已压缩分支的详细搜索记录
    for branch in trajectory.branches:
        if branch.status in ("done", "in_progress"):
            branch.search_actions = []  # 清除详细搜索记录
            # 保留 key_findings 的最近几条
            branch.key_findings = branch.key_findings[-3:]

    logger.info(f"RE-TRAC compression #{trajectory.compression_count}: "
                f"{len(summary)} chars, {trajectory.estimate_token_count()} est tokens remaining")

    return summary
</file>

<file path="src/collaboration/workflow/__init__.py">
"""
工作流状态机：四阶段 Explore / Outline / Drafting / Refine，LangGraph 最小图。
"""

from .graph import build_workflow_graph, run_workflow
from .states import WorkflowStage, StageConfig, STAGE_CONFIGS, get_stage_config
from .transitions import compute_next_stage

__all__ = [
    "WorkflowStage",
    "StageConfig",
    "STAGE_CONFIGS",
    "get_stage_config",
    "compute_next_stage",
    "build_workflow_graph",
    "run_workflow",
]
</file>

<file path="src/collaboration/workflow/graph.py">
"""
LangGraph 最小工作流图：根据当前阶段与意图路由到下一阶段并产出阶段系统提示。
"""

from typing import Any, Dict, TypedDict

from langgraph.graph import END, START, StateGraph

from .states import WorkflowStage, get_stage_config
from .transitions import compute_next_stage


class WorkflowState(TypedDict, total=False):
    current_stage: str
    intent_type: str
    topic: str
    outline: str
    context: str
    current_section: str
    section_guidance: str
    fragments: str
    draft_content: str
    next_stage: str
    system_prompt: str


def _route_stage(state: WorkflowState) -> Dict[str, Any]:
    current = state.get("current_stage") or WorkflowStage.EXPLORE.value
    intent = state.get("intent_type") or ""
    next_stage = compute_next_stage(current, intent)
    config = get_stage_config(next_stage)
    system_prompt = ""
    if config:
        system_prompt = config.system_prompt_template.format(
            topic=state.get("topic") or "",
            outline=state.get("outline") or "",
            context=state.get("context") or "",
            current_section=state.get("current_section") or "",
            section_guidance=state.get("section_guidance") or "",
            fragments=state.get("fragments") or "",
            draft_content=state.get("draft_content") or "",
        )
    return {"next_stage": next_stage, "system_prompt": system_prompt}


def build_workflow_graph() -> Any:
    """构建并编译最小工作流图。"""
    builder = StateGraph(WorkflowState)
    builder.add_node("route_stage", _route_stage)
    builder.add_edge(START, "route_stage")
    builder.add_edge("route_stage", END)
    return builder.compile()


def run_workflow(
    current_stage: str,
    intent_type: str,
    *,
    topic: str = "",
    context: str = "",
    **kwargs: str,
) -> Dict[str, Any]:
    """
    运行工作流：返回 next_stage 与 system_prompt（已填充的阶段提示）。
    """
    graph = build_workflow_graph()
    state: WorkflowState = {
        "current_stage": current_stage,
        "intent_type": intent_type,
        "topic": topic,
        "context": context,
        **kwargs,
    }
    result = graph.invoke(state)
    return {
        "next_stage": result.get("next_stage") or current_stage,
        "system_prompt": result.get("system_prompt") or "",
    }
</file>

<file path="src/collaboration/workflow/states.py">
"""
工作流阶段定义与配置。
"""

from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional


class WorkflowStage(Enum):
    EXPLORE = "explore"
    OUTLINE = "outline"
    DRAFTING = "drafting"
    REFINE = "refine"


@dataclass
class StageConfig:
    """阶段配置"""

    name: WorkflowStage
    description: str
    system_prompt_template: str
    allowed_intents: List[str]
    on_enter_actions: Optional[List[str]] = None
    transition_triggers: Optional[Dict[WorkflowStage, str]] = None

    def __post_init__(self) -> None:
        if self.on_enter_actions is None:
            self.on_enter_actions = []
        if self.transition_triggers is None:
            self.transition_triggers = {}


STAGE_CONFIGS: Dict[WorkflowStage, StageConfig] = {
    WorkflowStage.EXPLORE: StageConfig(
        name=WorkflowStage.EXPLORE,
        description="探索阶段：帮助用户发散思维，确定综述范围",
        system_prompt_template="""你是一个学术研究助手，正在帮助用户探索综述主题。
当前主题: {topic}

你的任务是：
1. 回答用户的开放式问题
2. 提供领域概览和关键研究方向
3. 在适当时候建议用户进入大纲构建阶段

请基于以下检索到的信息回答:
{context}
""",
        allowed_intents=["chat", "deep_research"],
        transition_triggers={WorkflowStage.OUTLINE: "用户明确表示开始构建大纲"},
    ),
    WorkflowStage.OUTLINE: StageConfig(
        name=WorkflowStage.OUTLINE,
        description="大纲阶段：与用户共同敲定综述骨架",
        system_prompt_template="""你是一个学术写作助手，正在帮助用户构建综述大纲。
主题: {topic}
当前大纲: {outline}

你的任务是：
1. 帮助生成或修改大纲结构
2. 对每个章节进行信息充足度分析
3. 识别需要补充检索的信息缺口

请基于以下信息协助用户:
{context}
""",
        allowed_intents=["chat", "deep_research"],
        on_enter_actions=["auto_generate_outline_if_empty"],
        transition_triggers={
            WorkflowStage.DRAFTING: "大纲确认完成且无重大缺口",
            WorkflowStage.EXPLORE: "用户要求重新探索",
        },
    ),
    WorkflowStage.DRAFTING: StageConfig(
        name=WorkflowStage.DRAFTING,
        description="写作阶段：逐段生成高质量草稿",
        system_prompt_template="""你是一个学术写作助手，正在帮助用户撰写综述。
主题: {topic}
当前章节: {current_section}
章节要点: {section_guidance}

写作要求：
1. 使用学术语言，逻辑清晰
2. 每个观点必须有文献支撑
3. 使用 [cite_key] 格式标注引用

可用的知识片段:
{fragments}

请撰写该章节内容。
""",
        allowed_intents=["chat", "deep_research"],
        transition_triggers={
            WorkflowStage.REFINE: "所有章节初稿完成",
            WorkflowStage.OUTLINE: "需要修改大纲结构",
        },
    ),
    WorkflowStage.REFINE: StageConfig(
        name=WorkflowStage.REFINE,
        description="精修阶段：优化语言、检查逻辑、统一格式",
        system_prompt_template="""你是一个学术编辑助手，正在帮助用户精修综述。
主题: {topic}

你的任务是：
1. 优化语言表达，使其更加学术化
2. 检查段落间的逻辑连贯性
3. 统一引用格式
4. 检查是否有遗漏的重要内容

当前内容:
{draft_content}

请基于以下信息协助用户:
{context}
""",
        allowed_intents=["chat", "deep_research"],
        transition_triggers={WorkflowStage.DRAFTING: "需要重写某些章节"},
    ),
}


def get_stage_config(stage: str) -> Optional[StageConfig]:
    """按阶段名（str）获取配置，无效则返回 None。"""
    try:
        s = WorkflowStage(stage)
        return STAGE_CONFIGS.get(s)
    except ValueError:
        return None
</file>

<file path="src/collaboration/workflow/transitions.py">
"""
根据当前阶段与意图计算下一阶段（简化版：Chat vs Deep Research）。
"""

from .states import WorkflowStage


# 意图 -> 目标阶段（仅 Deep Research 会强制切换阶段）
INTENT_TO_STAGE: dict[str, str] = {
    "deep_research": WorkflowStage.DRAFTING.value,
    # 兼容旧值
    "auto_complete": WorkflowStage.DRAFTING.value,
    "outline_generate": WorkflowStage.OUTLINE.value,
    "draft_section": WorkflowStage.DRAFTING.value,
    "edit_text": WorkflowStage.REFINE.value,
    "export_document": WorkflowStage.REFINE.value,
}


def compute_next_stage(current_stage: str, intent_type: str) -> str:
    """
    根据当前阶段与意图返回下一阶段。
    Chat 模式保持当前阶段不变；Deep Research 切换到 DRAFTING。
    """
    intent_val = getattr(intent_type, "value", intent_type) if intent_type else ""
    next_stage = INTENT_TO_STAGE.get(intent_val)
    if next_stage is not None:
        return next_stage
    return current_stage if current_stage else WorkflowStage.EXPLORE.value
</file>

<file path="src/collaboration/__init__.py">
# Collaboration layer - multi-turn dialogue and survey canvas
</file>

<file path="src/collaboration/auto_complete.py">
"""
自动完成综述服务。

根据检索结果和 LLM 自动执行：检索 -> 生成大纲 -> 逐章写作 -> 返回完整 Markdown。
"""

import re
import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from src.collaboration.canvas.canvas_manager import (
    create_canvas,
    get_canvas,
    upsert_draft,
    upsert_outline,
    update_canvas,
)
from src.collaboration.canvas.models import DraftBlock, OutlineSection, SurveyCanvas
from src.collaboration.citation.manager import resolve_response_citations, sync_evidence_to_canvas
from src.collaboration.citation.formatter import format_reference_list
from src.retrieval.service import RetrievalService, get_retrieval_service


@dataclass
class AutoCompleteResult:
    """自动完成结果"""

    session_id: str = ""
    canvas_id: str = ""
    markdown: str = ""
    outline: List[str] = field(default_factory=list)
    citations: List[Any] = field(default_factory=list)  # Citation 对象列表
    total_time_ms: float = 0.0
    dashboard: Optional[Dict[str, Any]] = None  # 研究进度仪表盘（Agent 模式）


def _parse_outline_from_llm(text: str) -> List[Dict[str, Any]]:
    """从 LLM 返回的文本解析大纲结构。"""
    sections: List[Dict[str, Any]] = []
    lines = (text or "").strip().split("\n")
    order = 0
    for line in lines:
        line = line.strip()
        if not line:
            continue
        # 匹配 "1. 引言" / "1.1 背景" / "- 引言" / "* 背景"
        m = re.match(r"^([\d\.\-\*]+)\s*(.+)$", line)
        if m:
            prefix, title = m.group(1), m.group(2).strip()
            level = 1
            if re.match(r"^\d", prefix):
                parts = prefix.split(".")
                level = min(len(parts), 3)
            sections.append({"title": title, "level": level, "order": order})
            order += 1
    return sections


class AutoCompleteService:
    """自动完成综述服务"""

    def __init__(
        self,
        llm_client: Any,
        retrieval_service: Optional[RetrievalService] = None,
        max_sections: int = 6,
        max_words_per_section: int = 500,
        include_abstract: bool = True,
    ):
        self.llm = llm_client
        self.retrieval = retrieval_service
        self.max_sections = max_sections
        self.max_words_per_section = max_words_per_section
        self.include_abstract = include_abstract

    def complete(
        self,
        topic: str,
        canvas_id: Optional[str] = None,
        session_id: str = "",
        search_mode: str = "hybrid",
        existing_outline: Optional[List[OutlineSection]] = None,
        user_id: str = "",
        filters: Optional[Dict[str, Any]] = None,
        clarification_answers: Optional[Dict[str, str]] = None,
        output_language: Optional[str] = None,
        step_models: Optional[Dict[str, Optional[str]]] = None,
        use_agent: bool = False,
    ) -> AutoCompleteResult:
        """
        自动完成综述（Deep Research 引擎）。

        Args:
            topic: 综述主题
            canvas_id: 可选，已有画布 ID
            session_id: 可选，会话 ID
            search_mode: 检索模式 local | web | hybrid
            existing_outline: 可选，已有大纲（跳过生成大纲）
            user_id: 用户 ID
            filters: UI 检索参数透传（web_providers, llm_provider, model_override, final_top_k 等）
            clarification_answers: 澄清问题回答 {question_id: answer_text}
            use_agent: 是否使用递归研究 Agent（LangGraph）

        Returns:
            AutoCompleteResult
        """
        t0 = time.perf_counter()
        result = AutoCompleteResult(session_id=session_id, canvas_id=canvas_id or "")

        # ── 新引擎：递归研究 Agent ──
        if use_agent:
            return self._run_agent(topic, canvas_id, session_id, user_id,
                                   search_mode, filters, clarification_answers, output_language, step_models, t0)
        filters = filters or {}
        collection = (filters.get("collection") or "").strip() or None
        retrieval = self.retrieval or get_retrieval_service(collection=collection, top_k=15)

        # 从 filters 取 top_k，Deep Research 默认更大
        main_top_k = filters.get("final_top_k") or 15
        section_top_k = max(main_top_k // 2, 10)

        # 1. 检索主题资料（透传完整 filters）
        pack = retrieval.search(
            query=topic, mode=search_mode, top_k=main_top_k, filters=filters or None,
        )
        context_str = pack.to_context_string(max_chunks=min(main_top_k, 20))

        # 2. 创建或获取画布
        if canvas_id:
            canvas = get_canvas(canvas_id)
            if canvas is None:
                canvas = create_canvas(session_id=session_id, topic=topic, user_id=user_id)
                result.canvas_id = canvas.id
        else:
            canvas = create_canvas(session_id=session_id, topic=topic, user_id=user_id)
            result.canvas_id = canvas.id

        # 3. 同步引用到画布
        sync_evidence_to_canvas(result.canvas_id, pack)

        # 4. 生成或使用大纲（融入澄清回答）
        if existing_outline and len(existing_outline) > 0:
            outline_sections = existing_outline
        else:
            outline_sections = self._generate_outline(topic, context_str, clarification_answers)
            if outline_sections:
                upsert_outline(result.canvas_id, outline_sections)
            else:
                # fallback: 单章
                outline_sections = [
                    OutlineSection(id="s1", title="正文", level=1, order=0),
                ]
                upsert_outline(result.canvas_id, outline_sections)

        result.outline = [s.title for s in outline_sections]
        sorted_sections = sorted(outline_sections, key=lambda s: (s.order, s.level))
        doc_key_to_cite_key: Dict[str, str] = {}
        existing_cite_keys: set[str] = set()
        cited_pool: Dict[str, Any] = {}

        # 5. 逐章写作
        markdown_parts: List[str] = []
        markdown_parts.append(f"# {topic}\n")

        if self.include_abstract:
            abstract = self._generate_abstract(topic, context_str)
            if abstract:
                markdown_parts.append(f"## 摘要\n{abstract}\n")

        for section in sorted_sections[: self.max_sections]:
            section_query = f"{topic} {section.title}"
            section_pack = retrieval.search(
                query=section_query, mode=search_mode,
                top_k=section_top_k, filters=filters or None,
            )
            section_context = section_pack.to_context_string(max_chunks=min(section_top_k, 15))

            content = self._generate_section(
                topic=topic,
                section_title=section.title,
                context=section_context,
            )
            if content:
                # 章节级 hash 引文后处理，并跨章节复用 cite_key 映射
                content, section_citations, _ = resolve_response_citations(
                    content,
                    section_pack.chunks,
                    doc_key_to_cite_key=doc_key_to_cite_key,
                    existing_cite_keys=existing_cite_keys,
                    include_unreferenced_documents=False,
                )
                for c in section_citations:
                    key = c.cite_key or c.id
                    if key and key not in cited_pool:
                        cited_pool[key] = c
                sync_evidence_to_canvas(result.canvas_id, section_pack)
                markdown_parts.append(f"## {section.title}\n{content}\n")

                block = DraftBlock(
                    section_id=section.id,
                    content_md=content,
                    version=1,
                    used_fragment_ids=[],
                    used_citation_ids=[],
                )
                upsert_draft(result.canvas_id, block)

        # 6. 参考文献
        if cited_pool:
            markdown_parts.append("## 参考文献\n")
            markdown_parts.append(format_reference_list(list(cited_pool.values())).strip())
        else:
            canvas = get_canvas(result.canvas_id)
            if canvas and canvas.citation_pool:
                markdown_parts.append("## 参考文献\n")
                markdown_parts.append(format_reference_list(list(canvas.citation_pool.values())).strip())

        result.markdown = "\n".join(markdown_parts).strip() + "\n"
        update_canvas(result.canvas_id, stage="refine")
        result.total_time_ms = (time.perf_counter() - t0) * 1000
        # 最终引用列表（优先使用正文替换后实际引用到的文档级 citation）
        if cited_pool:
            result.citations = list(cited_pool.values())
        else:
            final_canvas = get_canvas(result.canvas_id)
            if final_canvas and final_canvas.citation_pool:
                result.citations = list(final_canvas.citation_pool.values())
        return result

    def _generate_outline(
        self, topic: str, context: str,
        clarification_answers: Optional[Dict[str, str]] = None,
    ) -> List[OutlineSection]:
        """LLM 生成大纲（可融入用户澄清回答）"""
        answers_block = ""
        if clarification_answers:
            lines = [f"- {k}: {v}" for k, v in clarification_answers.items() if v]
            if lines:
                answers_block = "\n用户补充信息：\n" + "\n".join(lines) + "\n"

        prompt = f"""根据以下参考资料，为综述主题「{topic}」生成大纲结构。
{answers_block}
要求：
1. 列出 3-6 个主要章节，每行一个
2. 格式示例：1. 引言  2. 研究背景  3. 核心内容  4. 展望
3. 只输出章节列表，不要其他文字

参考资料：
{context[:3000] if len(context) > 3000 else context}
"""
        try:
            resp = self.llm.chat(
                [
                    {"role": "system", "content": "你是学术写作助手，只返回大纲列表。"},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=512,
            )
            text = (resp.get("final_text") or "").strip()
            parsed = _parse_outline_from_llm(text)
            sections = []
            for i, p in enumerate(parsed[: self.max_sections]):
                sections.append(
                    OutlineSection(
                        id=f"s{i+1}",
                        title=p.get("title", f"章节{i+1}"),
                        level=p.get("level", 1),
                        order=i,
                        status="todo",
                    )
                )
            return sections
        except Exception:
            return []

    def _generate_abstract(self, topic: str, context: str) -> str:
        """LLM 生成摘要"""
        prompt = f"""根据以下参考资料，为综述主题「{topic}」撰写 150-250 字的摘要。
只输出摘要内容，不要其他文字。

参考资料：
{context[:2500] if len(context) > 2500 else context}
"""
        try:
            resp = self.llm.chat(
                [
                    {"role": "system", "content": "你是学术写作助手。"},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=400,
            )
            return (resp.get("final_text") or "").strip()
        except Exception:
            return ""

    def _generate_section(
        self,
        topic: str,
        section_title: str,
        context: str,
    ) -> str:
        """LLM 生成章节内容"""
        prompt = f"""根据以下参考资料，撰写综述「{topic}」中「{section_title}」章节的内容。
要求：
1. 字数约 {self.max_words_per_section} 字
2. 使用学术语言，逻辑清晰
3. 引用时使用方括号标记；若参考资料中有 [ref_hash] 请直接引用该标记（后续会自动替换为正式 cite_key）
4. 只输出章节正文，不要标题

参考资料：
{context}
"""
        try:
            resp = self.llm.chat(
                [
                    {"role": "system", "content": "你是学术写作助手，基于参考资料撰写综述章节。"},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=1200,
            )
            return (resp.get("final_text") or "").strip()
        except Exception:
            return ""

    def _run_agent(
        self,
        topic: str,
        canvas_id: Optional[str],
        session_id: str,
        user_id: str,
        search_mode: str,
        filters: Optional[Dict[str, Any]],
        clarification_answers: Optional[Dict[str, str]],
        output_language: Optional[str],
        step_models: Optional[Dict[str, Optional[str]]],
        t0: float,
    ) -> AutoCompleteResult:
        """使用递归研究 Agent (LangGraph) 执行 Deep Research"""
        from src.collaboration.research.agent import run_deep_research

        agent_result = run_deep_research(
            topic=topic,
            llm_client=self.llm,
            canvas_id=canvas_id,
            session_id=session_id,
            user_id=user_id,
            search_mode=search_mode,
            filters=filters,
            max_iterations=self.max_sections * 5,
            clarification_answers=clarification_answers,
            output_language=output_language or "auto",
            step_models=step_models,
        )

        # 更新 Canvas
        cid = agent_result.get("canvas_id", canvas_id or "")
        if cid:
            try:
                update_canvas(cid, markdown=agent_result["markdown"])
            except Exception:
                pass

        return AutoCompleteResult(
            session_id=session_id,
            canvas_id=cid,
            markdown=agent_result["markdown"],
            outline=agent_result.get("outline", []),
            citations=agent_result.get("citations", []),
            total_time_ms=(time.perf_counter() - t0) * 1000,
            dashboard=agent_result.get("dashboard"),
        )
</file>

<file path="src/evaluation/__init__.py">
"""
评估模块
"""

from src.evaluation.dataset import EvalCase, load_dataset
from src.evaluation.metrics import extract_citations, recall_at_k, rouge_l_f1, token_f1
from src.evaluation.runner import evaluate_case, evaluate_dataset

__all__ = [
    "EvalCase",
    "load_dataset",
    "extract_citations",
    "recall_at_k",
    "rouge_l_f1",
    "token_f1",
    "evaluate_case",
    "evaluate_dataset",
]
</file>

<file path="src/evaluation/dataset.py">
"""
评测数据集加载与规范化
"""

from __future__ import annotations

import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Tuple


@dataclass
class EvalCase:
    id: str
    query: str
    mode: str = "local"
    expected_doc_ids: List[str] = field(default_factory=list)
    expected_citations: List[str] = field(default_factory=list)
    reference_answer: str = ""
    tags: List[str] = field(default_factory=list)
    meta: Dict[str, Any] = field(default_factory=dict)


def load_dataset(path: str | Path) -> Tuple[List[EvalCase], Dict[str, Any]]:
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Dataset not found: {path}")

    if path.suffix.lower() == ".jsonl":
        cases = []
        for line in path.read_text(encoding="utf-8").splitlines():
            line = line.strip()
            if not line:
                continue
            cases.append(json.loads(line))
        raw = {"cases": cases}
    else:
        raw = json.loads(path.read_text(encoding="utf-8"))
        if isinstance(raw, list):
            raw = {"cases": raw}

    cases = raw.get("cases") or []
    meta = {k: v for k, v in raw.items() if k != "cases"}
    normalized: List[EvalCase] = []
    for i, c in enumerate(cases, 1):
        normalized.append(_normalize_case(c, i))
    return normalized, meta


def _normalize_case(raw: Dict[str, Any], index: int) -> EvalCase:
    cid = str(raw.get("id") or f"case_{index:03d}")
    query = (raw.get("query") or "").strip()
    mode = (raw.get("mode") or "local").strip()
    expected_doc_ids = list(raw.get("expected_doc_ids") or [])
    expected_citations = list(raw.get("expected_citations") or [])
    reference_answer = (raw.get("reference_answer") or "").strip()
    tags = list(raw.get("tags") or [])
    meta = dict(raw.get("meta") or {})
    return EvalCase(
        id=cid,
        query=query,
        mode=mode,
        expected_doc_ids=expected_doc_ids,
        expected_citations=expected_citations,
        reference_answer=reference_answer,
        tags=tags,
        meta=meta,
    )
</file>

<file path="src/evaluation/metrics.py">
"""
评测指标：检索/生成/引用相关的基础度量
"""

from __future__ import annotations

import re
from collections import Counter
from typing import Iterable, List, Tuple


_TOKEN_RE = re.compile(r"[a-z0-9]+|[\u4e00-\u9fff]")
_CITE_RE = re.compile(r"\[([^\]]+)\]")


def tokenize(text: str) -> List[str]:
    text = (text or "").lower()
    return _TOKEN_RE.findall(text)


def _precision_recall_f1(pred_count: Counter, ref_count: Counter) -> Tuple[float, float, float]:
    if not pred_count and not ref_count:
        return 1.0, 1.0, 1.0
    if not pred_count or not ref_count:
        return 0.0, 0.0, 0.0
    common = pred_count & ref_count
    num_same = sum(common.values())
    if num_same == 0:
        return 0.0, 0.0, 0.0
    precision = num_same / max(1, sum(pred_count.values()))
    recall = num_same / max(1, sum(ref_count.values()))
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0
    return precision, recall, f1


def token_f1(pred: str, ref: str) -> Tuple[float, float, float]:
    pred_tokens = tokenize(pred)
    ref_tokens = tokenize(ref)
    return _precision_recall_f1(Counter(pred_tokens), Counter(ref_tokens))


def rouge_l_f1(pred: str, ref: str) -> Tuple[float, float, float]:
    pred_tokens = tokenize(pred)
    ref_tokens = tokenize(ref)
    if not pred_tokens and not ref_tokens:
        return 1.0, 1.0, 1.0
    if not pred_tokens or not ref_tokens:
        return 0.0, 0.0, 0.0
    lcs = _lcs_length(pred_tokens, ref_tokens)
    precision = lcs / max(1, len(pred_tokens))
    recall = lcs / max(1, len(ref_tokens))
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) else 0.0
    return precision, recall, f1


def _lcs_length(a: List[str], b: List[str]) -> int:
    if not a or not b:
        return 0
    m, n = len(a), len(b)
    dp = [0] * (n + 1)
    for i in range(1, m + 1):
        prev = 0
        for j in range(1, n + 1):
            tmp = dp[j]
            if a[i - 1] == b[j - 1]:
                dp[j] = prev + 1
            else:
                dp[j] = max(dp[j], dp[j - 1])
            prev = tmp
    return dp[n]


def extract_citations(text: str) -> List[str]:
    """
    从回答中抽取 [xxx] 形式的引用标记。
    支持 [A; B] / [A, B] / [A B]
    """
    out: List[str] = []
    for block in _CITE_RE.findall(text or ""):
        parts = re.split(r"[;,/|\s]+", block.strip())
        for p in parts:
            key = p.strip()
            if key:
                out.append(key)
    return out


def safe_mean(values: Iterable[float | None]) -> float | None:
    vals = [v for v in values if v is not None]
    if not vals:
        return None
    return sum(vals) / len(vals)


def recall_at_k(retrieved_ids: List[str], expected_ids: List[str], k: int) -> Tuple[float | None, float | None]:
    if not expected_ids:
        return None, None
    expected = set(expected_ids)
    topk = retrieved_ids[:k]
    hit = 1.0 if any(x in expected for x in topk) else 0.0
    recall = sum(1 for x in expected if x in topk) / max(1, len(expected))
    return recall, hit
</file>

<file path="src/evaluation/runner.py">
"""
评测执行器：检索/生成/引用质量统计
"""

from __future__ import annotations

from dataclasses import asdict
from typing import Any, Dict, List, Optional, Tuple

from src.collaboration.citation.manager import CiteKeyGenerator, chunk_to_citation
from src.evaluation.dataset import EvalCase
from src.evaluation.metrics import (
    extract_citations,
    recall_at_k,
    rouge_l_f1,
    safe_mean,
    token_f1,
)
from src.retrieval.evidence import EvidencePack
from src.retrieval.service import RetrievalService


def evaluate_dataset(
    cases: List[EvalCase],
    retrieval: RetrievalService,
    llm_client: Any | None = None,
    mode_override: Optional[str] = None,
    top_k: int = 10,
    max_context_chunks: int = 8,
    max_context_chars: int = 800,
    model_override: Optional[str] = None,
) -> Dict[str, Any]:
    results = []
    for case in cases:
        result = evaluate_case(
            case=case,
            retrieval=retrieval,
            llm_client=llm_client,
            mode_override=mode_override,
            top_k=top_k,
            max_context_chunks=max_context_chunks,
            max_context_chars=max_context_chars,
            model_override=model_override,
        )
        results.append(result)

    summary = _aggregate_results(results)
    return {"summary": summary, "results": results}


def evaluate_case(
    case: EvalCase,
    retrieval: RetrievalService,
    llm_client: Any | None = None,
    mode_override: Optional[str] = None,
    top_k: int = 10,
    max_context_chunks: int = 8,
    max_context_chars: int = 800,
    model_override: Optional[str] = None,
) -> Dict[str, Any]:
    mode = mode_override or case.mode or "local"
    pack = retrieval.search(case.query, mode=mode, top_k=top_k)

    retrieved_doc_ids = _unique_doc_ids(pack)
    recall, hit = recall_at_k(retrieved_doc_ids, case.expected_doc_ids, top_k)

    retrieval_metrics = {
        "recall_at_k": recall,
        "hit_at_k": hit,
        "top_k": top_k,
        "retrieval_time_ms": pack.retrieval_time_ms,
        "total_candidates": pack.total_candidates,
        "sources_used": pack.sources_used,
        "retrieved_doc_ids": retrieved_doc_ids[:top_k],
    }

    generation_metrics = None
    citation_metrics = None
    answer = None

    if llm_client and case.reference_answer:
        context, cite_keys = _build_context(pack, max_context_chunks, max_context_chars)
        answer = _generate_answer(
            llm_client=llm_client,
            query=case.query,
            context=context,
            cite_keys=cite_keys,
            model_override=model_override,
        )
        generation_metrics = _score_generation(answer, case.reference_answer)
        citation_metrics = _score_citations(answer, cite_keys, case.expected_citations)

    return {
        "case": asdict(case),
        "mode": mode,
        "retrieval": retrieval_metrics,
        "generation": generation_metrics,
        "citation": citation_metrics,
        "answer": answer,
    }


def _unique_doc_ids(pack: EvidencePack) -> List[str]:
    out = []
    seen = set()
    for chunk in pack.chunks:
        doc_id = (chunk.doc_id or "").strip()
        if not doc_id or doc_id in seen:
            continue
        seen.add(doc_id)
        out.append(doc_id)
    return out


def _build_context(
    pack: EvidencePack,
    max_chunks: int,
    max_chars: int,
) -> Tuple[str, List[str]]:
    gen = CiteKeyGenerator()
    lines = []
    keys: List[str] = []
    for chunk in pack.chunks[:max_chunks]:
        citation = chunk_to_citation(chunk, generator=gen)
        cite_key = citation.cite_key or citation.id
        title = citation.title or chunk.doc_id or chunk.chunk_id
        header = f"[{cite_key}] {title}"
        text = (chunk.text or "").strip()
        if not text:
            continue
        if max_chars > 0:
            text = text[:max_chars]
        lines.append(f"{header}\n{text}")
        keys.append(cite_key)
    return "\n\n".join(lines), keys


def _generate_answer(
    llm_client: Any,
    query: str,
    context: str,
    cite_keys: List[str],
    model_override: Optional[str] = None,
) -> str:
    system_prompt = (
        "你是严谨的科研助手，只能基于提供的证据回答。"
        "回答中需要在句末使用引用标记，例如 [Smith2023]。"
        "若证据不足，请直接说明无法确定。"
    )
    keys_str = ", ".join(cite_keys[:30])
    user_prompt = (
        "以下是证据列表（每条以引用标记开头）：\n\n"
        f"{context}\n\n"
        f"可用引用标记: {keys_str}\n\n"
        f"问题: {query}\n\n"
        "请给出简洁回答，并在相关句子末尾标注引用。"
    )
    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_prompt},
    ]
    resp = llm_client.chat(messages, model=model_override, max_tokens=800)
    return (resp.get("final_text") or "").strip()


def _score_generation(answer: str, reference: str) -> Dict[str, float]:
    p1, r1, f1 = token_f1(answer, reference)
    p2, r2, f2 = rouge_l_f1(answer, reference)
    return {
        "token_precision": p1,
        "token_recall": r1,
        "token_f1": f1,
        "rouge_l_precision": p2,
        "rouge_l_recall": r2,
        "rouge_l_f1": f2,
    }


def _score_citations(
    answer: str,
    allowed_cite_keys: List[str],
    expected_citations: List[str],
) -> Dict[str, float | int | None]:
    found = extract_citations(answer)
    if not found:
        return {
            "citations_found": 0,
            "valid_ratio": 0.0,
            "precision": 0.0,
            "recall": 0.0 if expected_citations else None,
        }
    allowed = set(allowed_cite_keys)
    valid = [c for c in found if c in allowed]
    precision = len(valid) / max(1, len(found))
    recall = None
    if expected_citations:
        expected = set(expected_citations)
        recall = len(set(found) & expected) / max(1, len(expected))
    return {
        "citations_found": len(found),
        "valid_ratio": precision,
        "precision": precision,
        "recall": recall,
    }


def _aggregate_results(results: List[Dict[str, Any]]) -> Dict[str, Any]:
    retrieval_recalls = []
    retrieval_hits = []
    token_f1s = []
    rouge_f1s = []
    citation_precisions = []
    citation_recalls = []

    for r in results:
        retrieval = r.get("retrieval") or {}
        retrieval_recalls.append(retrieval.get("recall_at_k"))
        retrieval_hits.append(retrieval.get("hit_at_k"))

        gen = r.get("generation") or {}
        token_f1s.append(gen.get("token_f1"))
        rouge_f1s.append(gen.get("rouge_l_f1"))

        cite = r.get("citation") or {}
        citation_precisions.append(cite.get("precision"))
        citation_recalls.append(cite.get("recall"))

    return {
        "retrieval_recall_at_k_mean": safe_mean(retrieval_recalls),
        "retrieval_hit_at_k_mean": safe_mean(retrieval_hits),
        "token_f1_mean": safe_mean(token_f1s),
        "rouge_l_f1_mean": safe_mean(rouge_f1s),
        "citation_precision_mean": safe_mean(citation_precisions),
        "citation_recall_mean": safe_mean(citation_recalls),
        "case_count": len(results),
    }
</file>

<file path="src/generation/__init__.py">
# 生成模块
</file>

<file path="src/generation/context_packer.py">
"""
Context Packing 模块
- QA 模式：短答场景，top chunks + 引用头
- Longform 模式：报告/综述场景，按 section 聚类，每节选 top N
"""

from __future__ import annotations

from collections import defaultdict
from typing import List, Dict, Any


def pack_qa_context(
    chunks: List[Dict],
    top_n: int = 6,
    separator: str = "\n\n---\n\n",
) -> str:
    """
    QA 模式：top chunks + 引用头
    
    Args:
        chunks: 检索结果（含 content 和 metadata）
        top_n: 取前 N 个 chunk
        separator: chunk 间分隔符
    
    Returns:
        拼接后的 context 字符串
    """
    context_parts = []
    for c in chunks[:top_n]:
        content = c.get("content") or c.get("raw_content") or ""
        if not content:
            continue
        meta = c.get("metadata", {}) or {}
        doc_id = meta.get("doc_id") or meta.get("paper_id") or "N/A"
        page = meta.get("page_range") or [meta.get("page", 0)]
        page_str = f"p.{page[0]}" if isinstance(page, (list, tuple)) and page else f"p.{meta.get('page', 0)}"
        section_path = meta.get("section_path") or ""
        header = f"[{doc_id} | {page_str} | {section_path}]"
        context_parts.append(f"{header}\n{content}")
    return separator.join(context_parts)


def pack_longform_context(
    chunks: List[Dict],
    chunks_per_section: int = 3,
    max_bundles: int = 8,
) -> List[Dict[str, Any]]:
    """
    Longform 模式：按 section 聚类，每节选 top N
    
    Args:
        chunks: 检索结果（含 content 和 metadata）
        chunks_per_section: 每 section 保留的 chunk 数
        max_bundles: 最多保留的 section 数
    
    Returns:
        [{"section": str, "evidence": [str, ...]}, ...]
    """
    groups: Dict[str, List[Dict]] = defaultdict(list)
    for c in chunks:
        meta = c.get("metadata", {}) or {}
        section = meta.get("section_path") or "(root)"
        groups[section].append(c)

    bundles = []
    for section, section_chunks in groups.items():
        if len(bundles) >= max_bundles:
            break
        top_in_section = section_chunks[:chunks_per_section]
        evidence = [
            (c.get("content") or c.get("raw_content") or "").strip()
            for c in top_in_section
        ]
        evidence = [e for e in evidence if e]
        if evidence:
            bundles.append({"section": section, "evidence": evidence})
    return bundles
</file>

<file path="src/generation/evidence_synthesizer.py">
"""
证据综合模块 (Evidence Synthesizer)

在检索和生成之间插入结构化的证据综合层：
- 时间线排序：按发表年份排序，标注时间标签
- 来源分组：区分本地文献 vs 网络来源，标注交叉验证
- 证据强度分级：从 section_title 推断 finding/method/interpretation/background/summary
- 矛盾/一致性标注：检测同一 doc_id 的多来源覆盖

核心原则：不额外调用 LLM，通过规则化证据组织 + 增强 system prompt 让 LLM 自身完成综合。
零延迟增加、零额外 token 消耗。

引用流程：
  1. 上下文中每条证据用 chunk 的 ref_hash (8位十六进制) 作为引用标记
  2. LLM 在回答中使用 [ref_hash] 引用
  3. 生成后由 citation/manager.resolve_response_citations() 将 hash 替换为
     正式 cite_key（numbered / author_date / hash 格式），同时输出文档级引文列表

使用方法:
---------
from src.generation.evidence_synthesizer import EvidenceSynthesizer

synthesizer = EvidenceSynthesizer()
context_str, synthesis_meta = synthesizer.synthesize(pack)
"""

from __future__ import annotations

import re
from collections import defaultdict
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

from src.retrieval.evidence import EvidenceChunk, EvidencePack


# ============================================================
# 证据类型分类规则
# ============================================================

# section_title 关键词 → 证据类型映射
_EVIDENCE_TYPE_RULES: List[Tuple[List[str], str]] = [
    # finding（最强）：实验结果、数据
    (
        ["result", "finding", "data", "observation", "outcome",
         "analysis", "statistical", "measurement"],
        "finding",
    ),
    # method：方法论、材料
    (
        ["method", "material", "procedure", "protocol", "experiment",
         "sample", "sampling", "technique", "approach", "design"],
        "method",
    ),
    # interpretation：讨论、结论
    (
        ["discussion", "conclusion", "implication", "interpretation",
         "perspective", "significance", "limitation", "future"],
        "interpretation",
    ),
    # background：引言、背景
    (
        ["introduction", "background", "overview", "context",
         "literature", "review", "state of the art", "related work"],
        "background",
    ),
    # summary：摘要
    (
        ["abstract", "summary", "highlight", "graphical abstract"],
        "summary",
    ),
]

# 证据类型展示名（中文）
_EVIDENCE_TYPE_LABELS = {
    "finding": "实验发现",
    "method": "方法描述",
    "interpretation": "讨论解读",
    "background": "背景信息",
    "summary": "摘要概述",
}

# 证据强度排序（用于排序，越小越强）
_EVIDENCE_STRENGTH_ORDER = {
    "finding": 0,
    "method": 1,
    "interpretation": 2,
    "summary": 3,
    "background": 4,
}

# 本地来源类型
_LOCAL_SOURCES = {"dense", "sparse", "graph"}


def classify_evidence_type(section_title: Optional[str]) -> str:
    """
    从 section_title 推断证据类型。

    Args:
        section_title: 章节路径/标题（如 "Results > Table 3"）

    Returns:
        证据类型: finding | method | interpretation | background | summary
    """
    if not section_title:
        return "background"

    lower = section_title.lower()
    for keywords, etype in _EVIDENCE_TYPE_RULES:
        for kw in keywords:
            if kw in lower:
                return etype
    return "background"


# ============================================================
# SynthesisMeta
# ============================================================

@dataclass
class SynthesisMeta:
    """证据综合元数据"""

    year_range: Tuple[Optional[int], Optional[int]] = (None, None)
    source_breakdown: Dict[str, int] = field(default_factory=dict)
    evidence_type_breakdown: Dict[str, int] = field(default_factory=dict)
    cross_validated_count: int = 0
    total_documents: int = 0

    def to_dict(self) -> Dict[str, Any]:
        return {
            "year_range": list(self.year_range),
            "source_breakdown": self.source_breakdown,
            "evidence_type_breakdown": self.evidence_type_breakdown,
            "cross_validated_count": self.cross_validated_count,
            "total_documents": self.total_documents,
        }


# ============================================================
# EvidenceSynthesizer
# ============================================================

class EvidenceSynthesizer:
    """
    证据综合器。

    对 EvidencePack 做结构化综合：时间线排序、来源分组、证据强度分级、
    交叉验证标注。输出增强的 context string + 综合元数据。
    """

    def __init__(self, max_chunks: int = 10):
        self.max_chunks = max_chunks

    def synthesize(
        self,
        pack: EvidencePack,
        max_chunks: Optional[int] = None,
    ) -> Tuple[str, SynthesisMeta]:
        """
        对 EvidencePack 做结构化综合。

        Args:
            pack: 检索结果包
            max_chunks: 最大使用的 chunk 数量

        Returns:
            (context_string, synthesis_meta)
        """
        limit = max_chunks or self.max_chunks
        chunks = pack.chunks[:limit]

        if not chunks:
            return "（本轮暂无检索结果）", SynthesisMeta()

        # 1. 为每个 chunk 分类证据类型
        for chunk in chunks:
            if not chunk.evidence_type:
                chunk.evidence_type = classify_evidence_type(chunk.section_title)

        # 2. 计算元数据
        meta = self._compute_meta(chunks)

        # 3. 检测交叉验证的 doc_id
        cross_validated_docs = self._find_cross_validated(chunks)
        meta.cross_validated_count = len(cross_validated_docs)

        # 4. 排序：先按年份（升序），同年按证据强度（finding 优先）
        sorted_chunks = sorted(
            chunks,
            key=lambda c: (
                c.year or 9999,
                _EVIDENCE_STRENGTH_ORDER.get(c.evidence_type or "background", 4),
            ),
        )

        # 5. 构建 context string（使用 ref_hash 作为引用标记）
        context = self._build_context(sorted_chunks, meta, cross_validated_docs)

        # 6. 回写到 pack
        pack.synthesis_meta = meta.to_dict()

        return context, meta

    def _compute_meta(self, chunks: List[EvidenceChunk]) -> SynthesisMeta:
        """计算综合元数据"""
        years = [c.year for c in chunks if c.year is not None]
        year_range = (min(years), max(years)) if years else (None, None)

        # 来源统计（合并为 local / web）
        source_counts: Dict[str, int] = defaultdict(int)
        for c in chunks:
            key = "local" if c.source_type in _LOCAL_SOURCES else "web"
            source_counts[key] += 1

        # 证据类型统计
        type_counts: Dict[str, int] = defaultdict(int)
        for c in chunks:
            type_counts[c.evidence_type or "background"] += 1

        # 独立文献数
        doc_ids = {c.doc_id for c in chunks if c.doc_id}

        return SynthesisMeta(
            year_range=year_range,
            source_breakdown=dict(source_counts),
            evidence_type_breakdown=dict(type_counts),
            cross_validated_count=0,
            total_documents=len(doc_ids),
        )

    def _find_cross_validated(self, chunks: List[EvidenceChunk]) -> set:
        """
        检测本地+网络双重覆盖的 doc_id。

        当同一 doc_id 同时有 local 和 web 来源的 chunk 时，标记为交叉验证。
        """
        doc_sources: Dict[str, set] = defaultdict(set)
        for c in chunks:
            if not c.doc_id:
                continue
            key = "local" if c.source_type in _LOCAL_SOURCES else "web"
            doc_sources[c.doc_id].add(key)

        return {
            doc_id
            for doc_id, sources in doc_sources.items()
            if len(sources) > 1
        }

    def _build_context(
        self,
        sorted_chunks: List[EvidenceChunk],
        meta: SynthesisMeta,
        cross_validated_docs: set,
    ) -> str:
        """构建结构化 context string"""
        parts = []

        # === Header: 综合概览 ===
        parts.append(self._build_header(meta))

        # === Body: 按时间排序的证据 ===
        parts.append("\n=== 证据（按时间排序）===\n")

        for chunk in sorted_chunks:
            entry = self._format_chunk(chunk, cross_validated_docs)
            if entry:
                parts.append(entry)

        return "\n".join(parts)

    def _build_header(self, meta: SynthesisMeta) -> str:
        """构建综合概览 header"""
        lines = ["=== 证据综合 ==="]

        # 时间跨度
        y_min, y_max = meta.year_range
        if y_min and y_max:
            if y_min == y_max:
                lines.append(f"时间跨度: {y_min} ({meta.total_documents}篇文献)")
            else:
                lines.append(f"时间跨度: {y_min}–{y_max} ({meta.total_documents}篇文献)")
        elif meta.total_documents:
            lines.append(f"涉及文献: {meta.total_documents}篇")

        # 来源构成
        local_n = meta.source_breakdown.get("local", 0)
        web_n = meta.source_breakdown.get("web", 0)
        source_parts = []
        if local_n:
            source_parts.append(f"本地 {local_n} 条")
        if web_n:
            source_parts.append(f"网络 {web_n} 条")
        if meta.cross_validated_count:
            source_parts.append(f"交叉验证 {meta.cross_validated_count} 条")
        if source_parts:
            lines.append(f"来源构成: {' | '.join(source_parts)}")

        # 证据类型
        type_parts = []
        for etype in ["finding", "method", "interpretation", "background", "summary"]:
            count = meta.evidence_type_breakdown.get(etype, 0)
            if count:
                label = _EVIDENCE_TYPE_LABELS.get(etype, etype)
                type_parts.append(f"{label} {count}")
        if type_parts:
            lines.append(f"证据类型: {' | '.join(type_parts)}")

        return "\n".join(lines)

    def _format_chunk(
        self,
        chunk: EvidenceChunk,
        cross_validated_docs: set,
    ) -> str:
        """格式化单个 chunk，使用 ref_hash 作为引用标记。"""
        text = (chunk.text or "").strip()
        if not text:
            return ""

        # 标签部分
        tags = []

        # 年份
        if chunk.year:
            tags.append(str(chunk.year))

        # 证据类型
        etype = chunk.evidence_type or "background"
        tags.append(etype)

        # 来源
        is_local = chunk.source_type in _LOCAL_SOURCES
        source_label = "local" if is_local else "web"

        # 交叉验证标记
        if chunk.doc_id and chunk.doc_id in cross_validated_docs:
            source_label = "local+web ✓"

        tags.append(source_label)

        tag_str = " | ".join(tags)

        # 引用标记：使用稳定的 8 位哈希
        ref = chunk.ref_hash

        return f"[{tag_str}] [{ref}]\n{text}"


# ============================================================
# 增强 system prompt
# ============================================================

SYNTHESIS_SYSTEM_PROMPT = (
    "你是一个基于检索增强的学术助手。参考资料已按时间排序并标注了来源类型和证据强度。\n"
    "每条证据前有一个方括号引用标记（如 [a1b2c3d4]），请在行文中使用该标记引用对应证据。\n"
    "请注意：\n"
    "1. 引用格式：在需要引用时使用方括号标记，如「…结果显示X [a1b2c3d4]」。\n"
    "2. 若有多个时间点的证据，请体现认知演进（'早期研究发现…[hash1]，近期研究表明…[hash2]'）\n"
    "3. 若不同来源结论不一致，请明确指出并分析可能原因\n"
    "4. 标注 [✓] 的证据有本地文献和网络来源双重支持，可信度更高\n"
    "5. 优先引用 finding 类型的证据，background 类型仅作为补充\n"
    "6. 保持多轮对话连贯。若参考资料与问题相关，请结合引用；若无直接相关，可基于常识简要回答并说明。\n\n"
    "参考资料：\n"
)


def build_synthesis_system_prompt(context: str) -> str:
    """构建包含综合证据的 system prompt"""
    return SYNTHESIS_SYSTEM_PROMPT + (context or "（本轮暂无检索结果）")
</file>

<file path="src/generation/llm_client.py">
"""
统一 LLM 调用：兼容层

本模块保留 call_llm() 函数签名，内部委托给 src/llm/llm_manager.py。
旧脚本可继续使用，无需修改调用代码。
"""

from typing import Optional

from src.llm.llm_manager import get_manager, LLMManager


def call_llm(
    provider: str,
    system: str,
    user_prompt: str,
    model_override: Optional[str] = None,
    max_tokens: int = 2000,
) -> str:
    """
    按 provider 调用对应 LLM，返回生成文本。

    Args:
        provider: config 中定义的 provider key (如 openai, deepseek, gemini-vision)
        system: system prompt
        user_prompt: user message
        model_override: 若传入则覆盖 config 中的 model
        max_tokens: 最大 token 数

    Returns:
        模型返回的文本；失败时返回以 [ERROR] 开头的字符串。
    """
    try:
        manager = get_manager()
    except Exception as e:
        return f"[ERROR] 加载 LLM 配置失败: {e}"

    # 检查 provider 是否可用
    if not manager.is_available(provider):
        return f"[ERROR] 未配置 {provider} 的 API Key，请在 config/rag_config.json 或环境变量中设置"

    try:
        client = manager.get_client(provider)
    except ValueError as e:
        return f"[ERROR] {e}"

    # 构建消息
    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user_prompt},
    ]

    try:
        resp = client.chat(messages, model=model_override, max_tokens=max_tokens)
        return (resp.get("final_text") or "").strip()
    except Exception as e:
        return f"[ERROR] {provider} 调用失败: {e}"
</file>

<file path="src/graph/__init__.py">
# HippoRAG 风格知识图谱模块
from src.graph.hippo_rag import HippoRAG, get_hippo_rag

__all__ = ["HippoRAG", "get_hippo_rag"]
</file>

<file path="src/graph/hippo_rag.py">
"""
HippoRAG 风格知识图谱增强检索

核心思想（受海马体索引理论启发）：
1. 从文档中抽取实体和关系，构建知识图谱
2. 检索时先通过向量检索找到种子节点
3. 使用 Personalized PageRank 在图上扩展
4. 融合向量检索和图检索结果

参考：https://github.com/OSU-NLP-Group/HippoRAG
"""

import json
import re
from pathlib import Path
from typing import List, Dict, Set, Tuple, Optional
from dataclasses import dataclass, field
from collections import defaultdict

import networkx as nx

from src.chunking.chunker import ChunkConfig, chunk_blocks
from src.log import get_logger


@dataclass
class Entity:
    """实体"""
    name: str
    type: str  # SPECIES, LOCATION, PHENOMENON, METHOD, SUBSTANCE 等
    mentions: List[str] = field(default_factory=list)  # 出现的 chunk_ids


@dataclass
class Relation:
    """关系"""
    source: str
    target: str
    relation_type: str  # 如 FOUND_IN, PRODUCES, LIVES_AT 等
    chunk_id: str


class HippoRAG:
    """
    HippoRAG 知识图谱检索增强

    使用 NetworkX 构建图谱，支持：
    - 实体抽取（基于规则或 LLM）
    - 关系抽取
    - Personalized PageRank 检索扩展
    """

    def __init__(self, graph_path: Optional[Path] = None):
        self.logger = get_logger(__name__)
        self.G = nx.DiGraph()
        self.entities: Dict[str, Entity] = {}
        self.chunk_to_entities: Dict[str, Set[str]] = defaultdict(set)
        self.entity_to_chunks: Dict[str, Set[str]] = defaultdict(set)
        self.graph_path = graph_path

        if graph_path and graph_path.exists():
            self.load(graph_path)

    # ========== 实体抽取 ==========

    def extract_entities_rule_based(self, text: str, chunk_id: str) -> List[Entity]:
        """
        基于规则的实体抽取（适用于深海科研领域）

        可扩展为 LLM 抽取，但规则方法更快且可控
        """
        entities = []

        # 深海相关实体模式
        patterns = {
            "LOCATION": [
                r"(马里亚纳海沟|冲绳海槽|南海|东太平洋|大西洋中脊)",
                r"(热液喷口|冷泉|深海平原|海山|海沟)",
                r"(hydrothermal vent|cold seep|deep sea|trench)",
            ],
            "SPECIES": [
                r"(管虫|贻贝|蟹类|虾类|细菌|古菌)",
                r"(tube worm|mussel|crab|shrimp|bacteria|archaea)",
                r"([A-Z][a-z]+ [a-z]+)",  # 拉丁学名
            ],
            "PHENOMENON": [
                r"(化能合成|光合作用|共生|甲烷渗漏|热液活动)",
                r"(chemosynthesis|symbiosis|methane seep)",
            ],
            "SUBSTANCE": [
                r"(硫化氢|甲烷|二氧化碳|氧气|硫化物)",
                r"(H2S|CH4|CO2|sulfide|methane)",
            ],
            "METHOD": [
                r"(ROV|AUV|深潜器|采样器|声呐)",
                r"(remotely operated vehicle|autonomous underwater vehicle)",
            ],
        }

        for entity_type, pattern_list in patterns.items():
            for pattern in pattern_list:
                matches = re.findall(pattern, text, re.IGNORECASE)
                for match in matches:
                    name = match if isinstance(match, str) else match[0]
                    name = name.strip().lower()
                    if len(name) > 2:  # 过滤太短的匹配
                        entity = Entity(name=name, type=entity_type, mentions=[chunk_id])
                        entities.append(entity)

        return entities

    def extract_entities_llm(self, text: str, chunk_id: str, llm_client=None) -> List[Entity]:
        """
        基于 LLM 的实体抽取（更准确但更慢）

        需要传入 LLM client，返回结构化实体列表
        """
        if llm_client is None:
            return self.extract_entities_rule_based(text, chunk_id)

        prompt = f"""从以下深海科研文本中抽取实体，返回 JSON 格式：

文本：
{text[:2000]}

请抽取以下类型的实体：
- LOCATION: 地理位置、海域、地质构造
- SPECIES: 物种名称（包括拉丁学名）
- PHENOMENON: 自然现象、生物过程
- SUBSTANCE: 化学物质、元素
- METHOD: 研究方法、设备

返回格式：
[{{"name": "实体名", "type": "类型"}}]

仅返回 JSON，不要其他内容："""

        try:
            # 这里假设使用 Claude API
            response = llm_client.messages.create(
                model="claude-sonnet-4-20250514",
                max_tokens=1000,
                messages=[{"role": "user", "content": prompt}]
            )
            result = json.loads(response.content[0].text)
            return [
                Entity(name=e["name"].lower(), type=e["type"], mentions=[chunk_id])
                for e in result
            ]
        except Exception:
            return self.extract_entities_rule_based(text, chunk_id)

    # ========== 图谱构建 ==========

    def add_chunk(self, chunk_id: str, content: str, paper_id: str, use_llm: bool = False, llm_client=None):
        """
        处理单个 chunk，抽取实体并添加到图谱
        """
        # 抽取实体
        if use_llm and llm_client:
            entities = self.extract_entities_llm(content, chunk_id, llm_client)
        else:
            entities = self.extract_entities_rule_based(content, chunk_id)

        # 添加实体节点
        for entity in entities:
            if entity.name not in self.entities:
                self.entities[entity.name] = entity
                self.G.add_node(entity.name, type=entity.type)
            else:
                self.entities[entity.name].mentions.append(chunk_id)

            self.chunk_to_entities[chunk_id].add(entity.name)
            self.entity_to_chunks[entity.name].add(chunk_id)

        # 添加 chunk 节点
        self.G.add_node(chunk_id, type="CHUNK", paper_id=paper_id)

        # 实体 -> chunk 边
        for entity in entities:
            self.G.add_edge(entity.name, chunk_id, relation="MENTIONED_IN")

        # 同一 chunk 内的实体之间建立共现关系
        entity_names = [e.name for e in entities]
        for i, e1 in enumerate(entity_names):
            for e2 in entity_names[i + 1:]:
                if self.G.has_edge(e1, e2):
                    self.G[e1][e2]["weight"] = self.G[e1][e2].get("weight", 1) + 1
                else:
                    self.G.add_edge(e1, e2, relation="CO_OCCURS", weight=1)
                if self.G.has_edge(e2, e1):
                    self.G[e2][e1]["weight"] = self.G[e2][e1].get("weight", 1) + 1
                else:
                    self.G.add_edge(e2, e1, relation="CO_OCCURS", weight=1)

    def build_from_parsed_docs(
        self,
        parsed_dir: Path,
        use_llm: bool = False,
        llm_client=None,
        chunk_config: Optional[ChunkConfig] = None,
    ):
        """
        从解析后的文档构建图谱。递归扫描 parsed_dir 下的 enriched.json，
        使用与 03_index 一致的 content_flow + chunk_blocks 分块，保证 chunk_id 与 Milvus 一致。
        """
        json_files = list(parsed_dir.rglob("enriched.json"))
        self.logger.info(f"构建知识图谱，共 {len(json_files)} 个文档...")

        cfg = chunk_config or ChunkConfig()

        for json_path in json_files:
            with open(json_path, "r", encoding="utf-8") as f:
                doc = json.load(f)

            doc_id = doc.get("doc_id", json_path.parent.name)
            paper_id = doc_id
            content_flow = doc.get("content_flow", [])

            chunks = chunk_blocks(content_flow, doc_id=doc_id, config=cfg)
            for c in chunks:
                if c.text.strip():
                    self.add_chunk(c.chunk_id, c.text, paper_id, use_llm, llm_client)

        self.logger.info(f"图谱构建完成: {self.G.number_of_nodes()} 节点, {self.G.number_of_edges()} 边")

    # ========== 检索增强 ==========

    def get_seed_entities(self, query: str) -> List[str]:
        """
        从查询中抽取实体作为种子节点
        """
        entities = self.extract_entities_rule_based(query, "query")
        seed_names = [e.name for e in entities if e.name in self.entities]
        return seed_names

    def personalized_pagerank(
        self,
        seed_entities: List[str],
        alpha: float = 0.85,
        top_k: int = 20
    ) -> List[Tuple[str, float]]:
        """
        Personalized PageRank

        从种子实体出发，在图上扩展找到相关节点
        """
        if not seed_entities:
            return []

        # 构建 personalization 向量
        personalization = {node: 0.0 for node in self.G.nodes()}
        for entity in seed_entities:
            if entity in personalization:
                personalization[entity] = 1.0 / len(seed_entities)

        # 运行 PPR
        try:
            scores = nx.pagerank(
                self.G,
                alpha=alpha,
                personalization=personalization,
                max_iter=100
            )
        except nx.PowerIterationFailedConvergence:
            # 如果不收敛，返回空
            return []

        # 过滤出 chunk 节点并排序
        chunk_scores = [
            (node, score)
            for node, score in scores.items()
            if self.G.nodes[node].get("type") == "CHUNK"
        ]
        chunk_scores.sort(key=lambda x: x[1], reverse=True)

        return chunk_scores[:top_k]

    def retrieve_with_graph(
        self,
        query: str,
        vector_hits: List[Dict],
        top_k: int = 10,
        graph_weight: float = 0.3
    ) -> List[Dict]:
        """
        融合向量检索和图检索结果

        Args:
            query: 查询文本
            vector_hits: 向量检索结果（需包含 chunk_id）
            top_k: 返回数量
            graph_weight: 图检索权重（0-1）

        Returns:
            融合后的检索结果
        """
        # 1. 从查询抽取种子实体
        seed_entities = self.get_seed_entities(query)

        # 2. 如果有种子实体，进行 PPR 扩展
        graph_scores = {}
        if seed_entities:
            ppr_results = self.personalized_pagerank(seed_entities, top_k=top_k * 2)
            max_score = ppr_results[0][1] if ppr_results else 1.0
            for chunk_id, score in ppr_results:
                graph_scores[chunk_id] = score / max_score  # 归一化到 0-1

        # 3. 融合向量检索分数和图检索分数
        fused_results = []
        for hit in vector_hits:
            chunk_id = hit.get("metadata", {}).get("chunk_id", "")
            vector_score = hit.get("score", 0)

            # 归一化向量分数（假设已经是 0-1 范围）
            graph_score = graph_scores.get(chunk_id, 0)

            # 加权融合
            fused_score = (1 - graph_weight) * vector_score + graph_weight * graph_score

            fused_hit = hit.copy()
            fused_hit["score"] = fused_score
            fused_hit["vector_score"] = vector_score
            fused_hit["graph_score"] = graph_score
            fused_results.append(fused_hit)

        # 4. 添加仅图检索命中的结果
        vector_chunk_ids = {h.get("metadata", {}).get("chunk_id", "") for h in vector_hits}
        for chunk_id, score in graph_scores.items():
            if chunk_id not in vector_chunk_ids:
                paper_id = self.G.nodes[chunk_id].get("paper_id", "")
                fused_results.append({
                    "chunk_id": chunk_id,
                    "metadata": {"chunk_id": chunk_id, "paper_id": paper_id},
                    "score": graph_weight * score,
                    "vector_score": 0,
                    "graph_score": score,
                    "source": "graph_only"
                })

        # 5. 排序并返回
        fused_results.sort(key=lambda x: x["score"], reverse=True)
        return fused_results[:top_k]

    # ========== 持久化 ==========

    def save(self, path: Path):
        """保存图谱到文件"""
        data = {
            "nodes": list(self.G.nodes(data=True)),
            "edges": list(self.G.edges(data=True)),
            "entities": {k: {"name": v.name, "type": v.type, "mentions": v.mentions}
                        for k, v in self.entities.items()},
            "chunk_to_entities": {k: list(v) for k, v in self.chunk_to_entities.items()},
            "entity_to_chunks": {k: list(v) for k, v in self.entity_to_chunks.items()},
        }
        with open(path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        self.logger.info(f"图谱已保存: {path}")

    def load(self, path: Path):
        """从文件加载图谱"""
        with open(path, "r", encoding="utf-8") as f:
            data = json.load(f)

        self.G = nx.DiGraph()
        for node, attrs in data["nodes"]:
            self.G.add_node(node, **attrs)
        for u, v, attrs in data["edges"]:
            self.G.add_edge(u, v, **attrs)

        self.entities = {
            k: Entity(name=v["name"], type=v["type"], mentions=v["mentions"])
            for k, v in data["entities"].items()
        }
        self.chunk_to_entities = defaultdict(set, {k: set(v) for k, v in data["chunk_to_entities"].items()})
        self.entity_to_chunks = defaultdict(set, {k: set(v) for k, v in data["entity_to_chunks"].items()})

        self.logger.info(f"图谱已加载: {self.G.number_of_nodes()} 节点, {self.G.number_of_edges()} 边")

    # ========== 统计 ==========

    def stats(self) -> Dict:
        """返回图谱统计信息"""
        entity_nodes = [n for n, d in self.G.nodes(data=True) if d.get("type") != "CHUNK"]
        chunk_nodes = [n for n, d in self.G.nodes(data=True) if d.get("type") == "CHUNK"]

        return {
            "total_nodes": self.G.number_of_nodes(),
            "total_edges": self.G.number_of_edges(),
            "entity_count": len(entity_nodes),
            "chunk_count": len(chunk_nodes),
            "entity_types": dict(defaultdict(int, {
                d.get("type"): 1 for _, d in self.G.nodes(data=True) if d.get("type") != "CHUNK"
            })),
        }


# 全局实例（懒加载）
_hippo_rag: Optional[HippoRAG] = None


def get_hippo_rag(graph_path: Optional[Path] = None) -> HippoRAG:
    """获取 HippoRAG 单例"""
    global _hippo_rag
    if _hippo_rag is None:
        _hippo_rag = HippoRAG(graph_path)
    return _hippo_rag
</file>

<file path="src/graphs/__init__.py">
"""LangGraph 工作流"""

from src.graphs.ingestion_graph import IngestionState, build_ingestion_graph

__all__ = ["IngestionState", "build_ingestion_graph"]
</file>

<file path="src/graphs/ingestion_graph.py">
"""
LangGraph 离线入库图：解析 PDF → 切块 → 向量化 → Milvus upsert → HippoRAG 建图
支持 chunk_id 主键 true upsert，重复跑同一批文档不产生重复数据。
"""

import json
from pathlib import Path
from typing import TypedDict, Literal, Any

from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.sqlite import SqliteSaver
try:
    from langchain_core.runnables import RunnableConfig
except ImportError:
    RunnableConfig = dict  # type: ignore

from src.chunking.chunker import ChunkConfig, chunk_blocks
from src.graph.hippo_rag import HippoRAG


def _truncate(content: str, max_len: int = 65000) -> str:
    return content[:max_len] if len(content) > max_len else content


class IngestionState(TypedDict, total=False):
    pdf_paths: list[str]
    current_index: int
    parsed_paths: dict[str, str]
    total_chunks: int
    total_upserted: int
    errors: list[dict]
    build_graph: bool
    run_id: str
    artifact_path: str | None


def _list_pdfs(state: IngestionState, *, config: RunnableConfig) -> dict:
    raw_papers = Path(config["configurable"]["raw_papers_path"])
    max_docs = config["configurable"].get("max_docs")
    paths = sorted(str(p) for p in raw_papers.glob("*.pdf"))
    if max_docs is not None:
        paths = paths[: max_docs]
    return {
        "pdf_paths": paths,
        "current_index": 0,
        "parsed_paths": {},
        "total_chunks": 0,
        "total_upserted": 0,
    }


def _parse_one_pdf(state: IngestionState, *, config: RunnableConfig) -> dict:
    cfg = config["configurable"]
    processor = cfg["processor"]
    skip_enrichment = cfg.get("skip_enrichment", True)
    parsed_dir = Path(cfg["parsed_dir"])
    pdf_paths = state["pdf_paths"]
    idx = state["current_index"]
    pdf_path = Path(pdf_paths[idx])
    paper_id = pdf_path.stem
    output_dir = parsed_dir / paper_id
    errors = list(state.get("errors") or [])

    try:
        processor.process(
            pdf_path,
            output_dir=output_dir,
            skip_enrichment=skip_enrichment,
        )
        parsed_paths = dict(state.get("parsed_paths") or {})
        parsed_paths[paper_id] = str(output_dir)
        return {"parsed_paths": parsed_paths}
    except Exception as e:
        errors.append({"paper_id": paper_id, "stage": "parse", "error": str(e)})
        return {"errors": errors}


def _chunk_embed_upsert_one_doc(state: IngestionState, *, config: RunnableConfig) -> dict:
    cfg = config["configurable"]
    embedder = cfg["embedder"]
    milvus = cfg["milvus"]
    collection_name = cfg["collection_name"]
    chunk_config = cfg.get("chunk_config") or {}
    pdf_paths = state["pdf_paths"]
    idx = state["current_index"]
    paper_id = Path(pdf_paths[idx]).stem
    parsed_paths = state.get("parsed_paths") or {}
    json_path = Path(parsed_paths.get(paper_id, "")) / "enriched.json"
    errors = list(state.get("errors") or [])

    out = {
        "total_chunks": state.get("total_chunks", 0),
        "total_upserted": state.get("total_upserted", 0),
        "current_index": idx + 1,
    }

    if not json_path.exists():
        errors.append({"paper_id": paper_id, "stage": "chunk_embed_upsert", "error": f"missing {json_path}"})
        return {**out, "errors": errors}

    try:
        with open(json_path, "r", encoding="utf-8") as f:
            doc = json.load(f)
        doc_id = doc.get("doc_id", paper_id)
        content_flow = doc.get("content_flow", [])
        ccfg = ChunkConfig(
            target_chars=chunk_config.get("target_chars", 1000),
            min_chars=chunk_config.get("min_chars", 200),
            max_chars=chunk_config.get("max_chars", 1800),
            overlap_sentences=chunk_config.get("overlap_sentences", 2),
            table_rows_per_chunk=chunk_config.get("table_rows_per_chunk", 10),
        )
        chunks = chunk_blocks(content_flow, doc_id=doc_id, config=ccfg)

        rows = []
        for c in chunks:
            text = _truncate(c.text)
            meta = c.meta or {}
            page_range = meta.get("page_range", [0, 0])
            page = page_range[0] if isinstance(page_range, (list, tuple)) else meta.get("page", 0)
            rows.append({
                "paper_id": doc_id,
                "chunk_id": c.chunk_id,
                "content": text,
                "raw_content": text,
                "domain": "global",
                "content_type": c.content_type,
                "chunk_type": ",".join(meta.get("block_types", []))[:64] or "paragraph",
                "section_path": str(meta.get("section_path", ""))[:512],
                "page": int(page) if isinstance(page, (int, float)) else 0,
                "_text_for_embed": text,
            })

        if not rows:
            return out

        texts = [r["_text_for_embed"] for r in rows]
        batch_size = 32
        for i in range(0, len(texts), batch_size):
            batch = texts[i : i + batch_size]
            emb = embedder.encode(batch)
            for k, j in enumerate(range(i, min(i + batch_size, len(rows)))):
                rows[j]["dense_vector"] = emb["dense"][k].tolist()
                sp = emb["sparse"]._getrow(k).tocoo()
                rows[j]["sparse_vector"] = {int(col): float(val) for col, val in zip(sp.col, sp.data)}
        for r in rows:
            del r["_text_for_embed"]

        upsert_batch = 100
        for start in range(0, len(rows), upsert_batch):
            batch = rows[start : start + upsert_batch]
            milvus.upsert(collection_name, batch)

        out["total_chunks"] = out["total_chunks"] + len(rows)
        out["total_upserted"] = out["total_upserted"] + len(rows)
    except Exception as e:
        errors.append({"paper_id": paper_id, "stage": "chunk_embed_upsert", "error": str(e)})
        out["errors"] = errors

    return out


def _route_after_upsert(state: IngestionState) -> Literal["parse_one_pdf", "build_hippo_graph", "write_artifact"]:
    pdf_paths = state.get("pdf_paths") or []
    idx = state.get("current_index", 0)
    if idx < len(pdf_paths):
        return "parse_one_pdf"
    if state.get("build_graph"):
        return "build_hippo_graph"
    return "write_artifact"


def _build_hippo_graph(state: IngestionState, *, config: RunnableConfig) -> dict:
    cfg = config["configurable"]
    parsed_dir = Path(cfg["parsed_dir"])
    graph_output_path = Path(cfg["graph_output_path"])
    chunk_config = cfg.get("chunk_config") or {}
    ccfg = ChunkConfig(
        target_chars=chunk_config.get("target_chars", 1000),
        min_chars=chunk_config.get("min_chars", 200),
        max_chars=chunk_config.get("max_chars", 1800),
        overlap_sentences=chunk_config.get("overlap_sentences", 2),
        table_rows_per_chunk=chunk_config.get("table_rows_per_chunk", 10),
    )
    hippo = HippoRAG()
    hippo.build_from_parsed_docs(parsed_dir, use_llm=False, chunk_config=ccfg)
    graph_output_path.parent.mkdir(parents=True, exist_ok=True)
    hippo.save(graph_output_path)
    return {}


def _write_artifact(state: IngestionState, *, config: RunnableConfig) -> dict:
    cfg = config["configurable"]
    run_id = state.get("run_id") or cfg.get("run_id", "")
    artifacts_dir = Path(cfg["artifacts_dir"])
    artifacts_dir.mkdir(parents=True, exist_ok=True)
    artifact_path = artifacts_dir / f"06_ingest_{run_id}.json"
    artifact = {
        "run_id": run_id,
        "input_count": len(state.get("pdf_paths") or []),
        "total_chunks": state.get("total_chunks", 0),
        "total_upserted": state.get("total_upserted", 0),
        "errors": state.get("errors") or [],
        "build_graph": state.get("build_graph", False),
    }
    with open(artifact_path, "w", encoding="utf-8") as f:
        json.dump(artifact, f, ensure_ascii=False, indent=2)
    return {"artifact_path": str(artifact_path)}


def _route_after_list(state: IngestionState) -> Literal["parse_one_pdf", "write_artifact"]:
    if state.get("pdf_paths"):
        return "parse_one_pdf"
    return "write_artifact"


def build_ingestion_graph(*, checkpointer: Any = None):
    """构建离线入库图。checkpointer 为 SqliteSaver 实例时支持可续跑；由调用方 with SqliteSaver.from_conn_string(path) as cp 传入。"""
    builder = StateGraph(IngestionState)

    builder.add_node("list_pdfs", _list_pdfs)
    builder.add_node("parse_one_pdf", _parse_one_pdf)
    builder.add_node("chunk_embed_upsert_one_doc", _chunk_embed_upsert_one_doc)
    builder.add_node("build_hippo_graph", _build_hippo_graph)
    builder.add_node("write_artifact", _write_artifact)

    builder.add_edge(START, "list_pdfs")
    builder.add_conditional_edges("list_pdfs", _route_after_list)
    builder.add_edge("parse_one_pdf", "chunk_embed_upsert_one_doc")
    builder.add_conditional_edges("chunk_embed_upsert_one_doc", _route_after_upsert)
    builder.add_edge("build_hippo_graph", "write_artifact")
    builder.add_edge("write_artifact", END)

    return builder.compile(checkpointer=checkpointer)
</file>

<file path="src/indexing/__init__.py">
from src.indexing.milvus_ops import milvus
from src.indexing.embedder import embedder

__all__ = ["milvus", "embedder"]
</file>

<file path="src/indexing/embedder.py">
"""
Embedding 服务封装
自动根据环境选择设备
"""

from pymilvus.model.hybrid import BGEM3EmbeddingFunction
from pymilvus.model.reranker import BGERerankFunction
from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)


class Embedder:
    """Embedding 服务（单例）"""

    _instance = None
    _ef = None
    _reranker = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @property
    def ef(self) -> BGEM3EmbeddingFunction:
        """BGE-M3 Embedding（懒加载）"""
        if self._ef is None:
            logger.info(f"加载 BGE-M3... (device={settings.model.device})")
            kwargs = {}
            if settings.model.embedding_cache_dir:
                kwargs["cache_dir"] = settings.model.embedding_cache_dir
            if settings.model.local_files_only:
                kwargs["local_files_only"] = True
            self._ef = BGEM3EmbeddingFunction(
                model_name=settings.model.embedding_model,
                device=settings.model.device,
                use_fp16=settings.model.use_fp16,
                **kwargs,
            )
            logger.info("[OK] BGE-M3 就绪")
        return self._ef

    @property
    def reranker(self) -> BGERerankFunction:
        """BGE-Reranker（懒加载）"""
        if self._reranker is None:
            logger.info(f"加载 BGE-Reranker... (device={settings.model.device})")
            kwargs = {}
            if settings.model.reranker_cache_dir:
                kwargs["cache_dir"] = settings.model.reranker_cache_dir
            if settings.model.local_files_only:
                kwargs["local_files_only"] = True
            self._reranker = BGERerankFunction(
                model_name=settings.model.reranker_model,
                device=settings.model.device,
                **kwargs,
            )
            logger.info("[OK] BGE-Reranker 就绪")
        return self._reranker

    def encode(self, texts: list) -> dict:
        """生成 Dense + Sparse 向量"""
        return self.ef(texts)

    def rerank(self, query: str, docs: list, top_k: int = None) -> list:
        """重排序"""
        top_k = top_k or settings.search.rerank_top_k
        return self.reranker(query, docs, top_k=top_k)


# 全局实例
embedder = Embedder()
</file>

<file path="src/indexing/ingest_job_store.py">
"""
Ingest 任务状态持久化（支持前端断连后重连查看进度）。
存储位置：src/data/ingest_jobs.db
"""

from __future__ import annotations

import json
import sqlite3
import time
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional


_DB_PATH = Path(__file__).resolve().parents[1] / "data" / "ingest_jobs.db"


def _db() -> sqlite3.Connection:
    _DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(str(_DB_PATH), timeout=30)
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA journal_mode=WAL")
    _init_schema(conn)
    return conn


def _init_schema(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS ingest_jobs (
            job_id          TEXT PRIMARY KEY,
            collection      TEXT NOT NULL,
            status          TEXT NOT NULL DEFAULT 'pending',
            total_files     INTEGER NOT NULL DEFAULT 0,
            processed_files INTEGER NOT NULL DEFAULT 0,
            failed_files    INTEGER NOT NULL DEFAULT 0,
            total_chunks    INTEGER NOT NULL DEFAULT 0,
            total_upserted  INTEGER NOT NULL DEFAULT 0,
            current_file    TEXT NOT NULL DEFAULT '',
            current_stage   TEXT NOT NULL DEFAULT '',
            message         TEXT NOT NULL DEFAULT '',
            error_message   TEXT NOT NULL DEFAULT '',
            payload_json    TEXT NOT NULL DEFAULT '{}',
            created_at      REAL NOT NULL,
            updated_at      REAL NOT NULL,
            finished_at     REAL
        )
        """
    )
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS ingest_job_events (
            id         INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id     TEXT NOT NULL,
            event      TEXT NOT NULL,
            data_json  TEXT NOT NULL DEFAULT '{}',
            created_at REAL NOT NULL
        )
        """
    )
    conn.execute(
        """
        CREATE INDEX IF NOT EXISTS idx_ingest_job_events_job_id_id
        ON ingest_job_events(job_id, id)
        """
    )
    conn.execute(
        """
        CREATE INDEX IF NOT EXISTS idx_ingest_jobs_created_at
        ON ingest_jobs(created_at DESC)
        """
    )
    conn.commit()


def create_job(collection: str, payload: Dict[str, Any], total_files: int) -> Dict[str, Any]:
    now = time.time()
    job_id = uuid.uuid4().hex
    with _db() as conn:
        conn.execute(
            """
            INSERT INTO ingest_jobs (
                job_id, collection, status, total_files, processed_files, failed_files,
                total_chunks, total_upserted, payload_json, created_at, updated_at
            ) VALUES (?, ?, 'pending', ?, 0, 0, 0, 0, ?, ?, ?)
            """,
            (job_id, collection, int(total_files), json.dumps(payload, ensure_ascii=False), now, now),
        )
        conn.commit()
    return get_job(job_id) or {}


def update_job(job_id: str, **fields: Any) -> Optional[Dict[str, Any]]:
    if not fields:
        return get_job(job_id)
    fields["updated_at"] = time.time()
    keys = list(fields.keys())
    set_expr = ", ".join([f"{k} = ?" for k in keys])
    vals = [fields[k] for k in keys]
    vals.append(job_id)
    with _db() as conn:
        conn.execute(f"UPDATE ingest_jobs SET {set_expr} WHERE job_id = ?", vals)
        conn.commit()
    return get_job(job_id)


def append_event(job_id: str, event: str, data: Dict[str, Any]) -> int:
    now = time.time()
    with _db() as conn:
        cur = conn.execute(
            """
            INSERT INTO ingest_job_events (job_id, event, data_json, created_at)
            VALUES (?, ?, ?, ?)
            """,
            (job_id, event, json.dumps(data, ensure_ascii=False), now),
        )
        conn.execute("UPDATE ingest_jobs SET updated_at = ? WHERE job_id = ?", (now, job_id))
        conn.commit()
        return int(cur.lastrowid or 0)


def get_job(job_id: str) -> Optional[Dict[str, Any]]:
    with _db() as conn:
        row = conn.execute("SELECT * FROM ingest_jobs WHERE job_id = ?", (job_id,)).fetchone()
        if not row:
            return None
        out = dict(row)
        try:
            out["payload"] = json.loads(out.get("payload_json") or "{}")
        except Exception:
            out["payload"] = {}
        out.pop("payload_json", None)
        return out


def list_jobs(limit: int = 20, status: Optional[str] = None) -> List[Dict[str, Any]]:
    limit = max(1, min(int(limit), 200))
    with _db() as conn:
        if status:
            rows = conn.execute(
                """
                SELECT * FROM ingest_jobs
                WHERE status = ?
                ORDER BY created_at DESC
                LIMIT ?
                """,
                (status, limit),
            ).fetchall()
        else:
            rows = conn.execute(
                """
                SELECT * FROM ingest_jobs
                ORDER BY created_at DESC
                LIMIT ?
                """,
                (limit,),
            ).fetchall()
        result: List[Dict[str, Any]] = []
        for r in rows:
            item = dict(r)
            try:
                item["payload"] = json.loads(item.get("payload_json") or "{}")
            except Exception:
                item["payload"] = {}
            item.pop("payload_json", None)
            result.append(item)
        return result


def list_events(job_id: str, after_id: int = 0, limit: int = 500) -> List[Dict[str, Any]]:
    after_id = max(0, int(after_id))
    limit = max(1, min(int(limit), 2000))
    with _db() as conn:
        rows = conn.execute(
            """
            SELECT id, event, data_json, created_at
            FROM ingest_job_events
            WHERE job_id = ? AND id > ?
            ORDER BY id ASC
            LIMIT ?
            """,
            (job_id, after_id, limit),
        ).fetchall()
    out: List[Dict[str, Any]] = []
    for r in rows:
        data: Dict[str, Any]
        try:
            data = json.loads(r["data_json"] or "{}")
        except Exception:
            data = {}
        data["event_id"] = int(r["id"])
        out.append(
            {
                "event_id": int(r["id"]),
                "event": str(r["event"]),
                "created_at": float(r["created_at"]),
                "data": data,
            }
        )
    return out
</file>

<file path="src/indexing/paper_metadata_store.py">
"""
论文元数据持久化存储 (SQLite)

提供 DOI / title / authors / year 的持久化查询与写入，
替代原来的 paper_metadata.json 平面文件。

特性:
- SQLite 标准库，零外部依赖
- DOI + normalized_title 双索引，O(1) 查询
- 线程安全（SQLite WAL 模式 + check_same_thread=False）
- 首次启动自动迁移旧 paper_metadata.json
- 单例模式，全局复用

使用:
    from src.indexing.paper_metadata_store import paper_meta_store

    paper_meta_store.upsert("paper_id", doi="10.1038/xxx", title="Full Title", ...)
    row = paper_meta_store.get("paper_id")
    doi_set = paper_meta_store.all_dois()
    title_set = paper_meta_store.all_normalized_titles()
"""

from __future__ import annotations

import json
import logging
import re
import sqlite3
import threading
from pathlib import Path
from typing import Any, Dict, List, Optional, Set, Tuple

logger = logging.getLogger(__name__)

_DB_PATH = Path("data/paper_metadata.db")
_JSON_PATH = Path("data/paper_metadata.json")


def _normalize_doi(doi: Optional[str]) -> str:
    if not doi or not isinstance(doi, str):
        return ""
    d = doi.strip().lower()
    d = re.sub(r"^https?://(?:dx\.)?doi\.org/", "", d)
    return d.rstrip("/.")


def _normalize_title(title: Optional[str]) -> str:
    if not title or not isinstance(title, str):
        return ""
    return re.sub(r"[^a-z0-9]+", "", title.lower())


class PaperMetadataStore:
    """SQLite-backed paper metadata store (singleton)."""

    _instance: Optional["PaperMetadataStore"] = None
    _lock = threading.Lock()

    def __new__(cls, db_path: Optional[Path] = None):
        with cls._lock:
            if cls._instance is None:
                inst = super().__new__(cls)
                inst._initialized = False
                cls._instance = inst
            return cls._instance

    def __init__(self, db_path: Optional[Path] = None):
        if self._initialized:
            return
        self._db_path = db_path or _DB_PATH
        self._db_path.parent.mkdir(parents=True, exist_ok=True)
        self._conn = sqlite3.connect(
            str(self._db_path),
            check_same_thread=False,
            isolation_level="DEFERRED",
        )
        self._conn.execute("PRAGMA journal_mode=WAL")
        self._conn.execute("PRAGMA synchronous=NORMAL")
        self._create_table()
        self._maybe_migrate_json()
        self._initialized = True
        count = self._conn.execute("SELECT COUNT(*) FROM paper_metadata").fetchone()[0]
        logger.info("PaperMetadataStore ready: %s (%d entries)", self._db_path, count)

    def _create_table(self) -> None:
        self._conn.executescript("""
            CREATE TABLE IF NOT EXISTS paper_metadata (
                paper_id        TEXT PRIMARY KEY,
                doi             TEXT DEFAULT '',
                normalized_doi  TEXT DEFAULT '',
                title           TEXT DEFAULT '',
                normalized_title TEXT DEFAULT '',
                authors         TEXT DEFAULT '',
                year            INTEGER,
                source          TEXT DEFAULT '',
                extra           TEXT DEFAULT '{}'
            );
            CREATE INDEX IF NOT EXISTS idx_pm_ndoi ON paper_metadata(normalized_doi) WHERE normalized_doi != '';
            CREATE INDEX IF NOT EXISTS idx_pm_ntitle ON paper_metadata(normalized_title) WHERE normalized_title != '';

            CREATE TABLE IF NOT EXISTS crossref_cache (
                normalized_title TEXT PRIMARY KEY,
                doi              TEXT DEFAULT '',
                title            TEXT DEFAULT '',
                authors          TEXT DEFAULT '',
                year             INTEGER,
                venue            TEXT DEFAULT '',
                created_at       REAL DEFAULT (julianday('now'))
            );
        """)

    def _maybe_migrate_json(self) -> None:
        """首次启动：若 SQLite 为空且旧 JSON 存在，自动导入。"""
        count = self._conn.execute("SELECT COUNT(*) FROM paper_metadata").fetchone()[0]
        if count > 0:
            return
        if not _JSON_PATH.exists():
            return
        try:
            with open(_JSON_PATH, "r", encoding="utf-8") as f:
                data: Dict[str, Dict[str, Any]] = json.load(f)
            if not data:
                return
            rows = []
            for paper_id, meta in data.items():
                doi = meta.get("doi") or ""
                title = meta.get("title") or ""
                authors = meta.get("authors")
                year = meta.get("year")
                source = meta.get("source") or ""
                authors_str = json.dumps(authors, ensure_ascii=False) if authors else ""
                rows.append((
                    paper_id,
                    doi,
                    _normalize_doi(doi),
                    title,
                    _normalize_title(title),
                    authors_str,
                    int(year) if year else None,
                    source,
                    "{}",
                ))
            self._conn.executemany(
                "INSERT OR IGNORE INTO paper_metadata "
                "(paper_id, doi, normalized_doi, title, normalized_title, authors, year, source, extra) "
                "VALUES (?,?,?,?,?,?,?,?,?)",
                rows,
            )
            self._conn.commit()
            logger.info("Migrated %d entries from paper_metadata.json -> SQLite", len(rows))
        except Exception as e:
            logger.warning("Failed to migrate paper_metadata.json: %s", e)

    # ── 写入 ──────────────────────────────────────────

    def upsert(
        self,
        paper_id: str,
        doi: Optional[str] = None,
        title: Optional[str] = None,
        authors: Optional[List[str]] = None,
        year: Optional[int] = None,
        source: Optional[str] = None,
        extra: Optional[Dict[str, Any]] = None,
    ) -> None:
        """插入或更新一条论文元数据。"""
        existing = self.get(paper_id)
        doi = doi or (existing.get("doi") if existing else "") or ""
        title = title or (existing.get("title") if existing else "") or ""
        authors_str = json.dumps(authors, ensure_ascii=False) if authors else (existing.get("authors_raw") if existing else "") or ""
        yr = year or (existing.get("year") if existing else None)
        src = source or (existing.get("source") if existing else "") or ""
        ext = json.dumps(extra or {}, ensure_ascii=False) if extra else (existing.get("extra_raw") if existing else "{}") or "{}"

        self._conn.execute(
            "INSERT INTO paper_metadata "
            "(paper_id, doi, normalized_doi, title, normalized_title, authors, year, source, extra) "
            "VALUES (?,?,?,?,?,?,?,?,?) "
            "ON CONFLICT(paper_id) DO UPDATE SET "
            "doi=excluded.doi, normalized_doi=excluded.normalized_doi, "
            "title=excluded.title, normalized_title=excluded.normalized_title, "
            "authors=excluded.authors, year=excluded.year, "
            "source=excluded.source, extra=excluded.extra",
            (
                paper_id,
                doi,
                _normalize_doi(doi),
                title,
                _normalize_title(title),
                authors_str,
                yr,
                src,
                ext,
            ),
        )
        self._conn.commit()

    def upsert_batch(self, records: List[Tuple[str, Dict[str, Any]]]) -> int:
        """批量 upsert [(paper_id, {doi, title, authors, year, source}), ...]。"""
        rows = []
        for paper_id, meta in records:
            doi = meta.get("doi") or ""
            title = meta.get("title") or ""
            authors = meta.get("authors")
            authors_str = json.dumps(authors, ensure_ascii=False) if authors else ""
            year = meta.get("year")
            source = meta.get("source") or ""
            rows.append((
                paper_id,
                doi,
                _normalize_doi(doi),
                title,
                _normalize_title(title),
                authors_str,
                int(year) if year else None,
                source,
                "{}",
            ))
        self._conn.executemany(
            "INSERT INTO paper_metadata "
            "(paper_id, doi, normalized_doi, title, normalized_title, authors, year, source, extra) "
            "VALUES (?,?,?,?,?,?,?,?,?) "
            "ON CONFLICT(paper_id) DO UPDATE SET "
            "doi=excluded.doi, normalized_doi=excluded.normalized_doi, "
            "title=excluded.title, normalized_title=excluded.normalized_title, "
            "authors=excluded.authors, year=excluded.year, "
            "source=excluded.source, extra=excluded.extra",
            rows,
        )
        self._conn.commit()
        return len(rows)

    # ── 查询 ──────────────────────────────────────────

    def get(self, paper_id: str) -> Optional[Dict[str, Any]]:
        """按 paper_id 查询单条。"""
        row = self._conn.execute(
            "SELECT paper_id, doi, title, authors, year, source, extra "
            "FROM paper_metadata WHERE paper_id = ?",
            (paper_id,),
        ).fetchone()
        if not row:
            return None
        return self._row_to_dict(row)

    def get_by_doi(self, doi: str) -> Optional[Dict[str, Any]]:
        """按 DOI 查询。"""
        ndoi = _normalize_doi(doi)
        if not ndoi:
            return None
        row = self._conn.execute(
            "SELECT paper_id, doi, title, authors, year, source, extra "
            "FROM paper_metadata WHERE normalized_doi = ? LIMIT 1",
            (ndoi,),
        ).fetchone()
        if not row:
            return None
        return self._row_to_dict(row)

    def has_doi(self, doi: str) -> bool:
        ndoi = _normalize_doi(doi)
        if not ndoi:
            return False
        r = self._conn.execute(
            "SELECT 1 FROM paper_metadata WHERE normalized_doi = ? LIMIT 1", (ndoi,)
        ).fetchone()
        return r is not None

    def has_title(self, title: str) -> bool:
        nt = _normalize_title(title)
        if not nt:
            return False
        r = self._conn.execute(
            "SELECT 1 FROM paper_metadata WHERE normalized_title = ? LIMIT 1", (nt,)
        ).fetchone()
        return r is not None

    def all_dois(self) -> Set[str]:
        """返回所有非空的 normalized_doi 集合。"""
        rows = self._conn.execute(
            "SELECT DISTINCT normalized_doi FROM paper_metadata WHERE normalized_doi != ''"
        ).fetchall()
        return {r[0] for r in rows}

    def all_normalized_titles(self) -> Set[str]:
        """返回所有非空的 normalized_title 集合。"""
        rows = self._conn.execute(
            "SELECT DISTINCT normalized_title FROM paper_metadata WHERE normalized_title != ''"
        ).fetchall()
        return {r[0] for r in rows}

    def count(self) -> int:
        return self._conn.execute("SELECT COUNT(*) FROM paper_metadata").fetchone()[0]

    def all_paper_ids_with_doi(self) -> Set[str]:
        """返回已有 DOI 的 paper_id 集合（用于 backfill 跳过已处理的）。"""
        rows = self._conn.execute(
            "SELECT paper_id FROM paper_metadata WHERE normalized_doi != ''"
        ).fetchall()
        return {r[0] for r in rows}

    def _row_to_dict(self, row: tuple) -> Dict[str, Any]:
        paper_id, doi, title, authors_str, year, source, extra_str = row
        authors = None
        if authors_str:
            try:
                authors = json.loads(authors_str)
            except Exception:
                pass
        return {
            "paper_id": paper_id,
            "doi": doi,
            "title": title,
            "authors": authors,
            "year": year,
            "source": source,
            "authors_raw": authors_str,
            "extra_raw": extra_str,
        }

    # ── CrossRef 缓存 ─────────────────────────────────────

    def crossref_get(self, title: str) -> Optional[Dict[str, Any]]:
        """按 normalized_title 查 CrossRef 缓存，命中返回 dict，未命中返回 None。"""
        nt = _normalize_title(title)
        if not nt:
            return None
        row = self._conn.execute(
            "SELECT doi, title, authors, year, venue FROM crossref_cache WHERE normalized_title = ?",
            (nt,),
        ).fetchone()
        if not row:
            return None
        doi, cr_title, authors_str, year, venue = row
        if not doi:
            return None
        authors = None
        if authors_str:
            try:
                authors = json.loads(authors_str)
            except Exception:
                pass
        return {"doi": doi, "title": cr_title, "authors": authors, "year": year, "venue": venue}

    def crossref_put(self, title: str, result: Optional[Dict[str, Any]]) -> None:
        """写入 CrossRef 查询结果到缓存。result=None 表示查询无结果（负缓存）。"""
        nt = _normalize_title(title)
        if not nt:
            return
        if result:
            doi = result.get("doi") or ""
            cr_title = result.get("title") or ""
            authors = result.get("authors")
            authors_str = json.dumps(authors, ensure_ascii=False) if authors else ""
            year = result.get("year")
            venue = result.get("venue") or ""
        else:
            doi = cr_title = authors_str = venue = ""
            year = None
        self._conn.execute(
            "INSERT INTO crossref_cache (normalized_title, doi, title, authors, year, venue) "
            "VALUES (?,?,?,?,?,?) "
            "ON CONFLICT(normalized_title) DO UPDATE SET "
            "doi=excluded.doi, title=excluded.title, authors=excluded.authors, "
            "year=excluded.year, venue=excluded.venue, created_at=julianday('now')",
            (nt, doi, cr_title, authors_str, year, venue),
        )
        self._conn.commit()

    def crossref_has(self, title: str) -> bool:
        """是否已缓存（包括负缓存）。"""
        nt = _normalize_title(title)
        if not nt:
            return False
        r = self._conn.execute(
            "SELECT 1 FROM crossref_cache WHERE normalized_title = ? LIMIT 1", (nt,)
        ).fetchone()
        return r is not None

    def close(self) -> None:
        if self._conn:
            self._conn.close()


# 全局单例
paper_meta_store = PaperMetadataStore()
</file>

<file path="src/indexing/paper_store.py">
"""
Paper 元数据持久化：记录每个集合中入库的文件信息，支持文件级查询和删除。
存储位置：src/data/papers.db
"""

import sqlite3
import time
from pathlib import Path
from typing import List, Optional

from src.log import get_logger

logger = get_logger(__name__)

_DB_PATH = Path(__file__).resolve().parents[1] / "data" / "papers.db"


def _db():
    _DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(str(_DB_PATH))
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA journal_mode=WAL")
    _init_schema(conn)
    return conn


def _init_schema(conn: sqlite3.Connection):
    conn.execute("""
        CREATE TABLE IF NOT EXISTS papers (
            id            INTEGER PRIMARY KEY AUTOINCREMENT,
            collection    TEXT    NOT NULL,
            paper_id      TEXT    NOT NULL,
            filename      TEXT    NOT NULL DEFAULT '',
            file_path     TEXT    NOT NULL DEFAULT '',
            file_size     INTEGER NOT NULL DEFAULT 0,
            chunk_count   INTEGER NOT NULL DEFAULT 0,
            row_count     INTEGER NOT NULL DEFAULT 0,
            enrich_tables_enabled INTEGER NOT NULL DEFAULT 0,
            enrich_figures_enabled INTEGER NOT NULL DEFAULT 0,
            table_count   INTEGER NOT NULL DEFAULT 0,
            figure_count  INTEGER NOT NULL DEFAULT 0,
            table_success INTEGER NOT NULL DEFAULT 0,
            figure_success INTEGER NOT NULL DEFAULT 0,
            status        TEXT    NOT NULL DEFAULT 'done',
            error_message TEXT    DEFAULT '',
            created_at    REAL    NOT NULL,
            UNIQUE(collection, paper_id)
        )
    """)
    conn.execute("""
        CREATE INDEX IF NOT EXISTS idx_papers_collection
        ON papers (collection)
    """)
    try:
        conn.execute("ALTER TABLE papers ADD COLUMN content_hash TEXT")
    except sqlite3.OperationalError:
        pass  # column already exists
    for col_sql in [
        "ALTER TABLE papers ADD COLUMN enrich_tables_enabled INTEGER NOT NULL DEFAULT 0",
        "ALTER TABLE papers ADD COLUMN enrich_figures_enabled INTEGER NOT NULL DEFAULT 0",
        "ALTER TABLE papers ADD COLUMN table_count INTEGER NOT NULL DEFAULT 0",
        "ALTER TABLE papers ADD COLUMN figure_count INTEGER NOT NULL DEFAULT 0",
        "ALTER TABLE papers ADD COLUMN table_success INTEGER NOT NULL DEFAULT 0",
        "ALTER TABLE papers ADD COLUMN figure_success INTEGER NOT NULL DEFAULT 0",
    ]:
        try:
            conn.execute(col_sql)
        except sqlite3.OperationalError:
            pass
    conn.commit()


# ── 写入 ──

def upsert_paper(
    collection: str,
    paper_id: str,
    filename: str = "",
    file_path: str = "",
    file_size: int = 0,
    chunk_count: int = 0,
    row_count: int = 0,
    enrich_tables_enabled: bool = False,
    enrich_figures_enabled: bool = False,
    table_count: int = 0,
    figure_count: int = 0,
    table_success: int = 0,
    figure_success: int = 0,
    status: str = "done",
    error_message: str = "",
    content_hash: str = "",
):
    """插入或更新 paper 记录"""
    now = time.time()
    with _db() as conn:
        conn.execute("""
            INSERT INTO papers (collection, paper_id, filename, file_path, file_size,
                                chunk_count, row_count,
                                enrich_tables_enabled, enrich_figures_enabled,
                                table_count, figure_count, table_success, figure_success,
                                status, error_message, created_at, content_hash)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ON CONFLICT(collection, paper_id) DO UPDATE SET
                filename      = excluded.filename,
                file_path     = excluded.file_path,
                file_size     = CASE WHEN excluded.file_size > 0 THEN excluded.file_size ELSE file_size END,
                chunk_count   = CASE WHEN excluded.chunk_count > 0 THEN excluded.chunk_count ELSE chunk_count END,
                row_count     = CASE WHEN excluded.row_count > 0 THEN excluded.row_count ELSE row_count END,
                enrich_tables_enabled = excluded.enrich_tables_enabled,
                enrich_figures_enabled = excluded.enrich_figures_enabled,
                table_count   = excluded.table_count,
                figure_count  = excluded.figure_count,
                table_success = excluded.table_success,
                figure_success = excluded.figure_success,
                status        = excluded.status,
                error_message = excluded.error_message,
                created_at    = CASE WHEN status = 'done' THEN created_at ELSE excluded.created_at END,
                content_hash  = excluded.content_hash
        """, (
            collection, paper_id, filename, file_path, file_size,
            chunk_count, row_count,
            int(bool(enrich_tables_enabled)), int(bool(enrich_figures_enabled)),
            int(table_count), int(figure_count), int(table_success), int(figure_success),
            status, error_message, now, content_hash or None
        ))
        conn.commit()


# ── 查询 ──

def list_papers(collection: str) -> List[dict]:
    """列出指定集合中的所有 paper"""
    with _db() as conn:
        rows = conn.execute("""
            SELECT paper_id, filename, file_size, chunk_count, row_count,
                   enrich_tables_enabled, enrich_figures_enabled,
                   table_count, figure_count, table_success, figure_success,
                   status, error_message, created_at, content_hash
            FROM papers
            WHERE collection = ?
            ORDER BY created_at DESC
        """, (collection,)).fetchall()
        return [dict(r) for r in rows]


def get_paper(collection: str, paper_id: str) -> Optional[dict]:
    """获取单个 paper 信息"""
    with _db() as conn:
        row = conn.execute("""
            SELECT paper_id, filename, file_size, chunk_count, row_count,
                   enrich_tables_enabled, enrich_figures_enabled,
                   table_count, figure_count, table_success, figure_success,
                   status, error_message, created_at, content_hash
            FROM papers
            WHERE collection = ? AND paper_id = ?
        """, (collection, paper_id)).fetchone()
        return dict(row) if row else None


# ── 删除 ──

def delete_paper(collection: str, paper_id: str) -> bool:
    """删除 paper 记录"""
    with _db() as conn:
        cur = conn.execute(
            "DELETE FROM papers WHERE collection = ? AND paper_id = ?",
            (collection, paper_id),
        )
        conn.commit()
        return cur.rowcount > 0


def delete_collection_papers(collection: str) -> int:
    """删除整个集合的所有 paper 记录"""
    with _db() as conn:
        cur = conn.execute(
            "DELETE FROM papers WHERE collection = ?",
            (collection,),
        )
        conn.commit()
        return cur.rowcount
</file>

<file path="src/llm/__init__.py">
"""
LLM 统一管理模块

提供:
- LLMManager: 配置加载与客户端获取
- BaseChatClient: 统一调用接口
- RawLogStore: 原始响应日志
"""

from .llm_manager import (
    LLMManager,
    BaseChatClient,
    HTTPChatClient,
    DryRunChatClient,
    RawLogStore,
    ProviderConfig,
    LLMConfig,
)

__all__ = [
    "LLMManager",
    "BaseChatClient",
    "HTTPChatClient",
    "DryRunChatClient",
    "RawLogStore",
    "ProviderConfig",
    "LLMConfig",
]
</file>

<file path="src/log/__init__.py">
"""统一日志管理：分级、按运行实例命名、自动清理。"""
from .log_manager import (
    LogManager,
    get_logger,
    init_logging,
    cleanup_logs,
)

__all__ = ["LogManager", "get_logger", "init_logging", "cleanup_logs"]
</file>

<file path="src/log/log_manager.py">
"""
日志管理模块：分级日志、按运行实例命名、自动清理。
"""
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any

# 默认配置
DEFAULT_LEVEL = "INFO"
DEFAULT_MAX_SIZE_MB = 100
DEFAULT_MAX_AGE_DAYS = 30
DEFAULT_MIN_KEEP_MB = 20
DEFAULT_CONSOLE_OUTPUT = True
LOG_DIR_NAME = "app"


class LogManager:
    """
    统一日志管理：分级（DEBUG/INFO/WARNING/ERROR）、按运行实例命名文件、
    控制台+文件双输出、按配置自动清理（上限 100MB，低于 20MB 不删）。
    """

    def __init__(self, config: dict[str, Any] | None = None):
        config = config or {}
        base = Path(__file__).resolve().parent.parent.parent
        self.log_dir = Path(config.get("log_dir")) if config.get("log_dir") else base / "logs" / LOG_DIR_NAME
        self.log_dir.mkdir(parents=True, exist_ok=True)

        self.max_size_mb = int(config.get("max_size_mb", DEFAULT_MAX_SIZE_MB))
        self.max_age_days = int(config.get("max_age_days", DEFAULT_MAX_AGE_DAYS))
        self.min_keep_mb = int(config.get("min_keep_mb", DEFAULT_MIN_KEEP_MB))
        self.console_output = config.get("console_output", DEFAULT_CONSOLE_OUTPUT)
        level_name = (config.get("level") or DEFAULT_LEVEL).upper()
        self.level = getattr(logging, level_name, logging.INFO)

        self._run_log_path: Path | None = None
        self._formatter = logging.Formatter(
            "%(asctime)s.%(msecs)03d | %(levelname)s | %(name)s | %(message)s",
            datefmt="%Y-%m-%d %H:%M:%S",
        )
        self._formatter.default_msec_format = "%s.%03d"

    def _run_file(self) -> Path:
        """当前运行的日志文件路径（按启动时间命名，进程内复用）."""
        if self._run_log_path is None:
            self._run_log_path = self.log_dir / datetime.now().strftime("%Y-%m-%d_%H-%M-%S.log")
        return self._run_log_path

    def get_logger(self, name: str) -> logging.Logger:
        """获取具名 logger，自动绑定当前运行日志文件与控制台."""
        logger = logging.getLogger(name)
        if logger.handlers:
            return logger

        logger.setLevel(self.level)
        logger.propagate = False

        if self.console_output:
            ch = logging.StreamHandler()
            ch.setLevel(self.level)
            ch.setFormatter(self._formatter)
            logger.addHandler(ch)

        fh = logging.FileHandler(self._run_file(), encoding="utf-8")
        fh.setLevel(self.level)
        fh.setFormatter(self._formatter)
        logger.addHandler(fh)

        return logger

    def cleanup(self) -> dict[str, Any]:
        """
        按配置清理日志文件。
        - 总大小 < min_keep_mb 时不删除。
        - 先删超过 max_age_days 的文件，再按最旧优先删至不超过 max_size_mb。
        """
        report: dict[str, Any] = {"deleted_by_age": [], "deleted_by_size": [], "remaining_mb": 0.0}
        if not self.log_dir.exists():
            return report

        log_files = sorted(
            (f for f in self.log_dir.iterdir() if f.is_file() and f.suffix == ".log"),
            key=lambda p: p.stat().st_mtime,
        )
        min_bytes = self.min_keep_mb * 1024 * 1024
        total = sum(f.stat().st_size for f in log_files)
        if total < min_bytes:
            report["remaining_mb"] = total / (1024 * 1024)
            return report

        cutoff = datetime.now() - timedelta(days=self.max_age_days)
        remaining: list[Path] = []
        for f in log_files:
            mtime = datetime.fromtimestamp(f.stat().st_mtime)
            if mtime < cutoff:
                report["deleted_by_age"].append(f.name)
                f.unlink()
            else:
                remaining.append(f)

        max_bytes = self.max_size_mb * 1024 * 1024
        while remaining:
            total = sum(f.stat().st_size for f in remaining)
            if total <= max_bytes:
                break
            oldest = remaining.pop(0)
            report["deleted_by_size"].append(oldest.name)
            oldest.unlink()

        if remaining:
            report["remaining_mb"] = sum(f.stat().st_size for f in remaining) / (1024 * 1024)
        return report


# 模块级单例，便于 get_logger 使用
_manager: LogManager | None = None


def _get_manager(config: dict[str, Any] | None = None) -> LogManager:
    global _manager
    if _manager is None:
        _manager = LogManager(config)
    return _manager


def _load_logging_config(path: Path) -> dict[str, Any]:
    import json
    if not path.exists():
        return {}
    raw = json.loads(path.read_text(encoding="utf-8"))
    cfg = raw.get("logging") or {}
    local_path = path.with_name(f"{path.stem}.local{path.suffix}")
    if local_path.exists():
        local_raw = json.loads(local_path.read_text(encoding="utf-8"))
        local_cfg = local_raw.get("logging") or {}
        if local_cfg:
            cfg = {**cfg, **local_cfg}
    return cfg


def init_logging(config: dict[str, Any] | None = None, config_path: str | Path | None = None) -> LogManager:
    """初始化日志（可选从配置文件加载）. 显式调用时可传入 config 或 config_path."""
    cfg = config
    if cfg is None and config_path is not None:
        path = Path(config_path)
        cfg = _load_logging_config(path)
    elif cfg is None:
        default_path = Path(__file__).resolve().parent.parent.parent / "config" / "rag_config.json"
        cfg = _load_logging_config(default_path)
    global _manager
    _manager = LogManager(cfg)
    return _manager


def get_logger(name: str, config: dict[str, Any] | None = None) -> logging.Logger:
    """获取 logger。若尚未初始化则用 config 或 rag_config.json 的 logging 段初始化."""
    if _manager is None:
        init_logging(config=config)
    return _get_manager().get_logger(name)


def cleanup_logs(config: dict[str, Any] | None = None) -> dict[str, Any]:
    """执行日志清理，返回清理报告。若未初始化则用默认或 config 初始化 LogManager 再清理."""
    if _manager is None:
        init_logging(config=config)
    return _get_manager().cleanup()
</file>

<file path="src/mcp/__init__.py">
"""
MCP (Model Context Protocol) Server。

将 RAG 系统的 Tool 和 Resource 暴露给 Claude Desktop / Cursor 等 MCP 客户端。
"""
</file>

<file path="src/observability/metrics.py">
"""
Prometheus metrics 定义。

所有自定义指标集中定义，业务模块通过 `from src.observability import metrics` 引用。
"""

from prometheus_client import Counter, Histogram, Gauge, Info


class _Metrics:
    """集中管理所有 Prometheus 指标"""

    def __init__(self):
        # ── HTTP 请求 ──
        self.http_requests_total = Counter(
            "rag_http_requests_total",
            "HTTP 请求总数",
            ["method", "endpoint", "status_code"],
        )
        self.http_request_duration_seconds = Histogram(
            "rag_http_request_duration_seconds",
            "HTTP 请求延迟 (秒)",
            ["method", "endpoint"],
            buckets=(0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0, 60.0),
        )

        # ── 检索 ──
        self.retrieval_total = Counter(
            "rag_retrieval_total",
            "检索请求总数",
            ["mode"],  # local / web / hybrid
        )
        self.retrieval_duration_seconds = Histogram(
            "rag_retrieval_duration_seconds",
            "检索延迟 (秒)",
            ["mode"],
            buckets=(0.1, 0.25, 0.5, 1.0, 2.5, 5.0, 10.0, 30.0),
        )
        self.retrieval_chunks_returned = Histogram(
            "rag_retrieval_chunks_returned",
            "每次检索返回的 chunk 数",
            ["mode"],
            buckets=(0, 1, 3, 5, 10, 20, 50),
        )

        # ── LLM ──
        self.llm_requests_total = Counter(
            "rag_llm_requests_total",
            "LLM 调用总数",
            ["provider", "model"],
        )
        self.llm_duration_seconds = Histogram(
            "rag_llm_duration_seconds",
            "LLM 调用延迟 (秒)",
            ["provider", "model"],
            buckets=(0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0),
        )
        self.llm_tokens_used = Counter(
            "rag_llm_tokens_total",
            "LLM token 消耗",
            ["provider", "model", "direction"],  # direction: input / output
        )
        self.llm_errors_total = Counter(
            "rag_llm_errors_total",
            "LLM 调用失败数",
            ["provider", "model"],
        )

        # ── 内容抓取 ──
        self.content_fetch_total = Counter(
            "rag_content_fetch_total",
            "全文抓取总数",
            ["strategy", "success"],  # strategy: trafilatura / brightdata / playwright
        )

        # ── Ingest ──
        self.ingest_total = Counter(
            "rag_ingest_total",
            "文档 ingest 总数",
            ["status"],  # success / failed
        )
        self.ingest_duration_seconds = Histogram(
            "rag_ingest_duration_seconds",
            "单文档 ingest 延迟 (秒)",
            buckets=(1, 5, 10, 30, 60, 120, 300),
        )

        # ── 系统 ──
        self.active_connections = Gauge(
            "rag_active_connections",
            "当前活跃 SSE 连接数",
        )
        self.app_info = Info(
            "rag_app",
            "应用元信息",
        )


# 单例
metrics = _Metrics()
</file>

<file path="src/observability/middleware.py">
"""
FastAPI 中间件：自动采集 HTTP 请求延迟 / 计数 / 状态码，并注入 trace context。
"""

import time
from starlette.middleware.base import BaseHTTPMiddleware
from starlette.requests import Request
from starlette.responses import Response

from src.observability.metrics import metrics
from src.observability.tracing import tracer


def _normalize_path(path: str) -> str:
    """
    将 path 中的动态 ID 替换为占位符，防止高基数指标。
    e.g. /canvas/abc123/snapshots → /canvas/{id}/snapshots
    """
    parts = path.strip("/").split("/")
    normalized = []
    skip_next = False
    for i, part in enumerate(parts):
        if skip_next:
            normalized.append("{id}")
            skip_next = False
            continue
        # 常见的 RESTful 资源路径：下一个 segment 是 ID
        if part in ("canvas", "projects", "sessions", "graph"):
            normalized.append(part)
            skip_next = True
        else:
            normalized.append(part)
    return "/" + "/".join(normalized)


class ObservabilityMiddleware(BaseHTTPMiddleware):
    """采集每个 HTTP 请求的延迟和计数指标，并创建 trace span。"""

    async def dispatch(self, request: Request, call_next):
        method = request.method
        path = _normalize_path(request.url.path)

        # 跳过 /metrics 和 /health 本身，避免自引用噪音
        if request.url.path in ("/metrics", "/health"):
            return await call_next(request)

        with tracer.start_as_current_span(
            f"{method} {path}",
            attributes={"http.method": method, "http.url": str(request.url)},
        ) as span:
            start = time.perf_counter()
            response: Response = await call_next(request)
            elapsed = time.perf_counter() - start

            status_code = str(response.status_code)
            span.set_attribute("http.status_code", response.status_code)

            metrics.http_requests_total.labels(
                method=method, endpoint=path, status_code=status_code
            ).inc()
            metrics.http_request_duration_seconds.labels(
                method=method, endpoint=path
            ).observe(elapsed)

            return response
</file>

<file path="src/observability/setup.py">
"""
一键初始化 Observability：注册中间件 + /metrics 端点 + 应用元信息。
"""

from fastapi import FastAPI
from fastapi.responses import PlainTextResponse
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST

from src.observability.middleware import ObservabilityMiddleware
from src.observability.metrics import metrics
from src.log import get_logger

logger = get_logger(__name__)


def setup_observability(app: FastAPI) -> None:
    """
    在 FastAPI app 上挂载 Observability 组件。

    应在 router 注册之后、启动之前调用。
    """
    # 1. 注册中间件
    app.add_middleware(ObservabilityMiddleware)

    # 2. 注册 /metrics 端点（Prometheus 拉取）
    @app.get("/metrics", include_in_schema=False)
    def prometheus_metrics():
        return PlainTextResponse(
            content=generate_latest(),
            media_type=CONTENT_TYPE_LATEST,
        )

    # 3. 增强 /health 端点（带组件状态）
    @app.get("/health/detailed", tags=["observability"])
    def health_detailed():
        """详细健康检查：各组件可达性"""
        checks = {}

        # Milvus
        try:
            from src.indexing.milvus_ops import milvus
            milvus.list_collections()
            checks["milvus"] = "ok"
        except Exception as e:
            checks["milvus"] = f"error: {e}"

        # LLM
        try:
            from src.llm.llm_manager import get_manager
            m = get_manager()
            checks["llm"] = "ok" if m else "not_configured"
        except Exception as e:
            checks["llm"] = f"error: {e}"

        # HippoRAG graph
        try:
            from config.settings import settings
            graph_path = settings.path.data / "hippo_graph.json"
            checks["hippo_graph"] = "available" if graph_path.exists() else "not_built"
        except Exception:
            checks["hippo_graph"] = "unknown"

        overall = "ok" if all(v == "ok" or v == "available" or v == "not_built" for v in checks.values()) else "degraded"
        return {"status": overall, "components": checks}

    # 4. 设置应用元信息
    metrics.app_info.info({"version": "0.1.0", "service": "deepsea-rag"})

    logger.info("[observability] middleware + /metrics + /health/detailed registered")
</file>

<file path="src/parser/__init__.py">
"""Parser 模块：PDF 解析与 EnrichedDoc 生成"""

from src.parser.pdf_parser import (
    PDFProcessor,
    ParserConfig,
    EnrichedDoc,
    ContentBlock,
    BlockType,
    TableData,
    FigureData,
)

__all__ = [
    "PDFProcessor",
    "ParserConfig",
    "EnrichedDoc",
    "ContentBlock",
    "BlockType",
    "TableData",
    "FigureData",
]
</file>

<file path="src/pipelines/__init__.py">
<<<<<<< Current (Your changes)
=======
"""LangGraph 工作流"""

from src.pipelines.ingestion_graph import IngestionState, build_ingestion_graph

__all__ = ["IngestionState", "build_ingestion_graph"]
>>>>>>> Incoming (Background Agent changes)
</file>

<file path="src/prompts/coherence_refine.txt">
You are a senior academic editor.
Rewrite the full review to improve global coherence across sections.
{language_instruction}

Hard constraints:
- Keep factual meaning and citation keys unchanged (e.g., [abc123], [evidence limited])
- Preserve section hierarchy and major headings
- Reduce redundancy across sections, improve transitions and terminology consistency
- Do not fabricate claims or references
- {lang_hard_rule}
- Return markdown only

Document:
{body_md}
</file>

<file path="src/prompts/evaluate_sufficiency.txt">
Evaluate information sufficiency for this section.

Section: {section_title}
Research topic: {topic}
Collected sources: {source_count}
Findings so far:
{findings}

Return JSON:
{{
  "coverage_score": 0.0-1.0,
  "gaps": ["missing information points"],
  "sufficient": true/false
}}
</file>

<file path="src/prompts/extract_claims.txt">
从以下文本中提取所有事实性声明（factual claims）。

只提取可验证的事实性陈述，忽略观点性表述和过渡句。

文本:
{text}

返回 JSON 对象，格式:
{{"claims": [{{"claim": "声明文本", "has_citation": true/false, "citation_keys": ["key1"]}}]}}

只返回 JSON 对象。
</file>

<file path="src/prompts/generate_abstract.txt">
Generate a 150-250 word abstract for the review below.
{language_instruction}

{full_md}
</file>

<file path="src/prompts/generate_claims.txt">
Based on the following evidence for section "{section_title}", extract 3-5 core claims (Claims).
Each claim MUST retain the [ref_hash] citation markers from the evidence.
Output format: numbered list, one claim per line. Do not add extra commentary.

Evidence:
{evidence}
</file>

<file path="src/prompts/generate_queries.txt">
You are an academic search query specialist.
Generate TWO categories of queries for this review section.

Research topic: {topic}
Scope: {scope}
Full outline (context only):
{outline_block}
Current section: {section_title}
Known gaps: {gaps_block}
User materials: {temp_block}
{user_context_block}

Category A — RECALL queries ({recall_budget} queries):
Purpose: cast a wide net. Use short queries (3-6 keywords), include synonyms,
abbreviations, alternative naming conventions, different research schools.
Example: "deep-sea mussel symbiosis immune" vs "Bathymodiolus innate immunity"

Category B — PRECISION queries ({precision_budget} queries):
Purpose: find specific evidence. Use longer queries (6-12 keywords) with
method/technique, data type, model organism, time period, or mechanism constraints.
Example: "Bathymodiolus transcriptome toll-like receptor signaling pathway 2020-2024"

Format (strict, no numbering):
RECALL:
query 1
query 2
PRECISION:
query 1
query 2

Rules:
- Focus ONLY on section: {section_title}
- Do not repeat gap queries
- Avoid generic suffixes: review, survey, overview
- Avoid overlap with sections: {avoid_overlap}
</file>

<file path="src/prompts/limitations_section.txt">
Based on the research review and the identified gaps/conflicts/limitations below,
write the BODY content for section "{lim_title}" (200-400 words).

The review:
{full_md}

Identified issues:
{insight_block}

Requirements:
- Acknowledge key limitations objectively
- Propose specific future research directions based on the gaps
- Explicitly mention evidence-scarce sections and avoid overstating conclusions for them
- Integrate conflict_notes attribution and write one dedicated paragraph labeled "观点交锋与实验条件差异 (Debate & Divergence):"
- In that dedicated paragraph, compare experimental-condition variables (e.g., sampling depth, sequencing platform/instrument, time span, sample size, cohort/region, statistical pipeline) and explain plausible causes of divergence
- Be constructive and forward-looking
- Output section body ONLY (no markdown heading, no duplicated title line)
{language_instruction}
</file>

<file path="src/prompts/open_gaps_agenda.txt">
You are an academic strategist. Based on the review and the OPEN GAPS below,
write the BODY content for section "{agenda_title}" (250-500 words).
{language_instruction}

Review excerpt:
{full_md}

Open gaps:
{gap_lines}

Requirements:
- Organize into 2-4 thematic directions
- For each direction include: (a) key unanswered question, (b) why it matters, (c) suggested methods/data
- Prioritize by feasibility and expected impact
- Be concrete and avoid generic wording
- Do not invent unsupported facts; keep this as forward-looking proposal
- Output section body ONLY (no markdown heading, no duplicated title line)
</file>

<file path="src/prompts/plan_outline.txt">
Based on the research brief and initial retrieval context, generate a 3-6 section review outline.

Research brief:
- Topic: {topic}
- Scope: {scope}
- Key questions: {key_questions}

Initial retrieval context:
{context}

Return a numbered list, one section title per line:
1. Section title
2. Section title
...
</file>

<file path="src/prompts/scope_research.txt">
You are a scientific review planning expert. Create a structured research brief.

Topic: {topic}
User clarifications:
{clarification_block}

Return strict JSON:
{{
  "scope": "One-sentence scope",
  "success_criteria": ["criterion 1", "criterion 2", "criterion 3"],
  "key_questions": ["question 1", "question 2", "question 3"],
  "exclusions": ["out-of-scope topics"],
  "time_range": "publication time range (for example: 2020-2025)",
  "source_priority": ["peer-reviewed papers", "reviews", "datasets"]
}}
</file>

<file path="src/prompts/translate_content.txt">
Translate the following markdown content into {target_lang}.

Section type: {section_label}

Hard constraints:
- Preserve markdown structure
- Keep bracketed citation/evidence tags unchanged (e.g., [abc123], [evidence limited])
- Do not add new facts
- Output translated markdown only

Content:
{raw}
</file>

<file path="src/prompts/verify_claims.txt">
请验证以下声明是否有证据支撑。

声明列表:
{claims}

可用的参考资料:
{evidence}

对每个声明判断：
- 是否有充分证据支撑
- 置信度 (high/medium/low)
- 如果不支撑，建议的修订或补充搜索方向
- 当发现多篇文献对同一问题结论冲突时，必须提取双方关键【实验条件变量】
  （如采样深度、测序仪器、时间跨度、样本量、地域/人群、统计方法等），
  并给出深度归因分析（Attribution Analysis），解释潜在差异来源，而不是只指出"有冲突"。
- 归因分析必须写入 revision_note 或 attribution_analysis 字段；如果存在冲突，请在 conflict_notes 中列出结构化要点。

返回 JSON 对象:
{{"verifications": [{{"claim_index": 0, "confidence": "high|medium|low", "evidence_found": "支撑证据概要", "needs_revision": false, "revision_note": "", "attribution_analysis": "", "conflict_notes": [], "supplementary_query": ""}}]}}

只返回 JSON 对象。
</file>

<file path="src/prompts/write_section.txt">
Write the section "{section_title}" for the review using the evidence below.

Requirements:
- Use academic writing style and coherent logic
- Mark citations with [ref_hash] as shown in evidence snippets
- Length: around 400-600 words
- Keep consistency with other sections
- Cross-check key data points against the verification evidence block before finalizing claims
{language_instruction}
{user_context_block}
{triangulation_block}
{caution_block}
{quantitative_block}
{claims_block}

Available evidence:
{evidence}

Verification evidence for data points/citations (secondary check, high priority):
{verification_evidence}

Relevant temporary materials from user:
{temp_context}
{supplement_block}
</file>

<file path="src/retrieval/__init__.py">
# 检索模块
from src.retrieval.hybrid_retriever import HybridRetriever, RetrievalConfig, retriever
from src.retrieval.web_search import TavilySearcher, web_searcher

__all__ = ["HybridRetriever", "RetrievalConfig", "retriever", "TavilySearcher", "web_searcher"]
</file>

<file path="src/retrieval/colbert_reranker.py">
"""
ColBERT 应用层精排器（MaxSim）
不存储向量，检索时对候选文档实时编码并计算 MaxSim，与 BGE-Reranker 接口对齐。
"""

from typing import List, Any
from dataclasses import dataclass

from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)


@dataclass
class RerankResult:
    """与 BGE-Reranker 一致的单个结果：index 为在原文 docs 列表中的下标，score 为分数"""
    index: int
    score: float


class ColBERTReranker:
    """ColBERT 精排器（懒加载，单例）"""

    _instance = None
    _model = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @property
    def model(self):
        """懒加载 ColBERT 模型（RAGatouille）"""
        if self._model is None:
            model_name = getattr(settings.search, "colbert_model", "colbert-ir/colbertv2.0")
            n_gpu = -1
            if settings.model.device and settings.model.device != "cpu":
                n_gpu = 1
            try:
                from ragatouille import RAGPretrainedModel
                logger.info(f"加载 ColBERT... ({model_name}, device={settings.model.device})")
                kwargs = {}
                if settings.model.colbert_cache_dir:
                    kwargs["cache_dir"] = settings.model.colbert_cache_dir
                if settings.model.local_files_only:
                    kwargs["local_files_only"] = True
                self._model = RAGPretrainedModel.from_pretrained(
                    model_name,
                    n_gpu=n_gpu,
                    verbose=0,
                    **kwargs,
                )
                logger.info("[OK] ColBERT 就绪")
            except Exception as e:
                raise RuntimeError(f"ColBERT 加载失败: {e}") from e
        return self._model

    def rerank(
        self,
        query: str,
        docs: List[str],
        top_k: int = 10,
        bsize: int = 64,
    ) -> List[RerankResult]:
        """
        对 docs 做 ColBERT MaxSim 精排，返回与 embedder.rerank 相同语义的结果：
        列表元素为 RerankResult(index=在 docs 中的下标, score=分数)，按 score 降序。
        """
        if not docs:
            return []
        top_k = min(top_k, len(docs))
        raw = self.model.rerank(
            query=query,
            documents=docs,
            k=top_k,
            zero_index_ranks=False,
            bsize=bsize,
        )
        # 单 query 时 ragatouille 返回 list[dict]，每个 dict 含 content, score, rank
        if isinstance(raw, list) and raw and isinstance(raw[0], list):
            raw = raw[0]
        out: List[RerankResult] = []
        doc_to_idx = {d: i for i, d in enumerate(docs)}
        for item in raw:
            content = item.get("content", "")
            score = float(item.get("score", 0.0))
            idx = doc_to_idx.get(content, -1)
            if idx >= 0:
                out.append(RerankResult(index=idx, score=score))
        return out


# 全局实例
colbert_reranker = ColBERTReranker()
</file>

<file path="src/retrieval/query_optimizer.py">
"""
Provider-specific query optimizer for web search.
"""

import re
from typing import Tuple


_QWORDS_EN = {
    "what", "why", "how", "when", "where", "which", "who",
    "is", "are", "was", "were", "do", "does", "did",
    "can", "could", "should", "would", "may", "might",
}

_QWORDS_ZH = {
    "什么", "为何", "为什么", "如何", "怎么", "怎样", "是否", "可以", "能否", "原理", "机制",
}


def _normalize(text: str) -> str:
    text = (text or "").strip()
    text = re.sub(r"\s+", " ", text)
    return text


def _strip_question(text: str) -> Tuple[str, bool]:
    """
    Remove common question leading words and punctuation.
    Returns (keywords, was_question).
    """
    t = _normalize(text)
    if not t:
        return "", False
    was_question = "?" in t or "？" in t

    # English question prefix
    lowered = t.lower()
    if any(lowered.startswith(w + " ") for w in _QWORDS_EN):
        was_question = True
        lowered = re.sub(r"^(what|why|how|when|where|which|who|is|are|was|were|do|does|did|can|could|should|would|may|might)\b\s*", "", lowered)
        t = lowered

    # Chinese question prefix
    for w in sorted(_QWORDS_ZH, key=len, reverse=True):
        if t.startswith(w):
            was_question = True
            t = t[len(w):].lstrip()
            break

    # Remove punctuation
    t = re.sub(r"[?？。．，,;；:：!！()（）\[\]{}\"']", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t, was_question


def optimize_query(provider: str, query: str) -> str:
    """
    Provider-specific query optimization.
    """
    q = _normalize(query)
    if not q:
        return q
    keywords, was_question = _strip_question(q)
    provider = (provider or "").lower()

    if provider in ("scholar", "semantic"):
        base = keywords or q
        # Do not force "review"/"survey" suffixes for academic engines.
        # They often bias results toward generic surveys and reduce snippet specificity.
        # 不再加双引号，避免 URL 编码导致 Scholar/Playwright 检索失败
        return base

    if provider == "google":
        base = keywords or q
        # Keep Google queries concise; avoid forcing "overview".
        return base

    if provider == "tavily":
        base = keywords or q
        if was_question and "overview" not in base.lower():
            base = f"{base} overview"
        return base

    return q
</file>

<file path="src/retrieval/semantic_scholar.py">
"""
Semantic Scholar API 搜索（ai4scholar 代理）。
输出与 unified_web_search 兼容的 RAG 格式。
"""

import asyncio
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

import aiohttp

from config.settings import settings
from src.log import get_logger
from src.utils.cache import TTLCache, _make_key, get_cache

logger = get_logger(__name__)


def _get_semantic_scholar_config() -> Dict[str, Any]:
    try:
        ss = getattr(settings, "semantic_scholar", None)
        if ss is not None:
            return {
                "enabled": getattr(ss, "enabled", False),
                "api_key": getattr(ss, "api_key", "") or "",
                "base_url": getattr(ss, "base_url", "https://ai4scholar.net/graph/v1"),
                "max_results": getattr(ss, "max_results", 5),
                "timeout_seconds": getattr(ss, "timeout_seconds", 30),
            }
    except Exception:
        pass
    return {}


def _domain_from_url(url: str) -> str:
    try:
        parsed = urlparse(url or "")
        netloc = (parsed.netloc or "").strip()
        if netloc.startswith("www."):
            netloc = netloc[4:]
        return netloc or ""
    except Exception:
        return ""


@dataclass
class SemanticScholarSearcher:
    _config: Dict[str, Any] = field(default_factory=dict)
    _cache: Optional[TTLCache] = field(default=None, repr=False)
    _session: Optional[aiohttp.ClientSession] = field(default=None, repr=False)

    BASE_URL = "https://ai4scholar.net/graph/v1"
    PAPER_FIELDS = [
        "paperId",
        "title",
        "abstract",
        "year",
        "citationCount",
        "authors",
        "url",
        "openAccessPdf",
        "publicationVenue",
        "externalIds",
    ]

    def __post_init__(self):
        if not self._config:
            self._config = _get_semantic_scholar_config()
        perf = getattr(settings, "perf_web_search", None)
        self._cache = (
            get_cache(
                getattr(perf, "cache_enabled", False),
                getattr(perf, "cache_ttl_seconds", 3600),
                prefix="semantic_scholar",
            )
            if perf else None
        )

    @property
    def enabled(self) -> bool:
        return bool(self._config.get("enabled") and (self._config.get("api_key") or "").strip())

    async def _ensure_session(self) -> None:
        if self._session and not self._session.closed:
            return
        timeout_s = int(self._config.get("timeout_seconds", 30))
        timeout = aiohttp.ClientTimeout(total=timeout_s)
        self._session = aiohttp.ClientSession(timeout=timeout)

    async def _fetch_json(self, url: str, params: Dict[str, Any], headers: Dict[str, str]) -> Dict[str, Any]:
        await self._ensure_session()
        assert self._session is not None
        async with self._session.get(url, params=params, headers=headers) as resp:
            if resp.status == 200:
                return await resp.json()
            text = await resp.text()
            raise RuntimeError(f"Semantic Scholar API error: {resp.status} - {text[:200]}")

    def _parse_paper(self, paper: Dict[str, Any], query: str) -> Dict[str, Any]:
        authors = [a.get("name", "") for a in paper.get("authors", []) if a.get("name")]
        open_pdf = (paper.get("openAccessPdf") or {}).get("url")
        external_ids = paper.get("externalIds") or {}
        doi = external_ids.get("DOI")
        arxiv = external_ids.get("ArXiv")
        url = paper.get("url") or ""
        if not url and doi:
            url = f"https://doi.org/{doi}"
        if not url and arxiv:
            url = f"https://arxiv.org/abs/{arxiv}"
        domain = _domain_from_url(url)
        venue = (paper.get("publicationVenue") or {}).get("name")

        metadata = {
            "source": "semantic",
            "doc_id": paper.get("title") or paper.get("paperId") or url or "semantic",
            "title": paper.get("title") or "",
            "url": url,
            "domain": domain,
            "search_query": query,
        }
        if authors:
            metadata["authors"] = authors
        if paper.get("year"):
            metadata["year"] = paper.get("year")
        if paper.get("citationCount") is not None:
            metadata["cited_by"] = paper.get("citationCount")
        if open_pdf:
            metadata["pdf_url"] = open_pdf
        if doi:
            metadata["doi"] = doi
        if arxiv:
            metadata["arxiv_id"] = arxiv
        if venue:
            metadata["venue"] = venue
        if paper.get("paperId"):
            metadata["paper_id"] = paper.get("paperId")

        return {
            "content": (paper.get("abstract") or paper.get("title") or "").strip(),
            "score": 0.9,
            "metadata": metadata,
        }

    async def search(
        self,
        query: str,
        limit: Optional[int] = None,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        if not self.enabled:
            return []
        q = (query or "").strip()
        if not q:
            return []
        limit = limit or int(self._config.get("max_results", 5))
        cache_key = _make_key("semantic_scholar", q, limit, year_start, year_end)
        if self._cache:
            cached = self._cache.get(cache_key)
            if cached is not None:
                return cached
        base_url = (self._config.get("base_url") or self.BASE_URL).rstrip("/")
        url = f"{base_url}/paper/search"
        params: Dict[str, Any] = {
            "query": q,
            "limit": min(int(limit), 100),
            "fields": ",".join(self.PAPER_FIELDS),
        }
        if year_start or year_end:
            if year_start and year_end:
                params["year"] = f"{year_start}-{year_end}"
            elif year_start:
                params["year"] = f"{year_start}-"
            else:
                params["year"] = f"-{year_end}"
        headers = {
            "Accept": "application/json",
            "User-Agent": "DeepSea-RAG/1.0",
        }
        api_key = (self._config.get("api_key") or "").strip()
        if api_key:
            headers["Authorization"] = f"Bearer {api_key}"

        try:
            data = await self._fetch_json(url, params, headers)
            papers = data.get("data") or []
            results = [self._parse_paper(p, q) for p in papers[:limit]]
            if self._cache:
                self._cache.set(cache_key, results)
            return results
        except asyncio.TimeoutError:
            logger.warning("Semantic Scholar search timeout")
            return []
        except Exception as e:
            logger.error(f"Semantic Scholar search failed: {e}")
            return []

    async def close(self) -> None:
        if self._session and not self._session.closed:
            await self._session.close()
        self._session = None


# 全局单例
semantic_scholar_searcher = SemanticScholarSearcher()
</file>

<file path="src/utils/cache.py">
"""
内存 TTL 缓存：按 key 缓存，过期自动失效，不落盘。
"""

from __future__ import annotations

import hashlib
import json
import time
from threading import RLock
from typing import Any, Callable, Optional, TypeVar

T = TypeVar("T")


def _make_key(prefix: str, *parts: Any) -> str:
    """生成缓存键。parts 会做稳定序列化。"""
    raw = [prefix]
    for p in parts:
        if p is None:
            raw.append("")
        elif isinstance(p, (str, int, float, bool)):
            raw.append(str(p))
        else:
            try:
                raw.append(json.dumps(p, sort_keys=True, default=str))
            except Exception:
                raw.append(repr(p))
    blob = "|".join(raw).encode("utf-8")
    return hashlib.sha256(blob).hexdigest()


class TTLCache:
    """
    线程安全的内存 TTL 缓存。
    - maxsize: 最大条目数，超过时淘汰最久未访问的。
    - ttl_seconds: 条目过期时间，0 表示不过期。
    """

    __slots__ = ("_store", "_expiry", "_order", "_maxsize", "_ttl", "_lock")

    def __init__(self, maxsize: int = 1024, ttl_seconds: int = 3600):
        self._store: dict[str, Any] = {}
        self._expiry: dict[str, float] = {}
        self._order: list[str] = []
        self._maxsize = max(1, maxsize)
        self._ttl = max(0, ttl_seconds)
        self._lock = RLock()

    def _evict_if_needed(self) -> None:
        now = time.monotonic()
        while self._order and len(self._store) >= self._maxsize:
            k = self._order.pop(0)
            self._store.pop(k, None)
            self._expiry.pop(k, None)
        # drop expired
        expired = [k for k, t in self._expiry.items() if self._ttl > 0 and (now - t) > self._ttl]
        for k in expired:
            self._store.pop(k, None)
            self._expiry.pop(k, None)
            if k in self._order:
                self._order.remove(k)

    def get(self, key: str) -> Optional[Any]:
        with self._lock:
            if key not in self._store:
                return None
            if self._ttl > 0 and (time.monotonic() - self._expiry.get(key, 0)) > self._ttl:
                self._store.pop(key, None)
                self._expiry.pop(key, None)
                if key in self._order:
                    self._order.remove(key)
                return None
            if key in self._order:
                self._order.remove(key)
            self._order.append(key)
            return self._store[key]

    def set(self, key: str, value: Any) -> None:
        with self._lock:
            self._evict_if_needed()
            self._store[key] = value
            self._expiry[key] = time.monotonic()
            if key in self._order:
                self._order.remove(key)
            self._order.append(key)

    def delete(self, key: str) -> None:
        with self._lock:
            self._store.pop(key, None)
            self._expiry.pop(key, None)
            if key in self._order:
                self._order.remove(key)

    def get_or_set(self, key: str, factory: Callable[[], T]) -> T:
        val = self.get(key)
        if val is not None:
            return val
        val = factory()
        self.set(key, val)
        return val


def get_cache(
    enabled: bool,
    ttl_seconds: int = 3600,
    maxsize: int = 1024,
    prefix: str = "cache",
) -> Optional[TTLCache]:
    """若 enabled 为 False 返回 None，否则返回一个 TTLCache。"""
    if not enabled:
        return None
    return TTLCache(maxsize=maxsize, ttl_seconds=ttl_seconds)
</file>

<file path="src/utils/limiter.py">
"""
并发限制：信号量包装，用于限制同时执行的任务数。
"""

from __future__ import annotations

import threading
from contextlib import contextmanager
from typing import Optional


class ConcurrencyLimiter:
    """基于 threading.Semaphore 的并发限制器。"""

    def __init__(self, max_parallel: int = 1):
        self._sem = threading.Semaphore(max(1, max_parallel))

    @contextmanager
    def acquire(self):
        self._sem.acquire()
        try:
            yield
        finally:
            self._sem.release()

    def run(self, fn, *args, **kwargs):
        """在限制下执行 fn(*args, **kwargs)。"""
        with self.acquire():
            return fn(*args, **kwargs)


_global_limiter: Optional[ConcurrencyLimiter] = None
_limiter_lock = threading.Lock()


def get_global_limiter(max_parallel: int = 5) -> ConcurrencyLimiter:
    """获取或创建全局并发限制器（按首次调用的 max_parallel）。"""
    global _global_limiter
    with _limiter_lock:
        if _global_limiter is None:
            _global_limiter = ConcurrencyLimiter(max_parallel)
        return _global_limiter


def reset_global_limiter() -> None:
    """测试用：重置全局限制器。"""
    global _global_limiter
    with _limiter_lock:
        _global_limiter = None
</file>

<file path="src/utils/model_sync.py">
"""
本地模型管理：检查是否存在 + 同步/升级模型缓存。
"""

from __future__ import annotations

from dataclasses import dataclass
import os
from pathlib import Path
from typing import List, Optional

from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)


@dataclass
class ModelStatusItem:
    name: str
    model_id: str
    cache_dir: str
    exists: bool
    local_files_only: bool
    error: Optional[str] = None


@dataclass
class ModelSyncItem:
    name: str
    model_id: str
    cache_dir: str
    local_files_only: bool
    updated: bool
    status: str
    message: Optional[str] = None
    error: Optional[str] = None
    resolved_path: Optional[str] = None


def _get_model_targets() -> List[tuple[str, str, str]]:
    """返回需要管理的模型列表：(name, model_id, cache_dir)."""
    targets = [
        ("bge_m3", settings.model.embedding_model, settings.model.embedding_cache_dir or ""),
        ("bge_reranker", settings.model.reranker_model, settings.model.reranker_cache_dir or ""),
    ]
    if settings.search.use_colbert_reranker:
        targets.append(
            ("colbert", settings.search.colbert_model, settings.model.colbert_cache_dir or "")
        )
    return targets


def check_models(local_files_only: Optional[bool] = None) -> List[ModelStatusItem]:
    from huggingface_hub import snapshot_download

    local_only = (
        settings.model.local_files_only
        if local_files_only is None
        else bool(local_files_only)
    )
    results: List[ModelStatusItem] = []
    for name, model_id, cache_dir in _get_model_targets():
        try:
            snapshot_download(
                repo_id=model_id,
                cache_dir=cache_dir,
                local_files_only=True,
                local_dir_use_symlinks=False,
            )
            results.append(
                ModelStatusItem(
                    name=name,
                    model_id=model_id,
                    cache_dir=cache_dir,
                    exists=True,
                    local_files_only=local_only,
                )
            )
        except Exception as e:
            results.append(
                ModelStatusItem(
                    name=name,
                    model_id=model_id,
                    cache_dir=cache_dir,
                    exists=False,
                    local_files_only=local_only,
                    error=str(e),
                )
            )
    return results


def _get_cache_root(cache_dir: str) -> Path:
    from huggingface_hub.constants import HUGGINGFACE_HUB_CACHE

    if cache_dir:
        return Path(cache_dir).expanduser()
    return Path(os.getenv("HF_HUB_CACHE", HUGGINGFACE_HUB_CACHE)).expanduser()


def _get_repo_cache_dir(model_id: str, cache_dir: str) -> Path:
    return _get_cache_root(cache_dir) / f"models--{model_id.replace('/', '--')}"


def _read_local_main_revision(model_id: str, cache_dir: str) -> Optional[str]:
    refs_main = _get_repo_cache_dir(model_id, cache_dir) / "refs" / "main"
    try:
        if refs_main.exists():
            revision = refs_main.read_text(encoding="utf-8").strip()
            return revision or None
    except Exception:
        return None
    return None


def _normalize_endpoint(endpoint: Optional[str]) -> Optional[str]:
    if not endpoint:
        return None
    ep = endpoint.strip()
    if not ep:
        return None
    if "://" not in ep:
        ep = f"https://{ep}"
    return ep.rstrip("/")


def _get_hf_endpoint_candidates() -> List[Optional[str]]:
    """
    获取 HuggingFace endpoint 候选列表（按优先级）：
    1) RAG_HF_ENDPOINTS=ep1,ep2（显式多候选）
    2) HF_ENDPOINT / HF_MIRROR（兼容单 endpoint）
    3) RAG_HF_USE_CN_MIRROR=true 时附加 hf-mirror.com
    4) 最后回退官方 endpoint（None）
    """
    raw_multi = os.getenv("RAG_HF_ENDPOINTS", "").strip()
    candidates: List[Optional[str]] = []

    if raw_multi:
        for item in raw_multi.split(","):
            ep = _normalize_endpoint(item)
            if ep:
                candidates.append(ep)
    else:
        ep = _normalize_endpoint(os.getenv("HF_ENDPOINT", "")) or _normalize_endpoint(
            os.getenv("HF_MIRROR", "")
        )
        if ep:
            candidates.append(ep)

    use_cn_mirror = os.getenv("RAG_HF_USE_CN_MIRROR", "false").lower() == "true"
    if use_cn_mirror:
        candidates.append("https://hf-mirror.com")

    # 官方 endpoint 作为最终回退
    candidates.append(None)

    deduped: List[Optional[str]] = []
    seen = set()
    for ep in candidates:
        key = ep or "__default__"
        if key in seen:
            continue
        seen.add(key)
        deduped.append(ep)
    return deduped


def _fetch_remote_main_revision_with_fallback(model_id: str) -> Optional[str]:
    from huggingface_hub import HfApi

    for endpoint in _get_hf_endpoint_candidates():
        try:
            client = HfApi(endpoint=endpoint) if endpoint else HfApi()
            info = client.model_info(repo_id=model_id)
            sha = getattr(info, "sha", None)
            if sha:
                return sha
        except Exception as e:
            label = endpoint or "https://huggingface.co"
            logger.warning(
                f"[model_sync] fetch remote revision failed for {model_id} via {label}: {e}"
            )
    return None


def _snapshot_download_with_fallback(
    *,
    model_id: str,
    cache_dir: str,
    local_files_only: bool,
    force_download: bool,
) -> tuple[str, Optional[str]]:
    from huggingface_hub import snapshot_download

    if local_files_only:
        path = snapshot_download(
            repo_id=model_id,
            cache_dir=cache_dir,
            local_files_only=True,
            force_download=force_download,
            local_dir_use_symlinks=False,
            resume_download=True,
        )
        return path, None

    last_error: Optional[Exception] = None
    for endpoint in _get_hf_endpoint_candidates():
        try:
            kwargs = {}
            if endpoint:
                kwargs["endpoint"] = endpoint
            path = snapshot_download(
                repo_id=model_id,
                cache_dir=cache_dir,
                local_files_only=False,
                force_download=force_download,
                local_dir_use_symlinks=False,
                resume_download=True,
                **kwargs,
            )
            return path, endpoint
        except Exception as e:
            last_error = e
            label = endpoint or "https://huggingface.co"
            logger.warning(
                f"[model_sync] snapshot download failed for {model_id} via {label}: {e}"
            )
    raise RuntimeError(f"all endpoints failed for {model_id}: {last_error}")


def sync_models(
    force_update: bool = False,
    local_files_only: Optional[bool] = None,
) -> List[ModelSyncItem]:
    """
    同步模型缓存：
    - force_update: True 时强制下载最新版本
    - local_files_only: True 时仅检查本地，False 时允许联网下载
    """
    local_only = (
        settings.model.local_files_only
        if local_files_only is None
        else bool(local_files_only)
    )
    results: List[ModelSyncItem] = []

    for name, model_id, cache_dir in _get_model_targets():
        try:
            local_revision_before = _read_local_main_revision(model_id, cache_dir)

            # 在线且非强制更新时：已是最新版本则跳过。
            if not local_only and not force_update:
                remote_revision = _fetch_remote_main_revision_with_fallback(model_id)
                if (
                    remote_revision
                    and local_revision_before
                    and remote_revision == local_revision_before
                ):
                    results.append(
                        ModelSyncItem(
                            name=name,
                            model_id=model_id,
                            cache_dir=cache_dir,
                            local_files_only=local_only,
                            updated=False,
                            status="already_latest",
                            message="已是最新版本，已跳过",
                        )
                    )
                    continue

            path, endpoint_used = _snapshot_download_with_fallback(
                model_id=model_id,
                cache_dir=cache_dir,
                local_files_only=local_only,
                force_download=bool(force_update),
            )
            local_revision_after = _read_local_main_revision(model_id, cache_dir)
            changed_revision = (
                bool(local_revision_before)
                and bool(local_revision_after)
                and local_revision_before != local_revision_after
            )
            first_download = (not local_revision_before) and bool(local_revision_after)
            updated = bool(force_update) or changed_revision or first_download
            if local_only:
                message = "离线模式：已校验本地缓存"
            elif updated:
                message = "模型已升级到最新版本" if changed_revision else "模型已下载到本地"
            else:
                message = "模型已是最新版本，未执行升级"
            if endpoint_used:
                message = f"{message}（endpoint: {endpoint_used}）"
            results.append(
                ModelSyncItem(
                    name=name,
                    model_id=model_id,
                    cache_dir=cache_dir,
                    local_files_only=local_only,
                    updated=updated,
                    status="ok",
                    message=message,
                    resolved_path=path,
                )
            )
        except Exception as e:
            logger.warning(f"[model_sync] {name} failed: {e}")
            results.append(
                ModelSyncItem(
                    name=name,
                    model_id=model_id,
                    cache_dir=cache_dir,
                    local_files_only=local_only,
                    updated=False,
                    status="failed",
                    message="模型同步失败",
                    error=str(e),
                )
            )
    return results
</file>

<file path="src/utils/prompt_manager.py">
"""Prompt asset manager — singleton that loads and caches .txt prompt templates."""

from __future__ import annotations

import threading
from pathlib import Path
from typing import Dict

_PROMPTS_DIR = Path(__file__).resolve().parents[1] / "prompts"


class PromptManager:
    """Singleton prompt template manager.

    Loads ``.txt`` templates from ``src/prompts/``, renders them with
    ``str.format(**kwargs)``, and keeps loaded templates in an in-memory cache
    so each file is read from disk at most once per process lifetime.

    Usage::

        pm = PromptManager()
        text = pm.render("scope_research.txt", topic="deep-sea biology", clarification_block="...")
    """

    _instance: PromptManager | None = None
    _lock: threading.Lock = threading.Lock()

    def __new__(cls) -> "PromptManager":
        if cls._instance is None:
            with cls._lock:
                if cls._instance is None:
                    inst = super().__new__(cls)
                    inst._cache: Dict[str, str] = {}
                    cls._instance = inst
        return cls._instance

    def render(self, template_name: str, **kwargs: object) -> str:
        """Render a prompt template by name.

        Args:
            template_name: File name relative to ``src/prompts/`` (e.g. ``"scope_research.txt"``).
            **kwargs: Variables substituted into the template via ``str.format``.

        Returns:
            Rendered prompt string.
        """
        if template_name not in self._cache:
            path = _PROMPTS_DIR / template_name
            self._cache[template_name] = path.read_text(encoding="utf-8")
        return self._cache[template_name].format(**kwargs)

    def invalidate(self, template_name: str | None = None) -> None:
        """Clear one or all cached templates (useful in tests or hot-reload scenarios)."""
        if template_name is None:
            self._cache.clear()
        else:
            self._cache.pop(template_name, None)
</file>

<file path="src/utils/storage_cleaner.py">
"""
持久化存储清理工具：按生命周期和总大小限制清理过期/超额数据。
支持 Canvas、Session、Persistent 三个 SQLite 数据库。
"""

import os
import sqlite3
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Tuple

from src.log import get_logger

logger = get_logger(__name__)


def _get_db_paths() -> List[Path]:
    """获取所有需要管理的 SQLite 数据库路径"""
    data_dir = Path(__file__).resolve().parents[2] / "data"
    return [
        data_dir / "canvas.db",
        data_dir / "sessions.db",
        data_dir / "persistent.db",
    ]


def _ensure_canvas_archive_column(conn: sqlite3.Connection) -> None:
    """Ensure canvases has archived column (migration for cleanup)."""
    cur = conn.execute("PRAGMA table_info(canvases)")
    columns = {row[1] for row in cur.fetchall()}
    if "archived" not in columns:
        conn.execute("ALTER TABLE canvases ADD COLUMN archived INTEGER NOT NULL DEFAULT 0")


def _get_total_size_bytes(paths: List[Path]) -> int:
    """计算所有数据库文件的总大小（字节）"""
    total = 0
    for p in paths:
        if p.exists():
            total += p.stat().st_size
    return total


def _cleanup_by_age_canvas(db_path: Path, cutoff: datetime, batch_size: int) -> int:
    """按时间清理 canvas.db 中过期的 canvas（及关联表），跳过已存档。"""
    if not db_path.exists():
        return 0
    removed = 0
    cutoff_str = cutoff.isoformat()
    with sqlite3.connect(db_path) as conn:
        _ensure_canvas_archive_column(conn)
        # 查找过期且未存档的 canvas
        rows = conn.execute(
            "SELECT id FROM canvases WHERE updated_at < ? AND (archived = 0 OR archived IS NULL) LIMIT ?",
            (cutoff_str, batch_size),
        ).fetchall()
        canvas_ids = [r[0] for r in rows]
        for cid in canvas_ids:
            conn.execute("DELETE FROM canvas_versions WHERE canvas_id = ?", (cid,))
            conn.execute("DELETE FROM canvas_citations WHERE canvas_id = ?", (cid,))
            conn.execute("DELETE FROM draft_blocks WHERE canvas_id = ?", (cid,))
            conn.execute("DELETE FROM outline_sections WHERE canvas_id = ?", (cid,))
            conn.execute("DELETE FROM canvases WHERE id = ?", (cid,))
            removed += 1
        conn.commit()
    return removed


def _cleanup_by_age_sessions(db_path: Path, cutoff: datetime, batch_size: int) -> int:
    """按时间清理 sessions.db 中过期的 session（及关联 turns）"""
    if not db_path.exists():
        return 0
    removed = 0
    cutoff_str = cutoff.isoformat()
    with sqlite3.connect(db_path) as conn:
        rows = conn.execute(
            "SELECT session_id FROM sessions WHERE updated_at < ? LIMIT ?",
            (cutoff_str, batch_size),
        ).fetchall()
        session_ids = [r[0] for r in rows]
        for sid in session_ids:
            conn.execute("DELETE FROM turns WHERE session_id = ?", (sid,))
            conn.execute("DELETE FROM sessions WHERE session_id = ?", (sid,))
            removed += 1
        conn.commit()
    return removed


def _cleanup_by_age_persistent(db_path: Path, cutoff: datetime, batch_size: int) -> int:
    """按时间清理 persistent.db 中过期的用户项目记录"""
    if not db_path.exists():
        return 0
    removed = 0
    cutoff_str = cutoff.isoformat()
    with sqlite3.connect(db_path) as conn:
        # user_projects 有 updated_at
        cur = conn.execute(
            "DELETE FROM user_projects WHERE updated_at < ? LIMIT ?",
            (cutoff_str, batch_size),
        )
        removed += cur.rowcount
        conn.commit()
    return removed


def cleanup_by_age(max_age_days: int, batch_size: int = 100) -> Tuple[int, int, int]:
    """
    按时间清理所有数据库中超过 max_age_days 的记录。
    返回 (canvas_removed, session_removed, project_removed)
    """
    cutoff = datetime.now() - timedelta(days=max_age_days)
    data_dir = Path(__file__).resolve().parents[2] / "data"
    c = _cleanup_by_age_canvas(data_dir / "canvas.db", cutoff, batch_size)
    s = _cleanup_by_age_sessions(data_dir / "sessions.db", cutoff, batch_size)
    p = _cleanup_by_age_persistent(data_dir / "persistent.db", cutoff, batch_size)
    if c or s or p:
        logger.info(f"[storage] age cleanup: canvas={c}, session={s}, project={p} (cutoff={cutoff.date()})")
    return c, s, p


def _get_oldest_canvas(db_path: Path) -> List[Tuple[str, str]]:
    """获取最旧的未存档 canvas (id, updated_at)，按 updated_at 升序"""
    if not db_path.exists():
        return []
    with sqlite3.connect(db_path) as conn:
        _ensure_canvas_archive_column(conn)
        rows = conn.execute(
            "SELECT id, updated_at FROM canvases WHERE (archived = 0 OR archived IS NULL) ORDER BY updated_at ASC LIMIT 50"
        ).fetchall()
    return [(r[0], r[1]) for r in rows]


def _get_oldest_sessions(db_path: Path) -> List[Tuple[str, str]]:
    """获取最旧的 session (session_id, updated_at)，按 updated_at 升序"""
    if not db_path.exists():
        return []
    with sqlite3.connect(db_path) as conn:
        rows = conn.execute(
            "SELECT session_id, updated_at FROM sessions ORDER BY updated_at ASC LIMIT 50"
        ).fetchall()
    return [(r[0], r[1]) for r in rows]


def cleanup_by_size(max_size_gb: float, batch_size: int = 100) -> int:
    """
    按总大小清理：若数据库总大小超过 max_size_gb，则删除最旧的记录，
    优先删除 canvas（含快照），再删 session。
    返回删除的记录总数。
    """
    max_bytes = int(max_size_gb * 1024 * 1024 * 1024)
    db_paths = _get_db_paths()
    removed_total = 0
    
    while True:
        current_size = _get_total_size_bytes(db_paths)
        if current_size <= max_bytes:
            break
        
        # 优先删 canvas
        data_dir = Path(__file__).resolve().parents[2] / "data"
        canvas_db = data_dir / "canvas.db"
        oldest = _get_oldest_canvas(canvas_db)
        if oldest:
            cid = oldest[0][0]
            with sqlite3.connect(canvas_db) as conn:
                conn.execute("DELETE FROM canvas_versions WHERE canvas_id = ?", (cid,))
                conn.execute("DELETE FROM canvas_citations WHERE canvas_id = ?", (cid,))
                conn.execute("DELETE FROM draft_blocks WHERE canvas_id = ?", (cid,))
                conn.execute("DELETE FROM outline_sections WHERE canvas_id = ?", (cid,))
                conn.execute("DELETE FROM canvases WHERE id = ?", (cid,))
                conn.commit()
            removed_total += 1
            continue
        
        # canvas 空了，删 session
        sessions_db = data_dir / "sessions.db"
        oldest_sessions = _get_oldest_sessions(sessions_db)
        if oldest_sessions:
            sid = oldest_sessions[0][0]
            with sqlite3.connect(sessions_db) as conn:
                conn.execute("DELETE FROM turns WHERE session_id = ?", (sid,))
                conn.execute("DELETE FROM sessions WHERE session_id = ?", (sid,))
                conn.commit()
            removed_total += 1
            continue
        
        # 都空了，退出
        break
    
    if removed_total:
        final_size = _get_total_size_bytes(db_paths)
        logger.info(f"[storage] size cleanup: removed={removed_total}, size={final_size / 1024 / 1024:.1f}MB (limit={max_size_gb}GB)")
    return removed_total


def run_cleanup(max_age_days: int = 30, max_size_gb: float = 5.0, batch_size: int = 100) -> dict:
    """
    执行完整清理流程：先按时间清理，再按大小清理。
    返回清理统计。
    """
    c, s, p = cleanup_by_age(max_age_days, batch_size)
    size_removed = cleanup_by_size(max_size_gb, batch_size)
    
    db_paths = _get_db_paths()
    final_size = _get_total_size_bytes(db_paths)
    
    return {
        "age_cleanup": {"canvas": c, "session": s, "project": p},
        "size_cleanup": size_removed,
        "final_size_mb": round(final_size / 1024 / 1024, 2),
    }


def vacuum_databases() -> None:
    """对所有数据库执行 VACUUM，回收空间"""
    for db_path in _get_db_paths():
        if db_path.exists():
            try:
                with sqlite3.connect(db_path) as conn:
                    conn.execute("VACUUM")
                logger.debug(f"[storage] vacuumed {db_path.name}")
            except Exception as e:
                logger.warning(f"[storage] vacuum failed for {db_path.name}: {e}")


def get_storage_stats() -> dict:
    """获取存储统计信息"""
    db_paths = _get_db_paths()
    stats = {
        "total_size_mb": 0.0,
        "databases": {},
    }
    for p in db_paths:
        if p.exists():
            size_mb = p.stat().st_size / 1024 / 1024
            stats["databases"][p.name] = round(size_mb, 2)
            stats["total_size_mb"] += size_mb
    stats["total_size_mb"] = round(stats["total_size_mb"], 2)
    return stats
</file>

<file path="src/utils/task_runner.py">
"""
Background task worker: polls SQLite job queues and executes tasks via asyncio.to_thread.

Replaces in-memory ThreadPoolExecutor so that pending tasks survive process restarts.
Worker is started once inside the FastAPI lifespan hook.
"""

from __future__ import annotations

import asyncio
import json
import os
import sqlite3
import time
from pathlib import Path
from typing import Any, Dict, List

from src.log import get_logger

logger = get_logger(__name__)

# ── DB paths (mirror the paths used by job_store / ingest_job_store) ──
_DATA_DIR = Path(__file__).resolve().parents[1] / "data"
_DR_DB = _DATA_DIR / "deep_research_jobs.db"
_INGEST_DB = _DATA_DIR / "ingest_jobs.db"

# ── Concurrency caps (same as the old ThreadPoolExecutor max_workers) ──
_DR_MAX_CONCURRENT = 2
_INGEST_MAX_CONCURRENT = 2
_POLL_INTERVAL = 5  # seconds

# ── worker instance id (used by db resume-queue routing) ──
_WORKER_INSTANCE_ID = f"{os.getenv('HOSTNAME', 'local')}:{os.getpid()}"


def get_worker_instance_id() -> str:
    return _WORKER_INSTANCE_ID


# ────────────────────────────────────────────────
# Startup cleanup
# ────────────────────────────────────────────────

def cleanup_stale_jobs() -> None:
    """Reset running/cancelling jobs to error — they were interrupted by a process restart."""
    now = time.time()
    if _DR_DB.exists():
        try:
            conn = sqlite3.connect(str(_DR_DB), timeout=10)
            conn.execute("PRAGMA journal_mode=WAL")
            cur = conn.execute(
                """
                UPDATE deep_research_jobs
                SET status = 'error',
                    error_message = '服务重启，任务中断',
                    message = '服务重启，任务中断',
                    updated_at = ?,
                    finished_at = ?
                WHERE status IN ('running', 'cancelling', 'waiting_review')
                """,
                (now, now),
            )
            if cur.rowcount > 0:
                logger.warning(
                    "[task_runner] reset %d stale deep-research job(s) to error",
                    cur.rowcount,
                )
            conn.commit()
            # Reset stale resume-queue entries for current process crash.
            try:
                conn.execute(
                    """
                    UPDATE deep_research_resume_queue
                    SET status = 'error',
                        message = '服务重启，恢复请求失效',
                        updated_at = ?
                    WHERE status = 'running'
                    """,
                    (now,),
                )
                conn.commit()
            except Exception:
                # resume queue table may not exist before first migration; ignore safely
                pass
            conn.close()
        except Exception as e:
            logger.warning("[task_runner] cleanup deep-research stale jobs failed: %s", e)
    if _INGEST_DB.exists():
        try:
            conn = sqlite3.connect(str(_INGEST_DB), timeout=10)
            conn.execute("PRAGMA journal_mode=WAL")
            cur = conn.execute(
                """
                UPDATE ingest_jobs
                SET status = 'error',
                    error_message = '服务重启，任务中断',
                    message = '服务重启，任务中断',
                    updated_at = ?,
                    finished_at = ?
                WHERE status IN ('running', 'cancelling')
                """,
                (now, now),
            )
            if cur.rowcount > 0:
                logger.warning(
                    "[task_runner] reset %d stale ingest job(s) to error",
                    cur.rowcount,
                )
            conn.commit()
            conn.close()
        except Exception as e:
            logger.warning("[task_runner] cleanup ingest stale jobs failed: %s", e)


# ────────────────────────────────────────────────
# Pending-job fetch (lightweight, no schema init)
# ────────────────────────────────────────────────

def _fetch_pending(
    db_path: Path,
    table: str,
    json_col: str,
    limit: int,
    exclude: set[str],
) -> List[Dict[str, Any]]:
    """Read pending rows from *table*, skipping IDs already tracked in *exclude*."""
    if not db_path.exists() or limit <= 0:
        return []
    try:
        conn = sqlite3.connect(str(db_path), timeout=10)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA journal_mode=WAL")
        rows = conn.execute(
            f"""
            SELECT job_id, {json_col} FROM {table}
            WHERE status = 'pending'
            ORDER BY created_at ASC
            LIMIT ?
            """,
            (limit + len(exclude),),
        ).fetchall()
        conn.close()

        result: list[Dict[str, Any]] = []
        for row in rows:
            jid = row["job_id"]
            if jid in exclude:
                continue
            try:
                data = json.loads(row[json_col] or "{}")
            except Exception:
                data = {}
            result.append({"job_id": jid, "data": data})
            if len(result) >= limit:
                break
        return result
    except Exception as e:
        logger.debug("[task_runner] fetch pending from %s failed: %s", table, e)
        return []


def _claim_pending_job(db_path: Path, table: str, job_id: str) -> bool:
    """
    Atomically claim a pending job by switching pending -> running.
    Returns True only when the row was still pending and got claimed.
    """
    if not db_path.exists():
        return False
    now = time.time()
    try:
        conn = sqlite3.connect(str(db_path), timeout=10)
        conn.execute("PRAGMA journal_mode=WAL")
        cur = conn.execute(
            f"""
            UPDATE {table}
            SET status = 'running',
                message = 'Worker 已领取任务，准备执行',
                updated_at = ?
            WHERE job_id = ? AND status = 'pending'
            """,
            (now, job_id),
        )
        conn.commit()
        conn.close()
        return cur.rowcount > 0
    except Exception as e:
        logger.warning("[task_runner] claim job failed table=%s job_id=%s err=%s", table, job_id, e)
        return False


# ────────────────────────────────────────────────
# Per-job executors (each runs in a worker thread)
# ────────────────────────────────────────────────

async def _exec_dr_job(job_id: str, request: dict) -> None:
    """Execute a deep-research job; wraps the existing sync business logic."""
    try:
        from src.api.routes_chat import _run_deep_research_job_safe
        from src.api.schemas import DeepResearchConfirmRequest

        optional_user_id = request.get("_worker_user_id")
        body_data = {k: v for k, v in request.items() if k != "_worker_user_id"}
        body = DeepResearchConfirmRequest(**body_data)

        await asyncio.to_thread(
            _run_deep_research_job_safe,
            job_id=job_id,
            body=body,
            optional_user_id=optional_user_id,
        )
    except Exception as e:
        logger.error("[task_runner] DR job %s failed: %s", job_id, e, exc_info=True)
        try:
            from src.collaboration.research.job_store import update_job
            update_job(
                job_id,
                status="error",
                error_message=str(e),
                message=f"Worker 执行失败: {e}",
                finished_at=time.time(),
            )
        except Exception:
            pass


async def _exec_resume(job_id: str) -> None:
    """Resume a suspended deep-research job (in-memory langgraph state)."""
    try:
        from src.api.routes_chat import _dr_get_suspended_runtime, _resume_suspended_job

        if not _dr_get_suspended_runtime(job_id):
            raise RuntimeError("当前实例不存在挂起上下文，无法恢复")

        await asyncio.to_thread(_resume_suspended_job, job_id)
    except Exception as e:
        logger.error("[task_runner] DR resume %s failed: %s", job_id, e, exc_info=True)
        raise


async def _exec_ingest_job(job_id: str, cfg: dict) -> None:
    """Execute an ingest job; wraps the existing sync business logic."""
    try:
        from src.api.routes_ingest import _run_ingest_job_safe

        await asyncio.to_thread(_run_ingest_job_safe, job_id, cfg)
    except Exception as e:
        logger.error("[task_runner] ingest job %s failed: %s", job_id, e, exc_info=True)
        try:
            from src.indexing.ingest_job_store import update_job
            update_job(
                job_id,
                status="error",
                error_message=str(e),
                message=f"Worker 执行失败: {e}",
                finished_at=time.time(),
            )
        except Exception:
            pass


# ────────────────────────────────────────────────
# Main polling loop
# ────────────────────────────────────────────────

async def run_background_worker() -> None:
    """Poll SQLite for pending tasks every POLL_INTERVAL seconds and dispatch them."""
    logger.info(
        "[task_runner] background worker started (instance=%s, poll=%ds, dr_max=%d, ingest_max=%d)",
        _WORKER_INSTANCE_ID,
        _POLL_INTERVAL,
        _DR_MAX_CONCURRENT,
        _INGEST_MAX_CONCURRENT,
    )

    dr_tasks: dict[str, asyncio.Task] = {}
    ingest_tasks: dict[str, asyncio.Task] = {}

    while True:
        try:
            # ── Prune completed tasks ──
            for jid in [k for k, t in dr_tasks.items() if t.done()]:
                dr_tasks.pop(jid, None)
            for jid in [k for k, t in ingest_tasks.items() if t.done()]:
                ingest_tasks.pop(jid, None)

            # ── Deep Research: process resume queue first (same DR slots) ──
            dr_available = _DR_MAX_CONCURRENT - len(dr_tasks)
            if dr_available > 0:
                try:
                    from src.collaboration.research.job_store import (
                        claim_resume_requests,
                        complete_resume_request,
                    )
                    resume_rows = claim_resume_requests(_WORKER_INSTANCE_ID, limit=dr_available)
                    for row in resume_rows:
                        rid = int(row.get("id") or 0)
                        jid = str(row.get("job_id") or "")
                        if not jid or rid <= 0:
                            complete_resume_request(rid, "error", "恢复请求缺少 job_id")
                            continue
                        existing = dr_tasks.get(jid)
                        if existing and not existing.done():
                            complete_resume_request(rid, "error", "任务正在执行，无法恢复")
                            continue

                        async def _run_resume_request(resume_id: int, job_id: str) -> None:
                            try:
                                await _exec_resume(job_id)
                                complete_resume_request(resume_id, "done", "恢复请求执行完成")
                            except Exception as e:
                                complete_resume_request(resume_id, "error", f"恢复请求失败: {e}")

                        logger.info("[task_runner] claimed resume request id=%s job=%s", rid, jid)
                        dr_tasks[jid] = asyncio.create_task(_run_resume_request(rid, jid))
                except Exception as e:
                    logger.warning("[task_runner] resume queue poll failed: %s", e)

            # ── Deep Research: claim pending jobs ──
            dr_available = _DR_MAX_CONCURRENT - len(dr_tasks)
            if dr_available > 0:
                pending = _fetch_pending(
                    _DR_DB,
                    "deep_research_jobs",
                    "request_json",
                    limit=dr_available,
                    exclude=set(dr_tasks.keys()),
                )
                for job in pending:
                    jid = job["job_id"]
                    if not _claim_pending_job(_DR_DB, "deep_research_jobs", jid):
                        continue
                    logger.info("[task_runner] claimed DR job %s", jid)
                    dr_tasks[jid] = asyncio.create_task(_exec_dr_job(jid, job["data"]))

            # ── Ingest: claim pending jobs ──
            ingest_available = _INGEST_MAX_CONCURRENT - len(ingest_tasks)
            if ingest_available > 0:
                pending = _fetch_pending(
                    _INGEST_DB,
                    "ingest_jobs",
                    "payload_json",
                    limit=ingest_available,
                    exclude=set(ingest_tasks.keys()),
                )
                for job in pending:
                    jid = job["job_id"]
                    if not _claim_pending_job(_INGEST_DB, "ingest_jobs", jid):
                        continue
                    logger.info("[task_runner] claimed ingest job %s", jid)
                    ingest_tasks[jid] = asyncio.create_task(
                        _exec_ingest_job(jid, job["data"])
                    )

        except Exception as e:
            logger.error("[task_runner] poll cycle error: %s", e, exc_info=True)

        await asyncio.sleep(_POLL_INTERVAL)
</file>

<file path="src/__init__.py">
# 深海科研知识库 RAG 系统
</file>

<file path="tests/__init__.py">

</file>

<file path="tests/conftest.py">
"""
共享 Fixtures: Mock LLM / Embedder / Milvus 等。
"""

import pytest
from unittest.mock import MagicMock


@pytest.fixture
def mock_llm_client():
    """模拟 LLM 客户端"""
    client = MagicMock()

    def _chat(messages=None, model=None, max_tokens=None, **kwargs):
        return {"final_text": '{"intent": "chat", "confidence": 0.9, "params": {}}'}

    client.chat.side_effect = _chat
    return client


@pytest.fixture
def sample_text_blocks():
    """示例文本 blocks"""
    return [
        {
            "block_type": "text",
            "heading_path": ["Introduction"],
            "text": "Deep-sea hydrothermal vents are one of the most extreme environments on Earth. "
                    "These systems support unique chemosynthetic ecosystems. "
                    "Tube worms and mussels are among the dominant fauna.",
            "page_index": 0,
            "block_id": "b001",
        },
        {
            "block_type": "text",
            "heading_path": ["Introduction"],
            "text": "The discovery of hydrothermal vents in 1977 revolutionized our understanding of life. "
                    "Organisms at these vents derive energy from chemical reactions rather than sunlight.",
            "page_index": 1,
            "block_id": "b002",
        },
        {
            "block_type": "text",
            "heading_path": ["Methods"],
            "text": "Samples were collected using ROV at depths of 2500-3000m. "
                    "DNA extraction was performed using the DNeasy protocol. "
                    "Metagenomic sequencing was conducted on Illumina NovaSeq platform.",
            "page_index": 2,
            "block_id": "b003",
        },
    ]


@pytest.fixture
def sample_table_block():
    """示例表格 block"""
    return {
        "block_type": "table",
        "heading_path": ["Results", "Species Distribution"],
        "text": "",
        "table_data": {
            "markdown": "| Species | Count | Depth |\n|---|---|---|\n| Tube worm | 150 | 2600m |\n| Mussel | 230 | 2800m |",
        },
        "page_index": 3,
        "block_id": "t001",
    }


@pytest.fixture
def sample_figure_block():
    """示例图表 block"""
    return {
        "block_type": "figure",
        "heading_path": ["Results"],
        "text": "",
        "figure_data": {"caption": "Distribution of hydrothermal vent species at various depths"},
        "page_index": 4,
        "block_id": "f001",
    }


@pytest.fixture
def sample_claims():
    """示例 claims"""
    return [
        {
            "claim_id": "claim_001",
            "text": "Tube worms dominate at shallower vents",
            "evidence": "Survey data shows...",
            "source_block_ids": ["b001", "b002"],
        },
        {
            "claim_id": "claim_002",
            "text": "Chemosynthesis is the primary energy source",
            "evidence": "Isotopic analysis...",
            "source_block_ids": ["b002"],
        },
    ]
</file>

<file path="tests/test_chunker.py">
"""
chunker 单元测试：chunk_blocks / 句子切分 / 表格 / 图片 / claims 注入
"""

import pytest
from src.chunking.chunker import (
    Chunk,
    ChunkConfig,
    chunk_blocks,
    chunk_table_block,
    chunk_image_block,
    _sentence_tokenize,
    _generate_stable_id,
    _merge_metadata,
    _match_claims_to_chunk,
)


# ── 句子切分 ──

class TestSentenceTokenize:
    def test_basic_split(self):
        result = _sentence_tokenize("Hello world. How are you? Fine.")
        assert len(result) == 3

    def test_chinese_split(self):
        # 句子切分 regex 要求标点后有空格，中文句号后无空格时不拆分
        result = _sentence_tokenize("深海热泉是极端环境。 生物依靠化能合成。")
        assert len(result) == 2

    def test_empty_input(self):
        assert _sentence_tokenize("") == []
        assert _sentence_tokenize("   ") == []

    def test_single_sentence(self):
        result = _sentence_tokenize("No punctuation at end")
        assert result == ["No punctuation at end"]


# ── 稳定 ID 生成 ──

class TestGenerateStableId:
    def test_deterministic(self):
        id1 = _generate_stable_id("doc1", "Introduction", 0, "hello", 0)
        id2 = _generate_stable_id("doc1", "Introduction", 0, "hello", 0)
        assert id1 == id2

    def test_different_inputs(self):
        id1 = _generate_stable_id("doc1", "Introduction", 0, "hello", 0)
        id2 = _generate_stable_id("doc2", "Introduction", 0, "hello", 0)
        assert id1 != id2

    def test_length_16(self):
        cid = _generate_stable_id("doc", "sec", 1, "prefix", 0)
        assert len(cid) == 16


# ── 元数据合并 ──

class TestMergeMetadata:
    def test_basic(self, sample_text_blocks):
        meta = _merge_metadata(sample_text_blocks[:2], "test_doc")
        assert meta["doc_id"] == "test_doc"
        assert meta["page_range"] == [0, 1]
        assert meta["section_path"] == "Introduction"

    def test_empty(self):
        meta = _merge_metadata([], "doc1")
        assert meta["doc_id"] == "doc1"
        assert meta["page_range"] == [0, 0]


# ── chunk_blocks 主逻辑 ──

class TestChunkBlocks:
    def test_basic_chunking(self, sample_text_blocks):
        chunks = chunk_blocks(sample_text_blocks, "test_doc")
        assert len(chunks) > 0
        for c in chunks:
            assert isinstance(c, Chunk)
            assert c.text.strip()
            assert c.chunk_id

    def test_empty_blocks(self):
        chunks = chunk_blocks([], "empty_doc")
        assert chunks == []

    def test_blank_text_skipped(self):
        blocks = [{"block_type": "text", "heading_path": [], "text": "   ", "page_index": 0, "block_id": "b_empty"}]
        chunks = chunk_blocks(blocks, "doc")
        assert chunks == []

    def test_section_boundary_creates_new_chunk(self, sample_text_blocks):
        """不同 section 应导致 buffer 刷新"""
        cfg = ChunkConfig(target_chars=5000, min_chars=50, max_chars=10000)
        chunks = chunk_blocks(sample_text_blocks, "doc", config=cfg)
        sections = [c.meta.get("section_path") for c in chunks]
        # Introduction 和 Methods 应在不同 chunk
        assert "Methods" in sections

    def test_caption_footnote_skipped(self):
        blocks = [
            {"block_type": "caption", "heading_path": [], "text": "Figure 1 caption", "page_index": 0, "block_id": "c1"},
            {"block_type": "footnote", "heading_path": [], "text": "Footnote text", "page_index": 0, "block_id": "f1"},
        ]
        chunks = chunk_blocks(blocks, "doc")
        assert chunks == []

    def test_max_chars_split(self):
        """超长文本应被按句拆分"""
        long_text = ". ".join([f"Sentence number {i}" for i in range(100)]) + "."
        blocks = [{"block_type": "text", "heading_path": ["Long"], "text": long_text, "page_index": 0, "block_id": "long1"}]
        cfg = ChunkConfig(target_chars=200, min_chars=50, max_chars=300)
        chunks = chunk_blocks(blocks, "doc", config=cfg)
        assert len(chunks) > 1
        for c in chunks:
            assert c.content_type == "text"

    def test_mixed_block_types(self, sample_text_blocks, sample_table_block, sample_figure_block):
        all_blocks = sample_text_blocks + [sample_table_block, sample_figure_block]
        chunks = chunk_blocks(all_blocks, "doc")
        content_types = {c.content_type for c in chunks}
        assert "text" in content_types
        assert "table" in content_types


# ── 表格切块 ──

class TestChunkTableBlock:
    def test_basic_table(self, sample_table_block):
        chunks = chunk_table_block(sample_table_block, "doc", max_chars=5000)
        assert len(chunks) == 1
        assert chunks[0].content_type == "table"
        assert "Tube worm" in chunks[0].text

    def test_empty_table(self):
        block = {"block_type": "table", "heading_path": [], "text": "", "table_data": {}, "page_index": 0, "block_id": "t_e"}
        chunks = chunk_table_block(block, "doc", max_chars=5000)
        assert chunks == []

    def test_large_table_split(self):
        """大表格应被按行分片"""
        rows = "\n".join([f"| row{i} | val{i} |" for i in range(50)])
        md = "| Col1 | Col2 |\n|---|---|\n" + rows
        block = {"block_type": "table", "heading_path": [], "text": "", "table_data": {"markdown": md}, "page_index": 0, "block_id": "t_big"}
        chunks = chunk_table_block(block, "doc", max_chars=200, rows_per_chunk=5)
        assert len(chunks) > 1


# ── 图片切块 ──

class TestChunkImageBlock:
    def test_basic_figure(self, sample_figure_block):
        chunks = chunk_image_block(sample_figure_block, "doc")
        assert len(chunks) == 1
        assert chunks[0].content_type == "image_caption"
        assert "hydrothermal" in chunks[0].text

    def test_no_caption_skipped(self):
        block = {"block_type": "figure", "heading_path": [], "text": "", "figure_data": {}, "page_index": 0, "block_id": "f_nc"}
        chunks = chunk_image_block(block, "doc")
        assert chunks == []


# ── Claims 注入 ──

class TestClaimsInjection:
    def test_claims_matched(self, sample_text_blocks, sample_claims):
        chunks = chunk_blocks(sample_text_blocks, "test_doc", claims=sample_claims)
        claim_chunks = [c for c in chunks if c.meta.get("claims")]
        # 至少有一些 chunk 匹配到了 claims
        assert len(claim_chunks) > 0

    def test_no_claims(self, sample_text_blocks):
        chunks = chunk_blocks(sample_text_blocks, "doc", claims=None)
        for c in chunks:
            assert "claims" not in c.meta

    def test_match_claims_to_chunk_basic(self):
        chunk = Chunk(
            chunk_id="c1",
            text="test",
            content_type="text",
            meta={"source_uri": "doc#b001"},
        )
        claims = [
            {"claim_id": "cl1", "source_block_ids": ["b001"]},
            {"claim_id": "cl2", "source_block_ids": ["b999"]},
        ]
        matched = _match_claims_to_chunk(chunk, claims)
        assert matched == ["cl1"]

    def test_match_claims_empty(self):
        chunk = Chunk(chunk_id="c1", text="test", content_type="text", meta={"source_uri": "doc#b001"})
        assert _match_claims_to_chunk(chunk, []) == []
</file>

<file path="tests/test_citation_resolution.py">
"""
Citation resolution tests:
- ref_hash -> cite_key replacement
- document-level merge (multiple chunks from same doc)
- cross-stage stable cite_key reuse
"""

import sys
from pathlib import Path

# Support direct execution: `python tests/test_citation_resolution.py`
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.collaboration.citation.manager import resolve_response_citations
from src.retrieval.evidence import EvidenceChunk


def _mk_chunk(
    *,
    chunk_id: str,
    doc_id: str,
    text: str = "evidence text",
    title: str = "Sample Title",
    authors=None,
    year: int | None = 2024,
    url: str | None = None,
) -> EvidenceChunk:
    return EvidenceChunk(
        chunk_id=chunk_id,
        doc_id=doc_id,
        text=text,
        score=0.9,
        source_type="web" if url else "dense",
        doc_title=title,
        authors=authors or ["Smith, John"],
        year=year,
        url=url,
    )


def test_same_document_multiple_hashes_resolve_to_one_citation():
    c1 = _mk_chunk(chunk_id="chunk-a", doc_id="doc-1", title="Paper A")
    c2 = _mk_chunk(chunk_id="chunk-b", doc_id="doc-1", title="Paper A")
    c3 = _mk_chunk(chunk_id="chunk-c", doc_id="doc-2", title="Paper B", authors=["Jones, Amy"], year=2023)

    raw = f"Claim X [{c1.ref_hash}] and supporting Y [{c2.ref_hash}]. Compare Z [{c3.ref_hash}]."
    resolved, citations, ref_map = resolve_response_citations(raw, [c1, c2, c3], format="author_date")

    # same doc -> same cite_key
    assert ref_map[c1.ref_hash] == ref_map[c2.ref_hash]
    assert len(citations) == 2

    # both hashes replaced to same key
    assert f"[{c1.ref_hash}]" not in resolved
    assert f"[{c2.ref_hash}]" not in resolved
    assert f"[{c3.ref_hash}]" not in resolved
    assert resolved.count(f"[{ref_map[c1.ref_hash]}]") == 2
    assert resolved.count(f"[{ref_map[c3.ref_hash]}]") == 1


def test_cross_stage_reuse_keeps_stable_numeric_keys():
    # phase 1
    a1 = _mk_chunk(chunk_id="a-1", doc_id="doc-a", title="Doc A")
    b1 = _mk_chunk(chunk_id="b-1", doc_id="doc-b", title="Doc B", authors=["Brown, Kim"], year=2022)
    text1 = f"A [{a1.ref_hash}] B [{b1.ref_hash}]"

    shared_doc_map = {}
    shared_keys = set()
    resolved1, cites1, ref_map1 = resolve_response_citations(
        text1,
        [a1, b1],
        format="numeric",
        doc_key_to_cite_key=shared_doc_map,
        existing_cite_keys=shared_keys,
        include_unreferenced_documents=False,
    )

    assert resolved1 == "A [1] B [2]"
    assert len(cites1) == 2

    # phase 2: doc-a appears again + new doc-c
    a2 = _mk_chunk(chunk_id="a-2", doc_id="doc-a", title="Doc A")
    c1 = _mk_chunk(chunk_id="c-1", doc_id="doc-c", title="Doc C", authors=["Chen, Li"], year=2021)
    text2 = f"A-again [{a2.ref_hash}] and C [{c1.ref_hash}]"
    resolved2, cites2, ref_map2 = resolve_response_citations(
        text2,
        [a2, c1],
        format="numeric",
        doc_key_to_cite_key=shared_doc_map,
        existing_cite_keys=shared_keys,
        include_unreferenced_documents=False,
    )

    # doc-a keeps key [1], new doc-c gets next key [3]
    assert resolved2 == "A-again [1] and C [3]"
    assert ref_map1[a1.ref_hash] == "1"
    assert ref_map2[a2.ref_hash] == "1"
    assert ref_map2[c1.ref_hash] == "3"
    assert len(cites2) == 2


def test_uppercase_hash_is_supported_for_replacement():
    c = _mk_chunk(chunk_id="upper-1", doc_id="doc-upper", title="Upper Hash Doc")
    raw = f"Upper hash citation [{c.ref_hash.upper()}]."
    resolved, _citations, _ref_map = resolve_response_citations(raw, [c], format="numeric")
    assert resolved == "Upper hash citation [1]."


def test_can_exclude_unreferenced_documents_from_citation_list():
    c1 = _mk_chunk(chunk_id="r-1", doc_id="doc-ref", title="Referenced Doc")
    c2 = _mk_chunk(chunk_id="u-1", doc_id="doc-unref", title="Unreferenced Doc")
    raw = f"Only this one is used [{c1.ref_hash}]"

    resolved, citations, ref_map = resolve_response_citations(
        raw,
        [c1, c2],
        format="author_date",
        include_unreferenced_documents=False,
    )

    assert f"[{c1.ref_hash}]" not in resolved
    assert c2.ref_hash not in ref_map
    assert len(citations) == 1
    assert citations[0].doc_id == "doc-ref"


if __name__ == "__main__":
    import pytest

    raise SystemExit(pytest.main([__file__]))
</file>

<file path="tests/test_hybrid_retriever.py">
"""
HybridRetriever 单元测试：RRF 融合 / should_use_hipporag / HybridRetriever 构造
"""

import pytest
from unittest.mock import MagicMock, patch


# ── weighted_rrf (模块级纯函数) ──

class TestWeightedRRF:
    def test_basic_fusion(self):
        from src.retrieval.hybrid_retriever import weighted_rrf

        dense_hits = [
            {"chunk_id": "a", "score": 0.9},
            {"chunk_id": "b", "score": 0.7},
        ]
        sparse_hits = [
            {"chunk_id": "b", "score": 0.95},
            {"chunk_id": "c", "score": 0.6},
        ]
        fused = weighted_rrf(dense_hits, sparse_hits, w_dense=0.7, w_sparse=0.3, k=60)
        ids = [cid for cid, _ in fused]
        # b 出现在两个列表中，分数应最高
        assert ids[0] == "b"
        assert len(fused) == 3

    def test_empty_inputs(self):
        from src.retrieval.hybrid_retriever import weighted_rrf
        fused = weighted_rrf([], [], w_dense=0.7, w_sparse=0.3, k=60)
        assert fused == []

    def test_single_list(self):
        from src.retrieval.hybrid_retriever import weighted_rrf
        hits = [{"chunk_id": "x", "score": 1.0}]
        fused = weighted_rrf(hits, [], w_dense=1.0, w_sparse=0.0, k=60)
        assert len(fused) == 1
        assert fused[0][0] == "x"

    def test_returns_sorted_descending(self):
        from src.retrieval.hybrid_retriever import weighted_rrf
        dense = [{"chunk_id": "a"}, {"chunk_id": "b"}, {"chunk_id": "c"}]
        sparse = [{"chunk_id": "c"}, {"chunk_id": "a"}]
        fused = weighted_rrf(dense, sparse)
        scores = [s for _, s in fused]
        assert scores == sorted(scores, reverse=True)


# ── should_use_hipporag (模块级纯函数) ──

class TestShouldUseHippoRAG:
    def test_multi_entity_with_relation(self):
        from src.retrieval.hybrid_retriever import should_use_hipporag
        # 英文大写词算实体，"关系"是关系关键词 → True
        result = should_use_hipporag("Tube Worm 和 Mussel 的关系是什么")
        assert result is True

    def test_no_relation_keyword(self):
        from src.retrieval.hybrid_retriever import should_use_hipporag
        # 无关系关键词 → False
        result = should_use_hipporag("深海热泉环境描述")
        assert result is False

    def test_empty_query(self):
        from src.retrieval.hybrid_retriever import should_use_hipporag
        assert should_use_hipporag("") is False

    def test_single_entity_with_relation(self):
        from src.retrieval.hybrid_retriever import should_use_hipporag
        # 只有一个实体 → False（需要多实体才触发）
        result = should_use_hipporag("管虫的关系")
        assert result is False


# ── _count_entities (模块级纯函数) ──

class TestCountEntities:
    def test_chinese_entities(self):
        from src.retrieval.hybrid_retriever import _count_entities
        # 连续中文字符算一个实体，用空格/非中文字符分隔才算多个
        count = _count_entities("管虫 贻贝 热泉")
        assert count >= 3

    def test_english_entities(self):
        from src.retrieval.hybrid_retriever import _count_entities
        count = _count_entities("Tube worms and Mussels")
        assert count >= 2

    def test_empty(self):
        from src.retrieval.hybrid_retriever import _count_entities
        assert _count_entities("") == 0


# ── RetrievalConfig 数据类 ──

class TestRetrievalConfig:
    def test_defaults(self):
        from src.retrieval.hybrid_retriever import RetrievalConfig
        cfg = RetrievalConfig()
        assert cfg.mode == "hybrid"
        assert cfg.top_k == 10
        assert cfg.rerank is True
        assert cfg.graph_weight == 0.3

    def test_custom(self):
        from src.retrieval.hybrid_retriever import RetrievalConfig
        cfg = RetrievalConfig(mode="vector", top_k=20, rerank=False)
        assert cfg.mode == "vector"
        assert cfg.top_k == 20
        assert cfg.rerank is False
</file>

<file path="tests/test_intent_parser.py">
"""
IntentParser 单元测试：命令解析 / 自然语言分类 / 边界情况
"""

import json
import pytest
from unittest.mock import MagicMock

from src.collaboration.intent.parser import (
    IntentType,
    IntentParser,
    ParsedIntent,
    is_deep_research,
    is_retrieval_intent,
)


# ── IntentType 枚举 ──

class TestIntentType:
    def test_basic_values(self):
        assert IntentType.CHAT.value == "chat"
        assert IntentType.DEEP_RESEARCH.value == "deep_research"
        assert IntentType.UNCLEAR.value == "unclear"

    def test_legacy_mapping(self):
        """旧值应正确映射到新类型"""
        assert IntentType("search_exploratory") == IntentType.CHAT
        assert IntentType("auto_complete") == IntentType.DEEP_RESEARCH
        assert IntentType("outline_generate") == IntentType.CHAT
        assert IntentType("chitchat") == IntentType.CHAT

    def test_unknown_legacy_returns_none(self):
        with pytest.raises(ValueError):
            IntentType("nonexistent_intent")


# ── 命令解析 ──

class TestCommandParsing:
    @pytest.fixture
    def parser(self, mock_llm_client):
        return IntentParser(mock_llm_client)

    def test_auto_command(self, parser):
        result = parser.parse("/auto")
        assert result.intent_type == IntentType.DEEP_RESEARCH
        assert result.confidence == 1.0
        assert result.from_command is True

    def test_auto_with_args(self, parser):
        result = parser.parse("/auto 写一篇关于深海的综述")
        assert result.intent_type == IntentType.DEEP_RESEARCH
        assert "深海" in result.params.get("args", "")

    def test_search_command(self, parser):
        result = parser.parse("/search hydrothermal vents")
        assert result.intent_type == IntentType.CHAT
        assert result.from_command is True

    def test_outline_add_compound(self, parser):
        result = parser.parse("/outline add Introduction")
        assert result.intent_type == IntentType.CHAT
        assert result.from_command is True

    def test_unknown_command(self, parser):
        result = parser.parse("/unknown_cmd blah")
        assert result.intent_type == IntentType.CHAT
        assert result.confidence == 0.5

    def test_all_known_commands(self, parser):
        cmds = ["/search", "/explore", "/outline", "/draft", "/edit", "/status", "/export", "/set"]
        for cmd in cmds:
            result = parser.parse(cmd)
            assert result.intent_type == IntentType.CHAT
            assert result.from_command is True


# ── 自然语言解析 ──

class TestNaturalLanguageParsing:
    def test_chat_intent(self, mock_llm_client):
        mock_llm_client.chat.return_value = {
            "final_text": json.dumps({"intent": "chat", "confidence": 0.95, "params": {}})
        }
        parser = IntentParser(mock_llm_client)
        result = parser.parse("什么是深海热泉？")
        assert result.intent_type == IntentType.CHAT
        assert result.confidence >= 0.9

    def test_deep_research_intent(self, mock_llm_client):
        # 需要清除 conftest 中设的 side_effect，否则 return_value 不生效
        mock_llm_client.chat.side_effect = None
        mock_llm_client.chat.return_value = {
            "final_text": json.dumps({"intent": "deep_research", "confidence": 0.92, "params": {}})
        }
        parser = IntentParser(mock_llm_client)
        result = parser.parse("帮我写一篇关于深海化能合成的完整综述")
        assert result.intent_type == IntentType.DEEP_RESEARCH

    def test_llm_failure_fallback(self, mock_llm_client):
        """LLM 调用失败时应回退到 CHAT"""
        mock_llm_client.chat.side_effect = Exception("API timeout")
        parser = IntentParser(mock_llm_client)
        result = parser.parse("test query")
        assert result.intent_type == IntentType.CHAT
        assert result.confidence <= 0.5

    def test_llm_invalid_json(self, mock_llm_client):
        """LLM 返回非法 JSON 应回退"""
        mock_llm_client.chat.return_value = {"final_text": "not valid json at all"}
        parser = IntentParser(mock_llm_client)
        result = parser.parse("some question")
        assert result.intent_type == IntentType.CHAT

    def test_empty_input(self, mock_llm_client):
        parser = IntentParser(mock_llm_client)
        result = parser.parse("")
        # 空输入走 NL 路径（不以 / 开头）
        assert isinstance(result, ParsedIntent)

    def test_with_history(self, mock_llm_client):
        mock_llm_client.chat.return_value = {
            "final_text": json.dumps({"intent": "chat", "confidence": 0.85, "params": {}})
        }
        parser = IntentParser(mock_llm_client)
        history = [{"role": "user", "content": "之前的问题"}, {"role": "assistant", "content": "之前的回答"}]
        result = parser.parse("继续", history=history)
        assert isinstance(result, ParsedIntent)


# ── 辅助函数 ──

class TestHelpers:
    def test_is_deep_research_true(self):
        p = ParsedIntent(intent_type=IntentType.DEEP_RESEARCH, confidence=1.0)
        assert is_deep_research(p) is True

    def test_is_deep_research_false(self):
        p = ParsedIntent(intent_type=IntentType.CHAT, confidence=1.0)
        assert is_deep_research(p) is False

    def test_is_retrieval_intent_compat(self):
        """兼容函数应与 is_deep_research 行为一致"""
        p = ParsedIntent(intent_type=IntentType.DEEP_RESEARCH, confidence=1.0)
        assert is_retrieval_intent(p) == is_deep_research(p)
</file>

<file path="tests/test_research_agent_state_compaction.py">
"""
Research agent state compaction tests:
- _accumulate_evidence_chunks stores shallow copies
- original chunks are not mutated
- text/raw_content are overwritten by configured empty value
"""

import sys
from pathlib import Path
from types import SimpleNamespace

# Support direct execution: `python tests/test_research_agent_state_compaction.py`
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.collaboration.research.agent import _accumulate_evidence_chunks
from src.retrieval.evidence import EvidenceChunk


def _mk_chunk(chunk_id: str, text: str = "original text") -> EvidenceChunk:
    return EvidenceChunk(
        chunk_id=chunk_id,
        doc_id=f"doc-{chunk_id}",
        text=text,
        score=0.88,
        source_type="dense",
        doc_title="Doc Title",
        authors=["Alice"],
        year=2024,
    )


def test_accumulate_compacts_text_and_keeps_original_untouched():
    state = {"evidence_chunks": []}
    chunk = _mk_chunk("c-1", text="very long evidence body")
    # Simulate optional payload fields that may exist on some chunk-like objects.
    chunk.raw_content = "raw payload"

    _accumulate_evidence_chunks(state, [chunk])

    assert len(state["evidence_chunks"]) == 1
    stored = state["evidence_chunks"][0]

    # Stored object should be a copy, not the original instance.
    assert stored is not chunk
    assert stored.chunk_id == chunk.chunk_id

    # Stored payload fields are compacted.
    assert stored.text == ""
    assert getattr(stored, "raw_content", None) == ""

    # Original chunk remains unchanged.
    assert chunk.text == "very long evidence body"
    assert chunk.raw_content == "raw payload"


def test_accumulate_uses_configurable_empty_value():
    state = {"evidence_chunks": [], "evidence_chunk_empty_value": None}
    chunk = SimpleNamespace(chunk_id="c-2", text="body", raw_content="raw")

    _accumulate_evidence_chunks(state, [chunk])

    stored = state["evidence_chunks"][0]
    assert stored is not chunk
    assert stored.text is None
    assert stored.raw_content is None


if __name__ == "__main__":
    import pytest

    raise SystemExit(pytest.main([__file__]))
</file>

<file path="tests/test_tools_routing.py">
"""
Tests for dynamic tool routing in src.llm.tools.
"""

import sys
from pathlib import Path

# Support direct execution: `python tests/test_tools_routing.py`
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.llm.tools import get_routed_skills, get_tools_by_names


def _tool_names(tools):
    return [t.name for t in tools]


def test_routing_local_always_on_when_search_enabled():
    names = _tool_names(
        get_routed_skills(
            message="随便聊聊",
            current_stage="explore",
            search_mode="local",
            allowed_web_providers=None,
        )
    )
    assert "search_local" in names


def test_routing_none_mode_can_be_tolless():
    names = _tool_names(
        get_routed_skills(
            message="随便聊聊",
            current_stage="explore",
            search_mode="none",
            allowed_web_providers=None,
        )
    )
    assert names == []


def test_routing_web_providers_subset_and_empty_list_semantics():
    subset = _tool_names(
        get_routed_skills(
            message="请检索外部信息",
            current_stage="explore",
            search_mode="hybrid",
            allowed_web_providers=["tavily", "scholar"],
        )
    )
    assert "search_local" in subset
    assert "search_web" in subset
    assert "search_scholar" in subset
    assert "search_ncbi" not in subset

    empty = _tool_names(
        get_routed_skills(
            message="请检索外部信息",
            current_stage="explore",
            search_mode="hybrid",
            allowed_web_providers=[],
        )
    )
    assert empty == ["search_local"]


def test_routing_analysis_and_graph_keywords():
    analysis = _tool_names(
        get_routed_skills(
            message="请对比不同方法并做统计计算",
            current_stage="explore",
            search_mode="local",
            allowed_web_providers=None,
        )
    )
    assert "compare_papers" in analysis
    assert "run_code" in analysis

    graph = _tool_names(
        get_routed_skills(
            message="帮我看下知识图谱关系网络",
            current_stage="explore",
            search_mode="local",
            allowed_web_providers=None,
        )
    )
    assert "explore_graph" in graph


def test_routing_collab_stage_and_keyword_trigger():
    by_stage = _tool_names(
        get_routed_skills(
            message="继续写",
            current_stage="refine",
            search_mode="local",
            allowed_web_providers=None,
        )
    )
    assert "canvas" in by_stage
    assert "get_citations" in by_stage

    by_keyword = _tool_names(
        get_routed_skills(
            message="更新画布草稿并整理引用",
            current_stage="explore",
            search_mode="local",
            allowed_web_providers=None,
        )
    )
    assert "canvas" in by_keyword
    assert "get_citations" in by_keyword


def test_get_tools_by_names_preserves_core_order_and_ignores_unknown():
    names = _tool_names(get_tools_by_names(["run_code", "search_local", "unknown_tool"]))
    # CORE_TOOLS order places search_local before run_code
    assert names == ["search_local", "run_code"]


if __name__ == "__main__":
    import pytest

    raise SystemExit(pytest.main([__file__]))
</file>

<file path="tests/test_tracing_modes.py">
"""
Minimal tracing integration tests:
- disabled mode: traceable is a no-op fallback
- enabled mode: importing react_loop with run_type="agent" raises no warning
"""

import importlib
import sys
import warnings
from pathlib import Path

import pytest


# Support direct execution: `python tests/test_tracing_modes.py`
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))


def _reload_module(module_name: str):
    """Drop module cache and import fresh."""
    sys.modules.pop(module_name, None)
    return importlib.import_module(module_name)


def test_tracing_disabled_uses_noop_traceable(monkeypatch):
    monkeypatch.delenv("LANGCHAIN_TRACING_V2", raising=False)

    tracing = _reload_module("src.observability.tracing")
    assert tracing.langsmith_enabled is False

    @tracing.traceable(run_type="llm", name="dummy")
    def _fn(value):
        return value + 1

    assert _fn(1) == 2


def test_tracing_enabled_imports_react_loop_without_agent_run_type_warning(monkeypatch):
    pytest.importorskip("langsmith")
    monkeypatch.setenv("LANGCHAIN_TRACING_V2", "true")

    # Load tracing first so react_loop consumes the wrapped traceable.
    _reload_module("src.observability.tracing")

    with warnings.catch_warnings(record=True) as caught:
        warnings.simplefilter("always")
        react_loop_module = _reload_module("src.llm.react_loop")
        assert callable(react_loop_module.react_loop)

    warning_texts = [str(w.message) for w in caught]
    assert not any("Unrecognized run_type: agent" in text for text in warning_texts)
</file>

<file path=".env.example">
# ============================================================
#  深海科研知识库 RAG 系统 - 环境配置
#  复制为 .env 并根据环境修改
# ============================================================

# ------------ 环境标识 ------------
RAG_ENV=dev
# dev  = Mac 开发环境
# prod = 服务器生产环境

# ------------ Milvus ------------
MILVUS_HOST=localhost
MILVUS_PORT=19530

# ------------ 设备 ------------
# Mac: mps | Linux GPU: cuda | CPU: cpu
COMPUTE_DEVICE=mps
USE_FP16=false

# ------------ 模型 ------------
EMBEDDING_MODEL=BAAI/bge-m3
RERANKER_MODEL=BAAI/bge-reranker-v2-m3

# ------------ LLM API ------------
ANTHROPIC_API_KEY=sk-ant-xxx
OPENAI_API_KEY=sk-xxx
DEFAULT_LLM=claude

# ------------ Collection 命名（统一，勿改） ------------
COLLECTION_LIFE=deepsea_life
COLLECTION_OCEAN=deepsea_ocean
COLLECTION_ENV=deepsea_env
COLLECTION_GLOBAL=deepsea_global

# ------------ 索引配置 ------------
# Mac: IVF_FLAT | 服务器 GPU: GPU_IVF_FLAT
INDEX_TYPE=IVF_FLAT
INDEX_NLIST=256

# ------------ 检索配置 ------------
SEARCH_TOP_K=10
RERANK_TOP_K=5
RRF_K=60

# ------------ 调试开关 ------------
# true 时不调用 LLM，只输出检索与 prompt 摘要
LLM_DRY_RUN=false
</file>

<file path=".gitignore">
# Environment
.env
.venv
venv/
env/

# Config
config/rag_config.local.json

# Python
__pycache__/
*.pyc
*.pyo
*.pyd
.pytest_cache/
.coverage

# Node / Frontend
node_modules/
frontend/node_modules/
frontend/dist/
frontend/.env
frontend/.env.local

# Data & Logs - Only structure + eval_mini.json; all runtime data stays local
# Data & Logs
logs/
artifacts/

# 1. 忽略 data 目录下的一切（默认规则）
data/*

# 2. 例外：保留必要的种子数据文件
!data/eval_mini.json
!data/.gitkeep

# 3. 例外：保留必要的目录结构（让 git 知道这些文件夹存在）
!data/raw_papers/
!data/parsed/
!data/parsed_test/
!data/checkpoints/

# 4. 但是：这些目录里面的内容还是要忽略（除了 .gitkeep）
data/raw_papers/*
!data/raw_papers/.gitkeep

data/parsed/*
!data/parsed/.gitkeep

data/parsed_test/*
!data/parsed_test/.gitkeep

data/checkpoints/*
!data/checkpoints/.gitkeep


# Database files
*.db
*.sqlite
*.sqlite3
*.db-journal
*.wal
*.shm

# Serialized data
*.pkl
*.pickle
*.pth
*.pt
*.bin
*.safetensors

# Cache and temporary files
.cache/
*.tmp
*.temp
*.log

# Docker Volumes
volumes/

# System & IDE
.DS_Store
.idea/
.vscode/
*.swp
.cursor/worktrees.json

#CapSolverExtension
extra_tools/CapSolverExtension/
</file>

<file path="docker-compose.yml">
# ============================================================
#  深海科研知识库 - Milvus 部署
#  Mac: docker compose --profile dev up -d
#  服务器: docker compose --profile prod up -d
# ============================================================

services:
  # ------------ 基础服务 ------------
  etcd:
    container_name: deepsea-etcd
    image: quay.io/coreos/etcd:v3.5.5
    environment:
      - ETCD_AUTO_COMPACTION_MODE=revision
      - ETCD_AUTO_COMPACTION_RETENTION=1000
      - ETCD_QUOTA_BACKEND_BYTES=4294967296
    volumes:
      - ./volumes/etcd:/etcd
    command: >
      etcd 
      -advertise-client-urls=http://etcd:2379 
      -listen-client-urls=http://0.0.0.0:2379 
      --data-dir=/etcd
    healthcheck:
      test: ["CMD", "etcdctl", "endpoint", "health"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - deepsea-net

  minio:
    container_name: deepsea-minio
    image: minio/minio:RELEASE.2024-12-18T13-15-44Z
    environment:
      MINIO_ACCESS_KEY: minioadmin
      MINIO_SECRET_KEY: minioadmin
    volumes:
      - ./volumes/minio:/minio_data
    command: minio server /minio_data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - deepsea-net

  # ------------ Mac 开发环境（CPU） ------------
  milvus-dev:
    container_name: deepsea-milvus
    image: milvusdb/milvus:v2.5.6
    profiles: ["dev"]
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - ./volumes/milvus:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - etcd
      - minio
    networks:
      - deepsea-net

  # ------------ 服务器生产环境（GPU） ------------
  milvus-prod:
    container_name: deepsea-milvus
    image: milvusdb/milvus:v2.5.6-gpu
    profiles: ["prod"]
    command: ["milvus", "run", "standalone"]
    environment:
      ETCD_ENDPOINTS: etcd:2379
      MINIO_ADDRESS: minio:9000
    volumes:
      - ./volumes/milvus:/var/lib/milvus
    ports:
      - "19530:19530"
      - "9091:9091"
    depends_on:
      - etcd
      - minio
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: ['0', '1']
              capabilities: [gpu]
    networks:
      - deepsea-net

networks:
  deepsea-net:
    name: deepsea-network
</file>

<file path="index.jsx">
import React, { useState, useEffect, useRef, useCallback } from 'react';
import { 
  Settings, Database, MessageSquare, UploadCloud, FileText, Search, ExternalLink, 
  ChevronLeft, ChevronRight, Cpu, Layers, CheckCircle, FileSearch, ArrowRight, 
  BarChart3, AlertCircle, Plus, Server, X, RefreshCw, Trash2, Loader2, Save, 
  Info, Image, Table, Activity, HardDrive, Users, LogOut, Lock, Shield, UserPlus, 
  Globe, Link, Sliders, Filter, MessageSquarePlus, Download, Copy, History, 
  Clock, MoreHorizontal, FileEdit, Network, GitBranch, GripVertical, PanelRightClose, 
  PanelRightOpen, PlugZap, Archive, ArchiveRestore, FileDown, FileType, Pin
} from 'lucide-react';

// --- 登录组件 ---
const LoginScreen = ({ onLogin }) => {
  const [username, setUsername] = useState('');
  const [password, setPassword] = useState('');
  const [error, setError] = useState('');
  const [isLoading, setIsLoading] = useState(false);

  const handleSubmit = (e) => {
    e.preventDefault();
    setIsLoading(true);
    setError('');
    // 模拟登录请求
    setTimeout(() => {
      if (username === 'admin' && password === 'admin') {
        onLogin({ username: 'Administrator', role: 'admin', avatar: 'https://api.dicebear.com/7.x/avataaars/svg?seed=admin' });
      } else if (username === 'user' && password === 'user') {
        onLogin({ username: 'Research User', role: 'user', avatar: 'https://api.dicebear.com/7.x/avataaars/svg?seed=user' });
      } else {
        setError('用户名或密码错误 (提示: admin/admin 或 user/user)');
        setIsLoading(false);
      }
    }, 800);
  };

  return (
    <div className="min-h-screen bg-gray-900 flex flex-col items-center justify-center p-4 relative overflow-hidden">
      {/* 背景装饰 */}
      <div className="absolute top-0 left-0 w-full h-full overflow-hidden z-0">
        <div className="absolute -top-[20%] -left-[10%] w-[50%] h-[50%] bg-blue-600/20 rounded-full blur-[120px]"></div>
        <div className="absolute top-[40%] -right-[10%] w-[40%] h-[60%] bg-purple-600/20 rounded-full blur-[120px]"></div>
      </div>
      <div className="bg-white/10 backdrop-blur-lg border border-white/20 p-8 rounded-3xl w-full max-w-md shadow-2xl z-10 animate-in fade-in zoom-in duration-500">
        <div className="text-center mb-8">
          <div className="w-16 h-16 bg-blue-600 rounded-2xl flex items-center justify-center mx-auto mb-4 shadow-lg shadow-blue-900/50">
            <Layers size={32} className="text-white" />
          </div>
          <h1 className="text-2xl font-bold text-white tracking-tight">深海科研 RAG 系统</h1>
          <p className="text-gray-400 text-sm mt-2">Remote Research & Collaboration Platform</p>
        </div>
        <form onSubmit={handleSubmit} className="space-y-5">
          <div>
            <label className="block text-xs font-medium text-gray-300 uppercase mb-2 ml-1">Account</label>
            <div className="relative">
              <Users size={18} className="absolute left-3 top-3.5 text-gray-400" />
              <input type="text" value={username} onChange={(e) => setUsername(e.target.value)} className="w-full bg-gray-800/50 border border-gray-600 text-white rounded-xl py-3 pl-10 pr-4 focus:ring-2 focus:ring-blue-500 focus:border-transparent outline-none transition-all placeholder-gray-500" placeholder="Username" />
            </div>
          </div>
          <div>
            <label className="block text-xs font-medium text-gray-300 uppercase mb-2 ml-1">Password</label>
            <div className="relative">
              <Lock size={18} className="absolute left-3 top-3.5 text-gray-400" />
              <input type="password" value={password} onChange={(e) => setPassword(e.target.value)} className="w-full bg-gray-800/50 border border-gray-600 text-white rounded-xl py-3 pl-10 pr-4 focus:ring-2 focus:ring-blue-500 focus:border-transparent outline-none transition-all placeholder-gray-500" placeholder="••••••••" />
            </div>
          </div>
          {error && <div className="flex items-center gap-2 text-red-400 text-xs bg-red-900/20 p-3 rounded-lg border border-red-900/30"><AlertCircle size={14} />{error}</div>}
          <button type="submit" disabled={isLoading} className="w-full bg-blue-600 hover:bg-blue-500 text-white font-bold py-3.5 rounded-xl transition-all shadow-lg shadow-blue-900/40 active:scale-[0.98] flex items-center justify-center gap-2">{isLoading ? <Loader2 size={20} className="animate-spin" /> : 'Sign In'}</button>
        </form>
      </div>
    </div>
  );
};

// --- 主应用组件 ---
const App = () => {
  // 用户鉴权状态
  const [currentUser, setCurrentUser] = useState(null); 
  
  const [activeTab, setActiveTab] = useState('chat');
  const [dbStatus, setDbStatus] = useState('disconnected'); 
  const [messages, setMessages] = useState([]);
  const [inputValue, setInputValue] = useState('');
  
  // 布局与尺寸状态 (Resizable)
  const [sidebarWidth, setSidebarWidth] = useState(340); // 稍微加宽以容纳详细配置
  const [canvasWidth, setCanvasWidth] = useState(500);
  const [isSidebarOpen, setIsSidebarOpen] = useState(true);
  const [isCanvasOpen, setIsCanvasOpen] = useState(false); 
  const [isHistoryOpen, setIsHistoryOpen] = useState(false); 

  // 拖拽逻辑
  const isResizingRef = useRef(false);
  const startResizingSidebar = useCallback(() => { isResizingRef.current = 'sidebar'; }, []);
  const startResizingCanvas = useCallback(() => { isResizingRef.current = 'canvas'; }, []);
  const stopResizing = useCallback(() => { isResizingRef.current = false; }, []);
  
  const resize = useCallback((e) => {
    if (isResizingRef.current === 'sidebar') {
      const newWidth = e.clientX;
      if (newWidth > 280 && newWidth < 500) setSidebarWidth(newWidth);
    } else if (isResizingRef.current === 'canvas') {
      const newWidth = window.innerWidth - e.clientX;
      if (newWidth > 350 && newWidth < 900) setCanvasWidth(newWidth);
    }
  }, []);

  useEffect(() => {
    window.addEventListener('mousemove', resize);
    window.addEventListener('mouseup', stopResizing);
    return () => {
      window.removeEventListener('mousemove', resize);
      window.removeEventListener('mouseup', stopResizing);
    };
  }, [resize, stopResizing]);

  // Local RAG Settings
  const [localTopK, setLocalTopK] = useState(5);
  const [enableHippoRAG, setEnableHippoRAG] = useState(false);
  const [enableReranker, setEnableReranker] = useState(true);
  
  // --- Web 检索状态 (升级版: 每源独立配置) ---
  const [webSearchEnabled, setWebSearchEnabled] = useState(false);
  const [webSources, setWebSources] = useState([
    { id: 'tavily', name: 'Tavily API', enabled: true, topK: 5, threshold: 0.5 },
    { id: 'google', name: 'Google Search', enabled: false, topK: 5, threshold: 0.4 },
    { id: 'scholar', name: 'Google Scholar', enabled: false, topK: 3, threshold: 0.6 },
    { id: 'semantic', name: 'Semantic Scholar', enabled: false, topK: 3, threshold: 0.7 },
  ]);

  // LangGraph Workflow State
  const [workflowStep, setWorkflowStep] = useState('idle'); // idle, explore, outline, drafting, refine
  const [canvasContent, setCanvasContent] = useState(''); // 画布内容

  // Modals
  const [showSettingsModal, setShowSettingsModal] = useState(false);
  const [showCreateCollectionModal, setShowCreateCollectionModal] = useState(false);
  const [showUserModal, setShowUserModal] = useState(false); 

  // Data
  const [files, setFiles] = useState([
    { id: 1, name: 'DeepSea_Mining_Env_Impact.pdf', status: 'Success', time: '10 min ago' },
    { id: 2, name: 'AUV_Navigation_Survey_2024.pdf', status: 'Success', time: '1 hr ago' }
  ]);
  
  // 历史记录 (增加 isArchived 字段)
  const [chatHistory, setChatHistory] = useState([
    { id: 'h1', title: '深海采矿环境影响综述', date: '2023-10-25 14:30', preview: '根据最新文献生成的环境评估...', isArchived: true },
    { id: 'h2', title: 'AUV 导航算法调研', date: '2023-10-24 09:15', preview: 'SLAM 与 USBL 融合方案...', isArchived: false },
  ]);
  
  const [userList, setUserList] = useState([
    { id: 1, username: 'Administrator', role: 'admin', status: 'Active', created: '2023-01-01' },
    { id: 2, username: 'Research User', role: 'user', status: 'Active', created: '2023-06-15' },
  ]);

  const [isUploading, setIsUploading] = useState(false);
  const [uploadProgress, setUploadProgress] = useState(0);
  const [dbAddress, setDbAddress] = useState('localhost:19530');
  const [currentCollection, setCurrentCollection] = useState('deepsea_research_v1');
  const [collections, setCollections] = useState(['deepsea_research_v1', 'general_ocean_v2']);
  
  const [toasts, setToasts] = useState([]);
  const addToast = (msg, type = 'info') => {
    const id = Date.now();
    setToasts(prev => [...prev, { id, msg, type }]);
    setTimeout(() => setToasts(prev => prev.filter(t => t.id !== id)), 3000);
  };

  // Auth Handlers
  const handleLogin = (user) => { setCurrentUser(user); addToast(`欢迎回来, ${user.username}`, 'success'); };
  const handleLogout = () => { setCurrentUser(null); setDbStatus('disconnected'); setActiveTab('chat'); addToast('您已安全注销', 'info'); };
  
  // Web 检索配置更新函数
  const toggleWebSource = (id) => {
    setWebSources(prev => prev.map(s => s.id === id ? { ...s, enabled: !s.enabled } : s));
  };
  const updateWebSourceParam = (id, field, value) => {
    setWebSources(prev => prev.map(s => s.id === id ? { ...s, [field]: value } : s));
  };

  // 历史记录归档
  const toggleArchiveHistory = (e, id) => {
    e.stopPropagation();
    setChatHistory(prev => prev.map(h => {
      if (h.id === id) {
        const newState = !h.isArchived;
        addToast(newState ? '项目已归档 (跳出清理周期)' : '项目已取消归档', 'success');
        return { ...h, isArchived: newState };
      }
      return h;
    }));
  };

  // Handlers
  const handleConnect = () => {
    setDbStatus('connecting');
    addToast(`正在连接 Milvus 服务 (${dbAddress})...`, 'info');
    setTimeout(() => {
      setDbStatus('connected');
      addToast('Docker 容器连接成功！', 'success');
      setMessages([{ role: 'assistant', content: '系统已就绪。我是您的深海科研助手，支持多源检索与协作式综述生成。' }]);
    }, 1500);
  };

  const handleSend = () => {
    if (!inputValue.trim()) return;
    const newMsg = { role: 'user', content: inputValue };
    setMessages(prev => [...prev, newMsg]);
    setInputValue('');
    
    // 模拟 LangGraph 工作流触发
    setWorkflowStep('explore');
    
    setTimeout(() => {
      // 步骤1：检索
      setWorkflowStep('outline');
      const activeWebSources = webSources.filter(s => s.enabled);
      const isWeb = webSearchEnabled && activeWebSources.length > 0;
      
      const sources = [
        { id: 1, title: 'DeepSea_Mining_Env_Impact.pdf', score: 0.98, snippet: 'Sediment plumes generated by mining vehicles...', path: '/local/docs/paper1.pdf', type: 'local' }
      ];

      // 模拟多源 Web 检索结果
      if (isWeb) {
        activeWebSources.forEach((src, idx) => {
           sources.push({ 
            id: 100 + idx, 
            title: `[Web] ${src.name} Result`, 
            score: (src.threshold + 0.1).toFixed(2), // 模拟分数
            snippet: `Results retrieved from ${src.name} (Top-${src.topK}). Content matched with threshold > ${src.threshold}...`, 
            path: `https://${src.id}.com/search`, 
            type: 'web' 
          });
        });
      }

      setMessages(prev => [...prev, { 
        role: 'assistant', 
        content: `已完成检索。本地命中 1 条，Web 检索启用 ${activeWebSources.length} 个源。正在构建综述大纲...`,
        sources: sources
      }]);

      // 步骤2：生成 Draft (更新 Canvas)
      setTimeout(() => {
        setWorkflowStep('drafting');
        setIsCanvasOpen(true); // 自动打开 Canvas
        setCanvasContent(`# ${newMsg.content} - Research Draft\n\n## 1. Introduction\nDeep sea mining presents significant environmental challenges...\n\n## 2. Sediment Plumes\nAccording to [1], the impact radius...\n\n## 3. Web Insights\nRecent studies from Google Scholar indicate...\n\n## 4. Policy Implications\n...`);
        
        setTimeout(() => {
          setWorkflowStep('refine');
          setMessages(prev => [...prev, { 
            role: 'assistant', 
            content: `初稿已生成到右侧 Canvas。您可以进一步提问以润色特定章节，或点击右上角导出文档。`
          }]);
          setTimeout(() => setWorkflowStep('idle'), 1000);
        }, 1500);
      }, 1500);

    }, 1000);
  };

  const handleNewChat = () => {
    if (messages.length > 0) {
      const newHistoryItem = {
        id: `h-${Date.now()}`,
        title: messages[0].content.substring(0, 15) + '...',
        date: new Date().toLocaleString(),
        preview: messages[messages.length - 1].content.substring(0, 20) + '...',
        isArchived: false
      };
      setChatHistory(prev => [newHistoryItem, ...prev]);
    }
    setMessages([]); 
    setCanvasContent('');
    setIsCanvasOpen(false);
    addToast('已开启新对话上下文', 'success');
  };

  const handleCreateUser = (userData) => {
    setShowUserModal(false);
    setUserList(prev => [...prev, { ...userData, id: Date.now(), status: 'Active', created: 'Just now' }]);
    addToast(`用户 ${userData.username} 创建成功`, 'success');
  };

  const handleUpload = () => {
    setIsUploading(true);
    setUploadProgress(0);
    const interval = setInterval(() => {
      setUploadProgress(prev => {
        if (prev >= 100) {
          clearInterval(interval);
          setIsUploading(false);
          setFiles(prev => [{ id: Date.now(), name: 'New_Uploaded_File.pdf', status: 'Success', time: '刚刚' }, ...prev]);
          addToast(`文件已写入集合: ${currentCollection}`, 'success');
          return 0;
        }
        return prev + 10;
      });
    }, 200);
  };

  const handleDeleteFile = (id) => {
    setFiles(prev => prev.filter(f => f.id !== id));
    addToast('文档已从索引中移除', 'info');
  };

  const handleTabChange = (tab) => {
    // 修复：允许在未连接状态下切换回 'chat'，因为 Chat 页面包含连接按钮
    if (dbStatus !== 'connected' && tab !== 'users' && tab !== 'chat') { 
      addToast('请先连接向量数据库以访问此功能', 'error');
      return;
    }
    setActiveTab(tab);
  };

  const handleCreateCollection = (name) => {
    setShowCreateCollectionModal(false);
    addToast(`正在初始化集合: ${name}...`, 'info');
    setTimeout(() => {
      setCollections(prev => [...prev, name]);
      setCurrentCollection(name);
      addToast(`集合 ${name} 创建成功`, 'success');
    }, 1000);
  };

  const handleRefresh = () => {
    if (dbStatus !== 'connected') return;
    addToast('正在同步 Docker 容器状态...', 'info');
    setTimeout(() => addToast('状态同步正常: Latency 2ms', 'success'), 1000);
  };

  const handleDeleteUser = (id) => {
    if (id === 1) {
      addToast('无法删除超级管理员账号', 'error');
      return;
    }
    setUserList(prev => prev.filter(u => u.id !== id));
    addToast('用户已删除', 'success');
  };

  const handleLoadHistory = (historyId) => {
    addToast(`正在恢复会话: ${historyId}`, 'info');
    const item = chatHistory.find(h => h.id === historyId);
    if (item) {
      setMessages([
        { role: 'user', content: `(恢复的对话) 关于 ${item.title} 的问题...` },
        { role: 'assistant', content: `这是历史会话 "${item.title}" 的上下文记录。您可以继续提问。` }
      ]);
    }
  };

  const handleDeleteHistory = (e, historyId) => {
    e.stopPropagation();
    setChatHistory(prev => prev.filter(h => h.id !== historyId));
    addToast('历史会话已删除', 'success');
  };

  const handleExportCanvas = (format) => {
    if (!canvasContent) {
      addToast('画布为空，无法导出', 'error');
      return;
    }
    const blob = new Blob([canvasContent], { type: 'text/plain' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `research_draft.${format}`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
    addToast(`文档已导出为 ${format.toUpperCase()}`, 'success');
  };

  const handleExportMessage = (content) => {
    const blob = new Blob([content], { type: 'text/plain' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = `chat_snippet.txt`;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
    addToast('消息片段已导出', 'success');
  };

  // 渲染如果未登录
  if (!currentUser) return <LoginScreen onLogin={handleLogin} />;

  return (
    <div className="flex h-screen bg-gray-50 text-gray-900 font-sans overflow-hidden">
      {/* Toast Container */}
      <div className="fixed bottom-5 right-5 z-50 flex flex-col gap-2 pointer-events-none">
        {toasts.map(t => (
          <div key={t.id} className={`px-4 py-3 rounded-lg shadow-lg text-sm font-medium animate-in slide-in-from-right fade-in duration-300 pointer-events-auto ${t.type === 'success' ? 'bg-green-600 text-white' : 'bg-gray-800 text-white'}`}>{t.msg}</div>
        ))}
      </div>

      {/* --- Modals --- */}
      {showUserModal && (
        <div className="fixed inset-0 bg-black/50 z-[70] flex items-center justify-center p-4 backdrop-blur-sm animate-in fade-in duration-200">
          <div className="bg-white rounded-2xl w-full max-w-sm p-6 shadow-2xl animate-in zoom-in-95 duration-200" onClick={(e) => e.stopPropagation()}>
            <h3 className="text-lg font-bold mb-4 flex items-center gap-2"><UserPlus size={20}/> 新建用户</h3>
            <form onSubmit={(e) => {
              e.preventDefault();
              const formData = new FormData(e.target);
              handleCreateUser({ username: formData.get('username'), role: formData.get('role') });
            }} className="space-y-4">
              <div><label className="text-xs font-medium text-gray-500 uppercase">用户名</label><input name="username" required type="text" className="w-full mt-1 border rounded-md p-2 text-sm focus:ring-2 focus:ring-blue-500 outline-none"/></div>
              <div><label className="text-xs font-medium text-gray-500 uppercase">初始密码</label><input name="password" required type="password" className="w-full mt-1 border rounded-md p-2 text-sm focus:ring-2 focus:ring-blue-500 outline-none"/></div>
              <div><label className="text-xs font-medium text-gray-500 uppercase">角色权限</label><select name="role" className="w-full mt-1 border rounded-md p-2 text-sm bg-white"><option value="user">普通用户 (Research User)</option><option value="admin">系统管理员 (Admin)</option></select></div>
              <div className="mt-6 flex justify-end gap-2"><button type="button" onClick={() => setShowUserModal(false)} className="px-4 py-2 text-gray-600 hover:bg-gray-100 rounded-lg text-sm">取消</button><button type="submit" className="px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 text-sm">创建账户</button></div>
            </form>
          </div>
        </div>
      )}
      {showCreateCollectionModal && (
        <div className="fixed inset-0 bg-black/50 z-[70] flex items-center justify-center p-4 backdrop-blur-sm animate-in fade-in duration-200">
          <div className="bg-white rounded-2xl w-full max-w-sm p-6 shadow-2xl animate-in zoom-in-95 duration-200" onClick={(e) => e.stopPropagation()}>
            <h3 className="text-lg font-bold mb-4">新建向量集合</h3>
            <div className="space-y-4"><div><label className="text-xs font-medium text-gray-500 uppercase">集合名称</label><input id="new-col-name" type="text" className="w-full mt-1 border rounded-md p-2 text-sm focus:ring-2 focus:ring-blue-500 outline-none"/></div><div><label className="text-xs font-medium text-gray-500 uppercase">向量维度</label><select className="w-full mt-1 border rounded-md p-2 text-sm bg-gray-50"><option>1536 (OpenAI)</option><option>1024 (BGE-Large)</option></select></div></div>
            <div className="mt-6 flex justify-end gap-2"><button onClick={() => setShowCreateCollectionModal(false)} className="px-4 py-2 text-gray-600 hover:bg-gray-100 rounded-lg text-sm">取消</button><button onClick={() => handleCreateCollection(document.getElementById('new-col-name').value || 'new_collection')} className="px-4 py-2 bg-blue-600 text-white rounded-lg hover:bg-blue-700 text-sm">创建</button></div>
          </div>
        </div>
      )}
      {showSettingsModal && (
        <div className="fixed inset-0 bg-black/50 z-[60] flex items-center justify-center p-4 backdrop-blur-sm animate-in fade-in duration-200">
          <div className="bg-white rounded-2xl w-full max-w-lg p-6 shadow-2xl animate-in zoom-in-95 duration-200 border border-gray-100" onClick={(e) => e.stopPropagation()}>
            <div className="flex justify-between items-center mb-6 pb-4 border-b"><h3 className="text-lg font-bold text-gray-900 flex items-center gap-2"><Shield size={20} className="text-blue-600"/> 系统高级配置</h3><button onClick={() => setShowSettingsModal(false)} className="p-2 hover:bg-gray-100 rounded-full"><X size={20}/></button></div>
            <div className="space-y-5"><div className="grid grid-cols-2 gap-4"><div><label className="text-sm font-medium text-gray-700 block mb-1">API Timeout (ms)</label><input type="number" defaultValue={30000} className="w-full bg-gray-50 border border-gray-200 rounded-lg p-2.5 text-sm"/></div><div><label className="text-sm font-medium text-gray-700 block mb-1">Max Tokens</label><input type="number" defaultValue={4096} className="w-full bg-gray-50 border border-gray-200 rounded-lg p-2.5 text-sm"/></div></div><div className="bg-yellow-50 p-3 rounded-lg flex gap-3 text-sm text-yellow-800 border border-yellow-100"><AlertCircle size={18} className="flex-shrink-0 mt-0.5" /><p>警告：修改这些参数会影响所有用户的请求行为。请谨慎操作。</p></div></div>
            <div className="mt-8 flex justify-end gap-3 pt-4 border-t"><button onClick={() => setShowSettingsModal(false)} className="px-5 py-2.5 text-gray-600 hover:bg-gray-100 rounded-xl text-sm font-medium">取消</button><button onClick={() => { setShowSettingsModal(false); addToast('系统全局配置已更新', 'success'); }} className="px-5 py-2.5 bg-gray-900 text-white rounded-xl hover:bg-black text-sm font-medium">保存更改</button></div>
          </div>
        </div>
      )}

      {/* --- 左侧边栏 (Resizable) --- */}
      <div 
        className="bg-white border-r flex flex-col relative flex-shrink-0 z-40 transition-none"
        style={{ width: isSidebarOpen ? sidebarWidth : 80 }}
      >
        {/* 拖拽把手 (Right Handle) */}
        {isSidebarOpen && (
          <div 
            className="absolute top-0 right-0 w-1 h-full cursor-col-resize hover:bg-blue-400 z-50 group transition-colors"
            onMouseDown={startResizingSidebar}
          >
            <div className="absolute top-1/2 -right-3 w-6 h-8 bg-white border rounded shadow flex items-center justify-center opacity-0 group-hover:opacity-100 transition-opacity pointer-events-none">
              <GripVertical size={12} className="text-gray-400"/>
            </div>
          </div>
        )}

        <div className="p-6 flex items-center gap-3 border-bottom h-20 overflow-hidden">
          <div className="bg-blue-600 p-2 rounded-lg text-white flex-shrink-0 shadow-lg shadow-blue-200">
            <Layers size={24} />
          </div>
          {isSidebarOpen && <span className="font-bold text-xl tracking-tight whitespace-nowrap">RAG Lab</span>}
        </div>

        {/* 侧边栏内容区 */}
        <div className="flex-1 overflow-y-auto p-4 space-y-8 scrollbar-hide">
          {/* 用户信息 */}
          <div className={`flex items-center gap-3 p-3 bg-gray-50 rounded-xl border border-gray-100 ${!isSidebarOpen && 'justify-center'}`}>
            <img src={currentUser.avatar} alt="Avatar" className="w-10 h-10 rounded-full bg-white border" />
            {isSidebarOpen && (
              <div className="flex-1 min-w-0">
                <div className="font-bold text-sm truncate text-gray-900">{currentUser.username}</div>
                <div className="text-xs text-gray-500 flex items-center gap-1"><Shield size={10} className="text-blue-600"/><span className="capitalize">{currentUser.role}</span></div>
              </div>
            )}
          </div>

          {/* 引擎配置 */}
          <section>
            <div className="flex items-center gap-2 mb-4 text-gray-500 font-semibold text-xs uppercase tracking-wider whitespace-nowrap">
              <Cpu size={14} /> {isSidebarOpen && '检索策略配置'}
            </div>
            {isSidebarOpen && (
              <div className="space-y-3 animate-in fade-in duration-300">
                
                {/* 核心检索参数 */}
                <div className="bg-gray-50 rounded-lg p-3 border border-gray-200 space-y-3">
                    {/* Top-K Slider */}
                    <div>
                      <div className="flex justify-between text-[10px] text-gray-500 mb-1">
                        <span>Local RAG Top-K</span>
                        <span className="font-mono bg-white px-1 rounded border">{localTopK}</span>
                      </div>
                      <input type="range" min="1" max="50" step="1" value={localTopK} onChange={(e) => setLocalTopK(e.target.value)} className="w-full accent-blue-600 h-1.5 bg-gray-200 rounded-lg appearance-none cursor-pointer"/>
                    </div>

                    {/* HippoRAG Toggle (New) */}
                    <div className="flex items-center justify-between">
                      <div className="flex items-center gap-2">
                        <Network size={14} className="text-purple-500"/>
                        <span className="text-sm text-gray-700">HippoRAG 图检索</span>
                      </div>
                      <input type="checkbox" checked={enableHippoRAG} onChange={(e) => setEnableHippoRAG(e.target.checked)} className="accent-purple-600 w-4 h-4 cursor-pointer" />
                    </div>

                    {/* Reranker Toggle (New) */}
                    <div className="flex items-center justify-between">
                      <div className="flex items-center gap-2">
                        <Filter size={14} className="text-orange-500"/>
                        <span className="text-sm text-gray-700">ColBERT 重排序</span>
                      </div>
                      <input type="checkbox" checked={enableReranker} onChange={(e) => setEnableReranker(e.target.checked)} className="accent-orange-600 w-4 h-4 cursor-pointer" />
                    </div>
                </div>
              </div>
            )}
          </section>

          {/* Web Search Config (Enhanced) */}
          <section>
            <div className="flex items-center gap-2 mb-4 text-gray-500 font-semibold text-xs uppercase tracking-wider whitespace-nowrap">
              <Globe size={14} /> {isSidebarOpen && 'Web 增强检索'}
            </div>
            {isSidebarOpen && (
              <div className="space-y-3 animate-in fade-in duration-300">
                {/* 总开关 */}
                <div className="border border-gray-200 rounded-xl p-3 hover:border-blue-200 hover:bg-blue-50/50 transition-all cursor-pointer">
                  <label className="flex items-center justify-between cursor-pointer w-full">
                    <div className="flex items-center gap-3">
                      <div className={`p-2 rounded-lg shadow-sm transition-colors ${webSearchEnabled ? 'bg-indigo-50 text-indigo-600' : 'bg-gray-100 text-gray-400'}`}>
                        <Globe size={16} />
                      </div>
                      <div>
                        <div className="text-sm font-medium text-gray-700">联网搜索</div>
                        <div className="text-[10px] text-gray-400">实时获取互联网信息</div>
                      </div>
                    </div>
                    <input 
                      type="checkbox" 
                      checked={webSearchEnabled}
                      onChange={(e) => {
                        setWebSearchEnabled(e.target.checked);
                        addToast(e.target.checked ? 'Web 检索已启用' : 'Web 检索已关闭', 'info');
                      }}
                      className="accent-blue-600 w-4 h-4 cursor-pointer" 
                    />
                  </label>
                </div>

                {webSearchEnabled && (
                  <div className="bg-white border border-gray-200 rounded-xl overflow-hidden animate-in slide-in-from-top-2 duration-200">
                    <div className="bg-gray-50 px-3 py-2 border-b text-[10px] font-bold text-gray-500 uppercase">Search Sources</div>
                    <div className="divide-y">
                      {webSources.map(source => (
                        <div key={source.id} className="p-3 hover:bg-gray-50 transition-colors">
                          <div className="flex items-center justify-between mb-2">
                            <span className={`text-sm font-medium ${source.enabled ? 'text-gray-800' : 'text-gray-400'}`}>{source.name}</span>
                            <input 
                              type="checkbox" 
                              checked={source.enabled} 
                              onChange={() => toggleWebSource(source.id)} 
                              className="accent-blue-600 w-4 h-4 cursor-pointer" 
                            />
                          </div>
                          {source.enabled && (
                            <div className="space-y-2 animate-in slide-in-from-top-1 duration-200">
                              <div className="flex items-center justify-between text-[10px] text-gray-500">
                                <span>Top-K: <span className="font-mono text-gray-900">{source.topK}</span></span>
                                <span>Threshold: <span className="font-mono text-gray-900">{source.threshold}</span></span>
                              </div>
                              <div className="grid grid-cols-2 gap-2">
                                <input 
                                  type="range" min="1" max="20" step="1" 
                                  value={source.topK} 
                                  onChange={(e) => updateWebSourceParam(source.id, 'topK', e.target.value)} 
                                  className="w-full accent-blue-600 h-1 bg-gray-200 rounded-lg appearance-none cursor-pointer"
                                />
                                <input 
                                  type="range" min="0" max="1" step="0.1" 
                                  value={source.threshold} 
                                  onChange={(e) => updateWebSourceParam(source.id, 'threshold', e.target.value)} 
                                  className="w-full accent-green-600 h-1 bg-gray-200 rounded-lg appearance-none cursor-pointer"
                                />
                              </div>
                            </div>
                          )}
                        </div>
                      ))}
                    </div>
                  </div>
                )}
              </div>
            )}
          </section>

          {/* Service Connection (Fixed: Always visible in sidebar) */}
          <section>
            <div className="flex items-center gap-2 mb-4 text-gray-500 font-semibold text-xs uppercase tracking-wider whitespace-nowrap">
              <Database size={14} /> {isSidebarOpen && '服务连接'}
            </div>
            {isSidebarOpen && (
              <div className="space-y-3 animate-in fade-in duration-300">
                <div>
                  <label className="block text-xs text-gray-400 mb-1">Service Address</label>
                  <div className="relative">
                    <input 
                      type="text" 
                      disabled={currentUser.role !== 'admin'}
                      className={`w-full bg-gray-50 border rounded-md p-2 text-sm pl-8 ${dbStatus === 'connected' ? 'text-green-700 border-green-200 bg-green-50' : ''} disabled:opacity-60 disabled:cursor-not-allowed`}
                      value={dbAddress}
                      onChange={(e) => setDbAddress(e.target.value)}
                    />
                    <Server size={14} className="absolute left-2.5 top-2.5 text-gray-400"/>
                  </div>
                </div>
                {dbStatus === 'connected' ? (
                  <div className="p-3 bg-gray-50 rounded-lg border border-gray-100">
                    <div className="flex justify-between items-center text-xs text-gray-500">
                      <span>Status:</span>
                      <span className="text-green-600 flex items-center gap-1"><div className="w-1.5 h-1.5 bg-green-500 rounded-full animate-pulse"></div> Online</span>
                    </div>
                  </div>
                ) : (
                  <button 
                    onClick={handleConnect} 
                    disabled={dbStatus === 'connecting'} 
                    className="w-full text-xs text-blue-600 font-medium py-2 border border-blue-100 rounded-md hover:bg-blue-50 flex justify-center items-center gap-2 transition-colors cursor-pointer"
                  >
                    {dbStatus === 'connecting' ? <Loader2 size={14} className="animate-spin"/> : <PlugZap size={14}/>}
                    {dbStatus === 'connecting' ? '连接中...' : '连接服务'}
                  </button>
                )}
              </div>
            )}
          </section>

          {/* Projects / History (Archived Support) */}
          <section>
             <div className="flex items-center justify-between mb-4 text-gray-500 font-semibold text-xs uppercase tracking-wider whitespace-nowrap">
              <div className="flex items-center gap-2"><History size={14} /> {isSidebarOpen && '历史项目'}</div>
            </div>
            {isSidebarOpen && (
              <div className="space-y-2 max-h-48 overflow-y-auto pr-1">
                {chatHistory.map(h => (
                  <div 
                    key={h.id} 
                    onClick={() => handleLoadHistory(h.id)}
                    className="group p-2 rounded-lg hover:bg-gray-100 cursor-pointer text-sm relative"
                  >
                    <div className="flex justify-between items-start">
                      <span className="font-medium text-gray-800 truncate flex-1">{h.title}</span>
                      {h.isArchived && <Pin size={12} className="text-yellow-500 fill-yellow-500 flex-shrink-0 ml-1"/>}
                    </div>
                    <div className="text-[10px] text-gray-400 flex items-center gap-1 mt-1"><Clock size={10}/> {h.date}</div>
                    
                    {/* Hover Actions */}
                    <div className="absolute right-1 top-1 hidden group-hover:flex gap-1 bg-white shadow-sm p-1 rounded border">
                       <button onClick={(e) => toggleArchiveHistory(e, h.id)} className="p-1 hover:text-yellow-600 text-gray-400" title={h.isArchived ? "取消归档" : "永久保存"}><Archive size={12}/></button>
                       <button onClick={(e) => handleDeleteHistory(e, h.id)} className="p-1 hover:text-red-600 text-gray-400" title="删除"><Trash2 size={12}/></button>
                    </div>
                  </div>
                ))}
              </div>
            )}
          </section>
        </div>
        
        {/* Footer Buttons */}
        <div className="p-4 border-t bg-gray-50/50 space-y-2">
          {currentUser.role === 'admin' && (
            <button onClick={() => setShowSettingsModal(true)} className={`w-full flex items-center justify-center gap-2 bg-gray-900 text-white py-3 rounded-xl text-sm font-medium hover:bg-black transition-all ${!isSidebarOpen && 'px-0'}`}><Settings size={18} />{isSidebarOpen && '高级配置'}</button>
          )}
          <button onClick={handleLogout} className={`w-full flex items-center justify-center gap-2 text-red-600 hover:bg-red-50 py-3 rounded-xl text-sm font-medium transition-all ${!isSidebarOpen && 'px-0'}`}><LogOut size={18} />{isSidebarOpen && '注销'}</button>
        </div>
      </div>

      {/* --- 中间主内容区 (Chat) --- */}
      <div className="flex-1 flex flex-col h-full overflow-hidden bg-white relative">
        {/* 顶部导航 */}
        <header className="bg-white border-b px-6 h-16 flex items-center justify-between flex-shrink-0 z-30">
          <div className="flex gap-6">
            <button onClick={() => setIsSidebarOpen(!isSidebarOpen)} className="text-gray-400 hover:text-gray-600 p-1"><MoreHorizontal size={20}/></button>
            <div className="h-8 w-[1px] bg-gray-200"></div>
            <button onClick={() => handleTabChange('chat')} className={`h-16 flex items-center gap-2 px-1 border-b-2 font-medium text-sm transition-all focus:outline-none ${activeTab === 'chat' ? 'border-blue-600 text-blue-600' : 'border-transparent text-gray-500'}`}><MessageSquare size={18} /> 智能问答</button>
            <button onClick={() => handleTabChange('ingest')} className={`h-16 flex items-center gap-2 px-1 border-b-2 font-medium text-sm transition-all focus:outline-none ${activeTab === 'ingest' ? 'border-blue-600 text-blue-600' : 'border-transparent text-gray-500'}`}><UploadCloud size={18} /> 数据入库</button>
            {currentUser.role === 'admin' && (<button onClick={() => handleTabChange('users')} className={`h-16 flex items-center gap-2 px-1 border-b-2 font-medium text-sm transition-all focus:outline-none ${activeTab === 'users' ? 'border-purple-600 text-purple-600' : 'border-transparent text-gray-400 hover:text-gray-600'}`}><Users size={18} />用户管理</button>)}
          </div>
          
          {/* Workflow Status Indicator */}
          {workflowStep !== 'idle' && (
            <div className="flex items-center gap-2 px-4 py-1.5 bg-blue-50 text-blue-700 rounded-full text-xs font-bold border border-blue-100 animate-pulse">
              <GitBranch size={14} />
              <span className="uppercase tracking-wide">Workflow: {workflowStep}</span>
            </div>
          )}

          <div className="flex items-center gap-3">
             {activeTab === 'chat' && dbStatus === 'connected' && (
               <>
                 <button onClick={handleNewChat} className="flex items-center gap-2 px-3 py-1.5 bg-gray-100 hover:bg-gray-200 text-gray-700 rounded-lg text-xs font-medium transition-colors border border-gray-200"><MessageSquarePlus size={14} /> 新对话</button>
                 <button onClick={() => setIsHistoryOpen(!isHistoryOpen)} className={`p-2 rounded-lg transition-colors cursor-pointer ${isHistoryOpen ? 'bg-blue-50 text-blue-600' : 'hover:bg-gray-100 text-gray-500'}`} title="切换历史记录"><History size={18} /></button>
               </>
             )}
             
             {/* 修复：顶部状态栏现在是一个可点击的按钮 */}
             {dbStatus === 'connected' ? (
                <button onClick={handleRefresh} className="flex items-center gap-2 px-3 py-1 bg-green-50 text-green-700 rounded-full text-xs font-medium border border-green-100 shadow-sm cursor-pointer hover:bg-green-100 transition-colors">
                  <div className="w-2 h-2 bg-green-500 rounded-full animate-pulse"></div>
                  System: Online
                </button>
              ) : (
                <button 
                  onClick={handleConnect}
                  className="flex items-center gap-2 px-3 py-1 bg-red-50 text-red-700 hover:bg-red-100 rounded-full text-xs font-medium border border-red-100 shadow-sm transition-colors cursor-pointer animate-pulse"
                  title="点击连接数据库"
                >
                  <PlugZap size={14} />
                  Disconnected (Click to Connect)
                </button>
              )}

             <button 
                onClick={() => setIsCanvasOpen(!isCanvasOpen)} 
                className={`flex items-center gap-2 px-3 py-1.5 rounded-lg text-xs font-medium transition-colors border ${isCanvasOpen ? 'bg-blue-100 border-blue-200 text-blue-700' : 'bg-white border-gray-200 text-gray-600 hover:bg-gray-50'}`}
                title="Toggle Canvas"
             >
               {isCanvasOpen ? <PanelRightClose size={14}/> : <PanelRightOpen size={14}/>} Canvas
             </button>
          </div>
        </header>

        {/* 聊天内容滚动区 */}
        <main className="flex-1 overflow-y-auto bg-gray-50/50 p-8 scrollbar-thin scrollbar-thumb-gray-200">
           {/* 未连接状态 (非用户管理Tab) */}
           {dbStatus === 'disconnected' && activeTab !== 'users' && (
              <div className="absolute inset-0 flex flex-col items-center justify-center p-8 animate-in fade-in zoom-in duration-300 z-10 bg-gray-50/90 backdrop-blur-sm">
                <div className="bg-white p-12 rounded-3xl shadow-xl max-w-2xl w-full text-center border border-gray-100"><div className="w-20 h-20 bg-blue-50 text-blue-600 rounded-full flex items-center justify-center mx-auto mb-6 shadow-sm"><Database size={40} /></div><h2 className="text-3xl font-bold text-gray-900 mb-3">连接远程服务</h2><p className="text-gray-500 mb-10 max-w-md mx-auto">请连接至 Docker 容器以访问向量数据。</p><div className="max-w-xs mx-auto"><button onClick={handleConnect} className="w-full group p-4 rounded-xl border-2 border-blue-600 bg-blue-600 text-white hover:bg-blue-700 transition-all active:scale-95 cursor-pointer flex items-center justify-center gap-3 shadow-lg shadow-blue-200"><Server size={20} /><span className="font-bold">连接服务节点</span></button></div></div>
              </div>
            )}

           {activeTab === 'chat' ? (
             <div className="max-w-3xl mx-auto space-y-6 pb-24">
                {messages.length === 0 && (
                  <div className="flex flex-col items-center justify-center h-64 text-gray-400">
                    <div className="w-16 h-16 bg-gray-100 rounded-2xl flex items-center justify-center mb-4"><MessageSquare size={32} className="opacity-40"/></div>
                    <p>开启新的科研对话...</p>
                  </div>
                )}
                {messages.map((msg, idx) => (
                  <div key={idx} className={`flex ${msg.role === 'user' ? 'justify-end' : 'justify-start'} animate-in slide-in-from-bottom-2 duration-300`}>
                    <div className={`max-w-[90%] rounded-2xl p-5 shadow-sm group ${msg.role === 'user' ? 'bg-blue-600 text-white' : 'bg-white border text-gray-800'}`}>
                      <p className="text-[15px] leading-relaxed whitespace-pre-wrap">{msg.content}</p>
                      
                      {/* 助手消息底部工具栏 */}
                      {msg.role === 'assistant' && (
                        <div className="mt-3 pt-2 border-t border-gray-100 flex items-center justify-end gap-2 opacity-0 group-hover:opacity-100 transition-opacity">
                           <button onClick={() => { navigator.clipboard.writeText(msg.content); addToast('内容已复制', 'info'); }} className="p-1.5 text-gray-400 hover:text-blue-600 hover:bg-blue-50 rounded-lg transition-colors cursor-pointer" title="复制内容"><Copy size={14} /></button>
                           <button onClick={() => handleExportMessage(msg.content)} className="p-1.5 text-gray-400 hover:text-blue-600 hover:bg-blue-50 rounded-lg transition-colors cursor-pointer" title="导出片段"><Download size={14} /></button>
                        </div>
                      )}

                      {msg.sources && (
                        <div className="mt-4 pt-3 border-t border-gray-100/50 space-y-2">
                          <div className="text-[10px] font-bold uppercase tracking-wider opacity-70 flex items-center gap-1"><FileSearch size={10}/> References</div>
                          {msg.sources.map(src => (
                            <div key={src.id} className="flex items-center justify-between bg-black/5 p-2 rounded-lg text-xs hover:bg-black/10 transition-colors cursor-pointer">
                              <span className="truncate flex-1 font-medium">{src.title}</span>
                              <span className="ml-2 text-[10px] opacity-70 border px-1 rounded">{src.type === 'web' ? `Thresh>${src.score}` : `Score:${src.score}`}</span>
                            </div>
                          ))}
                        </div>
                      )}
                    </div>
                  </div>
                ))}
             </div>
           ) : activeTab === 'ingest' ? (
             <div className="max-w-5xl mx-auto space-y-8 animate-in fade-in">
               <div className="grid grid-cols-1 md:grid-cols-3 gap-6">
                  <div className="col-span-1 bg-gray-900 text-white rounded-2xl p-6 shadow-lg flex flex-col justify-between relative overflow-hidden"><div className="absolute top-0 right-0 p-4 opacity-10"><Database size={100} /></div><div><div className="flex items-center gap-2 text-gray-400 text-xs font-bold uppercase mb-2"><HardDrive size={14} /> Node Status</div><div className="text-2xl font-bold font-mono tracking-tight">{dbAddress}</div></div><div className="mt-6 space-y-2 text-sm text-gray-300"><div className="flex justify-between"><span>RAM:</span> <span className="text-white">4.2 GB / 16 GB</span></div><div className="flex justify-between"><span>Role:</span> <span className="text-green-400">Read/Write</span></div></div></div>
                  <div className="col-span-2 bg-white border rounded-2xl p-6 shadow-sm flex flex-col justify-center"><div className="flex justify-between items-center mb-6"><div><h3 className="text-lg font-bold text-gray-900">选择集合 (Collection)</h3><p className="text-xs text-gray-500">将文档上传至指定的知识库分区</p></div>{currentUser.role === 'admin' && (<button onClick={() => setShowCreateCollectionModal(true)} className="flex items-center gap-2 px-4 py-2 bg-blue-50 text-blue-600 rounded-lg text-sm font-bold hover:bg-blue-100 transition-colors"><Plus size={16} /> 新建集合</button>)}</div><div className="flex gap-3"><select value={currentCollection} onChange={(e) => setCurrentCollection(e.target.value)} className="flex-1 bg-gray-50 border border-gray-200 rounded-xl p-3 font-medium text-gray-700 outline-none focus:ring-2 focus:ring-blue-500">{collections.map(c => <option key={c} value={c}>{c}</option>)}</select></div></div>
                </div>
               {/* Ingest UI */}
               <div className="bg-white p-8 rounded-2xl border shadow-sm text-center">
                 <UploadCloud size={48} className="mx-auto text-blue-500 mb-4"/>
                 <h2 className="text-xl font-bold">Upload Documents</h2>
                 <p className="text-gray-500 mt-2 mb-6">Drag and drop PDFs to add to {currentCollection}</p>
                 {isUploading ? (<div className="w-full max-w-md mx-auto mt-6"><div className="flex justify-between text-xs mb-1 text-gray-500"><span>Indexing...</span><span>{uploadProgress}%</span></div><div className="h-2 bg-gray-100 rounded-full overflow-hidden"><div className="h-full bg-blue-600 transition-all duration-300" style={{ width: `${uploadProgress}%` }}></div></div></div>) : (<button onClick={handleUpload} className="bg-gray-900 text-white px-6 py-2 rounded-lg text-sm font-medium">Select Files</button>)}
               </div>
               
               {/* History Table */}
              <div className="bg-white border rounded-2xl overflow-hidden shadow-sm">
                 <div className="px-6 py-4 border-b bg-gray-50 flex justify-between items-center"><span className="font-bold text-sm">集合数据概览</span></div>
                <table className="w-full text-sm">
                  <thead><tr className="text-left text-gray-400 border-b"><th className="px-6 py-3 font-medium">文件名</th><th className="px-6 py-3 font-medium">状态</th><th className="px-6 py-3 font-medium text-right">操作</th></tr></thead>
                  <tbody className="divide-y">
                    {files.map((item) => (
                      <tr key={item.id} className="hover:bg-gray-50 transition-colors"><td className="px-6 py-4 font-medium flex items-center gap-2"><FileText size={16} className="text-gray-400" /> {item.name}</td><td className="px-6 py-4"><span className={`px-2 py-1 rounded-full text-[10px] font-bold uppercase ${item.status === 'Success' ? 'bg-green-50 text-green-600' : 'bg-blue-50 text-blue-600'}`}>{item.status}</span></td><td className="px-6 py-4 text-right"><button onClick={() => handleDeleteFile(item.id)} className="text-gray-300 hover:text-red-500"><Trash2 size={16}/></button></td></tr>
                    ))}
                  </tbody>
                </table>
              </div>
             </div>
           ) : (
             <div className="max-w-5xl mx-auto space-y-8 p-8 animate-in fade-in duration-300">
               <div className="flex justify-between items-end">
                <div><h2 className="text-2xl font-bold text-gray-900">用户权限管理</h2><p className="text-gray-500 mt-1">管理系统访问权限与角色分配</p></div>
                <button onClick={() => setShowUserModal(true)} className="flex items-center gap-2 bg-blue-600 text-white px-5 py-2.5 rounded-xl font-medium shadow-lg shadow-blue-200 hover:bg-blue-700 transition-all"><UserPlus size={18} /> 新建用户</button>
               </div>
               <div className="bg-white border rounded-2xl overflow-hidden shadow-sm"><table className="w-full text-sm"><thead className="bg-gray-50"><tr className="text-left text-gray-500 border-b"><th className="px-6 py-4 font-medium">用户名 / 角色</th><th className="px-6 py-4 font-medium">状态</th><th className="px-6 py-4 font-medium">创建时间</th><th className="px-6 py-4 font-medium text-right">管理操作</th></tr></thead><tbody className="divide-y">{userList.map((user) => (<tr key={user.id} className="hover:bg-gray-50 transition-colors"><td className="px-6 py-4"><div className="flex items-center gap-3"><div className={`w-8 h-8 rounded-full flex items-center justify-center ${user.role === 'admin' ? 'bg-purple-100 text-purple-600' : 'bg-blue-100 text-blue-600'}`}>{user.role === 'admin' ? <Shield size={14}/> : <Users size={14}/>}</div><div><div className="font-bold text-gray-900">{user.username}</div><div className="text-xs text-gray-500 capitalize">{user.role}</div></div></div></td><td className="px-6 py-4"><span className={`px-2.5 py-1 rounded-full text-[10px] font-bold uppercase tracking-wide ${user.status === 'Active' ? 'bg-green-50 text-green-700 border border-green-100' : 'bg-gray-100 text-gray-500 border border-gray-200'}`}>{user.status}</span></td><td className="px-6 py-4 text-gray-500 font-mono text-xs">{user.created}</td><td className="px-6 py-4 text-right"><button onClick={() => handleDeleteUser(user.id)} className="text-gray-400 hover:text-red-600 hover:bg-red-50 p-2 rounded-lg transition-colors" title="删除用户"><Trash2 size={16} /></button></td></tr>))}</tbody></table></div>
             </div>
           )}
        </main>

        {/* 底部输入框 */}
        {activeTab === 'chat' && (
          <div className="p-6 bg-white border-t z-30">
            <div className="max-w-3xl mx-auto relative">
              <input type="text" value={inputValue} onChange={(e) => setInputValue(e.target.value)} onKeyPress={(e) => e.key === 'Enter' && handleSend()} placeholder="输入研究问题，例如：'生成关于深海采矿羽流扩散的综述大纲'..." className="w-full bg-gray-50 border border-gray-300 rounded-xl py-4 pl-5 pr-14 shadow-sm focus:ring-2 focus:ring-blue-500 focus:border-blue-500 outline-none transition-all" />
              <button onClick={handleSend} className="absolute right-2 top-2 bottom-2 aspect-square bg-blue-600 text-white rounded-lg flex items-center justify-center hover:bg-blue-700 transition-colors"><ArrowRight size={20}/></button>
            </div>
          </div>
        )}
      </div>

      {/* --- 右侧 Canvas 面板 (Resizable) --- */}
      {isCanvasOpen && (
        <div 
          className="bg-white border-l flex flex-col relative flex-shrink-0 z-40 shadow-xl"
          style={{ width: canvasWidth }}
        >
          {/* 拖拽把手 (Left Handle) */}
          <div 
            className="absolute top-0 left-0 w-1 h-full cursor-col-resize hover:bg-blue-400 z-50 group transition-colors"
            onMouseDown={startResizingCanvas}
          >
             <div className="absolute top-1/2 -left-3 w-6 h-8 bg-white border rounded shadow flex items-center justify-center opacity-0 group-hover:opacity-100 transition-opacity pointer-events-none">
              <GripVertical size={12} className="text-gray-400"/>
            </div>
          </div>

          <div className="h-14 border-b flex items-center justify-between px-4 bg-gray-50">
            <div className="flex items-center gap-2 font-bold text-sm text-gray-700">
              <FileEdit size={16} className="text-blue-600"/> 
              Research Canvas
              {workflowStep === 'drafting' && <Loader2 size={12} className="animate-spin text-gray-400"/>}
            </div>
            <div className="flex items-center gap-2">
              <button onClick={() => handleExportCanvas('md')} className="p-1.5 hover:bg-white rounded text-gray-500" title="Export Markdown"><FileDown size={14}/></button>
              <button onClick={() => handleExportCanvas('pdf')} className="p-1.5 hover:bg-white rounded text-gray-500" title="Export PDF"><FileType size={14}/></button>
              <button onClick={() => setIsCanvasOpen(false)} className="p-1.5 hover:bg-white rounded text-gray-500"><X size={14}/></button>
            </div>
          </div>

          <div className="flex-1 overflow-y-auto p-6 bg-white">
            {canvasContent ? (
              <div className="prose prose-sm max-w-none prose-headings:font-bold prose-h1:text-xl prose-h2:text-lg prose-p:text-gray-600">
                <pre className="whitespace-pre-wrap font-sans text-sm">{canvasContent}</pre>
              </div>
            ) : (
              <div className="flex flex-col items-center justify-center h-full text-gray-400 text-xs">
                <FileEdit size={32} className="mb-2 opacity-20"/>
                <p>Generated content will appear here</p>
              </div>
            )}
          </div>
        </div>
      )}

      {/* --- 右侧历史记录边栏 (Float) --- */}
      {activeTab === 'chat' && isHistoryOpen && dbStatus === 'connected' && (
        <div className={`absolute top-16 bottom-0 right-0 w-80 bg-white border-l flex flex-col animate-in slide-in-from-right duration-300 shadow-2xl z-50`}>
          <div className="p-4 border-b flex items-center justify-between bg-gray-50"><span className="font-bold text-sm text-gray-700 flex items-center gap-2"><History size={16} /> 历史对话</span><button onClick={() => setIsHistoryOpen(false)} className="text-gray-400 hover:text-gray-600"><X size={16} /></button></div>
          <div className="flex-1 overflow-y-auto p-2 space-y-2">
            {chatHistory.length === 0 ? (<div className="text-center text-gray-400 py-10 text-xs">暂无历史记录</div>) : (chatHistory.map(history => (
                <div key={history.id} onClick={() => handleLoadHistory(history.id)} className="group p-3 rounded-xl border border-transparent hover:border-gray-200 hover:bg-gray-50 cursor-pointer transition-all relative">
                  <div className="flex justify-between items-start">
                    <h4 className="text-sm font-medium text-gray-800 line-clamp-1 pr-2">{history.title}</h4>
                    {history.isArchived && <Archive size={12} className="text-yellow-500 fill-yellow-500 flex-shrink-0"/>}
                  </div>
                  <div className="flex items-center gap-1 text-[10px] text-gray-400 mt-1"><Clock size={10} /> {history.date}</div>
                  <p className="text-xs text-gray-500 mt-2 line-clamp-2">{history.preview}</p>
                  <div className="absolute top-8 right-2 flex gap-1 opacity-0 group-hover:opacity-100 transition-all">
                    <button onClick={(e) => toggleArchiveHistory(e, history.id)} className="p-1.5 text-gray-300 hover:text-yellow-600 hover:bg-white rounded-full" title={history.isArchived ? "取消永久保存" : "永久保存 (跳出清理)"}>{history.isArchived ? <ArchiveRestore size={12}/> : <Archive size={12}/>}</button>
                    <button onClick={(e) => handleDeleteHistory(e, history.id)} className="p-1.5 text-gray-300 hover:text-red-500 hover:bg-white rounded-full" title="删除"><Trash2 size={12} /></button>
                  </div>
                </div>
              ))
            )}
          </div>
          <div className="p-4 border-t bg-gray-50"><button onClick={() => {setChatHistory([]); addToast('历史记录已清空', 'info');}} className="w-full text-xs text-red-500 hover:text-red-600 flex items-center justify-center gap-1 py-2 hover:bg-red-50 rounded-lg transition-colors"><Trash2 size={12} /> 清空所有记录</button></div>
        </div>
      )}
    </div>
  );
};

export default App;
</file>

<file path="install.md">
# 安装与依赖核对指南

本文档同时覆盖"安装步骤"和"依赖是否安装正确"的核对方法。

如需 Ubuntu 生产发布/迁移，请优先参考：`docs/release_migration_ubuntu.md`。

## 1. 依赖基线

### Python 运行时

- Python：`3.10+`（推荐 3.10）
- PyTorch / Torchvision / timm：必须走 Conda 安装
- 关键约束（见 `requirements.txt`）：
  - `transformers>=4.42.0,<5.0.0`
  - `huggingface-hub==0.36.0`
  - `fastapi>=0.100.0,<1.0.0`
  - `pydantic>=2.0.0,<3.0.0`
  - `openai>=1.0.0,<3.0.0`
  - `anthropic>=0.18.0`
  - `mcp>=1.26.0`
  - `langgraph>=0.2.0`
  - `pymilvus[model]>=2.5.0`

### 前端运行时

- Node.js：`^20.19.0 || >=22.12.0`（由 Vite 7 的引擎要求决定）
- npm：建议使用随 Node LTS 的版本
- 前端锁文件：`frontend/package-lock.json`（lockfileVersion=3）

### 系统依赖（Ubuntu）

```bash
sudo apt install -y \
  git curl wget unzip ca-certificates gnupg lsb-release \
  build-essential pkg-config python3-dev \
  libssl-dev libffi-dev libsqlite3-dev \
  docker.io docker-compose-plugin
```

## 2. 创建环境并安装（后端）

```bash
conda create -n deepsea-rag python=3.10 -y
conda activate deepsea-rag

# 必须 Conda 安装（不要放进 requirements.txt）
conda install -c pytorch -c conda-forge "pytorch>=2.6.0" "torchvision>=0.21.0" timm -y

# Python 依赖
pip install -r requirements.txt --no-cache-dir

# 浏览器自动化（Google/Scholar 功能依赖）
playwright install chromium
```

## 3. 安装（前端）

```bash
cd frontend
npm install
cd ..
```

若要严格按 lock 还原环境，可用：

```bash
cd frontend
npm ci
cd ..
```

## 4. 环境配置

```bash
# 复制配置模板
cp config/rag_config.example.json config/rag_config.json
cp config/rag_config.local.example.json config/rag_config.local.json
cp .env.example .env

# 编辑 .env，设置 Milvus 地址、设备类型等
# 编辑 config/rag_config.local.json，填入各 LLM provider 的 API Key
```

密钥注入优先级：

1. 环境变量（推荐生产）：`RAG_LLM__OPENAI__API_KEY`、`RAG_LLM__DEEPSEEK__API_KEY` 等
2. `config/rag_config.local.json`（推荐开发）
3. `config/rag_config.json`（不建议写真实密钥）

## 5. 依赖核对（推荐执行）

优先使用一键脚本：

```bash
bash scripts/verify_dependencies.sh
```

### 核对后端关键依赖

```bash
conda run -n deepsea-rag python --version
conda run -n deepsea-rag python -c "import transformers, huggingface_hub; print(transformers.__version__, huggingface_hub.__version__)"
conda run -n deepsea-rag python -c "import fastapi, mcp; print(fastapi.__version__, mcp.__version__)"
conda run -n deepsea-rag python -c "import torch, torchvision; print(torch.__version__, torchvision.__version__)"
conda run -n deepsea-rag python -c "import langgraph; print(langgraph.__version__)"
```

### 核对前端依赖与 Node 版本

```bash
node -v
npm -v
cd frontend && npm ls --depth=0 && cd ..
```

## 6. 启动前检查

```bash
bash scripts/00_preflight_check.sh
# 开发环境（Mac CPU）
docker compose --profile dev up -d
# 生产环境（Linux GPU）
# docker compose --profile prod up -d
bash scripts/00_healthcheck_docker.sh
```

## 7. 初始化与入库

```bash
python scripts/01_init_env.py
python scripts/02_parse_papers.py
python scripts/03_index_papers.py
python scripts/03b_build_graph.py
```

## 8. 启动服务

```bash
# 一键全栈（开发）
bash scripts/start.sh

# 或只启动后端（支持热更新）
python scripts/08_run_api.py --reload

# 或分别启动
bash scripts/start.sh --backend-only
bash scripts/start.sh --frontend-only
```

- 前端：`http://localhost:5173`
- 后端 Swagger：`http://127.0.0.1:9999/docs`

### 生产部署建议（systemd）

发布环境建议使用 `systemd` 托管服务，不建议使用 `start.sh`（其后端默认包含 `--reload`，更适合开发）。
完整生产方案（含 Nginx 静态托管与 `/api` 反向代理）见 `docs/release_migration_ubuntu.md`。

后端 service 示例：

```ini
[Unit]
Description=DeepSea RAG FastAPI Service
After=network.target docker.service
Wants=docker.service

[Service]
Type=simple
User=YOUR_USER
Group=YOUR_USER
WorkingDirectory=/path/to/RAG
Environment=PYTHONUNBUFFERED=1
Environment=API_HOST=0.0.0.0
Environment=API_PORT=9999
EnvironmentFile=-/path/to/RAG/.env
Environment=CONDA_EXE=conda
ExecStart=/bin/bash -lc '$CONDA_EXE run -n deepsea-rag python scripts/08_run_api.py --host 0.0.0.0 --port 9999'
Restart=always
RestartSec=5
TimeoutStopSec=20

[Install]
WantedBy=multi-user.target
```

前端 service 示例：

```ini
[Unit]
Description=DeepSea RAG Frontend Preview
After=network.target

[Service]
Type=simple
User=YOUR_USER
Group=YOUR_USER
WorkingDirectory=/path/to/RAG/frontend
Environment=NODE_ENV=production
Environment=CONDA_EXE=conda
ExecStart=/bin/bash -lc '$CONDA_EXE run -n deepsea-rag npm run preview -- --host 0.0.0.0 --port 5173'
Restart=always
RestartSec=5
TimeoutStopSec=20

[Install]
WantedBy=multi-user.target
```

启用与管理：

```bash
sudo systemctl daemon-reload
sudo systemctl enable deepsea-rag-api deepsea-rag-frontend
sudo systemctl start deepsea-rag-api deepsea-rag-frontend
sudo journalctl -u deepsea-rag-api -f
```

> 若 `conda` 不在 systemd 的 PATH，可把 `CONDA_EXE` 改为实际路径（如 `/opt/anaconda3/bin/conda`）。

## 9. Deep Research 使用说明

当前版本 Deep Research 默认采用"后台任务模式"：

- 点击"开始研究"后，任务在后端持续执行，前端页面关闭/刷新不会自动中断
- 前端通过任务接口自动刷新状态；可在主界面查看进度
- 只有点击"停止任务"或调用取消接口时才会终止任务

### 启动前设置（前端 `⚙` 弹窗）

- `Research Depth`（lite / comprehensive）
- `Output Language`
- `Per-step Models`（scope / plan / research / evaluate / write / verify / synthesize）
- `Strict step model resolution`

以上设置持久化到本地（跨会话保留），对话框内仍可做"本次运行覆盖"。

### 审核与人工介入

- 章节审核：通过此章 / 需要修改 / 重新确认 / 全部通过并触发整合
- 章节缺口补充：
  - `material`：材料线索（文献/URL/数据线索）
  - `direct_info`：直接观点/约束
- 人工材料注入：上传 `pdf/md/txt` 作为临时材料（仅本次任务，不写入持久库）

### 最终整合流程

1. 全部审核通过 → 生成 Abstract
2. 生成 Limitations and Future Directions
3. 聚合 Open Gaps 研究议程
4. 全篇连贯性重写（global coherence refine）
5. 引用保护检查（失败则回退安全版本）
6. Canvas 切换到 `refine` 阶段

### 相关接口

- `POST /deep-research/submit`：提交后台任务，返回 `job_id`
- `GET /deep-research/jobs/{job_id}`：查询任务状态
- `GET /deep-research/jobs/{job_id}/events`：增量拉取进度事件
- `POST /deep-research/jobs/{job_id}/cancel`：停止任务
- `POST /deep-research/jobs/{job_id}/review`：提交章节审核
- `POST /deep-research/jobs/{job_id}/gap-supplement`：提交缺口补充
- `GET /deep-research/jobs/{job_id}/insights`：查看研究洞察

## 10. 常见问题

### Q1: 为什么 requirements.txt 不写 torch？

因为 pip/conda 混装容易导致底层 C++ 算子冲突，项目明确要求 torch 生态由 Conda 安装。

### Q2: 前端安装失败（Node 版本过低）

升级到 Node `20.19+` 或 `22.12+` 后重试 `npm install`。

### Q3: Google/Scholar 不可用

- 确认 `playwright install chromium` 已执行
- 确认 `config/rag_config.json` 中 `google_search.enabled=true`

### Q4: 离线模型如何运行？

```bash
export HF_LOCAL_FILES_ONLY=true
export MODEL_CACHE_ROOT=/path/to/hf_cache
```

### Q5: Milvus 启动失败

```bash
bash scripts/00_healthcheck_docker.sh
docker compose --profile dev logs milvus-dev
```

## 11. 下一步

安装完成后建议按顺序执行：

```bash
bash scripts/verify_dependencies.sh
python scripts/01_init_env.py
python scripts/02_parse_papers.py
python scripts/03_index_papers.py
python scripts/03b_build_graph.py
bash scripts/start.sh
```

生产环境请参考 `docs/release_migration_ubuntu.md` 使用 systemd + Nginx 部署。
</file>

<file path="pytest.ini">
[pytest]
testpaths = tests
python_files = test_*.py
python_functions = test_*
python_classes = Test*
addopts = -v --tb=short
</file>

<file path="README.md">
# DeepSea RAG

面向科研场景（尤其深海相关文献）的全栈 RAG 系统：支持 PDF 解析与入库、混合检索（向量 + 图谱 + 网络）、多轮对话、Deep Research、画布协作、引用管理、多文档对比，以及可观测性与 MCP 工具化接入。

## 你可以用它做什么

- 把本地 PDF 批量解析为结构化数据并向量入库
- 在聊天中做 `local / web / hybrid` 检索并给出可追溯引用
- 启用 Agent 模式让模型自动调用检索/画布/图谱/对比工具
- 对 2-5 篇论文做结构化比较（对话引文候选 + 本地文库候选）
- 在 Canvas 中持续编辑综述草稿并导出 Markdown
- 通过 Deep Research 完成从选题到综述终稿的全自动化研究流程
- 通过 `/metrics`、`/health/detailed` 监控运行状态
- 多语言支持（中/英）

## 当前技术栈

| 层 | 技术 |
|---|---|
| 后端框架 | FastAPI + Pydantic + SQLite + Milvus + NetworkX |
| 检索引擎 | Dense/Sparse + RRF + BGE-M3/ColBERT + HippoRAG + Web Search 聚合 |
| LLM 调度 | 统一 `LLMManager`（OpenAI / DeepSeek / Gemini / Claude / Kimi / Sonar） |
| Agent 框架 | ReAct 循环 + 统一 Tool 抽象 + LangGraph Deep Research |
| 前端 | React 19 + TypeScript + Zustand + Vite 7 + Tailwind CSS |
| 国际化 | i18next（中/英） |
| 可观测性 | OpenTelemetry + Prometheus + LangSmith |
| 工具化 | MCP Server（对外暴露工具/资源） |

## 快速开始

### 1) 安装依赖

```bash
conda create -n deepsea-rag python=3.10 -y
conda activate deepsea-rag
conda install -c pytorch -c conda-forge "pytorch>=2.6.0" "torchvision>=0.21.0" timm -y
pip install -r requirements.txt --no-cache-dir
cd frontend && npm install && cd ..
playwright install chromium
```

详细安装与依赖核对见 `install.md`。

### 2) 准备配置

```bash
cp config/rag_config.example.json config/rag_config.json
cp config/rag_config.local.example.json config/rag_config.local.json
cp .env.example .env
bash scripts/verify_dependencies.sh
```

- 敏感字段（API Key）建议放在 `config/rag_config.local.json`
- 或使用环境变量覆盖：`RAG_LLM__{PROVIDER}__API_KEY`

### 3) 启动基础服务（Milvus 等）

```bash
bash scripts/00_preflight_check.sh
# 开发环境（Mac CPU）
docker compose --profile dev up -d
# 生产环境（Linux GPU）
# docker compose --profile prod up -d
bash scripts/00_healthcheck_docker.sh
```

### 4) 执行离线入库流水线

```bash
python scripts/01_init_env.py
python scripts/02_parse_papers.py
python scripts/03_index_papers.py
python scripts/03b_build_graph.py
```

### 5) 启动全栈

```bash
bash scripts/start.sh
```

- 前端：`http://localhost:5173`
- 后端 Swagger：`http://127.0.0.1:9999/docs`

## 关键接口速览

| 功能域 | 接口 |
|---|---|
| 聊天 | `POST /chat`、`POST /chat/stream` |
| Deep Research | `/deep-research/start\|submit\|jobs/*\|review\|gap-supplement\|insights` |
| 画布 | `/canvas/*`（CRUD + 大纲 + 草稿 + 快照 + AI 编辑 + 引用管理） |
| 导出 | `POST /export` |
| 对比 | `POST /compare`、`GET /compare/candidates`、`GET /compare/papers` |
| 图谱 | `/graph/*`（统计 + 实体 + 邻居 + chunk 详情） |
| 在线入库 | `/ingest/*`（上传 + Collections + 任务管理） |
| 认证与项目 | `/auth/*`、`/admin/*`、`/projects/*` |
| 模型管理 | `GET /models/status`、`POST /models/sync`、`GET /llm/providers` |
| 自动补全 | `POST /auto-complete` |
| 可观测 | `GET /metrics`、`GET /health`、`GET /health/detailed`、`GET /storage/stats` |

完整接口见 `docs/api_reference.md`。

## Deep Research 核心能力

- **启动前 `⚙` 设置**：深度（lite / comprehensive）、输出语言、分步骤模型、strict step model
- **后台任务模式**：提交后前端可关闭/刷新，任务在后端持续运行
- **Drafting 人工审核**：通过 / 修改 / 重新确认 + 一键"全部通过并触发整合"
- **章节缺口补充**：支持"材料线索"与"直接观点"
- **人工介入**：上传临时材料（pdf/md/txt）或文本补充，仅用于本次任务
- **最终整合**：自动生成 Abstract + Limitations + Open Gaps 研究议程 + 全篇连贯性整合
- **引用保护**：若整合后引用/证据标签显著丢失，自动回退安全版本
- **成本监控**：心跳上报 + 预警阈值 + 强制摘要模式
- **循环防护**：动态迭代预算 + 收益曲线早停 + 3 级验证分流

## 目录结构

```text
.
├── README.md                 # 项目入口
├── install.md                # 安装与依赖核对
├── requirements.txt          # Python 依赖
├── docker-compose.yml        # Docker 服务（Milvus/etcd/MinIO）
├── .env.example              # 环境变量模板
│
├── config/                   # 配置中心
│   ├── rag_config.json       #   主配置（结构 + 默认值）
│   ├── rag_config.local.json #   本地覆盖（敏感信息，gitignored）
│   ├── rag_config.example.json
│   ├── rag_config.local.example.json
│   └── settings.py           #   Python 配置加载器
│
├── src/                      # 后端源码
│   ├── api/                  #   FastAPI 路由层
│   ├── llm/                  #   LLMManager + tools + react_loop
│   ├── retrieval/            #   混合检索 + web 聚合 + 重排
│   ├── collaboration/        #   协作核心
│   │   ├── canvas/           #     画布管理
│   │   ├── memory/           #     会话/工作/持久记忆
│   │   ├── intent/           #     意图解析与命令
│   │   ├── research/         #     Deep Research（LangGraph Agent）
│   │   ├── workflow/         #     状态机
│   │   ├── citation/         #     引用管理与格式化
│   │   └── export/           #     导出格式化
│   ├── indexing/             #   embed + Milvus + paper 管理
│   ├── parser/               #   PDF 解析 + 声明提取
│   ├── chunking/             #   结构化切块
│   ├── generation/           #   证据综合 + 上下文打包 + LLM 兼容层
│   ├── graph/                #   HippoRAG 图检索
│   ├── graphs/               #   LangGraph 流水线（入库图）
│   ├── mcp/                  #   MCP Server
│   ├── observability/        #   metrics + tracing + middleware
│   ├── evaluation/           #   评测执行与指标
│   ├── auth/                 #   认证（session + password）
│   ├── utils/                #   缓存/限流/清理/提示词管理/任务运行器
│   ├── log/                  #   日志管理
│   └── prompts/              #   LLM 提示词模板
│
├── frontend/                 # React 前端
│   ├── src/pages/            #   ChatPage / IngestPage / LoginPage / AdminPage
│   ├── src/components/       #   chat / canvas / compare / graph / workflow / research / settings / layout / ui
│   ├── src/stores/           #   Zustand 状态管理
│   ├── src/api/              #   后端接口封装
│   ├── src/types/            #   TypeScript 类型
│   └── src/i18n/             #   国际化（en/zh）
│
├── scripts/                  # 运行与测试脚本
├── tests/                    # pytest 测试
├── docs/                     # 文档中心
├── data/                     # 数据存储（raw/parsed/metadata/graph）
├── volumes/                  # Docker 持久卷（Milvus/etcd/MinIO）
└── logs/                     # 应用日志（含 LLM 原始响应日志）
```

## 文档导航

| 文档 | 说明 |
|---|---|
| `docs/README.md` | 文档总览与角色阅读路径 |
| `docs/developer_guide.md` | 开发总指南（模块职责、约定、扩展路径） |
| `docs/architecture.md` | 系统架构与关键数据流 |
| `docs/api_reference.md` | 按前缀分组的完整 API 参考 |
| `docs/configuration.md` | 配置项与环境变量说明 |
| `docs/scripts_guide.md` | 脚本用途、参数、推荐执行顺序 |
| `docs/operations_and_troubleshooting.md` | 启动、监控、运维、故障处理 |
| `docs/release_migration_ubuntu.md` | Ubuntu 发布与迁移全流程（systemd + Nginx） |
| `docs/testing_and_evaluation.md` | pytest 与评测体系 |
| `docs/dependency_matrix.md` | Python / 前端依赖矩阵与运行时要求 |

## LLM 调用约定（必须遵守）

统一走 `src/llm/llm_manager.py`：

```python
from src.llm import LLMManager

manager = LLMManager.from_json("config/rag_config.json")
client = manager.get_client("deepseek")
resp = client.chat(messages=[{"role": "user", "content": "你好"}])
text = resp["final_text"]
```

禁止直接在业务代码里实例化各家 SDK 客户端或硬编码密钥。

## License

MIT
</file>

<file path="config/rag_config.example.json">
{
  "llm": {
    "default": "claude",
    "dry_run": false,
    "providers": {
      "openai": {
        "api_key": "sk-proj-xxx",
        "base_url": "https://api.openai.com/v1",
        "default_model": "gpt-5.2",
        "models": {
          "gpt-5-mini": "gpt-5-mini",
          "gpt-5.2": "gpt-5.2"
        },
        "params": {}
      },
      "openai-thinking": {
        "api_key": "sk-proj-xxx",
        "base_url": "https://api.openai.com/v1",
        "default_model": "gpt-5.2",
        "models": {
          "gpt-5.2": "gpt-5.2"
        },
        "params": {
          "reasoning_effort": "high"
        }
      },
      "deepseek": {
        "api_key": "sk-xxx",
        "base_url": "https://api.deepseek.com/v1",
        "default_model": "deepseek-chat",
        "models": {
          "deepseek-chat": "deepseek-chat"
        },
        "params": {}
      },
      "deepseek-thinking": {
        "api_key": "sk-xxx",
        "base_url": "https://api.deepseek.com/v1",
        "default_model": "deepseek-reasoner",
        "models": {
          "deepseek-reasoner": "deepseek-reasoner"
        },
        "params": {}
      },
      "gemini": {
        "api_key": "AIzxxx",
        "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
        "default_model": "gemini-pro-latest",
        "models": {
          "gemini-pro-latest": "gemini-pro-latest",
          "gemini-flash-latest": "gemini-flash-latest"
        },
        "params": {}
      },
      "gemini-thinking": {
        "api_key": "AIzxxx",
        "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
        "default_model": "gemini-pro-latest",
        "models": {
          "gemini-pro-latest": "gemini-pro-latest",
          "gemini-flash-latest": "gemini-flash-latest"
        },
        "params": {
          "reasoning_effort": "high"
        }
      },
      "gemini-vision": {
        "api_key": "AIzxxx",
        "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
        "default_model": "gemini-2.5-flash",
        "models": {
          "gemini-pro-latest": "gemini-pro-latest",
          "gemini-flash-latest": "gemini-flash-latest",
          "gemini-2.5-flash": "gemini-2.5-flash"
        },
        "params": {
          "temperature": 0.2,
          "max_tokens": 8192
        }
      },
      "claude": {
        "api_key": "sk-ant-xxx",
        "base_url": "https://api.anthropic.com",
        "default_model": "claude-sonnet-4-5",
        "models": {
          "claude-sonnet-4-5": "claude-sonnet-4-5-20250929",
          "claude-haiku-4-5": "claude-haiku-4-5-20251001",
          "claude-opus-4-5": "claude-opus-4-5-20251101"
        },
        "params": {}
      },
      "claude-thinking": {
        "api_key": "sk-ant-xxx",
        "base_url": "https://api.anthropic.com",
        "default_model": "claude-sonnet-4-5",
        "models": {
          "claude-sonnet-4-5": "claude-sonnet-4-5-20250929",
          "claude-haiku-4-5": "claude-haiku-4-5-20251001",
          "claude-opus-4-5": "claude-opus-4-5-20251101"
        },
        "params": {
          "thinking": {
            "type": "enabled",
            "budget_tokens": 16000
          },
          "max_tokens": 32000
        }
      },
      "kimi": {
        "api_key": "sk-xxx",
        "base_url": "https://api.moonshot.ai/v1",
        "default_model": "kimi-k2.5",
        "models": {
          "kimi-k2.5": "kimi-k2.5"
        },
        "params": {}
      },
      "kimi-vision": {
        "api_key": "sk-xxx",
        "base_url": "https://api.moonshot.ai/v1",
        "default_model": "kimi-k2.5",
        "models": {
          "kimi-k2.5": "kimi-k2.5"
        },
        "params": {
          "response_format": {
            "type": "json_object"
          }
        }
      },
      "kimi-thinking": {
        "api_key": "sk-xxx",
        "base_url": "https://api.moonshot.ai/v1",
        "default_model": "kimi-k2.5",
        "models": {
          "kimi-k2.5": "kimi-k2.5"
        },
        "params": {
          "enable_reasoning": true,
          "reasoning_effort": "high"
        }
      }
    }
  },
  "parser": {
    "version": "1.0.0",
    "docling_ocr": false,
    "column_gap_min_ratio": 0.02,
    "column_gap_max_ratio": 0.15,
    "gravity_max_v_down_ratio": 0.06,
    "gravity_min_x_overlap": 0.30,
    "gravity_cross_page_penalty": 0.20,
    "caption_patterns": ["^(Fig\\.|Figure)\\s*\\d+", "^图\\s*\\d+"],
    "table_title_patterns": ["^Table\\s*\\d+", "^Tab\\.\\s*\\d+", "^表\\s*\\d+"],
    "table_footnote_patterns": ["^Note:", "^\\*", "^注[：:]", "^Source:"],
    "llm_text_provider": "deepseek",
    "llm_vision_provider": "gemini-vision",
    "llm_text_model": null,
    "llm_vision_model": null,
    "llm_vision_concurrency": 1,
    "llm_text_max_tokens": 500,
    "llm_vision_max_tokens": 1024,
    "llm_json_repair_max_tokens": 800,
    "llm_temperature": 0.1,
    "llm_max_retries": 2,
    "enrich_tables": true,
    "enrich_figures": true
  },
  "chunk": {
    "target_chars": 1000,
    "min_chars": 200,
    "max_chars": 1800,
    "overlap_sentences": 2,
    "table_rows_per_chunk": 10
  },
  "search": {
    "top_k": 10,
    "rerank_top_k": 5,
    "rrf_k": 60,
    "dense_recall_k": 80,
    "sparse_recall_k": 80,
    "rrf_dense_weight": 0.6,
    "rrf_sparse_weight": 0.4,
    "rerank_input_k": 100,
    "rerank_output_k": 10,
    "per_doc_cap": 3,
    "reranker_mode": "cascade",
    "use_colbert_reranker": true,
    "colbert_model": "jinaai/jina-colbert-v2",
    "colbert_top_k": 30
  },
  "web_search": {
    "enabled": true,
    "provider": "tavily",
    "api_key": "",
    "search_depth": "advanced",
    "max_results": 5,
    "include_answer": true,
    "include_domains": [],
    "exclude_domains": [],
    "enable_query_optimizer": true,
    "enable_query_expansion": false,
    "query_expansion_llm": "deepseek",
    "max_queries": 4
  },
  "semantic_scholar": {
    "enabled": false,
    "api_key": "",
    "base_url": "https://ai4scholar.net/graph/v1",
    "max_results": 5,
    "timeout_seconds": 30
  },
  "ncbi": {
    "_comment": "NCBI PubMed E-Utilities（免费，api_key 可选——填写后速率从 3 req/s 提升至 10 req/s）",
    "_api_key_note": "在 https://www.ncbi.nlm.nih.gov/account/ 注册后可在账号设置中申请 API Key",
    "enabled": true,
    "api_key": "",
    "max_results": 5,
    "timeout_seconds": 20,
    "cache_ttl_seconds": 3600,
    "cache_maxsize": 256
  },
  "citation": {
    "key_format": "author_date",
    "hash_length": 12,
    "author_date_max_authors": 2,
    "merge_level": "document"
  },
  "auto_complete": {
    "enabled": true,
    "max_sections": 6,
    "max_words_per_section": 500,
    "include_abstract": true,
    "default_search_mode": "hybrid"
  },
  "performance": {
    "retrieval": {
      "timeout_seconds": 60,
      "cache_enabled": false,
      "cache_ttl_seconds": 3600,
      "parallel_dense_sparse": true,
      "max_workers": 4
    },
    "llm": {
      "timeout_seconds": 120,
      "max_retries": 2,
      "retry_backoff": 1.5,
      "cache_enabled": false,
      "cache_ttl_seconds": 3600,
      "max_concurrent_per_provider": 5
    },
    "web_search": {
      "timeout_seconds": 30,
      "cache_enabled": false,
      "cache_ttl_seconds": 3600
    },
    "unified_web_search": {
      "max_parallel_providers": 3,
      "per_provider_timeout_seconds": 30,
      "browser_providers_max_parallel": 1
    },
    "google_search": {
      "browser_reuse": true,
      "max_idle_seconds": 300,
      "max_pages_per_browser": 10,
      "cache_enabled": false,
      "cache_ttl_seconds": 1800
    }
  }
}
</file>

<file path="config/rag_config.json">
{
  "llm": {
    "default": "deepseek", 
    "dry_run": false,
    "providers": {
      "openai": {
        "api_key": "sk-xxx",
        "base_url": "https://api.openai.com/v1",
        "default_model": "gpt-5.2",
        "models": {
          "gpt-5-mini": "gpt-5-mini",
          "gpt-5.2": "gpt-5.2"
        },
        "params": {}
      },
      "openai-thinking": {
        "api_key": "sk-xxx",
        "base_url": "https://api.openai.com/v1",
        "default_model": "gpt-5.2",
        "models": {
          "gpt-5.2": "gpt-5.2"
        },
        "params": {
          "reasoning_effort": "high"
        }
      },
      "deepseek": {
        "api_key": "sk-xxx",
        "base_url": "https://api.deepseek.com/v1",
        "default_model": "deepseek-chat",
        "models": {
          "deepseek-chat": "deepseek-chat"
        },
        "params": {}
      },
      "deepseek-thinking": {
        "api_key": "sk-xxx",
        "base_url": "https://api.deepseek.com/v1",
        "default_model": "deepseek-reasoner",
        "models": {
          "deepseek-reasoner": "deepseek-reasoner"
        },
        "params": {}
      },
      "gemini": {
        "api_key": "AIzxxx",
        "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
        "default_model": "gemini-pro-latest",
        "models": {
          "gemini-pro-latest": "gemini-pro-latest",
          "gemini-flash-latest": "gemini-flash-latest"
        },
        "params": {}
      },
      "gemini-thinking": {
        "api_key": "AIzxxx",
        "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
        "default_model": "gemini-flash-latest",
        "default_model_note": "修改默认模型：将 default_model 改为 models 里的 key（如 gemini-pro-latest）",
        "models": {
          "gemini-pro-latest": "gemini-pro-latest",
          "gemini-flash-latest": "gemini-flash-latest"
        },
        "params": {
          "reasoning_effort": "high"
        }
      },
      "gemini-vision": {
        "api_key": "AIzxxx",
        "base_url": "https://generativelanguage.googleapis.com/v1beta/openai",
        "default_model": "gemini-2.5-flash",
        "models": {
          "gemini-pro-latest": "gemini-pro-latest",
          "gemini-flash-latest": "gemini-flash-latest",
          "gemini-2.5-flash": "gemini-2.5-flash"
        },
        "params": {
          "temperature": 0.2,
          "max_tokens": 8192
        }
      },
      "claude": {
        "api_key": "sk-ant-xxx",
        "base_url": "https://api.anthropic.com",
        "default_model": "claude-sonnet-4-5",
        "models": {
          "claude-sonnet-4-5": "claude-sonnet-4-5-20250929",
          "claude-haiku-4-5": "claude-haiku-4-5-20251001",
          "claude-opus-4-6": "claude-opus-4-6-20260205"
        },
        "params": {}
      },
      "claude-thinking": {
        "api_key": "sk-ant-xxx",
        "base_url": "https://api.anthropic.com",
        "default_model": "claude-sonnet-4-5",
        "models": {
          "claude-sonnet-4-5": "claude-sonnet-4-5-20250929",
          "claude-haiku-4-5": "claude-haiku-4-5-20251001",
          "claude-opus-4-6": "claude-opus-4-6-20260205"
        },
        "params": {
          "thinking": {
            "type": "enabled",
            "budget_tokens": 8000
          },
          "max_tokens": 16000
        }
      },
      "kimi": {
        "api_key": "sk-xxx",
        "base_url": "https://api.moonshot.ai/v1",
        "default_model": "kimi-k2.5",
        "models": {
          "kimi-k2.5": "kimi-k2.5"
        },
        "params": {}
      },
      "kimi-vision": {
        "api_key": "sk-xxx",
        "base_url": "https://api.moonshot.ai/v1",
        "default_model": "kimi-k2.5",
        "models": {
          "kimi-k2.5": "kimi-k2.5"
        },
        "params": {
          "response_format": {
            "type": "json_object"
          }
        }
      },
      "kimi-thinking": {
        "api_key": "sk-xxx",
        "base_url": "https://api.moonshot.ai/v1",
        "default_model": "kimi-k2.5",
        "models": {
          "kimi-k2.5": "kimi-k2.5"
        },
        "params": {
          "enable_reasoning": true,
          "reasoning_effort": "high"
        }
      },
      "sonar": {
        "api_key": "",
        "base_url": "https://api.perplexity.ai",
        "default_model": "sonar-pro",
        "models": {
          "sonar": "sonar",
          "sonar-pro": "sonar-pro",
          "sonar-reasoning-pro": "sonar-reasoning-pro"
        },
        "params": {}
      }
    }
  },
  "parser": {
    "version": "1.0.0",
    "docling_ocr": false,
    "column_gap_min_ratio": 0.02,
    "column_gap_max_ratio": 0.15,
    "gravity_max_v_down_ratio": 0.06,
    "gravity_min_x_overlap": 0.30,
    "gravity_cross_page_penalty": 0.20,
    "caption_patterns": ["^(Fig\\.|Figure)\\s*\\d+", "^图\\s*\\d+"],
    "table_title_patterns": ["^Table\\s*\\d+", "^Tab\\.\\s*\\d+", "^表\\s*\\d+"],
    "table_footnote_patterns": ["^Note:", "^\\*", "^注[：:]", "^Source:"],
    "llm_text_provider": "deepseek",
    "llm_vision_provider": "gemini-vision",
    "llm_text_model": null,
    "llm_vision_model": null,
    "llm_vision_concurrency": 1,
    "llm_text_max_tokens": 800,
    "llm_vision_max_tokens": 1536,
    "llm_json_repair_max_tokens": 1024,
    "llm_temperature": 0.1,
    "llm_max_retries": 2,
    "enrich_tables": true,
    "enrich_figures": true
  },
  "chunk": {
    "target_chars": 1000,
    "min_chars": 200,
    "max_chars": 1800,
    "overlap_sentences": 2,
    "table_rows_per_chunk": 10
  },
  "search": {
    "top_k": 20,
    "rerank_top_k": 20,
    "rrf_k": 60,
    "dense_recall_k": 80,
    "sparse_recall_k": 80,
    "rrf_dense_weight": 0.6,
    "rrf_sparse_weight": 0.4,
    "rerank_input_k": 100,
    "rerank_output_k": 20,
    "per_doc_cap": 5,
    "reranker_mode": "cascade",
    "use_colbert_reranker": true,
    "colbert_model": "jinaai/jina-colbert-v2",
    "colbert_top_k": 30
  },
  "web_search": {
    "enabled": true,
    "provider": "tavily",
    "api_key": "",
    "search_depth": "advanced",
    "max_results": 5,
    "include_answer": true,
    "include_domains": [],
    "exclude_domains": [],
    "enable_query_optimizer": true,
    "enable_query_expansion": false,
    "query_expansion_llm": "deepseek",
    "max_queries": 4,
    "smart_optimizer": {
      "enabled": true,
      "llm_provider": "deepseek",
      "max_queries_per_provider": 3,
      "enable_bilingual": true,
      "fallback_to_simple": true
    }
  },
  "google_search": {
    "enabled": true,
    "scholar_enabled": true,
    "google_enabled": false,
    "extension_path": "extra_tools/CapSolverExtension",
    "headless": null,
    "proxy": null,
    "timeout": 60000,
    "max_results": 5,
    "user_data_dir": null
  },
  "semantic_scholar": {
    "enabled": false,
    "api_key": "",
    "base_url": "https://ai4scholar.net/graph/v1",
    "max_results": 5,
    "timeout_seconds": 30
  },
  "ncbi": {
    "enabled": true,
    "api_key": "",
    "max_results": 5,
    "timeout_seconds": 20,
    "cache_ttl_seconds": 3600,
    "cache_maxsize": 256
  },
  "content_fetcher": {
    "enabled": false,
    "only_academic": false,
    "max_content_length": 8000,
    "timeout_seconds": 15,
    "brightdata_api_key": "",
    "brightdata_zone": "",
    "cache_enabled": true,
    "cache_ttl_seconds": 3600,
    "max_concurrent": 5
  },
  "api": {
    "host": "127.0.0.1",
    "port": 9999
  },
  "auth": {
    "secret_key": "change-me-in-local",
    "token_expire_hours": 24,
    "admin_username": "admin",
    "admin_default_password": "admin123"
  },
  "logging": {
    "level": "INFO",
    "max_size_mb": 100,
    "max_age_days": 30,
    "min_keep_mb": 20,
    "console_output": true
  },
  "storage": {
    "max_age_days": 30,
    "max_size_gb": 5,
    "cleanup_on_startup": true,
    "cleanup_batch_size": 100
  },
  "citation": {
    "key_format": "author_date",
    "hash_length": 12,
    "author_date_max_authors": 2,
    "merge_level": "document"
  },
  "deep_research": {
    "default_depth": "comprehensive",
    "depth_presets": {
      "lite": {
        "max_iterations_per_section": 3,
        "max_section_research_rounds": 3,
        "coverage_threshold": 0.60,
        "recall_queries_per_section": 2,
        "precision_queries_per_section": 2,
        "search_top_k_first": 18,
        "search_top_k_gap": 10,
        "search_top_k_write": 10,
        "verification_k": 12,
        "self_correction_trigger_coverage": 0.75,
        "self_correction_min_round": 3,
        "search_top_k_gap_decay_factor": 0.60,
        "search_top_k_gap_min": 6,
        "coverage_plateau_floor": 0.70,
        "coverage_plateau_min_gain": 0.03,
        "verify_light_threshold": 0.20,
        "verify_medium_threshold": 0.40,
        "verify_severe_threshold": 0.45,
        "review_gate_max_rounds": 80,
        "review_gate_base_sleep": 2,
        "review_gate_max_sleep": 15,
        "review_gate_early_stop_unchanged": 8,
        "recursion_limit": 200,
        "cost_warn_steps": 120,
        "cost_force_summary_steps": 180,
        "cost_tick_interval": 25
      },
      "comprehensive": {
        "max_iterations_per_section": 6,
        "max_section_research_rounds": 5,
        "coverage_threshold": 0.80,
        "recall_queries_per_section": 4,
        "precision_queries_per_section": 4,
        "search_top_k_first": 30,
        "search_top_k_gap": 15,
        "search_top_k_write": 12,
        "verification_k": 16,
        "self_correction_trigger_coverage": 0.78,
        "self_correction_min_round": 3,
        "search_top_k_gap_decay_factor": 0.70,
        "search_top_k_gap_min": 8,
        "coverage_plateau_floor": 0.78,
        "coverage_plateau_min_gain": 0.02,
        "verify_light_threshold": 0.15,
        "verify_medium_threshold": 0.30,
        "verify_severe_threshold": 0.35,
        "review_gate_max_rounds": 200,
        "review_gate_base_sleep": 2,
        "review_gate_max_sleep": 20,
        "review_gate_early_stop_unchanged": 12,
        "recursion_limit": 500,
        "cost_warn_steps": 300,
        "cost_force_summary_steps": 420,
        "cost_tick_interval": 30
      }
    }
  },
  "auto_complete": {
    "enabled": true,
    "max_sections": 6,
    "max_words_per_section": 500,
    "include_abstract": true,
    "default_search_mode": "hybrid"
  },
  "performance": {
    "retrieval": {
      "timeout_seconds": 60,
      "cache_enabled": false,
      "cache_ttl_seconds": 3600,
      "parallel_dense_sparse": true,
      "max_workers": 4
    },
    "llm": {
      "timeout_seconds": 120,
      "max_retries": 2,
      "retry_backoff": 1.5,
      "cache_enabled": false,
      "cache_ttl_seconds": 3600,
      "max_concurrent_per_provider": 5
    },
    "web_search": {
      "timeout_seconds": 30,
      "cache_enabled": false,
      "cache_ttl_seconds": 3600
    },
    "unified_web_search": {
      "max_parallel_providers": 3,
      "per_provider_timeout_seconds": 30,
      "browser_providers_max_parallel": 1
    },
    "google_search": {
      "browser_reuse": true,
      "max_idle_seconds": 300,
      "max_pages_per_browser": 10,
      "cache_enabled": false,
      "cache_ttl_seconds": 1800
    }
  }
}
</file>

<file path="config/rag_config.local.example.json">
{
  "llm": {
    "providers": {
      "openai": { "api_key": "sk-xxx" },
      "openai-thinking": { "api_key": "sk-xxx" },
      "deepseek": { "api_key": "sk-xxx" },
      "deepseek-thinking": { "api_key": "sk-xxx" },
      "gemini": { "api_key": "AIzxxx" },
      "gemini-thinking": { "api_key": "AIzxxx" },
      "gemini-vision": { "api_key": "AIzxxx" },
      "claude": { "api_key": "sk-ant-xxx" },
      "claude-thinking": { "api_key": "sk-ant-xxx" },
      "kimi": { "api_key": "sk-xxx" },
      "kimi-vision": { "api_key": "sk-xxx" },
      "kimi-thinking": { "api_key": "sk-xxx" }
    }
  },
  "web_search": {
    "api_key": "tvly-xxx"
  },
  "semantic_scholar": {
    "api_key": "ai4scholar-xxx"
  },
  "ncbi": {
    "api_key": ""
  }
}
</file>

<file path="config/settings.py">
"""
统一配置模块
- 配置文件: config/rag_config.json（LLM 等可调参数）
- 本地覆盖: config/rag_config.local.json（本地私密配置）
- 环境变量优先覆盖敏感项（API Key 等）
"""

import os
import json
from pathlib import Path
from dataclasses import dataclass, field
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

load_dotenv()

# 加载 config/rag_config.json + config/rag_config.local.json（本地覆盖）
_CONFIG_PATH = Path(__file__).parent / "rag_config.json"
_LOCAL_CONFIG_PATH = Path(__file__).parent / "rag_config.local.json"


def _load_json(path: Path) -> Dict[str, Any]:
    if not path.exists():
        return {}
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def _deep_merge(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    result = base.copy()
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = _deep_merge(result[key], value)
        else:
            result[key] = value
    return result


_RAW_CONFIG: Dict[str, Any] = _load_json(_CONFIG_PATH)
if _LOCAL_CONFIG_PATH.exists():
    _RAW_CONFIG = _deep_merge(_RAW_CONFIG, _load_json(_LOCAL_CONFIG_PATH))


@dataclass
class MilvusSettings:
    host: str = os.getenv("MILVUS_HOST", "localhost")
    port: int = int(os.getenv("MILVUS_PORT", "19530"))

    @property
    def uri(self) -> str:
        return f"http://{self.host}:{self.port}"


@dataclass
class ModelSettings:
    device: str = os.getenv("COMPUTE_DEVICE", "mps")
    use_fp16: bool = os.getenv("USE_FP16", "false").lower() == "true"
    embedding_model: str = os.getenv("EMBEDDING_MODEL", "BAAI/bge-m3")
    reranker_model: str = os.getenv("RERANKER_MODEL", "BAAI/bge-reranker-v2-m3")
    _default_cache_root: str = os.path.expanduser(os.getenv("MODEL_CACHE_ROOT", "~/Hug"))
    embedding_cache_dir: Optional[str] = os.getenv("EMBEDDING_CACHE_DIR") or _default_cache_root
    reranker_cache_dir: Optional[str] = os.getenv("RERANKER_CACHE_DIR") or _default_cache_root
    colbert_cache_dir: Optional[str] = os.getenv("COLBERT_CACHE_DIR") or _default_cache_root
    local_files_only: bool = os.getenv("HF_LOCAL_FILES_ONLY", "false").lower() == "true"


@dataclass
class CollectionSettings:
    """Collection 命名 - 全局统一，迁移时不变"""
    life: str = os.getenv("COLLECTION_LIFE", "deepsea_life")
    ocean: str = os.getenv("COLLECTION_OCEAN", "deepsea_ocean")
    env: str = os.getenv("COLLECTION_ENV", "deepsea_env")
    global_: str = os.getenv("COLLECTION_GLOBAL", "deepsea_global")

    def get(self, domain: str) -> str:
        mapping = {"life": self.life, "ocean": self.ocean, "env": self.env}
        return mapping.get(domain, self.global_)

    def all(self) -> List[str]:
        return [self.life, self.ocean, self.env, self.global_]


@dataclass
class IndexSettings:
    index_type: str = os.getenv("INDEX_TYPE", "IVF_FLAT")
    nlist: int = int(os.getenv("INDEX_NLIST", "256"))

    @property
    def params(self) -> dict:
        return {
            "index_type": self.index_type,
            "metric_type": "COSINE",
            "params": {"nlist": self.nlist}
        }


@dataclass
class ChunkSettings:
    target_chars: int = int(os.getenv("CHUNK_TARGET_CHARS", "1000"))
    min_chars: int = int(os.getenv("CHUNK_MIN_CHARS", "200"))
    max_chars: int = int(os.getenv("CHUNK_MAX_CHARS", "1800"))
    overlap_sentences: int = int(os.getenv("CHUNK_OVERLAP_SENTENCES", "2"))
    table_rows_per_chunk: int = int(os.getenv("CHUNK_TABLE_ROWS_PER_CHUNK", "10"))


@dataclass
class SearchSettings:
    top_k: int = int(os.getenv("SEARCH_TOP_K", "20"))
    rerank_top_k: int = int(os.getenv("RERANK_TOP_K", "20"))
    rrf_k: int = int(os.getenv("RRF_K", "60"))
    dense_recall_k: int = int(os.getenv("DENSE_RECALL_K", "80"))
    sparse_recall_k: int = int(os.getenv("SPARSE_RECALL_K", "80"))
    rrf_dense_weight: float = float(os.getenv("RRF_DENSE_WEIGHT", "0.6"))
    rrf_sparse_weight: float = float(os.getenv("RRF_SPARSE_WEIGHT", "0.4"))
    rerank_input_k: int = int(os.getenv("RERANK_INPUT_K", "100"))
    rerank_output_k: int = int(os.getenv("RERANK_OUTPUT_K", "20"))
    per_doc_cap: int = int(os.getenv("PER_DOC_CAP", "5"))
    # ColBERT 精排：bge_only | colbert_only | cascade
    reranker_mode: str = os.getenv("RERANKER_MODE", "cascade")
    use_colbert_reranker: bool = os.getenv("USE_COLBERT_RERANKER", "true").lower() == "true"
    colbert_model: str = os.getenv("COLBERT_MODEL", "colbert-ir/colbertv2.0")
    colbert_top_k: int = int(os.getenv("COLBERT_TOP_K", "30"))  # cascade 时 BGE 粗排输出条数，再送 ColBERT 精排


@dataclass
class WebSearchConfig:
    """Tavily 网络搜索配置"""
    enabled: bool = True
    provider: str = "tavily"
    api_key: str = ""
    search_depth: str = "advanced"
    max_results: int = 5
    include_answer: bool = True
    include_domains: List[str] = field(default_factory=list)
    exclude_domains: List[str] = field(default_factory=list)
    enable_query_optimizer: bool = True
    enable_query_expansion: bool = False
    query_expansion_llm: str = "deepseek"
    max_queries: int = 4


@dataclass
class GoogleSearchConfig:
    """Google Scholar / Google 搜索配置"""
    enabled: bool = True
    scholar_enabled: bool = True
    google_enabled: bool = False
    extension_path: str = "extra_tools/CapSolverExtension"
    headless: Optional[bool] = None
    proxy: Optional[str] = None
    timeout: int = 60000
    max_results: int = 5
    user_data_dir: Optional[str] = None


@dataclass
class SemanticScholarConfig:
    """Semantic Scholar API 配置（通过 ai4scholar 代理）"""
    enabled: bool = False
    api_key: str = ""
    base_url: str = "https://ai4scholar.net/graph/v1"
    max_results: int = 5
    timeout_seconds: int = 30


@dataclass
class NCBIConfig:
    """NCBI PubMed E-Utilities 配置（免费 API，api_key 可选但可提升速率上限）"""
    enabled: bool = True
    api_key: str = ""
    max_results: int = 5
    timeout_seconds: int = 20
    cache_ttl_seconds: int = 3600
    cache_maxsize: int = 256


@dataclass
class ContentFetcherConfig:
    """WebContentFetcher 全文抓取配置"""
    enabled: bool = False
    only_academic: bool = False
    max_content_length: int = 8000
    timeout_seconds: int = 15
    brightdata_api_key: str = ""
    brightdata_zone: str = ""
    cache_enabled: bool = True
    cache_ttl_seconds: int = 3600
    max_concurrent: int = 5


@dataclass
class ApiSettings:
    """API 服务配置"""
    host: str = os.getenv("API_HOST", "127.0.0.1")
    port: int = int(os.getenv("API_PORT", "9999"))


@dataclass
class CitationSettings:
    """引用格式配置"""
    key_format: str = "author_date"  # numeric | hash | author_date
    hash_length: int = 12
    author_date_max_authors: int = 2
    merge_level: str = "document"  # chunk | document（按 doc_id/URL 合并为文章级）


# LLM 环境变量映射（兼容旧变量名）
_LLM_ENV_KEYS = {
    "openai": "OPENAI_API_KEY",
    "deepseek": "DEEPSEEK_API_KEY",
    "gemini": "GEMINI_API_KEY",
    "claude": "ANTHROPIC_API_KEY",
    "kimi": "KIMI_API_KEY",
}

# 各 provider 默认 base_url / model（config 未填时回退）
_LLM_DEFAULTS = {
    "openai": {"base_url": "https://api.openai.com/v1", "default_model": "gpt-4o"},
    "deepseek": {"base_url": "https://api.deepseek.com/v1", "default_model": "deepseek-chat"},
    "gemini": {"base_url": "https://generativelanguage.googleapis.com/v1beta", "default_model": "gemini-1.5-pro"},
    "claude": {"base_url": "https://api.anthropic.com", "default_model": "claude-sonnet-4-20250514"},
}


def _llm_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("llm") or {})


def _llm_provider_raw(name: str) -> Dict[str, Any]:
    return (_llm_from_config().get("providers") or {}).get(name) or {}


class LLMSettings:
    """
    LLM 配置：config/rag_config.json 中 llm.providers 支持 openai / deepseek / gemini / claude。
    环境变量覆盖 api_key：OPENAI_API_KEY, DEEPSEEK_API_KEY, GEMINI_API_KEY, ANTHROPIC_API_KEY。
    脚本可通过参数 --llm / --model 指定本次使用的 provider 和模型。
    """

    def __init__(self):
        cfg = _llm_from_config()
        self.default: str = os.getenv("DEFAULT_LLM") or cfg.get("default") or "claude"
        self.dry_run: bool = (
            os.getenv("LLM_DRY_RUN", "").lower() == "true" or cfg.get("dry_run") is True
        )

    def get_provider(self, name: str) -> Dict[str, Any]:
        """
        按 provider 名（openai / deepseek / gemini / claude / kimi 等）返回配置：
        - api_key/base_url
        - default_model + models（支持一个 provider 多模型）
        - params（provider 额外参数）
        环境变量优先覆盖 api_key。支持变体名匹配环境变量（如 kimi-thinking 找 KIMI_API_KEY）。
        """
        raw = _llm_provider_raw(name)
        defaults = _LLM_DEFAULTS.get(name) or {}

        # 环境变量查找逻辑：
        # 1) RAG_LLM__{PROVIDER}__API_KEY（优先）
        # 2) 兼容旧变量名（如 OPENAI_API_KEY）
        normalized = name.upper().replace("-", "_")
        env_key = f"RAG_LLM__{normalized}__API_KEY"
        api_key = os.getenv(env_key)
        if not api_key:
            legacy_key = _LLM_ENV_KEYS.get(name)
            if not legacy_key and "-" in name:
                base_name = name.split("-")[0]
                legacy_key = _LLM_ENV_KEYS.get(base_name)
            api_key = (os.getenv(legacy_key) if legacy_key else None)
        api_key = api_key or raw.get("api_key") or ""
        models = raw.get("models") or {}
        if isinstance(models, list):
            models = {m: m for m in models}
        default_model = (
            raw.get("default_model")
            or raw.get("model")  # 兼容旧字段
            or defaults.get("default_model")
            or defaults.get("model")
            or ""
        )
        return {
            "api_key": api_key,
            "base_url": raw.get("base_url") or defaults.get("base_url") or "",
            "default_model": default_model,
            "models": models,
            "params": raw.get("params") or {},
        }

    def resolve_model(self, provider: str, model_override: str | None = None) -> str:
        """
        解析本次实际要调用的模型名：
        - 若传入 model_override：可为 alias（在 models 中）或直接 model 名
        - 若未传：使用 default_model
        """
        cfg = self.get_provider(provider)
        models: Dict[str, str] = cfg.get("models") or {}
        default_model: str = (cfg.get("default_model") or "").strip()

        if model_override:
            m = model_override.strip()
            if not m:
                return default_model
            if m in models:
                return str(models[m]).strip()
            # 允许直接给 model 名（无需事先写入 models）
            return m

        # 未指定 model：走 default_model；若 default_model 是 alias，则映射
        if default_model in models:
            return str(models[default_model]).strip()
        return default_model

    def is_available(self, name: str) -> bool:
        """检查某 provider 是否已配置 api_key（不做占位前缀过滤）"""
        p = self.get_provider(name)
        key = (p.get("api_key") or "").strip()
        return bool(key)


@dataclass
class PathSettings:
    base: Path = field(default_factory=lambda: Path(__file__).parent.parent)

    @property
    def data(self) -> Path:
        return self.base / "data"

    @property
    def raw_papers(self) -> Path:
        return self.data / "raw_papers"

    @property
    def parsed(self) -> Path:
        return self.data / "parsed"

    @property
    def logs(self) -> Path:
        return self.data / "logs"

    @property
    def artifacts(self) -> Path:
        return self.base / "artifacts"

    def ensure_dirs(self):
        for p in [self.data, self.raw_papers, self.parsed, self.logs, self.artifacts]:
            p.mkdir(parents=True, exist_ok=True)


def _chunk_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("chunk") or {})


def _search_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("search") or {})


def _web_search_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("web_search") or {})


def _google_search_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("google_search") or {})


def _api_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("api") or {})


def _citation_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("citation") or {})


def _semantic_scholar_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("semantic_scholar") or {})


def _ncbi_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("ncbi") or {})


def _content_fetcher_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("content_fetcher") or {})


def _performance_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("performance") or {})


def _storage_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("storage") or {})


def _auth_from_config() -> Dict[str, Any]:
    return (_RAW_CONFIG.get("auth") or {})


@dataclass
class RetrievalPerfSettings:
    """检索层：超时、缓存、并行"""
    timeout_seconds: int = 60
    cache_enabled: bool = False
    cache_ttl_seconds: int = 3600
    parallel_dense_sparse: bool = True
    max_workers: int = 4


@dataclass
class LLMPerfSettings:
    """LLM：超时、重试、并发限流、缓存"""
    timeout_seconds: int = 120
    max_retries: int = 2
    retry_backoff: float = 1.5
    cache_enabled: bool = False
    cache_ttl_seconds: int = 3600
    max_concurrent_per_provider: int = 5


@dataclass
class WebSearchPerfSettings:
    """Tavily 等：超时、缓存"""
    timeout_seconds: int = 30
    cache_enabled: bool = False
    cache_ttl_seconds: int = 3600


@dataclass
class UnifiedWebSearchPerfSettings:
    """统一网络搜索：并发、超时。Scholar/Google 因风控单独限流，默认 1，建议最大 2。"""
    max_parallel_providers: int = 3
    per_provider_timeout_seconds: int = 30
    browser_providers_max_parallel: int = 1  # Scholar+Google 同时最多几个，建议不超过 2


@dataclass
class GoogleSearchPerfSettings:
    """Google/Scholar：浏览器复用、缓存"""
    browser_reuse: bool = True
    max_idle_seconds: int = 300
    max_pages_per_browser: int = 10
    cache_enabled: bool = False
    cache_ttl_seconds: int = 1800


@dataclass
class StorageSettings:
    """持久化存储生命周期与容量限制"""
    max_age_days: int = 30          # 数据保留天数，默认 30 天
    max_size_gb: float = 5.0        # 总大小上限（GB），默认 5GB
    cleanup_on_startup: bool = True # 启动时是否自动清理
    cleanup_batch_size: int = 100   # 每批清理记录数


@dataclass
class AuthSettings:
    """认证配置：token 有效期、首次管理员账号（敏感项放 .local.json）"""
    secret_key: str = "change-me-in-local"
    token_expire_hours: float = 24.0
    admin_username: str = "admin"
    admin_default_password: str = "admin123"


class Settings:
    def __init__(self):
        self.env = os.getenv("RAG_ENV", "dev")
        self.milvus = MilvusSettings()
        self.model = ModelSettings()
        self.collection = CollectionSettings()
        self.index = IndexSettings()
        c = _chunk_from_config()
        self.chunk = ChunkSettings(
            target_chars=c.get("target_chars", 1000),
            min_chars=c.get("min_chars", 200),
            max_chars=c.get("max_chars", 1800),
            overlap_sentences=c.get("overlap_sentences", 2),
            table_rows_per_chunk=c.get("table_rows_per_chunk", 10),
        )
        s = _search_from_config()
        self.search = SearchSettings(
            top_k=s.get("top_k", int(os.getenv("SEARCH_TOP_K", "20"))),
            rerank_top_k=s.get("rerank_top_k", int(os.getenv("RERANK_TOP_K", "20"))),
            rrf_k=s.get("rrf_k", int(os.getenv("RRF_K", "60"))),
            dense_recall_k=s.get("dense_recall_k", 80),
            sparse_recall_k=s.get("sparse_recall_k", 80),
            rrf_dense_weight=s.get("rrf_dense_weight", 0.6),
            rrf_sparse_weight=s.get("rrf_sparse_weight", 0.4),
            rerank_input_k=s.get("rerank_input_k", 100),
            rerank_output_k=s.get("rerank_output_k", 20),
            per_doc_cap=s.get("per_doc_cap", 5),
            reranker_mode=s.get("reranker_mode", os.getenv("RERANKER_MODE", "bge_only")),
            use_colbert_reranker=s.get("use_colbert_reranker", os.getenv("USE_COLBERT_RERANKER", "false").lower() == "true"),
            colbert_model=s.get("colbert_model", os.getenv("COLBERT_MODEL", "colbert-ir/colbertv2.0")),
            colbert_top_k=s.get("colbert_top_k", int(os.getenv("COLBERT_TOP_K", "30"))),
        )
        w = _web_search_from_config()
        include = w.get("include_domains") or []
        exclude = w.get("exclude_domains") or []
        if isinstance(include, str):
            include = [x.strip() for x in include.split(",") if x.strip()]
        if isinstance(exclude, str):
            exclude = [x.strip() for x in exclude.split(",") if x.strip()]
        self.web_search = WebSearchConfig(
            enabled=w.get("enabled", True),
            provider=(w.get("provider") or "tavily").strip(),
            api_key=(w.get("api_key") or "").strip(),
            search_depth=(w.get("search_depth") or "advanced").strip(),
            max_results=min(int(w.get("max_results", 5)), 10),
            include_answer=w.get("include_answer", True),
            include_domains=include,
            exclude_domains=exclude,
            enable_query_optimizer=bool(w.get("enable_query_optimizer", True)),
            enable_query_expansion=w.get("enable_query_expansion", False),
            query_expansion_llm=(w.get("query_expansion_llm") or "deepseek").strip(),
            max_queries=min(int(w.get("max_queries", 4)), 8),
        )
        g = _google_search_from_config()
        self.google_search = GoogleSearchConfig(
            enabled=g.get("enabled", True),
            scholar_enabled=g.get("scholar_enabled", True),
            google_enabled=g.get("google_enabled", False),
            extension_path=(g.get("extension_path") or "extra_tools/CapSolverExtension").strip(),
            headless=g.get("headless"),
            proxy=g.get("proxy"),
            timeout=int(g.get("timeout", 60000)),
            max_results=min(int(g.get("max_results", 5)), 20),
            user_data_dir=g.get("user_data_dir"),
        )
        ss = _semantic_scholar_from_config()
        self.semantic_scholar = SemanticScholarConfig(
            enabled=bool(ss.get("enabled", False)),
            api_key=(ss.get("api_key") or "").strip(),
            base_url=(ss.get("base_url") or "https://ai4scholar.net/graph/v1").strip(),
            max_results=min(int(ss.get("max_results", 5)), 20),
            timeout_seconds=int(ss.get("timeout_seconds", 30)),
        )
        nc = _ncbi_from_config()
        self.ncbi = NCBIConfig(
            enabled=bool(nc.get("enabled", True)),
            api_key=(nc.get("api_key") or "").strip(),
            max_results=min(int(nc.get("max_results", 5)), 50),
            timeout_seconds=int(nc.get("timeout_seconds", 20)),
            cache_ttl_seconds=int(nc.get("cache_ttl_seconds", 3600)),
            cache_maxsize=int(nc.get("cache_maxsize", 256)),
        )
        cf = _content_fetcher_from_config()
        self.content_fetcher = ContentFetcherConfig(
            enabled=bool(cf.get("enabled", False)),
            only_academic=bool(cf.get("only_academic", False)),
            max_content_length=int(cf.get("max_content_length", 8000)),
            timeout_seconds=int(cf.get("timeout_seconds", 15)),
            brightdata_api_key=(cf.get("brightdata_api_key") or "").strip(),
            brightdata_zone=(cf.get("brightdata_zone") or "").strip(),
            cache_enabled=bool(cf.get("cache_enabled", True)),
            cache_ttl_seconds=int(cf.get("cache_ttl_seconds", 3600)),
            max_concurrent=int(cf.get("max_concurrent", 5)),
        )
        a = _api_from_config()
        self.api = ApiSettings(
            host=str(a.get("host", os.getenv("API_HOST", "127.0.0.1"))),
            port=int(a.get("port", os.getenv("API_PORT", "9999"))),
        )
        ct = _citation_from_config()
        self.citation = CitationSettings(
            key_format=str(ct.get("key_format", os.getenv("CITATION_KEY_FORMAT", "author_date"))),
            hash_length=int(ct.get("hash_length", 12)),
            author_date_max_authors=int(ct.get("author_date_max_authors", 2)),
            merge_level=str(ct.get("merge_level", "document")),
        )
        self.llm = LLMSettings()
        self.path = PathSettings()
        pf = _performance_from_config()
        rp = pf.get("retrieval") or {}
        lp = pf.get("llm") or {}
        wp = pf.get("web_search") or {}
        up = pf.get("unified_web_search") or {}
        gp = pf.get("google_search") or {}
        self.perf_retrieval = RetrievalPerfSettings(
            timeout_seconds=int(rp.get("timeout_seconds", 60)),
            cache_enabled=bool(rp.get("cache_enabled", False)),
            cache_ttl_seconds=int(rp.get("cache_ttl_seconds", 3600)),
            parallel_dense_sparse=bool(rp.get("parallel_dense_sparse", True)),
            max_workers=int(rp.get("max_workers", 4)),
        )
        self.perf_llm = LLMPerfSettings(
            timeout_seconds=int(lp.get("timeout_seconds", 120)),
            max_retries=int(lp.get("max_retries", 2)),
            retry_backoff=float(lp.get("retry_backoff", 1.5)),
            cache_enabled=bool(lp.get("cache_enabled", False)),
            cache_ttl_seconds=int(lp.get("cache_ttl_seconds", 3600)),
            max_concurrent_per_provider=int(lp.get("max_concurrent_per_provider", 5)),
        )
        self.perf_web_search = WebSearchPerfSettings(
            timeout_seconds=int(wp.get("timeout_seconds", 30)),
            cache_enabled=bool(wp.get("cache_enabled", False)),
            cache_ttl_seconds=int(wp.get("cache_ttl_seconds", 3600)),
        )
        browser_max = min(2, int(up.get("browser_providers_max_parallel", 1)))  # 风控建议不超过 2
        self.perf_unified_web = UnifiedWebSearchPerfSettings(
            max_parallel_providers=int(up.get("max_parallel_providers", 3)),
            per_provider_timeout_seconds=int(up.get("per_provider_timeout_seconds", 30)),
            browser_providers_max_parallel=max(1, browser_max),
        )
        self.perf_google_search = GoogleSearchPerfSettings(
            browser_reuse=bool(gp.get("browser_reuse", True)),
            max_idle_seconds=int(gp.get("max_idle_seconds", 300)),
            max_pages_per_browser=int(gp.get("max_pages_per_browser", 10)),
            cache_enabled=bool(gp.get("cache_enabled", False)),
            cache_ttl_seconds=int(gp.get("cache_ttl_seconds", 1800)),
        )
        st = _storage_from_config()
        self.storage = StorageSettings(
            max_age_days=int(st.get("max_age_days", 30)),
            max_size_gb=float(st.get("max_size_gb", 5.0)),
            cleanup_on_startup=bool(st.get("cleanup_on_startup", True)),
            cleanup_batch_size=int(st.get("cleanup_batch_size", 100)),
        )
        au = _auth_from_config()
        self.auth = AuthSettings(
            secret_key=str(au.get("secret_key", "change-me-in-local")),
            token_expire_hours=float(au.get("token_expire_hours", 24)),
            admin_username=str(au.get("admin_username", "admin")),
            admin_default_password=str(au.get("admin_default_password", "admin123")),
        )

    @property
    def is_prod(self) -> bool:
        return self.env == "prod"

    def print_info(self):
        print(f"""
========================================
  深海科研知识库 RAG 系统
========================================
  环境: {self.env}
  Milvus: {self.milvus.uri}
  设备: {self.model.device}
  FP16: {self.model.use_fp16}
  索引: {self.index.index_type}
  Collections: {', '.join(self.collection.all())}
========================================
        """)


# 全局单例
settings = Settings()
</file>

<file path="docs/api_reference.md">
# API 参考

本文档基于 `src/api/routes_*.py` 与 `src/api/server.py` 的实际路由整理。

更新时间：2026-02-19

## 基础信息

- 默认服务地址：`http://127.0.0.1:9999`
- Swagger：`/docs`
- OpenAPI：`/openapi.json`

## 全局与健康

| 方法 | 路径 | 说明 |
|---|---|---|
| `GET` | `/health` | 基础健康检查 |
| `GET` | `/health/detailed` | 组件级健康状态（检索/LLM/图谱等） |
| `GET` | `/storage/stats` | 存储统计 |
| `GET` | `/metrics` | Prometheus 指标导出 |

## 认证与用户（`/auth`、`/admin`）

| 方法 | 路径 | 说明 |
|---|---|---|
| `POST` | `/auth/login` | 用户登录，返回 token |
| `POST` | `/admin/users` | 管理员创建用户（需 admin token） |
| `GET` | `/admin/users` | 管理员查看用户列表（需 admin token） |

### 鉴权说明

- 需要鉴权的接口使用 Header：`Authorization: Bearer <token>`
- 普通用户与管理员权限由路由层依赖项判断

## 项目管理（`/projects`）

| 方法 | 路径 | 说明 |
|---|---|---|
| `GET` | `/projects` | 项目列表（可选 `include_archived`） |
| `POST` | `/projects/{canvas_id}/archive` | 存档 |
| `POST` | `/projects/{canvas_id}/unarchive` | 取消存档 |
| `DELETE` | `/projects/{canvas_id}` | 删除项目 |

## 聊天与会话

| 方法 | 路径 | 说明 |
|---|---|---|
| `POST` | `/chat` | 同步对话 |
| `POST` | `/chat/stream` | SSE 流式对话 |
| `POST` | `/intent/detect` | 意图检测（Chat vs Deep Research） |
| `GET` | `/sessions` | 会话列表 |
| `GET` | `/sessions/{session_id}` | 会话详情 |
| `DELETE` | `/sessions/{session_id}` | 删除会话 |

### `chat/stream` SSE 事件

- `meta`：元信息（session_id 等）
- `dashboard`：仪表盘数据
- `tool_trace`：工具调用轨迹
- `delta`：流式文本增量
- `done`：完成

## Deep Research

### 推荐后台任务模式

| 方法 | 路径 | 说明 |
|---|---|---|
| `POST` | `/deep-research/clarify` | 生成澄清问题（1-6 个关键问题） |
| `POST` | `/deep-research/context-files` | 上传临时上下文文件（pdf/md/txt），仅本次使用 |
| `POST` | `/deep-research/start` | Phase-1（Scope + Plan），返回可编辑 brief + outline |
| `POST` | `/deep-research/submit` | Phase-2 后台提交，返回 `job_id`（推荐） |
| `GET` | `/deep-research/jobs` | 列出后台任务（支持 `limit`、`status` 筛选） |
| `GET` | `/deep-research/jobs/{job_id}` | 查询单个任务状态与结果 |
| `GET` | `/deep-research/jobs/{job_id}/events` | 增量事件流（支持 `after_id`） |
| `POST` | `/deep-research/jobs/{job_id}/cancel` | 停止任务（前端关闭不会自动终止） |
| `POST` | `/deep-research/jobs/{job_id}/review` | 提交章节审核（approve / revise） |
| `GET` | `/deep-research/jobs/{job_id}/reviews` | 查看章节审核记录 |
| `POST` | `/deep-research/jobs/{job_id}/gap-supplement` | 提交章节缺口补充 |
| `GET` | `/deep-research/jobs/{job_id}/gap-supplements` | 查看补充记录（pending / consumed） |
| `GET` | `/deep-research/jobs/{job_id}/insights` | 查看研究洞察 |
| `POST` | `/deep-research/jobs/{job_id}/insights/{insight_id}/status` | 更新洞察状态 |

### Resume Queue 运维

| 方法 | 路径 | 说明 |
|---|---|---|
| `GET` | `/deep-research/resume-queue` | 查看 resume 队列（支持 status/owner/job 过滤） |
| `POST` | `/deep-research/resume-queue/cleanup` | 清理终态记录 |
| `POST` | `/deep-research/resume-queue/{resume_id}/retry` | 重试指定 resume 请求 |

### 兼容流式模式

| 方法 | 路径 | 说明 |
|---|---|---|
| `POST` | `/deep-research/confirm` | SSE 直连执行（旧模式，仍可用） |

### Phase-2 关键请求字段

`POST /deep-research/submit` 与 `/deep-research/confirm` 共用 `DeepResearchConfirmRequest`：

**基础字段：**
- `topic`、`session_id`、`canvas_id`、`search_mode`
- `confirmed_outline`、`confirmed_brief`
- `output_language`、`step_models`、`step_model_strict`
- `skip_draft_review`、`skip_refine_review`
- 检索参数（`web_providers`、`local_top_k`、`final_top_k` 等）

**研究深度（`depth`）：**
- `lite`：快速探索（~5-15 min）
- `comprehensive`：全面学术综述（~20-60 min，默认）

**人工介入字段：**
- `user_context`：用户补充观点/约束文本
- `user_context_mode`：`supporting`（补充上下文）| `direct_injection`（强提示直接注入）
- `user_documents`：`[{name, content}]` 临时材料（来自 `/deep-research/context-files`）

**`step_model_strict` 行为：**
- `false`（默认）：步骤模型解析失败时自动回退默认模型
- `true`：步骤模型解析失败时立即终止任务

### 人工审核字段

`POST /deep-research/jobs/{job_id}/review`：

```json
{
  "section_id": "string",
  "action": "approve | revise",
  "feedback": "optional string"
}
```

### 缺口补充字段

`POST /deep-research/jobs/{job_id}/gap-supplement`：

```json
{
  "section_id": "string",
  "gap_text": "string",
  "supplement_type": "material | direct_info",
  "content": {"text": "用户补充内容"}
}
```

### Resume Queue 运维字段

`POST /deep-research/resume-queue/cleanup`：

```json
{
  "statuses": ["done", "error", "cancelled"],
  "before_hours": 72,
  "owner_instance": "optional",
  "job_id": "optional"
}
```

`POST /deep-research/resume-queue/{resume_id}/retry`：

```json
{
  "message": "optional retry note"
}
```

### Deep Research 事件类型

`GET /deep-research/jobs/{job_id}/events` 返回的事件：

| 事件 | 说明 |
|---|---|
| `start` | 任务启动 |
| `progress` | 阶段进展（含 section/type/message） |
| `warning` | 风险或覆盖不足提醒 |
| `section_review` | 用户提交章节审核 |
| `gap_supplement` | 用户提交缺口补充 |
| `cancel_requested` | 收到停止请求 |
| `done` | 任务完成 |
| `cancelled` | 任务取消完成 |
| `error` | 任务失败 |

**`progress` 子类型（`type` 字段）：**

| type | 说明 |
|---|---|
| `evidence_insufficient` | 章节证据不足 |
| `section_degraded` | 章节降级写作 |
| `search_self_correction` | 补缺阶段自校正 |
| `coverage_plateau_early_stop` | 覆盖收益趋平，提前停止 |
| `section_evaluate_done` | 章节评估结果（含 coverage/gain/round/steps） |
| `write_verification_context` | 写作阶段二次取证 |
| `all_reviews_approved` | 所有章节审核通过 |
| `global_refine_done` | 全文连贯性整合完成 |
| `citation_guard_fallback` | 引用保护回退 |
| `step_model_resolved` | 步骤模型解析成功 |
| `step_model_fallback` | 步骤模型回退默认 |
| `cost_monitor_tick` | 成本监控心跳 |
| `cost_monitor_warn` | 成本预警 |
| `cost_monitor_force_summary` | 强制摘要模式 |

### Research Depth Presets

#### Iteration & Coverage

| 参数 | lite | comprehensive | 说明 |
|---|---|---|---|
| `max_iterations_per_section` | 3 | 6 | 每章迭代预算 |
| `max_section_research_rounds` | 3 | 5 | 每章最大研究轮次 |
| `coverage_threshold` | 0.60 | 0.80 | 覆盖度达标阈值 |

#### Query Strategy

| 参数 | lite | comprehensive | 说明 |
|---|---|---|---|
| `recall_queries_per_section` | 2 | 4 | 广撒网查询 |
| `precision_queries_per_section` | 2 | 4 | 定向深钻查询 |

#### Tiered search_top_k

| 参数 | lite | comprehensive | 说明 |
|---|---|---|---|
| `search_top_k_first` | 18 | 30 | 首轮广撒网 |
| `search_top_k_gap` | 10 | 15 | 补缺定点搜索 |
| `search_top_k_write` | 8 | 10 | 写作前精选 |

#### 3-tier Verification

| 参数 | lite | comprehensive | 说明 |
|---|---|---|---|
| `verify_light_threshold` | 0.20 | 0.15 | 轻微：仅标记 |
| `verify_medium_threshold` | 0.40 | 0.30 | 中等：记录 gaps |
| `verify_severe_threshold` | 0.45 | 0.35 | 严重：回到 research |

#### Review Gate

| 参数 | lite | comprehensive | 说明 |
|---|---|---|---|
| `review_gate_max_rounds` | 80 | 200 | 最大轮询轮数 |
| `review_gate_base_sleep` | 2s | 2s | 初始等待 |
| `review_gate_max_sleep` | 15s | 20s | 单轮等待上限 |
| `review_gate_early_stop_unchanged` | 8 | 12 | 无变化 N 轮后放行 |

#### LangGraph & Cost

| 参数 | lite | comprehensive | 说明 |
|---|---|---|---|
| `recursion_limit` | 200 | 500 | 递归上限 |
| `cost_warn_steps` | 120 | 300 | 成本预警步数 |
| `cost_force_summary_steps` | 180 | 420 | 强制摘要步数 |

> 阈值可在 `config/rag_config.json` → `deep_research.depth_presets` 自定义覆盖。

## Canvas（`/canvas`）

| 方法 | 路径 | 说明 |
|---|---|---|
| `POST` | `/canvas` | 创建画布 |
| `GET` | `/canvas/{canvas_id}` | 获取画布 |
| `PATCH` | `/canvas/{canvas_id}` | 更新画布基础字段 |
| `DELETE` | `/canvas/{canvas_id}` | 删除画布 |
| `POST` | `/canvas/{canvas_id}/outline` | 更新大纲 |
| `POST` | `/canvas/{canvas_id}/drafts` | 更新草稿 |
| `POST` | `/canvas/{canvas_id}/snapshot` | 创建快照 |
| `POST` | `/canvas/{canvas_id}/restore/{version_number}` | 恢复快照 |
| `GET` | `/canvas/{canvas_id}/snapshots` | 获取快照列表 |
| `GET` | `/canvas/{canvas_id}/export` | 按画布导出 |
| `POST` | `/canvas/{canvas_id}/refine-full` | 全文精炼 |
| `GET` | `/canvas/{canvas_id}/citations` | 引用列表 |
| `POST` | `/canvas/{canvas_id}/citations/filter` | 引用过滤 |
| `DELETE` | `/canvas/{canvas_id}/citations/{cite_key}` | 删除引用 |
| `POST` | `/canvas/{canvas_id}/ai-edit` | AI 段落编辑 |

## 导出（`/export`）

| 方法 | 路径 | 说明 |
|---|---|---|
| `POST` | `/export` | 导出入口（支持 `canvas_id` 或 `session_id`，当前仅 Markdown） |

## Auto Complete

| 方法 | 路径 | 说明 |
|---|---|---|
| `POST` | `/auto-complete` | 一键综述 |

## Compare 多文档对比（`/compare`）

| 方法 | 路径 | 说明 |
|---|---|---|
| `POST` | `/compare` | 对比 2-5 篇论文 |
| `GET` | `/compare/candidates` | 会话引文候选 |
| `GET` | `/compare/papers` | 本地文库分页/搜索 |

## Graph 图谱（`/graph`）

| 方法 | 路径 | 说明 |
|---|---|---|
| `GET` | `/graph/stats` | 图谱统计 |
| `GET` | `/graph/entities` | 实体查询 |
| `GET` | `/graph/neighbors/{entity_name}` | 实体邻居 |
| `GET` | `/graph/chunk/{chunk_id}` | chunk 详情 |
| `GET` | `/graph/pdf/{paper_id}` | PDF 原文访问 |

## Ingest 在线入库（`/ingest`）

| 方法 | 路径 | 说明 |
|---|---|---|
| `GET` | `/ingest/collections` | 集合列表 |
| `POST` | `/ingest/collections` | 创建集合 |
| `DELETE` | `/ingest/collections/{name}` | 删除集合 |
| `GET` | `/ingest/collections/{name}/papers` | 集合内论文列表 |
| `DELETE` | `/ingest/collections/{name}/papers/{paper_id}` | 删除论文 |
| `POST` | `/ingest/upload` | 上传文件 |
| `POST` | `/ingest/process` | 触发处理 |
| `GET` | `/ingest/jobs` | 任务列表 |
| `GET` | `/ingest/jobs/{job_id}` | 任务详情 |
| `POST` | `/ingest/jobs/{job_id}/cancel` | 取消任务 |
| `GET` | `/ingest/jobs/{job_id}/events` | 任务事件流 |

## Models 模型管理

| 方法 | 路径 | 说明 |
|---|---|---|
| `GET` | `/models/status` | 模型加载状态 |
| `POST` | `/models/sync` | 同步模型 |
| `GET` | `/llm/providers` | 可用 LLM provider 列表 |
</file>

<file path="docs/operations_and_troubleshooting.md">
# 运维与排障

本文档面向日常运行维护，覆盖启动、监控、常见故障和恢复建议。

发布与迁移全流程请参考：`release_migration_ubuntu.md`。

更新时间：2026-02-19

## 一、启动与运行

### 本地全栈

```bash
bash scripts/start.sh
```

可选：

- `bash scripts/start.sh --backend-only`
- `bash scripts/start.sh --frontend-only`
- `API_PORT=8000 bash scripts/start.sh`

### 仅 API

```bash
python scripts/08_run_api.py --reload
```

### Ubuntu 生产托管（systemd）

生产环境建议使用 `systemd` 托管后端与前端服务。
若使用 Nginx 托管前端静态资源，前端 preview service 可停用，仅保留后端 API service。
完整 Nginx + HTTPS 模板见 `release_migration_ubuntu.md`。

后端示例：`/etc/systemd/system/deepsea-rag-api.service`

```ini
[Unit]
Description=DeepSea RAG FastAPI Service
After=network.target docker.service
Wants=docker.service

[Service]
Type=simple
User=YOUR_USER
Group=YOUR_USER
WorkingDirectory=/path/to/RAG
Environment=PYTHONUNBUFFERED=1
Environment=API_HOST=0.0.0.0
Environment=API_PORT=9999
EnvironmentFile=-/path/to/RAG/.env
Environment=CONDA_EXE=conda
ExecStart=/bin/bash -lc '$CONDA_EXE run -n deepsea-rag python scripts/08_run_api.py --host 0.0.0.0 --port 9999'
Restart=always
RestartSec=5
TimeoutStopSec=20

[Install]
WantedBy=multi-user.target
```

前端示例：`/etc/systemd/system/deepsea-rag-frontend.service`

```ini
[Unit]
Description=DeepSea RAG Frontend Preview
After=network.target

[Service]
Type=simple
User=YOUR_USER
Group=YOUR_USER
WorkingDirectory=/path/to/RAG/frontend
Environment=NODE_ENV=production
Environment=CONDA_EXE=conda
ExecStart=/bin/bash -lc '$CONDA_EXE run -n deepsea-rag npm run preview -- --host 0.0.0.0 --port 5173'
Restart=always
RestartSec=5
TimeoutStopSec=20

[Install]
WantedBy=multi-user.target
```

启用命令：

```bash
sudo systemctl daemon-reload
sudo systemctl enable deepsea-rag-api deepsea-rag-frontend
sudo systemctl start deepsea-rag-api deepsea-rag-frontend
```

查看日志：

```bash
sudo journalctl -u deepsea-rag-api -f
sudo journalctl -u deepsea-rag-frontend -f
```

> 如果 `conda` 在 systemd 环境不可见，设置 `CONDA_EXE` 为实际路径（如 `/opt/anaconda3/bin/conda`）。

## 二、运行时检查

### 基础健康

| 端点 | 说明 |
|---|---|
| `GET /health` | 服务可用性 |
| `GET /health/detailed` | 组件状态（检索/LLM/图谱等） |
| `GET /metrics` | Prometheus 指标 |
| `GET /storage/stats` | 存储情况 |

### Deep Research 任务检查

| 端点 | 说明 |
|---|---|
| `GET /deep-research/jobs/{job_id}` | 任务状态（pending/running/cancelling/done/error/cancelled） |
| `GET /deep-research/jobs/{job_id}/events?after_id=...` | 增量事件流（排障主入口） |
| `GET /deep-research/jobs/{job_id}/reviews` | 章节审核状态 |
| `GET /deep-research/jobs/{job_id}/gap-supplements` | 缺口补充状态（pending/consumed） |
| `GET /deep-research/jobs/{job_id}/insights` | 研究洞察 |

### Resume Queue 运维检查

| 端点 | 说明 |
|---|---|
| `GET /deep-research/resume-queue` | 查看 resume 队列 |
| `POST /deep-research/resume-queue/cleanup` | 清理历史终态记录 |
| `POST /deep-research/resume-queue/{resume_id}/retry` | 重试指定请求 |

建议先看：

- `status=running`：是否存在长时间未更新的恢复请求
- `status=error`：是否出现异常积压
- `owner_instance`：请求是否绑定到预期实例

### Ingest 任务检查

| 端点 | 说明 |
|---|---|
| `GET /ingest/jobs/{job_id}` | 入库任务状态 |
| `GET /ingest/jobs/{job_id}/events` | 入库事件流 |
| `POST /ingest/jobs/{job_id}/cancel` | 取消入库 |

## 三、Deep Research Resume Queue SOP

### 场景 A：审核通过后任务未继续执行

1. 查看任务状态与事件：
   - `GET /deep-research/jobs/{job_id}`
   - `GET /deep-research/jobs/{job_id}/events`
2. 查看 resume 队列（按 job）：
   - `GET /deep-research/resume-queue?job_id={job_id}&limit=20`
3. 按结果处理：
   - 无队列项：前端重新提交 review
   - `pending` 长时间不变：检查 worker 是否存活
   - `running` 长时间不变：重启服务（会触发 stale 清理），再判断是否 retry
   - `error/cancelled`：走"场景 C 手动重试"

### 场景 B：批量清理历史记录

```bash
curl -X POST "http://127.0.0.1:9999/deep-research/resume-queue/cleanup" \
  -H "Content-Type: application/json" \
  -d '{"statuses": ["done", "error", "cancelled"], "before_hours": 72}'
```

> 不建议清理 `pending/running`，除非确认是僵尸数据。

### 场景 C：手动重试

前提：队列项当前为终态，同一 `job_id` 没有活跃项。

```bash
curl -X POST "http://127.0.0.1:9999/deep-research/resume-queue/{resume_id}/retry" \
  -H "Content-Type: application/json" \
  -d '{"message": "manual retry by oncall"}'
```

### 场景 D：实例重启后恢复

当前策略：

- 服务启动会将中断态任务重置为 `error`
- resume_queue 中 `running` 请求会重置为 `error`

值班建议：

1. 重启后巡检：`GET /deep-research/resume-queue?status=error&limit=100`
2. 需要继续的任务：按"场景 C"逐条 retry
3. 无需继续的：按"场景 B"批量清理

### 快速排障决策树

```text
任务没继续？
├── 看 job events 有无 section_review / resume_start
└── 看 resume_queue：
    ├── 没记录 → 重新提交 review
    ├── pending → 看 worker/实例绑定
    ├── running 卡住 → 重启并重试
    └── error → 直接 retry
```

## 四、日志与产物

| 路径 | 内容 |
|---|---|
| `logs/` | 运行日志（按模块拆分） |
| `logs/llm_raw/` | LLM 原始响应（JSONL，含请求/响应/耗时/token） |
| `artifacts/` | 评测/任务产物 |

## 五、常见问题与处理

### 1) Milvus 连接失败

排查顺序：

1. `bash scripts/00_healthcheck_docker.sh`
2. 检查 `MILVUS_HOST`、`MILVUS_PORT` 或 `config/rag_config.json`
3. 重启容器：`docker compose --profile dev up -d`

### 2) 检索结果为空

排查顺序：

1. 确认已执行 `02_parse_papers.py` 与 `03_index_papers.py`
2. 检查目标 collection 是否存在（`GET /ingest/collections`）
3. 用 `scripts/04_test_search.py` 单独验证检索链路

### 3) LLM 调用失败 / 401

排查：

- 检查 `RAG_LLM__{PROVIDER}__API_KEY`
- 检查 `GET /llm/providers` 返回
- 确认 `rag_config.local.json` 是否覆盖了错误值

### 4) Google / Scholar 不稳定

建议：

- 降低 `performance.unified_web_search.browser_providers_max_parallel`
- 安装并检查 Playwright：`playwright install chromium`
- 必要时切换 headless 策略或代理

### 5) Ingest 任务卡住

排查：

- `GET /ingest/jobs/{job_id}`
- `GET /ingest/jobs/{job_id}/events`
- 必要时 `POST /ingest/jobs/{job_id}/cancel`

### 6) Deep Research 全部通过后未进入最终整合

排查顺序：

1. 检查审核记录是否覆盖全部章节：`GET /deep-research/jobs/{job_id}/reviews`
2. 检查事件是否出现 `progress(type=all_reviews_approved)`
   - 若没有，通常是章节名不一致或仍有 `pending_sections`
3. 检查是否出现 `review_gate_timeout` / `review_gate_early_stop`
4. 确认 `job.status=done` 且 Canvas stage 已切换 `refine`

### 7) 最终整合后引用异常

系统已内置 citation guard：

- 正常：`progress(type=global_refine_done)`
- 保护回退：`progress(type=citation_guard_fallback)`

若频繁触发 `citation_guard_fallback`：

- 降低 `synthesize` 步骤模型温度/改用更稳模型
- 缩短单次整合输入规模
- 检查原始草稿中的引用 key 规范性

### 8) 成本预警

系统内置成本监控机制：

- `cost_monitor_tick`：每 N 步上报
- `cost_monitor_warn`：达到预警阈值
- `cost_monitor_force_summary`：强制进入摘要模式

处理建议：降低 `depth` 或缩小研究范围。

## 六、存储维护

自动清理配置：

- `storage.max_age_days`
- `storage.max_size_gb`
- `storage.cleanup_on_startup`

手动清理：

```bash
python scripts/19_cleanup_storage.py --vacuum
```

## 七、数据安全建议

- 生产环境关闭默认管理员密码（`auth.secret_key` 必须替换）
- 定期备份：
  - `data/parsed/`
  - `src/data/sessions.db`
  - `src/data/deep_research_jobs.db`
  - 关键配置文件（脱敏后）
- 变更前先导出核心画布内容（`POST /export`）
- 敏感配置不要提交到 Git（已在 `.gitignore` 中配置）
</file>

<file path="frontend/src/api/canvas.ts">
import client from './client';
import type { Canvas, Citation, OutlineSection, DraftBlock, Annotation } from '../types';

export async function createCanvas(data: {
  session_id?: string;
  topic?: string;
}): Promise<Canvas> {
  const res = await client.post<Canvas>('/canvas', data);
  return res.data;
}

export async function getCanvas(canvasId: string): Promise<Canvas> {
  const res = await client.get<Canvas>(`/canvas/${canvasId}`);
  return res.data;
}

/**
 * 更新 Canvas 基础字段（不包括 outline 和 drafts）。
 * 若需更新 outline，请使用 upsertOutline()；
 * 若需更新 drafts，请使用 upsertDraft()。
 */
export async function updateCanvas(
  canvasId: string,
  data: Partial<{
    session_id: string;
    topic: string;
    working_title: string;
    abstract: string;
    keywords: string[];
    stage: string;
    refined_markdown: string;
  }>
): Promise<Canvas> {
  const res = await client.patch<Canvas>(`/canvas/${canvasId}`, data);
  return res.data;
}

export async function deleteCanvas(canvasId: string): Promise<void> {
  await client.delete(`/canvas/${canvasId}`);
}

export interface ExportResponse {
  format: string;
  content: string;
  session_id: string;
  canvas_id: string;
}

export async function exportCanvas(
  canvasId: string,
  format: 'json' | 'markdown' = 'json'
): Promise<ExportResponse> {
  const res = await client.get<ExportResponse>(`/canvas/${canvasId}/export`, {
    params: { format },
  });
  return res.data;
}

export async function exportCanvasDocx(canvasId: string): Promise<Blob> {
  const res = await client.post<Blob>(
    '/export',
    {
      canvas_id: canvasId,
      format: 'docx',
    },
    {
      responseType: 'blob',
    }
  );
  return res.data;
}

export type CitationFormat = 'bibtex' | 'text' | 'ris' | 'both';

// 根据 format 参数返回不同结构
export type CitationsResult<T extends CitationFormat> = T extends 'bibtex'
  ? { format: 'bibtex'; content: string }
  : T extends 'text'
    ? { format: 'text'; content: string }
    : T extends 'ris'
      ? { format: 'ris'; content: string }
    : { format: 'both'; bibtex: string; reference_list: string; citations: Citation[] };

/**
 * 获取画布的引用列表。
 * @param format - bibtex: 返回 BibTeX 字符串; text: 返回文本参考列表; both: 返回两者及结构化数据
 */
export async function getCanvasCitations<T extends CitationFormat = 'both'>(
  canvasId: string,
  format?: T
): Promise<CitationsResult<T>> {
  const res = await client.get(`/canvas/${canvasId}/citations`, {
    params: { format: format ?? 'both' },
  });
  return res.data;
}

export async function createSnapshot(
  canvasId: string
): Promise<{ version_number: number }> {
  const res = await client.post(`/canvas/${canvasId}/snapshot`);
  return res.data;
}

export async function restoreSnapshot(
  canvasId: string,
  versionNumber: number
): Promise<void> {
  await client.post(`/canvas/${canvasId}/restore/${versionNumber}`);
}

export interface CanvasVersionItem {
  version_number: number;
  created_at: string;
}

export async function listCanvasSnapshots(
  canvasId: string,
  limit = 50
): Promise<CanvasVersionItem[]> {
  const res = await client.get<CanvasVersionItem[]>(`/canvas/${canvasId}/snapshots`, {
    params: { limit },
  });
  return res.data;
}

// ---- Outline & Drafts ----

/**
 * 批量更新/插入大纲章节。
 */
export async function upsertOutline(
  canvasId: string,
  sections: OutlineSection[]
): Promise<Canvas> {
  const res = await client.post<Canvas>(`/canvas/${canvasId}/outline`, { sections });
  return res.data;
}

/**
 * 更新/插入单个草稿块。
 */
export async function upsertDraft(
  canvasId: string,
  block: DraftBlock
): Promise<Canvas> {
  const res = await client.post<Canvas>(`/canvas/${canvasId}/drafts`, { block });
  return res.data;
}

// ---- Citation Management ----

export interface CitationFilterResponse {
  removed_count: number;
  remaining_keys: string[];
}

/**
 * 筛选引用池：保留或删除指定 cite_key。
 * keep_keys 和 remove_keys 二选一。
 */
export async function filterCitations(
  canvasId: string,
  options: { keep_keys?: string[]; remove_keys?: string[] }
): Promise<CitationFilterResponse> {
  const res = await client.post<CitationFilterResponse>(
    `/canvas/${canvasId}/citations/filter`,
    options
  );
  return res.data;
}

/**
 * 删除指定 cite_key 的引用。
 */
export async function deleteCitation(
  canvasId: string,
  citeKey: string
): Promise<{ ok: boolean; canvas_id: string; removed_cite_key: string }> {
  const res = await client.delete(`/canvas/${canvasId}/citations/${citeKey}`);
  return res.data;
}

// ---- AI Edit ----

export interface AIEditRequest {
  section_text: string;
  action: 'rewrite' | 'expand' | 'condense' | 'add_citations' | 'targeted_refine';
  context?: string;
  search_mode?: string;
  directive?: string;
  preserve_citations?: boolean;
}

export interface AIEditResponse {
  edited_text: string;
  citations_added: string[];
  citation_guard_triggered?: boolean;
  citation_guard_message?: string;
}

/**
 * AI 段落级编辑（重写/扩展/精简/添加引用）。
 */
export async function aiEditCanvas(
  canvasId: string,
  data: AIEditRequest
): Promise<AIEditResponse> {
  const res = await client.post<AIEditResponse>(
    `/canvas/${canvasId}/ai-edit`,
    data
  );
  return res.data;
}

export interface CanvasFullRefineRequest {
  content_md?: string;
  directives?: string[];
  save_snapshot_before?: boolean;
  locked_ranges?: Array<{ start: number; end: number; text: string }>;
}

export interface CanvasFullRefineResponse {
  edited_markdown: string;
  snapshot_version?: number | null;
  locked_applied?: number;
  locked_skipped?: number;
  lock_guard_triggered?: boolean;
  lock_guard_message?: string;
}

export async function refineCanvasFull(
  canvasId: string,
  data: CanvasFullRefineRequest
): Promise<CanvasFullRefineResponse> {
  const res = await client.post<CanvasFullRefineResponse>(
    `/canvas/${canvasId}/refine-full`,
    data,
    {
      // Full-document refine can take much longer than default 60s.
      timeout: 300000,
    }
  );
  return res.data;
}

// ---- Annotations ----

/**
 * 批量添加行内批注。
 */
export async function addAnnotations(
  canvasId: string,
  annotations: Annotation[]
): Promise<Canvas> {
  const res = await client.post<Canvas>(
    `/canvas/${canvasId}/annotations`,
    { annotations }
  );
  return res.data;
}

/**
 * 更新 Canvas 阶段跳过控制。
 */
export async function updateCanvasSkipSettings(
  canvasId: string,
  settings: { skip_draft_review?: boolean; skip_refine_review?: boolean }
): Promise<Canvas> {
  const res = await client.patch<Canvas>(`/canvas/${canvasId}`, settings);
  return res.data;
}
</file>

<file path="frontend/src/api/graph.ts">
import client from './client';

export interface GraphStats {
  available: boolean;
  total_nodes: number;
  total_edges: number;
  entity_count: number;
  chunk_count: number;
  entity_types: Record<string, number>;
}

export interface EntityItem {
  name: string;
  type: string;
  mention_count: number;
}

export interface GraphNode {
  id: string;
  type: string;
  paper_id?: string;
  is_center?: boolean;
}

export interface GraphEdge {
  source: string;
  target: string;
  relation: string;
  weight: number;
}

export interface NeighborGraph {
  center: string;
  depth: number;
  nodes: GraphNode[];
  edges: GraphEdge[];
}

export interface ChunkDetail {
  collection: string;
  chunk_id: string;
  paper_id: string;
  content: string;
  section_path?: string;
  page?: number | null;
  content_type?: string;
  chunk_type?: string;
  related_entities?: string[];
  bbox?: number[];
}

export async function getGraphStats(): Promise<GraphStats> {
  const res = await client.get<GraphStats>('/graph/stats');
  return res.data;
}

export async function getEntities(params?: {
  entity_type?: string;
  limit?: number;
  offset?: number;
  q?: string;
}): Promise<{ total: number; entities: EntityItem[] }> {
  const res = await client.get('/graph/entities', { params });
  return res.data;
}

export async function getNeighbors(entityName: string, depth = 1): Promise<NeighborGraph> {
  const res = await client.get<NeighborGraph>(
    `/graph/neighbors/${encodeURIComponent(entityName)}`,
    { params: { depth } },
  );
  return res.data;
}

export async function getChunkDetail(params: {
  chunk_id: string;
  collection?: string;
  paper_id?: string;
}): Promise<ChunkDetail> {
  const { chunk_id, collection, paper_id } = params;
  const res = await client.get<ChunkDetail>(`/graph/chunk/${encodeURIComponent(chunk_id)}`, {
    params: { collection, paper_id },
  });
  return res.data;
}
</file>

<file path="frontend/src/components/canvas/CanvasPanel.tsx">
import { useState } from 'react';
import { useTranslation } from 'react-i18next';
import {
  FileEdit,
  FileDown,
  FileType,
  X,
  Loader2,
  Plus,
  Telescope,
} from 'lucide-react';
import { useCanvasStore, useChatStore, useUIStore, useToastStore } from '../../stores';
import { createCanvas, exportCanvasDocx, getCanvasCitations } from '../../api/canvas';
import { StageStepper } from './StageStepper';
import { ExploreStage } from './ExploreStage';
import { OutlineStage } from './OutlineStage';
import { DraftingStage } from './DraftingStage';
import { RefineStage } from './RefineStage';
import type { CanvasStage } from '../../types';

interface CanvasPanelProps {
  onStartResize: () => void;
}

export function CanvasPanel({ onStartResize }: CanvasPanelProps) {
  const { t } = useTranslation();
  const {
    canvas,
    canvasContent,
    isLoading,
    isAIEditing,
    activeStage,
    setCanvas,
    setActiveStage,
  } = useCanvasStore();
  const { workflowStep, sessionId, setCanvasId, setShowDeepResearchDialog } = useChatStore();
  const { canvasWidth, setCanvasOpen } = useUIStore();
  const addToast = useToastStore((s) => s.addToast);
  const [isCreating, setIsCreating] = useState(false);

  const downloadBlob = (blob: Blob, filename: string) => {
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename;
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
  };

  const handleExport = async (format: 'md' | 'pdf' | 'docx' | 'ris') => {
    if (format === 'md') {
      if (!canvasContent) {
        addToast(t('canvas.emptyCannotExport'), 'error');
        return;
      }
      const blob = new Blob([canvasContent], { type: 'text/markdown' });
      downloadBlob(blob, 'research_draft.md');
      addToast(t('canvas.exportedMd'), 'success');
      return;
    }

    if (format === 'pdf') {
      addToast(t('canvas.pdfInDev'), 'info');
      return;
    }

    if (!canvas?.id) {
      addToast(t('canvas.emptyCannotExport'), 'error');
      return;
    }

    try {
      if (format === 'docx') {
        const blob = await exportCanvasDocx(canvas.id);
        downloadBlob(blob, 'research_draft.docx');
        addToast(t('canvas.exportedDocx'), 'success');
        return;
      }

      const ris = await getCanvasCitations(canvas.id, 'ris');
      if (!ris.content.trim()) {
        addToast(t('canvas.noCitationsToExport'), 'error');
        return;
      }
      const blob = new Blob([ris.content], { type: 'application/x-research-info-systems' });
      downloadBlob(blob, 'references.ris');
      addToast(t('canvas.exportedRis'), 'success');
    } catch (err) {
      console.error('[CanvasPanel] Export failed:', err);
      addToast(t('canvas.exportFailed'), 'error');
    }
  };

  // 创建空 Canvas（无需 Deep Research）
  const handleCreateEmptyCanvas = async () => {
    setIsCreating(true);
    try {
      const newCanvas = await createCanvas({
        session_id: sessionId || undefined,
        topic: '',
      });
      if (newCanvas) {
        setCanvas(newCanvas);
        setCanvasId(newCanvas.id);
        addToast(t('canvas.canvasCreated'), 'success');
      }
    } catch (err) {
      console.error('[CanvasPanel] Create canvas failed:', err);
      addToast(t('canvas.createFailed'), 'error');
    } finally {
      setIsCreating(false);
    }
  };

  // 当前 canvas 的真实阶段（用于 stepper 的"已完成"判断）
  const currentStage: CanvasStage = (canvas?.stage as CanvasStage) || 'explore';

  // 渲染对应阶段的内容
  const renderStageContent = () => {
    // 如果没有 canvas 数据，显示可操作的空状态
    if (!canvas) {
      return (
        <div className="flex flex-col items-center justify-center h-full text-slate-500 px-6">
          <div className="p-4 bg-slate-800/50 rounded-full mb-4 shadow-[0_0_20px_rgba(56,189,248,0.1)] border border-slate-700/50 animate-pulse-glow">
            <FileEdit size={36} className="text-sky-500/50" />
          </div>
          <p className="text-sm font-medium mb-1 text-slate-300">{t('canvas.emptyTitle')}</p>
          <p className="text-xs text-center mb-6 leading-relaxed text-slate-500 max-w-[240px]">
            {t('canvas.emptyDesc')}
          </p>
          <div className="flex flex-col gap-3 w-full max-w-48">
            <button
              onClick={() => setShowDeepResearchDialog(true)}
              className="flex items-center justify-center gap-2 px-4 py-2.5 bg-indigo-600/90 text-white text-xs font-medium rounded-lg hover:bg-indigo-500 transition-colors shadow-lg shadow-indigo-500/20 border border-indigo-400/30"
            >
              <Telescope size={14} />
              {t('canvas.startDeepResearch')}
            </button>
            <button
              onClick={handleCreateEmptyCanvas}
              disabled={isCreating}
              className="flex items-center justify-center gap-2 px-4 py-2.5 border border-slate-700 bg-slate-800/40 text-slate-400 text-xs font-medium rounded-lg hover:bg-slate-700 hover:text-slate-200 transition-colors disabled:opacity-50"
            >
              {isCreating ? <Loader2 size={14} className="animate-spin" /> : <Plus size={14} />}
              {t('canvas.createBlankCanvas')}
            </button>
          </div>
        </div>
      );
    }

    switch (activeStage) {
      case 'explore':
        return <ExploreStage canvas={canvas} />;
      case 'outline':
        return <OutlineStage canvas={canvas} />;
      case 'drafting':
        return <DraftingStage canvas={canvas} />;
      case 'refine':
        return <RefineStage canvas={canvas} />;
      default:
        return <ExploreStage canvas={canvas} />;
    }
  };

  return (
    <div
      className="bg-slate-900/95 backdrop-blur-md border-l border-slate-700/50 flex flex-col relative flex-shrink-0 z-40 shadow-[-5px_0_30px_rgba(0,0,0,0.3)] transition-[width] duration-100 ease-linear"
      style={{ width: canvasWidth }}
    >
      {/* Header */}
      <div className="h-12 border-b border-slate-700/50 flex items-center justify-between px-4 bg-slate-900/80 shadow-sm">
        <div className="flex items-center gap-2 font-bold text-sm text-slate-200 min-w-0">
          <FileEdit size={16} className="text-sky-500" />
          <span className="truncate">{t('canvas.researchCanvas')}</span>
          {(workflowStep === 'drafting' || isLoading || isAIEditing) && (
            <Loader2 size={12} className="animate-spin text-sky-400" />
          )}
        </div>
        <div className="flex items-center gap-1 shrink-0">
          <button
            onClick={() => void handleExport('md')}
            className="p-1.5 hover:bg-slate-800 rounded text-slate-400 hover:text-sky-400 transition-colors"
            title={t('canvas.exportMd')}
          >
            <FileDown size={14} />
          </button>
          <button
            onClick={() => void handleExport('docx')}
            className="p-1.5 hover:bg-slate-800 rounded text-slate-400 hover:text-sky-400 transition-colors"
            title={t('canvas.exportWord')}
          >
            <FileType size={14} />
          </button>
          <button
            onClick={() => void handleExport('ris')}
            className="p-1.5 hover:bg-slate-800 rounded text-slate-400 hover:text-sky-400 transition-colors"
            title={t('canvas.exportRis')}
          >
            <FileDown size={14} />
          </button>
          <button
            onClick={() => void handleExport('pdf')}
            className="p-1.5 hover:bg-slate-800 rounded text-slate-400 hover:text-sky-400 transition-colors"
            title={t('canvas.exportPdf')}
          >
            <FileType size={14} />
          </button>
          <button
            onClick={() => setCanvasOpen(false)}
            className="p-1.5 hover:bg-slate-800 rounded text-slate-400 hover:text-red-400 transition-colors"
          >
            <X size={14} />
          </button>
        </div>
      </div>

      {/* Stage Stepper — 只在有 canvas 时显示 */}
      {canvas && (
        <StageStepper
          currentStage={currentStage}
          activeStage={activeStage}
          onStageClick={(stage) => setActiveStage(stage)}
        />
      )}

      {/* Stage Content */}
      <div className="flex-1 overflow-hidden bg-slate-900/50">
        {renderStageContent()}
      </div>

      {/* Drag Handle */}
      <div
        onMouseDown={(e) => {
          e.preventDefault();
          onStartResize();
        }}
        className="absolute top-0 left-0 w-1 h-full cursor-col-resize hover:bg-sky-500/50 z-50 group transition-colors"
      >
        <div className="absolute top-1/2 -left-3 w-6 h-8 bg-slate-800 border border-slate-600 rounded shadow-sm flex items-center justify-center opacity-0 group-hover:opacity-100 transition-opacity pointer-events-none">
          <div className="text-slate-500 text-[10px]">&#x22ee;</div>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/chat/RetrievalDebugPanel.tsx">
import { useState } from 'react';
import { ChevronDown, ChevronRight, Database, Globe, Clock, BarChart3, Search, ShieldCheck } from 'lucide-react';
import type { EvidenceSummary } from '../../types';

interface Props {
  summary: EvidenceSummary;
}

const EVIDENCE_TYPE_LABELS: Record<string, string> = {
  finding: '实验发现',
  method: '方法描述',
  interpretation: '讨论解读',
  background: '背景信息',
  summary: '摘要概述',
};

export function RetrievalDebugPanel({ summary }: Props) {
  const [expanded, setExpanded] = useState(false);
  const diag = summary.diagnostics;

  const totalMs = Math.round(summary.retrieval_time_ms);

  return (
    <div className="border border-gray-200 rounded-lg bg-gray-50 text-xs mb-4 overflow-hidden">
      {/* Header - always visible */}
      <button
        onClick={() => setExpanded(!expanded)}
        className="w-full flex items-center justify-between px-3 py-2 hover:bg-gray-100 transition-colors cursor-pointer"
      >
        <div className="flex items-center gap-2 text-gray-600">
          <Search size={12} className="text-blue-500" />
          <span className="font-medium">检索诊断</span>
          <span className="text-gray-400">
            {summary.total_chunks} 条结果 · {totalMs}ms
          </span>
          {summary.sources_used.length > 0 && (
            <span className="text-gray-400">
              · {summary.sources_used.join('+')}
            </span>
          )}
          {diag?.cross_source_dedup && diag.cross_source_dedup.removed > 0 && (
            <span className="text-orange-500 flex items-center gap-0.5">
              · <ShieldCheck size={10} /> 去重 {diag.cross_source_dedup.removed}
            </span>
          )}
        </div>
        {expanded ? <ChevronDown size={14} className="text-gray-400" /> : <ChevronRight size={14} className="text-gray-400" />}
      </button>

      {/* Expanded content */}
      {expanded && (
        <div className="px-3 pb-3 pt-1 border-t border-gray-200 space-y-3">
          {/* Pipeline stages */}
          {diag?.stages && (
            <div>
              <div className="font-medium text-gray-500 flex items-center gap-1 mb-1.5">
                <BarChart3 size={11} /> 检索流水线
              </div>
              <div className="space-y-1">
                {Object.entries(diag.stages).map(([name, info]) => {
                  const maxMs = Math.max(
                    ...Object.values(diag.stages!).map((s) => s.time_ms),
                    1
                  );
                  const pct = Math.min((info.time_ms / maxMs) * 100, 100);
                  return (
                    <div key={name} className="flex items-center gap-2">
                      <span className="w-24 text-gray-500 truncate">{name}</span>
                      <div className="flex-1 bg-gray-200 rounded-full h-1.5 relative">
                        <div
                          className="absolute top-0 left-0 h-full bg-blue-400 rounded-full transition-all"
                          style={{ width: `${pct}%` }}
                        />
                      </div>
                      <span className="w-16 text-right text-gray-500">
                        {info.count}条 {Math.round(info.time_ms)}ms
                      </span>
                    </div>
                  );
                })}
              </div>
            </div>
          )}

          {/* Source breakdown */}
          {summary.source_breakdown && (
            <div>
              <div className="font-medium text-gray-500 flex items-center gap-1 mb-1.5">
                <Database size={11} /> 来源分布
              </div>
              <div className="flex gap-3">
                {Object.entries(summary.source_breakdown).map(([src, count]) => (
                  <span
                    key={src}
                    className={`px-2 py-0.5 rounded-full ${
                      src === 'local'
                        ? 'bg-blue-100 text-blue-700'
                        : 'bg-green-100 text-green-700'
                    }`}
                  >
                    {src === 'local' ? '本地' : '网络'} {count}
                  </span>
                ))}
                {summary.cross_validated_count ? (
                  <span className="px-2 py-0.5 rounded-full bg-purple-100 text-purple-700">
                    交叉验证 {summary.cross_validated_count}
                  </span>
                ) : null}
              </div>
            </div>
          )}

          {/* Cross-source dedup */}
          {diag?.cross_source_dedup && diag.cross_source_dedup.removed > 0 && (
            <div className="flex items-center gap-2 px-2 py-1.5 rounded bg-orange-50 border border-orange-200 text-orange-700">
              <ShieldCheck size={12} />
              <span>
                已通过本地指纹拦截 <strong>{diag.cross_source_dedup.removed}</strong> 条重复网络结果，保留 {diag.cross_source_dedup.remaining} 条增量信息
              </span>
            </div>
          )}

          {/* Web providers */}
          {diag?.web_providers && Object.keys(diag.web_providers).length > 0 && (
            <div>
              <div className="font-medium text-gray-500 flex items-center gap-1 mb-1.5">
                <Globe size={11} /> Web 来源
              </div>
              <div className="flex flex-wrap gap-2">
                {Object.entries(diag.web_providers).map(([prov, info]) => (
                  <span key={prov} className="px-2 py-0.5 rounded bg-gray-200 text-gray-600">
                    {prov}: {info.count}条 ({Math.round(info.time_ms)}ms)
                  </span>
                ))}
              </div>
              {diag.content_fetcher && (
                <div className="mt-1 text-gray-400">
                  全文抓取: {diag.content_fetcher.enriched}/{diag.content_fetcher.total}
                </div>
              )}
            </div>
          )}

          {/* Evidence types */}
          {summary.evidence_type_breakdown && (
            <div>
              <div className="font-medium text-gray-500 flex items-center gap-1 mb-1.5">
                <BarChart3 size={11} /> 证据类型
              </div>
              <div className="flex flex-wrap gap-2">
                {Object.entries(summary.evidence_type_breakdown).map(([etype, count]) => (
                  <span key={etype} className="px-2 py-0.5 rounded bg-gray-200 text-gray-600">
                    {EVIDENCE_TYPE_LABELS[etype] || etype} {count}
                  </span>
                ))}
              </div>
            </div>
          )}

          {/* Time range + documents */}
          {(summary.year_range || summary.total_documents) && (
            <div className="flex items-center gap-3 text-gray-400">
              {summary.year_range && summary.year_range[0] && (
                <span className="flex items-center gap-1">
                  <Clock size={10} />
                  {summary.year_range[0] === summary.year_range[1]
                    ? summary.year_range[0]
                    : `${summary.year_range[0]}–${summary.year_range[1]}`}
                </span>
              )}
              {summary.total_documents ? (
                <span>{summary.total_documents} 篇文献</span>
              ) : null}
            </div>
          )}
        </div>
      )}
    </div>
  );
}
</file>

<file path="frontend/src/i18n/locales/en.json">
{
  "common": {
    "cancel": "Cancel",
    "save": "Save",
    "confirm": "Confirm",
    "delete": "Delete",
    "create": "Create",
    "loading": "Loading...",
    "search": "Search",
    "export": "Export",
    "close": "Close",
    "retry": "Retry",
    "enabled": "Enabled",
    "disabled": "Disabled",
    "settings": "Settings",
    "status": "Status",
    "online": "Online",
    "offline": "Offline",
    "remove": "Remove",
    "back": "Back",
    "next": "Next",
    "skip": "Skip",
    "start": "Start",
    "stop": "Stop",
    "refresh": "Refresh"
  },
  "header": {
    "chat": "Chat",
    "ingest": "Ingest",
    "graph": "Knowledge Graph",
    "compare": "Doc Compare",
    "users": "Users",
    "newChat": "New Chat",
    "toggleHistory": "Toggle History",
    "systemOnline": "System: Online",
    "disconnected": "Disconnected (Click to Connect)",
    "connecting": "Connecting...",
    "connectService": "Connecting...",
    "serviceConnected": "Service connected!",
    "connectFailed": "Connection failed",
    "syncingStatus": "Syncing service status...",
    "syncOk": "Status synced",
    "disconnectedStatus": "Disconnected",
    "newChatCreated": "New conversation started",
    "workflow": "Workflow"
  },
  "sidebar": {
    "brand": "RAG Lab",
    "retrievalConfig": "Retrieval Config",
    "localKnowledge": "Local Knowledge Base",
    "localKnowledgeHelp": "Retrieve relevant chunks from ingested documents as conversation context. Disabling this will not use the local knowledge base.",
    "queryCollection": "Query Collection",
    "noCollection": "No collections available",
    "localTopK": "Local RAG Top-K",
    "localTopKTitle": "Enter larger Top-K (>60)",
    "hippoRAG": "HippoRAG Graph Retrieval",
    "hippoRAGHelp": "Knowledge graph-based retrieval using entities and relations to enhance recall. Requires graph construction first.",
    "colbertReranker": "ColBERT Reranker",
    "mergeParams": "Merge Retrieval Params",
    "finalTopK": "Final Top-K",
    "finalTopKDesc": "Final result count after merging and reranking Local + Web results",
    "threshold": "Similarity Threshold",
    "thresholdTitle": "Results below this threshold will be filtered",
    "thresholdDesc": "Applies to all retrieval results (Local + Web); results below threshold are filtered",
    "webSearch": "Web-Enhanced Retrieval",
    "webSearchToggle": "Web Search",
    "webSearchHelp": "Real-time retrieval from the internet (e.g. Perplexity). Can be used alongside local knowledge base (hybrid retrieval).",
    "webSearchDesc": "Fetch real-time internet information",
    "webSearchEnabled": "Web search enabled",
    "webSearchDisabled": "Web search disabled",
    "searchSources": "Search Sources",
    "queryEnhancement": "Query Enhancement",
    "queryOptimizer": "Query Optimizer",
    "queryOptimizerHelp": "Optimizes query format and multilingual queries for different search engines to improve retrieval quality.",
    "queryOptimizerDesc": "Smart multilingual query optimization",
    "queryOptimizerEnabled": "Query optimizer enabled",
    "queryOptimizerDisabled": "Query optimizer disabled",
    "queriesPerProvider": "Queries per provider per language",
    "settingsQueryTitle": "Set max query count per search engine",
    "queryCountSet": "Query count set to {{count}} (per language)",
    "contentFetcher": "Full-text Fetcher",
    "contentFetcherHelp": "Fetches full web page content from search results for more complete answers, but slower.",
    "contentFetcherDesc": "Fetch full webpage content (slower)",
    "contentFetcherEnabled": "Full-text fetcher enabled",
    "contentFetcherDisabled": "Full-text fetcher disabled",
    "agentMode": "Agent Mode",
    "reactAgent": "ReAct Agent (Chat)",
    "reactAgentHelp": "Only applies to regular Chat. When enabled, Chat uses ReAct loop: LLM autonomously decides whether to call search/graph tools. Suitable for complex, multi-step reasoning. Deep Research is not affected.",
    "reactAgentDesc": "Autonomous tool invocation",
    "agentEnabled": "Agent mode enabled",
    "agentDisabled": "Agent mode disabled",
    "agentNote": "Chat: Autonomous retrieval decisions\nDeep Research: Always auto-executes",
    "serviceConnection": "Service Connection",
    "serviceAddress": "Service Address",
    "serviceAddressHelp": "Backend API address. Changes take effect after reconnecting.",
    "connectService": "Connect",
    "connecting": "Connecting...",
    "connectingTo": "Connecting to service ({{address}})...",
    "connected": "Service connected!",
    "connectFailed": "Connection failed, please check service status",
    "localModels": "Local Models",
    "syncModels": "Sync / Upgrade Models",
    "syncing": "Syncing...",
    "checkModelStatus": "Check Model Status",
    "defaultCacheDir": "Default cache dir",
    "history": "History",
    "noHistory": "No history",
    "turns": "turns",
    "sessionDeleted": "Session deleted",
    "deleteFailed": "Delete failed",
    "sessionLoaded": "Session loaded",
    "loadSessionFailed": "Failed to load session",
    "confirmDeleteSession": "Delete this session?",
    "advancedConfig": "Advanced Config",
    "logout": "Sign Out",
    "loggedOut": "You have been signed out",
    "modelSyncComplete": "Model sync complete: {{upgraded}} upgraded, {{skipped}} up-to-date, {{failed}} failed",
    "modelSyncFailed": "Model sync failed, please check network or cache path",
    "modelStatusReady": "{{ok}} ready, {{failed}} missing",
    "modelStatusFailed": "Failed to get model status"
  },
  "chat": {
    "newConversation": "Start a new research conversation...",
    "copied": "Content copied",
    "exported": "Exported as Markdown",
    "copyContent": "Copy content",
    "exportMarkdown": "Export Markdown",
    "references": "References",
    "localDoc": "Local document",
    "addToCompare": "Add to compare",
    "addedToCompare": "Added to compare candidates. View in 'Doc Compare' tab",
    "onlyLocalCompare": "Only ingested local documents can be compared",
    "sendFailed": "Send failed, please retry",
    "requestError": "[Error] Request failed, please check network connection.",
    "bgResearchRunning": "Background Deep Research in progress (running)",
    "bgResearchStopping": "Background Deep Research in progress (stopping)",
    "recentLogs": "Recent Logs",
    "collapseLogs": "Collapse Logs",
    "viewTask": "View Task",
    "forceStop": "Force Stop",
    "noNewLogs": "No new logs",
    "confirmForceStop": "Force stop the current background Deep Research task?",
    "stopRequested": "Stop request sent, task is terminating",
    "stopFailed": "Failed to stop task, please retry",
    "coverageGaps": "Coverage gaps detected",
    "supplementAndContinue": "Supplement & Continue",
    "supplementHint": "You can supplement perspectives or upload temporary materials (pdf/md/txt) to continue research.",
    "unnamed": "Unnamed task",
    "stage": "Stage"
  },
  "chatInput": {
    "placeholder": "Type a question or / for commands...",
    "inputCommand": "Type",
    "viewCommands": "for commands",
    "deepResearch": "Deep Research",
    "pressEnter": "Press",
    "toSend": "to send",
    "enterTopic": "Please enter a research topic",
    "generatingQuestions": "Generating research planning questions...",
    "topicClear": "Topic is clear enough, skipping clarification",
    "clarifyFailed": "Clarification failed, using default questions",
    "drSettingsTitle": "Deep Research Default Settings",
    "defaultQuestion": "Please confirm the key objectives and scope of this research"
  },
  "chatPage": {
    "connectTitle": "Connect to Service",
    "connectDesc": "Please connect to the backend service to access the vector database.",
    "connectBtn": "Connect to Service Node",
    "connectingTo": "Connecting to service ({{address}})...",
    "connected": "Service connected!",
    "connectFailed": "Connection failed, please check service status"
  },
  "login": {
    "title": "DeepSea Research RAG",
    "subtitle": "Remote Research & Collaboration Platform",
    "account": "Account",
    "password": "Password",
    "usernamePlaceholder": "Username",
    "signIn": "Sign In"
  },
  "admin": {
    "title": "User Management",
    "subtitle": "Manage system access and role assignments",
    "createUser": "Create User",
    "username": "Username",
    "usernameRole": "Username / Role",
    "initialPassword": "Initial Password",
    "rolePermission": "Role",
    "normalUser": "Research User",
    "adminUser": "System Admin",
    "createAccount": "Create Account",
    "createdTime": "Created At",
    "management": "Actions",
    "fetchUsersFailed": "Failed to fetch user list",
    "userCreated": "User {{name}} created",
    "createFailed": "Failed to create user",
    "cannotDeleteAdmin": "Cannot delete super admin account",
    "deleteInDev": "Delete user feature in development",
    "deleteUser": "Delete User"
  },
  "canvas": {
    "researchCanvas": "Research Canvas",
    "exportMd": "Export Markdown",
    "exportWord": "Export Word",
    "exportRis": "Export RIS",
    "exportPdf": "Export PDF",
    "emptyTitle": "Research Canvas",
    "emptyDesc": "Plan research, organize literature, and write reviews here.\nFindings from Chat will automatically accumulate in the canvas.",
    "startDeepResearch": "Start Deep Research",
    "createBlankCanvas": "Create Blank Canvas",
    "canvasCreated": "Research canvas created",
    "createFailed": "Failed to create canvas",
    "emptyCannotExport": "Canvas is empty, cannot export",
    "exportedMd": "Document exported as Markdown",
    "exportedDocx": "Document exported as Word",
    "exportedRis": "Citations exported as RIS",
    "noCitationsToExport": "No citations available to export",
    "exportFailed": "Export failed, please retry",
    "pdfInDev": "PDF export in development"
  },
  "settings": {
    "advancedConfig": "Advanced Config",
    "citationFormat": "Citation Format",
    "citeKeyFormat": "Cite Key Format",
    "mergeLevel": "Citation Merge Level",
    "docLevel": "Document",
    "docLevelDesc": "Merge same article/website into one",
    "chunkLevel": "Chunk",
    "chunkLevelDesc": "Each chunk as separate citation",
    "rerankerStrategy": "Reranker Strategy",
    "rerankerMode": "Reranker Mode",
    "enableReranker": "Enable Reranker",
    "knowledgeGraph": "Knowledge Graph (HippoRAG)",
    "enableHippoRAG": "Enable HippoRAG",
    "hippoRAGDesc": "Auto-fuse graph retrieval for multi-entity + relation queries",
    "saved": "Advanced config saved"
  },
  "research": {
    "waiting": "Waiting for Deep Research to start...",
    "progressTitle": "Deep Research Progress",
    "confidence": "Confidence",
    "confidenceLow": "Low",
    "confidenceMedium": "Medium",
    "confidenceHigh": "High",
    "overallProgress": "Overall Progress",
    "sources": "Sources",
    "coverage": "Coverage",
    "iterations": "Iterations",
    "sections": "Sections",
    "coverageGaps": "Coverage Gaps",
    "conflicts": "Source Conflicts",
    "pending": "Pending",
    "researching": "Researching",
    "writing": "Writing",
    "reviewing": "Reviewing",
    "done": "Done",
    "sourcesCount": "{{count}} src"
  },
  "ingest": {
    "cannotConnect": "Cannot connect to backend service",
    "confirmBackendRunning": "Please ensure the backend service is running",
    "reconnect": "Reconnect",
    "status": "Status:",
    "connected": "Connected",
    "collections": "Collections:",
    "currentCollection": "Current Collection:",
    "docCount": "Documents:",
    "selectCollection": "Select Collection",
    "uploadToCollection": "Upload documents to the specified knowledge base partition",
    "refreshCollections": "Refresh collection list",
    "createCollection": "Create Collection",
    "records": "records",
    "noCollections": "No collections",
    "createFirst": "Please create a vector collection first",
    "createFirstBtn": "Create First Collection",
    "indexedFiles": "Indexed Files",
    "files": "files",
    "perPage": "Per page",
    "items": "items",
    "refreshFiles": "Refresh file list",
    "loading": "Loading...",
    "noFilesInCollection": "No indexed files in this collection",
    "fileName": "File Name",
    "fileSize": "Size",
    "chunks": "Chunks",
    "chartParse": "Chart Parse",
    "fileStatus": "Status",
    "ingestTime": "Ingest Time",
    "operations": "Actions",
    "table": "Table:",
    "on": "On",
    "off": "Off",
    "image": "Image:",
    "indexed": "Indexed",
    "uploadDocs": "Upload Documents",
    "dropPdfHere": "Drop PDF files here",
    "selectCollectionFirst": "Please select or create a collection first",
    "selectFiles": "Select Files",
    "selectFolder": "Select Folder",
    "llmEnhance": "LLM Enhancement",
    "imageDesc": "Image Description",
    "imageDescDetail": "Vision model parses chart meaning",
    "selectProvider": "Select Provider",
    "defaultModel": "Default (Provider default)",
    "concurrency": "Concurrency",
    "tableParse": "Table Parse",
    "tableParseDetail": "LLM generates table semantic summary",
    "noEnhancement": "No enhancement selected, basic parsing only (faster)",
    "parseStatus": "Parse Status (Table/Image)",
    "fileList": "File List",
    "count": "",
    "pendingStatus": "Pending",
    "doneStatus": "Done",
    "failedStatus": "Failed",
    "retryFailed": "Retry Failed",
    "deletePending": "Delete Pending",
    "deleteFailed": "Delete Failed",
    "clearDone": "Clear Done",
    "cancelProcess": "Cancel",
    "cancelling": "Cancelling...",
    "startIngest": "Start Ingest",
    "startProcess": "Start Processing",
    "processingFiles": "{{count}} files...",
    "uploading": "Uploading files...",
    "uploadingProgress": "Uploading...",
    "waitingProcess": "Waiting to process...",
    "processing": "Processing...",
    "complete": "Complete",
    "taskCancelled": "Task cancelled",
    "ingestComplete": "Ingest complete",
    "recordsWritten": "{{count}} records written",
    "cancelled": "Cancelled, ready to restart",
    "noPendingFiles": "No pending files",
    "cannotReadFile": "Cannot read file, please re-select",
    "uploadFailed": "Upload failed:",
    "duplicateFiles": "Following files already exist in this collection (identical content)",
    "skipDuplicate": "Choose 'Skip' to avoid duplicate ingestion",
    "overwriteIngest": "Overwrite & Re-ingest",
    "applyToAll": "Apply same action to all future duplicates",
    "currentPref": "Current preference:",
    "skipExisting": "Skip existing",
    "clearPref": "Clear preference",
    "newCollection": "New Vector Collection",
    "collectionName": "Collection Name",
    "collectionPlaceholder": "Custom name, e.g. my_research_2026",
    "quickTemplate": "Quick Templates",
    "useV2Schema": "Use v2 schema",
    "confirmDeleteFile": "Delete '{{name}}'?",
    "deleteFileWarning": "All vector data for this file will be permanently deleted.",
    "deleting": "Deleting",
    "deleted": "Deleted",
    "deleteFailed2": "Delete failed:",
    "noPdf": "No PDF files found",
    "confirmDeleteCollection": "Delete collection '{{name}}'? This cannot be undone and all data will be permanently deleted.",
    "deletingCollection": "Deleting collection:",
    "collectionDeleted": "Collection {{name}} deleted",
    "deleteCollectionFailed": "Delete failed:",
    "creatingCollection": "Creating collection:",
    "collectionCreated": "Collection {{name}} created",
    "createCollectionFailed": "Create failed:",
    "resumedTask": "Resumed background task progress...",
    "connectedBgTask": "Connected to background task",
    "collectionTemplates": {
      "general": "General Deep-Sea Literature",
      "biology": "Deep-Sea Biology & Ecology",
      "ocean": "Oceanography & Geology",
      "environment": "Deep-Sea Environment & Conservation"
    }
  },
  "graph": {
    "notBuilt": "Knowledge graph not built yet. Please import and parse papers via Ingest first",
    "nodes": "Nodes",
    "edges": "Edges",
    "entities": "Entities",
    "searchEntities": "Search entities...",
    "depth": "Depth",
    "selectEntity": "Select an entity on the left to view the knowledge graph",
    "loadFailed": "Load failed",
    "chunkDetail": "Chunk Details",
    "loadingContent": "Loading content...",
    "loadChunkFailed": "Failed to load chunk details",
    "collection": "Collection:",
    "paperId": "Paper ID:",
    "page": "Page:",
    "type": "Type:",
    "chunkId": "Chunk ID:",
    "section": "Section:",
    "relatedEntities": "Related Entities (Top {{count}})",
    "content": "Content",
    "emptyContent": "Empty content",
    "chunkNotFound": "Chunk content not found"
  },
  "compare": {
    "title": "Multi-Document Compare",
    "selectPapers": "Select 2-5 papers",
    "generate": "Generate Comparison",
    "chatCitations": "Chat Citations",
    "localLibrary": "Local Library",
    "searchPlaceholder": "Search by title or paper_id",
    "searchBtn": "Search",
    "total": "Total",
    "papers": "papers",
    "page": "Page",
    "item": "items",
    "prevPage": "Previous",
    "nextPage": "Next",
    "citations": "Citations",
    "times": "times",
    "noAbstract": "No abstract",
    "notLocalCannotCompare": "Not local, cannot compare",
    "select": "Select",
    "compareFailed": "Compare failed",
    "analysis": "Analysis",
    "comparedPapers": "Compared papers:",
    "dimension": "Dimension",
    "emptyHint": "Select 2-5 papers then click 'Generate Comparison' to view results",
    "noChatCitations": "No chat citations yet. Please generate citations in chat first",
    "switchToLocal": "Switch to local library",
    "noLocalPapers": "No papers available. Please import via Ingest first",
    "switchToChat": "Switch to chat citations"
  },
  "deepResearch": {
    "title": "Deep Research",
    "subtitle": "Multi-step deep research - confirm outline and track progress",
    "step1": "1. Clarify",
    "step2": "2. Confirm Outline",
    "step3": "3. Execute Research",
    "topic": "Research Topic",
    "topicPlaceholder": "Enter review topic...",
    "supplementInfo": "Please supplement the following (optional, {{count}} questions)",
    "answerPlaceholder": "Enter answer...",
    "whyAsk": "Why we ask:",
    "generatingQuestions": "Generating clarification questions...",
    "outputLanguage": "Output Language",
    "chinese": "Chinese",
    "dragHint": "Drag the left icon to reorder sections.",
    "researchDepth": "Research Depth",
    "stageIntervention": "Stage Intervention",
    "clarifyStage": "Clarify Intent",
    "required": "Required",
    "confirmOutline": "Confirm Outline",
    "reviewEachSection": "Review Each Section",
    "skipped": "Skipped",
    "optional": "Optional",
    "refineDirectives": "Refine with Directives",
    "minimizeIntervention": "Minimize human intervention (keep only required steps)",
    "jobIdStrategy": "Job ID Strategy",
    "keepOldJobId": "Keep old job ID when starting new task (for recovery)",
    "bgTaskNote": "Current task will continue in background; new task uses a new job ID.",
    "intervention": "Intervention (Supplementary context, optional)",
    "interventionMode": "Text intervention mode",
    "asContext": "As supplementary context (default)",
    "asDirective": "As strong directive (high confidence)",
    "directivePlaceholder": "Enter high-confidence insights/constraints, system will treat as high-priority...",
    "contextPlaceholder": "Supplement perspectives, counterexamples, constraints, key references...",
    "uploadTemp": "Upload temporary materials (pdf/md/txt)",
    "tempMaterialNote": "These materials are only used for this task and will not be saved to persistent local knowledge base.",
    "skipClarify": "Skip Clarification",
    "generateOutline": "Generate Outline",
    "backToClarify": "Back to Clarify",
    "confirmAndStart": "Confirm & Start Research",
    "bgResearching": "Researching in Background",
    "stopping": "Stopping...",
    "stopTask": "Stop Task",
    "settingsAutoSave": "Settings auto-save and persist across sessions. Can be temporarily overridden in Deep Research dialog.",
    "allSectionsPass": "All sections passed, starting final integration...",
    "integrationDone": "Full-text coherence integration complete",
    "citationRollback": "Citation loss risk detected, auto-rolled back to safe version",
    "stopReceived": "Stop request received, task is terminating...",
    "completed": "Deep Research Complete",
    "stopped": "Deep Research Stopped",
    "failed": "Deep Research failed, please retry",
    "otherTaskRunning": "Another Deep Research task detected; proceeding with new topic outline confirmation.",
    "restored": "Deep Research background task state restored",
    "enterTopic": "Please enter a research topic",
    "planFailed": "Research plan generation failed, please retry",
    "keepOneSection": "Please keep at least one outline section",
    "movedToBackground": "Moved to background execution, safe to close frontend",
    "failedRetry": "Deep Research failed, please retry",
    "stopRequestSent": "Stop requested, task is terminating...",
    "stopFailed": "Failed to stop task, please retry",
    "invalidFileText": "No valid text extracted from file (supports pdf/md/txt)",
    "addedTempMaterials": "Added {{count}} temporary materials",
    "extractFailed": "Temporary material extraction failed, please retry"
  },
  "commands": {
    "deepResearch": "Deep Research",
    "deepResearchDesc": "One-click review (multi-step deep research)",
    "search": "Search",
    "searchDesc": "Search literature",
    "generateOutline": "Outline",
    "generateOutlineDesc": "Create review outline",
    "draftChapter": "Draft",
    "draftChapterDesc": "Generate specified chapter",
    "exportDoc": "Export",
    "exportDocDesc": "Export as Markdown",
    "viewStatus": "Status",
    "viewStatusDesc": "Current progress overview",
    "availableCommands": "Available Commands",
    "example": "e.g.",
    "press": "Press",
    "orClickToSelect": "or click to select command"
  },
  "workflow": {
    "explore": "Explore",
    "exploreDesc": "Retrieve literature, explore the field",
    "outline": "Outline",
    "outlineDesc": "Plan review structure",
    "drafting": "Draft",
    "draftingDesc": "Generate chapter content",
    "refine": "Refine",
    "refineDesc": "Polish, proofread, export"
  },
  "language": {
    "zh": "中文",
    "en": "English"
  }
}
</file>

<file path="frontend/src/i18n/locales/zh.json">
{
  "common": {
    "cancel": "取消",
    "save": "保存",
    "confirm": "确认",
    "delete": "删除",
    "create": "创建",
    "loading": "加载中...",
    "search": "搜索",
    "export": "导出",
    "close": "关闭",
    "retry": "重试",
    "enabled": "已启用",
    "disabled": "已关闭",
    "settings": "设置",
    "status": "状态",
    "online": "在线",
    "offline": "离线",
    "remove": "移除",
    "back": "返回",
    "next": "下一步",
    "skip": "跳过",
    "start": "开始",
    "stop": "停止",
    "refresh": "刷新"
  },
  "header": {
    "chat": "智能问答",
    "ingest": "数据入库",
    "graph": "知识图谱",
    "compare": "文档对比",
    "users": "用户管理",
    "newChat": "新对话",
    "toggleHistory": "切换历史记录",
    "systemOnline": "System: Online",
    "disconnected": "未连接 (点击连接)",
    "connecting": "正在连接...",
    "connectService": "连接服务...",
    "serviceConnected": "服务连接成功！",
    "connectFailed": "连接失败",
    "syncingStatus": "正在同步服务状态...",
    "syncOk": "状态同步正常",
    "disconnectedStatus": "连接断开",
    "newChatCreated": "已开启新对话上下文",
    "workflow": "工作流"
  },
  "sidebar": {
    "brand": "RAG Lab",
    "retrievalConfig": "检索策略配置",
    "localKnowledge": "本地知识库",
    "localKnowledgeHelp": "从已入库的文档中检索相关片段作为对话上下文。关闭后对话将不使用本地知识库。",
    "queryCollection": "查询知识库",
    "noCollection": "暂无可用知识库",
    "localTopK": "Local RAG Top-K",
    "localTopKTitle": "输入更大的 Top-K（>60）",
    "hippoRAG": "HippoRAG 图检索",
    "hippoRAGHelp": "基于知识图谱的检索，利用实体与关系增强召回。需先构建图谱。",
    "colbertReranker": "ColBERT 重排序",
    "mergeParams": "合并检索参数",
    "finalTopK": "最终保留数量 (Final Top-K)",
    "finalTopKDesc": "Local + Web 合并重排后保留的最终结果数",
    "threshold": "相似度阈值 (Threshold)",
    "thresholdTitle": "低于此阈值的结果将被过滤",
    "thresholdDesc": "适用于所有检索结果（Local + Web），低于阈值的结果会被过滤",
    "webSearch": "Web 增强检索",
    "webSearchToggle": "联网搜索",
    "webSearchHelp": "从互联网实时检索（如 Perplexity 等），与本地知识库可同时开启（混合检索）。",
    "webSearchDesc": "实时获取互联网信息",
    "webSearchEnabled": "Web 检索已启用",
    "webSearchDisabled": "Web 检索已关闭",
    "searchSources": "Search Sources",
    "queryEnhancement": "Query Enhancement",
    "queryOptimizer": "查询优化器",
    "queryOptimizerHelp": "针对不同搜索引擎优化查询格式与分语言查询数，可能提高检索质量。",
    "queryOptimizerDesc": "智能优化多语言查询",
    "queryOptimizerEnabled": "查询优化器已启用",
    "queryOptimizerDisabled": "查询优化器已关闭",
    "queriesPerProvider": "单引擎单语种查询数",
    "settingsQueryTitle": "设置每个搜索引擎的最大查询数",
    "queryCountSet": "查询数设置为 {{count}}（中文+英文各自）",
    "contentFetcher": "全文抓取",
    "contentFetcherHelp": "对网络搜索结果抓取网页正文再参与回答，结果更完整但速度更慢。",
    "contentFetcherDesc": "深入抓取网页正文（较慢）",
    "contentFetcherEnabled": "全文抓取已启用",
    "contentFetcherDisabled": "全文抓取已关闭",
    "agentMode": "Agent 模式",
    "reactAgent": "ReAct Agent (Chat)",
    "reactAgentHelp": "仅对普通 Chat 生效。开启后，Chat 使用 ReAct 循环：LLM 自主决定是否调用搜索/图谱等工具，适合复杂、多步推理问题。Deep Research 不受此开关影响。",
    "reactAgentDesc": "智能体自主工具调用",
    "agentEnabled": "Agent 模式已启用",
    "agentDisabled": "Agent 模式已关闭",
    "agentNote": "Chat: 自主决策是否检索\nDeep Research: 始终自动执行",
    "serviceConnection": "服务连接",
    "serviceAddress": "Service Address",
    "serviceAddressHelp": "后端 API 地址，修改后需点击连接以生效。",
    "connectService": "连接服务",
    "connecting": "连接中...",
    "connectingTo": "正在连接服务 ({{address}})...",
    "connected": "服务连接成功！",
    "connectFailed": "连接失败，请检查服务状态",
    "localModels": "本地模型",
    "syncModels": "同步/升级模型",
    "syncing": "同步中...",
    "checkModelStatus": "检查模型状态",
    "defaultCacheDir": "默认缓存目录",
    "history": "历史项目",
    "noHistory": "暂无历史记录",
    "turns": "轮",
    "sessionDeleted": "会话已删除",
    "deleteFailed": "删除失败",
    "sessionLoaded": "会话已加载",
    "loadSessionFailed": "加载会话失败",
    "confirmDeleteSession": "确定删除此会话？",
    "advancedConfig": "高级配置",
    "logout": "注销",
    "loggedOut": "您已安全注销",
    "modelSyncComplete": "模型同步完成：升级 {{upgraded}}，已是最新 {{skipped}}，失败 {{failed}}",
    "modelSyncFailed": "模型同步失败，请检查网络或缓存路径",
    "modelStatusReady": "已就绪 {{ok}}，缺失 {{failed}}",
    "modelStatusFailed": "获取模型状态失败"
  },
  "chat": {
    "newConversation": "开启新的科研对话...",
    "copied": "内容已复制",
    "exported": "内容已导出为 Markdown",
    "copyContent": "复制内容",
    "exportMarkdown": "导出 Markdown",
    "references": "参考文献",
    "localDoc": "本地文档",
    "addToCompare": "加入对比",
    "addedToCompare": "已加入对比候选，可在「多文档对比」页查看",
    "onlyLocalCompare": "仅支持已入库的本地文档加入对比",
    "sendFailed": "发送失败，请重试",
    "requestError": "[错误] 请求失败，请检查网络连接。",
    "bgResearchRunning": "后台 Deep Research 未完成（运行中）",
    "bgResearchStopping": "后台 Deep Research 未完成（正在停止）",
    "recentLogs": "最近日志",
    "collapseLogs": "收起日志",
    "viewTask": "查看任务",
    "forceStop": "强行停止",
    "noNewLogs": "暂无新日志",
    "confirmForceStop": "确认强行停止当前后台 Deep Research 任务吗？",
    "stopRequested": "已发送停止请求，任务正在终止",
    "stopFailed": "停止任务失败，请重试",
    "coverageGaps": "检测到研究缺口",
    "supplementAndContinue": "补充材料并继续",
    "supplementHint": "你可以补充观点或上传临时材料（pdf/md/txt）后继续研究。",
    "unnamed": "未命名任务",
    "stage": "阶段"
  },
  "chatInput": {
    "placeholder": "输入问题或 / 查看命令...",
    "inputCommand": "输入",
    "viewCommands": "查看命令",
    "deepResearch": "深度研究",
    "pressEnter": "按",
    "toSend": "发送",
    "enterTopic": "请输入研究主题",
    "generatingQuestions": "正在生成研究规划问题...",
    "topicClear": "当前主题已足够明确，跳过澄清问题",
    "clarifyFailed": "澄清问题生成失败，已回退到默认问题",
    "drSettingsTitle": "Deep Research 默认设置",
    "defaultQuestion": "请确认本次研究最关键的目标与范围边界"
  },
  "chatPage": {
    "connectTitle": "连接远程服务",
    "connectDesc": "请连接至后端服务以访问向量数据库。",
    "connectBtn": "连接服务节点",
    "connectingTo": "正在连接服务 ({{address}})...",
    "connected": "服务连接成功！",
    "connectFailed": "连接失败，请检查服务状态"
  },
  "login": {
    "title": "深海科研 RAG 系统",
    "subtitle": "Remote Research & Collaboration Platform",
    "account": "Account",
    "password": "Password",
    "usernamePlaceholder": "Username",
    "signIn": "Sign In"
  },
  "admin": {
    "title": "用户权限管理",
    "subtitle": "管理系统访问权限与角色分配",
    "createUser": "新建用户",
    "username": "用户名",
    "usernameRole": "用户名 / 角色",
    "initialPassword": "初始密码",
    "rolePermission": "角色权限",
    "normalUser": "普通用户 (Research User)",
    "adminUser": "系统管理员 (Admin)",
    "createAccount": "创建账户",
    "createdTime": "创建时间",
    "management": "管理操作",
    "fetchUsersFailed": "获取用户列表失败",
    "userCreated": "用户 {{name}} 创建成功",
    "createFailed": "创建用户失败",
    "cannotDeleteAdmin": "无法删除超级管理员账号",
    "deleteInDev": "删除用户功能开发中",
    "deleteUser": "删除用户"
  },
  "canvas": {
    "researchCanvas": "Research Canvas",
    "exportMd": "Export Markdown",
    "exportWord": "导出 Word",
    "exportRis": "导出 RIS",
    "exportPdf": "Export PDF",
    "emptyTitle": "研究画布",
    "emptyDesc": "在这里规划研究、整理文献、撰写综述。\nChat 中的发现会自动积累到画布中。",
    "startDeepResearch": "启动 Deep Research",
    "createBlankCanvas": "创建空白画布",
    "canvasCreated": "研究画布已创建",
    "createFailed": "创建画布失败",
    "emptyCannotExport": "画布为空，无法导出",
    "exportedMd": "文档已导出为 Markdown",
    "exportedDocx": "文档已导出为 Word",
    "exportedRis": "引文已导出为 RIS",
    "noCitationsToExport": "当前没有可导出的引文",
    "exportFailed": "导出失败，请重试",
    "pdfInDev": "PDF 导出功能开发中"
  },
  "settings": {
    "advancedConfig": "高级配置",
    "citationFormat": "引文格式",
    "citeKeyFormat": "引用键格式 (cite_key)",
    "mergeLevel": "引文合并级别",
    "docLevel": "文档级",
    "docLevelDesc": "同一文章/网站合并为一条",
    "chunkLevel": "段落级",
    "chunkLevelDesc": "每个检索片段独立引文",
    "rerankerStrategy": "重排序策略",
    "rerankerMode": "Reranker 模式",
    "enableReranker": "启用 Reranker",
    "knowledgeGraph": "知识图谱 (HippoRAG)",
    "enableHippoRAG": "启用 HippoRAG",
    "hippoRAGDesc": "多实体 + 关系类查询时自动融合图检索",
    "saved": "高级配置已保存"
  },
  "research": {
    "waiting": "等待 Deep Research 启动...",
    "progressTitle": "Deep Research 进度",
    "confidence": "置信度",
    "confidenceLow": "低",
    "confidenceMedium": "中",
    "confidenceHigh": "高",
    "overallProgress": "整体进度",
    "sources": "来源",
    "coverage": "覆盖",
    "iterations": "迭代",
    "sections": "章节",
    "coverageGaps": "信息缺口",
    "conflicts": "来源冲突",
    "pending": "待开始",
    "researching": "研究中",
    "writing": "撰写中",
    "reviewing": "审核中",
    "done": "已完成",
    "sourcesCount": "{{count}}源"
  },
  "ingest": {
    "cannotConnect": "无法连接后端服务",
    "confirmBackendRunning": "请确认后端服务已启动",
    "reconnect": "重新连接",
    "status": "状态:",
    "connected": "已连接",
    "collections": "集合数:",
    "currentCollection": "当前集合:",
    "docCount": "文档数:",
    "selectCollection": "选择集合 (Collection)",
    "uploadToCollection": "将文档上传至指定的知识库分区",
    "refreshCollections": "刷新集合列表",
    "createCollection": "新建集合",
    "records": "条记录",
    "noCollections": "暂无集合",
    "createFirst": "请先创建一个向量集合",
    "createFirstBtn": "创建第一个集合",
    "indexedFiles": "已入库文件",
    "files": "个文件",
    "perPage": "每页",
    "items": "条",
    "refreshFiles": "刷新文件列表",
    "loading": "加载中...",
    "noFilesInCollection": "该集合暂无已入库的文件",
    "fileName": "文件名",
    "fileSize": "大小",
    "chunks": "Chunks",
    "chartParse": "图表解析",
    "fileStatus": "状态",
    "ingestTime": "入库时间",
    "operations": "操作",
    "table": "表格:",
    "on": "已开",
    "off": "未开",
    "image": "图片:",
    "indexed": "已入库",
    "uploadDocs": "上传文档",
    "dropPdfHere": "拖放 PDF 文件到此处",
    "selectCollectionFirst": "请先选择或创建一个集合",
    "selectFiles": "选择文件",
    "selectFolder": "选择文件夹",
    "llmEnhance": "LLM 增强选项",
    "imageDesc": "图片描述",
    "imageDescDetail": "Vision 模型解析图表含义",
    "selectProvider": "选择 Provider",
    "defaultModel": "模型默认（Provider default）",
    "concurrency": "并发",
    "tableParse": "表格解析",
    "tableParseDetail": "LLM 生成表格语义摘要",
    "noEnhancement": "未选择增强项，将仅执行基础解析（更快）",
    "parseStatus": "解析状态（表格/图片）",
    "fileList": "文件列表",
    "count": "个",
    "pendingStatus": "待处理",
    "doneStatus": "完成",
    "failedStatus": "失败",
    "retryFailed": "重试失败",
    "deletePending": "删除待处理",
    "deleteFailed": "删除失败",
    "clearDone": "清除已完成",
    "cancelProcess": "取消处理",
    "cancelling": "取消中...",
    "startIngest": "开始入库",
    "startProcess": "开始处理",
    "processingFiles": "{{count}} 个文件...",
    "uploading": "上传文件...",
    "uploadingProgress": "上传中...",
    "waitingProcess": "等待处理...",
    "processing": "处理中...",
    "complete": "完成",
    "taskCancelled": "任务已取消",
    "ingestComplete": "入库完成",
    "recordsWritten": "{{count}} 条记录写入",
    "cancelled": "已取消，可重新开始",
    "noPendingFiles": "没有待处理的文件",
    "cannotReadFile": "无法读取文件，请重新选择",
    "uploadFailed": "上传失败:",
    "duplicateFiles": "以下文件已在本集合中（内容相同）",
    "skipDuplicate": "选择「跳过」不重复入库",
    "overwriteIngest": "覆盖并重新入库",
    "applyToAll": "对本次及之后所有重复项执行相同操作",
    "currentPref": "当前偏好：",
    "skipExisting": "跳过已存在",
    "clearPref": "清除偏好",
    "newCollection": "新建向量集合",
    "collectionName": "集合名称",
    "collectionPlaceholder": "自定义集合名称，例如 my_research_2026",
    "quickTemplate": "快速选择模板",
    "useV2Schema": "使用 v2 schema",
    "confirmDeleteFile": "确定删除「{{name}}」？",
    "deleteFileWarning": "将同时删除该文件在集合中的所有向量数据，不可恢复。",
    "deleting": "正在删除",
    "deleted": "已删除",
    "deleteFailed2": "删除失败:",
    "noPdf": "未找到 PDF 文件",
    "confirmDeleteCollection": "确定要删除集合「{{name}}」？此操作不可恢复，集合内所有数据将被永久删除。",
    "deletingCollection": "正在删除集合:",
    "collectionDeleted": "集合 {{name}} 已删除",
    "deleteCollectionFailed": "删除失败:",
    "creatingCollection": "正在创建集合:",
    "collectionCreated": "集合 {{name}} 创建成功",
    "createCollectionFailed": "创建失败:",
    "resumedTask": "已恢复后台任务进度...",
    "connectedBgTask": "已连接后台任务",
    "collectionTemplates": {
      "general": "通用深海文献库",
      "biology": "深海生物与生态",
      "ocean": "海洋学与地质",
      "environment": "深海环境与保护"
    }
  },
  "graph": {
    "notBuilt": "知识图谱尚未构建，请先通过 Ingest 导入论文并构建图谱",
    "nodes": "节点",
    "edges": "边",
    "entities": "实体",
    "searchEntities": "搜索实体...",
    "depth": "深度",
    "selectEntity": "选择左侧实体以查看知识图谱",
    "loadFailed": "加载失败",
    "chunkDetail": "Chunk 详情",
    "loadingContent": "正在加载内容...",
    "loadChunkFailed": "加载 chunk 详情失败",
    "collection": "Collection:",
    "paperId": "Paper ID:",
    "page": "Page:",
    "type": "Type:",
    "chunkId": "Chunk ID:",
    "section": "Section:",
    "relatedEntities": "关联实体（Top {{count}}）",
    "content": "内容",
    "emptyContent": "空内容",
    "chunkNotFound": "未找到 chunk 内容"
  },
  "compare": {
    "title": "多文档对比",
    "selectPapers": "选择 2-5 篇论文",
    "generate": "生成对比",
    "chatCitations": "对话引文",
    "localLibrary": "本地文库",
    "searchPlaceholder": "搜索标题或 paper_id",
    "searchBtn": "搜索",
    "total": "共",
    "papers": "篇",
    "page": "第",
    "item": "条",
    "prevPage": "上一页",
    "nextPage": "下一页",
    "citations": "引用",
    "times": "次",
    "noAbstract": "无摘要",
    "notLocalCannotCompare": "未在本地，无法对比",
    "select": "选择",
    "compareFailed": "对比失败",
    "analysis": "综合分析",
    "comparedPapers": "已对比论文：",
    "dimension": "维度",
    "emptyHint": "选择 2-5 篇论文后点击「生成对比」查看结果",
    "noChatCitations": "暂无对话引文，请先在对话中产生引用后再使用「对话引文」",
    "switchToLocal": "切换到本地文库选择论文",
    "noLocalPapers": "无可用论文，请先通过 Ingest 导入并解析论文",
    "switchToChat": "切换到对话引文"
  },
  "deepResearch": {
    "title": "Deep Research",
    "subtitle": "多步深度研究 - 可确认大纲并跟踪进度",
    "step1": "1. 澄清问题",
    "step2": "2. 确认大纲",
    "step3": "3. 执行研究",
    "topic": "研究主题",
    "topicPlaceholder": "输入综述主题...",
    "supplementInfo": "请补充以下信息（可选，共 {{count}} 题）",
    "answerPlaceholder": "输入回答...",
    "whyAsk": "为什么问：",
    "generatingQuestions": "正在生成澄清问题...",
    "outputLanguage": "Output Language",
    "chinese": "中文",
    "dragHint": "可拖拽左侧图标调整章节顺序。",
    "researchDepth": "Research Depth (研究深度)",
    "stageIntervention": "Stage Intervention (阶段介入)",
    "clarifyStage": "澄清意图 (Clarify)",
    "required": "必须",
    "confirmOutline": "确认大纲 (Confirm Outline)",
    "reviewEachSection": "逐章审阅 (Review Each Section)",
    "skipped": "已跳过",
    "optional": "可选",
    "refineDirectives": "精炼修改 (Refine with Directives)",
    "minimizeIntervention": "最小化人工介入（仅保留必须步骤）",
    "jobIdStrategy": "任务 ID 策略",
    "keepOldJobId": "开始新任务时保留旧任务 ID（便于后续恢复）",
    "bgTaskNote": "当前任务将继续在后台运行；新任务会使用新的 job id。",
    "intervention": "Intervention (补充上下文，可选)",
    "interventionMode": "文本介入模式",
    "asContext": "作为补充上下文（默认）",
    "asDirective": "作为强提示直接注入（我对内容非常自信）",
    "directivePlaceholder": "输入高置信观点/约束，系统会作为高优先级提示并要求显式验证...",
    "contextPlaceholder": "可补充新观点、反例、约束条件、重点文献线索...",
    "uploadTemp": "上传临时材料 (pdf/md/txt)",
    "tempMaterialNote": "这些材料仅用于本次任务，不写入持久本地知识库。",
    "skipClarify": "跳过澄清",
    "generateOutline": "生成大纲",
    "backToClarify": "返回澄清",
    "confirmAndStart": "确认并开始研究",
    "bgResearching": "后台研究中",
    "stopping": "停止中...",
    "stopTask": "停止任务",
    "settingsAutoSave": "设置自动保存，跨会话持久化。在 Deep Research 对话内可临时覆盖。",
    "allSectionsPass": "所有章节已通过，开始最终整合...",
    "integrationDone": "全文连贯性整合完成",
    "citationRollback": "检测到引用丢失风险，已自动回退到安全版本",
    "stopReceived": "收到停止请求，任务正在终止...",
    "completed": "Deep Research 完成",
    "stopped": "Deep Research 已停止",
    "failed": "Deep Research 失败，请重试",
    "otherTaskRunning": "检测到其他进行中的 Deep Research 任务；当前按新主题进入大纲确认流程。",
    "restored": "已恢复 Deep Research 后台任务状态",
    "enterTopic": "请输入研究主题",
    "planFailed": "研究规划生成失败，请重试",
    "keepOneSection": "请至少保留一个大纲章节",
    "movedToBackground": "已转为后台执行，可安全关闭当前前端页面",
    "failedRetry": "Deep Research 失败，请重试",
    "stopRequestSent": "已请求停止任务，正在终止...",
    "stopFailed": "停止任务失败，请重试",
    "invalidFileText": "未从文件提取到有效文本（支持 pdf/md/txt）",
    "addedTempMaterials": "已添加 {{count}} 份临时材料",
    "extractFailed": "临时材料提取失败，请重试"
  },
  "commands": {
    "deepResearch": "Deep Research",
    "deepResearchDesc": "一键综述（多步深度研究）",
    "search": "检索",
    "searchDesc": "搜索文献资料",
    "generateOutline": "生成大纲",
    "generateOutlineDesc": "创建综述大纲",
    "draftChapter": "撰写章节",
    "draftChapterDesc": "生成指定章节",
    "exportDoc": "导出文档",
    "exportDocDesc": "导出为 Markdown",
    "viewStatus": "查看状态",
    "viewStatusDesc": "当前进度概览",
    "availableCommands": "可用命令",
    "example": "例:",
    "press": "按",
    "orClickToSelect": "或点击选择命令"
  },
  "workflow": {
    "explore": "探索",
    "exploreDesc": "检索文献、了解领域",
    "outline": "大纲",
    "outlineDesc": "规划综述结构",
    "drafting": "撰写",
    "draftingDesc": "逐章生成内容",
    "refine": "精炼",
    "refineDesc": "润色、校对、导出"
  },
  "language": {
    "zh": "中文",
    "en": "English"
  }
}
</file>

<file path="frontend/src/stores/useChatStore.ts">
import { create } from 'zustand';
import type { Message, WorkflowStep, EvidenceSummary, Source, ClarifyQuestion, ResearchDashboardData, ToolTraceItem } from '../types';
import { getSession } from '../api/chat';

interface ChatState {
  sessionId: string | null;
  canvasId: string | null;
  messages: Message[];
  workflowStep: WorkflowStep;
  isStreaming: boolean;
  lastEvidenceSummary: EvidenceSummary | null;
  isLoadingSession: boolean;

  // Deep Research 状态
  deepResearchActive: boolean;          // 是否正在进行 Deep Research
  deepResearchTopic: string;            // Deep Research 主题
  clarificationQuestions: ClarifyQuestion[]; // 澄清问题列表
  showDeepResearchDialog: boolean;      // 是否显示澄清对话框
  researchDashboard: ResearchDashboardData | null;  // 研究进度仪表盘
  toolTrace: ToolTraceItem[] | null;                // Agent 工具调用轨迹

  // 命令面板
  showCommandPalette: boolean;

  // Actions
  setSessionId: (id: string | null) => void;
  setCanvasId: (id: string | null) => void;
  addMessage: (msg: Message) => void;
  updateLastMessage: (content: string) => void;
  appendToLastMessage: (delta: string) => void;
  setLastMessageSources: (sources: Message['sources']) => void;
  clearMessages: () => void;
  setWorkflowStep: (step: WorkflowStep) => void;
  setIsStreaming: (streaming: boolean) => void;
  setLastEvidenceSummary: (summary: EvidenceSummary | null) => void;
  newChat: () => void;
  loadSession: (sessionId: string) => Promise<void>;

  // Deep Research Actions
  setDeepResearchActive: (active: boolean) => void;
  setDeepResearchTopic: (topic: string) => void;
  setClarificationQuestions: (questions: ClarifyQuestion[]) => void;
  setShowDeepResearchDialog: (show: boolean) => void;
  setShowCommandPalette: (show: boolean) => void;
  setResearchDashboard: (dashboard: ResearchDashboardData | null) => void;
  setToolTrace: (trace: ToolTraceItem[] | null) => void;
}

export const useChatStore = create<ChatState>((set) => ({
  sessionId: null,
  canvasId: null,
  messages: [],
  workflowStep: 'idle',
  isStreaming: false,
  lastEvidenceSummary: null,
  isLoadingSession: false,

  // Deep Research 初始状态
  deepResearchActive: false,
  deepResearchTopic: '',
  clarificationQuestions: [],
  showDeepResearchDialog: false,
  researchDashboard: null,
  toolTrace: null,
  showCommandPalette: false,

  setSessionId: (id) => set({ sessionId: id }),
  setCanvasId: (id) => set({ canvasId: id }),

  addMessage: (msg) =>
    set((state) => ({
      messages: [...state.messages, { ...msg, timestamp: new Date().toISOString() }],
    })),

  updateLastMessage: (content) =>
    set((state) => {
      const messages = [...state.messages];
      if (messages.length > 0) {
        messages[messages.length - 1] = {
          ...messages[messages.length - 1],
          content,
        };
      }
      return { messages };
    }),

  appendToLastMessage: (delta) =>
    set((state) => {
      const messages = [...state.messages];
      if (messages.length > 0) {
        messages[messages.length - 1] = {
          ...messages[messages.length - 1],
          content: messages[messages.length - 1].content + delta,
        };
      }
      return { messages };
    }),

  setLastMessageSources: (sources) =>
    set((state) => {
      const messages = [...state.messages];
      if (messages.length > 0) {
        messages[messages.length - 1] = {
          ...messages[messages.length - 1],
          sources,
        };
      }
      return { messages };
    }),

  clearMessages: () => set({ messages: [] }),

  setWorkflowStep: (step) => set({ workflowStep: step }),
  setIsStreaming: (streaming) => set({ isStreaming: streaming }),
  setLastEvidenceSummary: (summary) => set({ lastEvidenceSummary: summary }),

  newChat: () =>
    set(() => ({
      sessionId: null,
      canvasId: null,
      messages: [],
      workflowStep: 'idle',
      lastEvidenceSummary: null,
      // 新对话 = 全新页面；旧 Deep Research 任务仍在后台运行，
      // 加载旧会话时会重新恢复 dashboard。
      deepResearchActive: false,
      deepResearchTopic: '',
      clarificationQuestions: [],
      showDeepResearchDialog: false,
      researchDashboard: null,
      toolTrace: null,
    })),

  // Deep Research Actions
  setDeepResearchActive: (active) => set({ deepResearchActive: active }),
  setDeepResearchTopic: (topic) => set({ deepResearchTopic: topic }),
  setClarificationQuestions: (questions) => set({ clarificationQuestions: questions }),
  setShowDeepResearchDialog: (show) => set({ showDeepResearchDialog: show }),
  setShowCommandPalette: (show) => set({ showCommandPalette: show }),
  setResearchDashboard: (dashboard) => set({ researchDashboard: dashboard }),
  setToolTrace: (trace) => set({ toolTrace: trace }),

  loadSession: async (sessionId: string) => {
    set({ isLoadingSession: true });
    try {
      const sessionInfo = await getSession(sessionId);
      const messages: Message[] = sessionInfo.turns.map((turn, index) => {
        const sources: Source[] = (turn.sources || []).map((s, sIndex) => ({
          id: s.cite_key || `${sessionId}-${index}-${sIndex}`,
          cite_key: s.cite_key || '',
          title: s.title || '',
          authors: s.authors || [],
          year: s.year,
          doc_id: s.doc_id,
          url: s.url,
          doi: s.doi,
          bbox: s.bbox,
          page_num: s.page_num,
          type: s.url ? 'web' : 'local',
        }));
        return {
          id: `${sessionId}-${index}`,
          role: turn.role as 'user' | 'assistant',
          content: turn.content,
          timestamp: new Date().toISOString(),
          sources,
        };
      });
      set({
        sessionId: sessionInfo.session_id,
        canvasId: sessionInfo.canvas_id || null,
        messages,
        workflowStep: 'idle',
        lastEvidenceSummary: null,
        deepResearchActive: false,
        deepResearchTopic: '',
        clarificationQuestions: [],
        showDeepResearchDialog: false,
        researchDashboard: sessionInfo.research_dashboard || null,
        showCommandPalette: false,
        isLoadingSession: false,
      });
    } catch (error) {
      set({ isLoadingSession: false });
      throw error;
    }
  },
}));
</file>

<file path="frontend/package.json">
{
  "name": "frontend",
  "private": true,
  "version": "0.0.0",
  "type": "module",
  "scripts": {
    "dev": "vite",
    "build": "tsc -b && vite build",
    "lint": "eslint .",
    "preview": "vite preview"
  },
  "dependencies": {
    "axios": "^1.13.4",
    "clsx": "^2.1.1",
    "date-fns": "^4.1.0",
    "i18next": "^25.8.7",
    "i18next-browser-languagedetector": "^8.2.1",
    "lucide-react": "^0.563.0",
    "react": "^19.2.0",
    "react-dom": "^19.2.0",
    "react-force-graph-2d": "^1.29.1",
    "react-i18next": "^16.5.4",
    "react-markdown": "^10.1.0",
    "react-pdf": "^10.3.0",
    "react-router-dom": "^7.13.0",
    "rehype-highlight": "^7.0.2",
    "remark-gfm": "^4.0.1",
    "tailwind-merge": "^3.4.0",
    "zustand": "^5.0.11"
  },
  "devDependencies": {
    "@eslint/js": "^9.39.1",
    "@tailwindcss/vite": "^4.1.18",
    "@types/node": "^24.10.1",
    "@types/react": "^19.2.5",
    "@types/react-dom": "^19.2.3",
    "@vitejs/plugin-react": "^5.1.1",
    "autoprefixer": "^10.4.24",
    "eslint": "^9.39.1",
    "eslint-plugin-react-hooks": "^7.0.1",
    "eslint-plugin-react-refresh": "^0.4.24",
    "globals": "^16.5.0",
    "postcss": "^8.5.6",
    "tailwindcss": "^4.1.18",
    "typescript": "~5.9.3",
    "typescript-eslint": "^8.46.4",
    "vite": "^7.2.4"
  }
}
</file>

<file path="scripts/03_index_papers.py">
#!/usr/bin/env python
"""步骤3: 切块 + 向量化 + 入库（结构化切块，enriched.json）"""

import sys
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict

sys.path.insert(0, ".")

from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)
from src.indexing.milvus_ops import milvus
from src.indexing.embedder import embedder
from src.chunking.chunker import Chunk, ChunkConfig, chunk_blocks


def truncate_content(content: str, max_length: int = 65000) -> str:
    """截断超长内容"""
    if len(content) > max_length:
        return content[:max_length]
    return content


def main():
    run_id = datetime.now().strftime("%Y%m%d_%H%M%S")
    settings.path.ensure_dirs()

    logger.info("=" * 60)
    logger.info("深海科研知识库 - 向量入库（结构化切块）")
    logger.info("=" * 60)

    parsed_dir = settings.path.parsed
    enriched_files = list(parsed_dir.rglob("enriched.json"))

    logger.info(f"解析目录: {parsed_dir}")
    logger.info(f"找到 enriched.json: {len(enriched_files)} 个")

    if not enriched_files:
        logger.warning("未找到 enriched.json")
        logger.warning("请先运行: python scripts/02_parse_papers.py")
        return

    cs = getattr(settings, "chunk", None)
    chunk_cfg = ChunkConfig(
        target_chars=getattr(cs, "target_chars", 1000) if cs else 1000,
        min_chars=getattr(cs, "min_chars", 200) if cs else 200,
        max_chars=getattr(cs, "max_chars", 1800) if cs else 1800,
        overlap_sentences=getattr(cs, "overlap_sentences", 2) if cs else 2,
        table_rows_per_chunk=getattr(cs, "table_rows_per_chunk", 10) if cs else 10,
    )

    artifact = {
        "run_id": run_id,
        "input_count": len(enriched_files),
        "total_chunks": 0,
        "inserted_count": 0,
        "truncated_count": 0,
        "collections": {},
    }

    collection_name = settings.collection.global_
    all_data: List[Dict] = []

    logger.info("处理文件并生成向量...")
    for i, json_path in enumerate(enriched_files, 1):
        logger.info(f"[{i}/{len(enriched_files)}] {json_path.parent.name}")

        with open(json_path, "r", encoding="utf-8") as f:
            doc = json.load(f)

        doc_id = doc.get("doc_id", json_path.parent.name)
        content_flow = doc.get("content_flow", [])
        doc_metadata = doc.get("doc_metadata") or {}
        doi = doc_metadata.get("doi") or ""
        doc_title = doc_metadata.get("title") or ""

        chunks = chunk_blocks(content_flow, doc_id=doc_id, config=chunk_cfg)
        logger.info(f"chunks: {len(chunks)}")

        for c in chunks:
            text = truncate_content(c.text)
            if len(c.text) > 65000:
                artifact["truncated_count"] += 1

            meta = c.meta or {}
            page_range = meta.get("page_range", [0, 0])
            page = page_range[0] if isinstance(page_range, (list, tuple)) else meta.get("page", 0)

            row = {
                "paper_id": doc_id,
                "chunk_id": c.chunk_id,
                "content": text,
                "raw_content": text,
                "domain": "global",
                "content_type": c.content_type,
                "chunk_type": ",".join(meta.get("block_types", []))[:64] or "paragraph",
                "section_path": str(meta.get("section_path", ""))[:512],
                "page": int(page) if isinstance(page, (int, float)) else 0,
                "_text_for_embed": text,
            }
            if doi:
                row["doi"] = doi
            if doc_title:
                row["doc_title"] = doc_title
            all_data.append(row)

        artifact["total_chunks"] += len(chunks)

    if not all_data:
        logger.warning("无有效内容")
        return

    # 批量生成向量
    logger.info(f"生成向量 (共 {len(all_data)} 个 chunks)...")
    texts = [d["_text_for_embed"] for d in all_data]

    batch_size = 32
    for batch_start in range(0, len(texts), batch_size):
        batch_end = min(batch_start + batch_size, len(texts))
        batch_texts = texts[batch_start:batch_end]

        embeddings = embedder.encode(batch_texts)

        for k, idx in enumerate(range(batch_start, batch_end)):
            all_data[idx]["dense_vector"] = embeddings["dense"][k].tolist()
            sparse_row = embeddings["sparse"]._getrow(k).tocoo()
            all_data[idx]["sparse_vector"] = {int(col): float(val) for col, val in zip(sparse_row.col, sparse_row.data)}

        logger.info(f"向量化: {batch_end}/{len(texts)}")

    for d in all_data:
        del d["_text_for_embed"]

    # 批量入库
    logger.info(f"入库到 {collection_name}...")
    insert_batch_size = 100
    for batch_start in range(0, len(all_data), insert_batch_size):
        batch_end = min(batch_start + insert_batch_size, len(all_data))
        batch = all_data[batch_start:batch_end]

        milvus.insert(collection_name, batch)
        artifact["inserted_count"] += len(batch)
        logger.info(f"插入: {batch_end}/{len(all_data)}")

    count = milvus.count(collection_name)
    artifact["collections"][collection_name] = count

    artifact_path = settings.path.artifacts / f"03_index_{run_id}.json"
    with open(artifact_path, "w", encoding="utf-8") as f:
        json.dump(artifact, f, ensure_ascii=False, indent=2)

    logger.info("=" * 60)
    logger.info(f"入库完成: {artifact['inserted_count']} 条")
    logger.info(f"Collection '{collection_name}' 总数: {count}")
    if artifact["truncated_count"] > 0:
        logger.warning(f"截断: {artifact['truncated_count']} 条")
    logger.info(f"产物已保存: {artifact_path}")
    logger.info("=" * 60)


if __name__ == "__main__":
    main()
</file>

<file path="src/api/routes_canvas.py">
"""
Canvas API：CRUD、大纲/草稿、快照、导出、AI 编辑。
"""

import uuid
from datetime import datetime
from pathlib import Path
import re
import requests

from fastapi import APIRouter, Depends, HTTPException, Query

from src.api.routes_auth import get_optional_user_id

from src.api.schemas import (
    CanvasAIEditRequest,
    CanvasAIEditResponse,
    CanvasCreateRequest,
    CanvasRefineRequest,
    CanvasRefineResponse,
    CanvasResponse,
    CanvasUpdateRequest,
    CanvasVersionItem,
    CitationFilterRequest,
    CitationFilterResponse,
    CitationResponse,
    DraftBlockSchema,
    DraftUpsertRequest,
    ExportResponse,
    OutlineSectionSchema,
    OutlineUpsertRequest,
)
from src.collaboration.canvas.canvas_manager import (
    create_canvas,
    create_snapshot,
    delete_canvas,
    delete_canvas_citation,
    export_canvas,
    filter_canvas_citations,
    get_canvas,
    get_canvas_citations,
    list_snapshots,
    restore_snapshot,
    update_canvas,
    upsert_draft,
    upsert_outline,
)
from src.collaboration.canvas.models import DraftBlock, OutlineSection
from src.collaboration.citation.formatter import format_bibtex, format_reference_list, format_ris
from src.collaboration.memory.session_memory import get_session_store

router = APIRouter(prefix="/canvas", tags=["canvas"])


def _canvas_to_response(c) -> CanvasResponse:
    from src.api.schemas import AnnotationSchema, ResearchBriefSchema

    # 序列化 annotations
    annotations_out = []
    for ann in (c.annotations or []):
        annotations_out.append(AnnotationSchema(
            id=ann.id,
            section_id=ann.section_id,
            target_text=ann.target_text,
            directive=ann.directive,
            status=ann.status,
            created_at=ann.created_at.isoformat() if hasattr(ann.created_at, 'isoformat') else str(ann.created_at),
        ))

    # 序列化 research_brief
    brief_out = None
    if c.research_brief:
        rb = c.research_brief
        brief_out = ResearchBriefSchema(
            scope=rb.scope,
            success_criteria=rb.success_criteria,
            key_questions=rb.key_questions,
            exclusions=rb.exclusions,
            time_range=rb.time_range,
            source_priority=rb.source_priority,
            action_plan=getattr(rb, 'action_plan', ''),
        )

    # 序列化 citation_pool
    citations_out = [
        {
            "id": cit.id,
            "cite_key": cit.cite_key or cit.id,
            "title": cit.title,
            "authors": cit.authors or [],
            "year": cit.year,
            "doi": cit.doi,
            "url": cit.url,
            "bibtex": cit.bibtex,
        }
        for cit in c.citation_pool.values()
    ]

    return CanvasResponse(
        id=c.id,
        session_id=c.session_id,
        topic=c.topic,
        working_title=c.working_title,
        abstract=c.abstract,
        keywords=c.keywords,
        stage=c.stage,
        refined_markdown=getattr(c, "refined_markdown", "") or "",
        outline=[
            {"id": s.id, "title": s.title, "level": s.level, "order": s.order, "parent_id": s.parent_id, "status": s.status, "guidance": s.guidance}
            for s in c.outline
        ],
        drafts={
            sid: {
                "section_id": b.section_id,
                "content_md": b.content_md,
                "version": b.version,
                "used_fragment_ids": b.used_fragment_ids,
                "used_citation_ids": b.used_citation_ids,
                "updated_at": b.updated_at.isoformat(),
            }
            for sid, b in c.drafts.items()
        },
        citation_pool=citations_out,
        identified_gaps=c.identified_gaps or [],
        user_directives=c.user_directives or [],
        annotations=annotations_out,
        research_brief=brief_out,
        skip_draft_review=getattr(c, 'skip_draft_review', False),
        skip_refine_review=getattr(c, 'skip_refine_review', False),
        version=c.version,
    )


@router.post("", response_model=CanvasResponse)
def canvas_create(
    body: CanvasCreateRequest,
    user_id: str | None = Depends(get_optional_user_id),
) -> CanvasResponse:
    c = create_canvas(session_id=body.session_id, topic=body.topic, user_id=user_id or "")
    if body.session_id:
        store = get_session_store()
        if store.get_session_meta(body.session_id):
            store.update_session_meta(body.session_id, {"canvas_id": c.id})
    return _canvas_to_response(c)


@router.get("/{canvas_id}", response_model=CanvasResponse)
def canvas_get(canvas_id: str) -> CanvasResponse:
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    return _canvas_to_response(c)


@router.patch("/{canvas_id}", response_model=CanvasResponse)
def canvas_update(canvas_id: str, body: CanvasUpdateRequest) -> CanvasResponse:
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    fields = body.model_dump(exclude_unset=True)
    update_canvas(canvas_id, **fields)
    c = get_canvas(canvas_id)
    return _canvas_to_response(c)


@router.delete("/{canvas_id}")
def canvas_delete(canvas_id: str) -> dict:
    if not delete_canvas(canvas_id):
        raise HTTPException(status_code=404, detail="canvas not found")
    return {"ok": True, "canvas_id": canvas_id}


@router.post("/{canvas_id}/outline", response_model=CanvasResponse)
def canvas_upsert_outline(canvas_id: str, body: OutlineUpsertRequest) -> CanvasResponse:
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    sections = [
        OutlineSection(
            id=s.id or str(uuid.uuid4())[:8],
            title=s.title,
            level=s.level,
            order=s.order,
            parent_id=s.parent_id,
            status=s.status,
            guidance=s.guidance,
        )
        for s in body.sections
    ]
    upsert_outline(canvas_id, sections)
    c = get_canvas(canvas_id)
    return _canvas_to_response(c)


@router.post("/{canvas_id}/drafts", response_model=CanvasResponse)
def canvas_upsert_draft(canvas_id: str, body: DraftUpsertRequest) -> CanvasResponse:
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    b = body.block
    block = DraftBlock(
        section_id=b.section_id,
        content_md=b.content_md,
        version=b.version,
        used_fragment_ids=b.used_fragment_ids,
        used_citation_ids=b.used_citation_ids,
        updated_at=datetime.now(),
    )
    upsert_draft(canvas_id, block)
    c = get_canvas(canvas_id)
    return _canvas_to_response(c)


@router.post("/{canvas_id}/snapshot")
def canvas_snapshot(canvas_id: str) -> dict:
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    try:
        ver = create_snapshot(canvas_id)
        return {"ok": True, "canvas_id": canvas_id, "version_number": ver}
    except ValueError as e:
        raise HTTPException(status_code=404, detail=str(e))


@router.post("/{canvas_id}/restore/{version_number}")
def canvas_restore(canvas_id: str, version_number: int) -> dict:
    if not restore_snapshot(canvas_id, version_number):
        raise HTTPException(status_code=404, detail="canvas or version not found")
    return {"ok": True, "canvas_id": canvas_id, "restored_version": version_number}


@router.get("/{canvas_id}/snapshots", response_model=list[CanvasVersionItem])
def canvas_list_snapshots(canvas_id: str, limit: int = Query(50, ge=1, le=200)) -> list[CanvasVersionItem]:
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    rows = list_snapshots(canvas_id, limit=limit)
    return [CanvasVersionItem(**r) for r in rows]


@router.get("/{canvas_id}/export", response_model=ExportResponse)
def canvas_export(canvas_id: str, format: str = "json") -> ExportResponse:
    """
    导出画布。
    - format=json: 返回完整 JSON 结构
    - format=markdown: 返回 Markdown 格式（大纲 + 草稿 + 引用）
    """
    import json as json_lib
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    if format == "markdown":
        if getattr(c, "refined_markdown", "").strip():
            content = c.refined_markdown
        else:
            # 生成 Markdown 格式
            lines = []
            if c.working_title:
                lines.append(f"# {c.working_title}\n")
            if c.abstract:
                lines.append(f"## 摘要\n\n{c.abstract}\n")
            if c.outline:
                lines.append("## 大纲\n")
                for s in c.outline:
                    indent = "  " * (s.level - 1)
                    lines.append(f"{indent}- {s.title}")
                lines.append("")
            if c.drafts:
                lines.append("## 正文\n")
                for section_id, block in c.drafts.items():
                    if block.content_md:
                        lines.append(block.content_md)
                        lines.append("")
            citations = get_canvas_citations(canvas_id)
            if citations:
                lines.append("## 参考文献\n")
                lines.append(format_reference_list(citations))
            content = "\n".join(lines)
    else:
        # JSON 格式：返回完整数据
        try:
            data = export_canvas(canvas_id)
            content = json_lib.dumps(data, ensure_ascii=False, indent=2)
        except ValueError as e:
            raise HTTPException(status_code=404, detail=str(e))
    return ExportResponse(
        format=format,
        content=content,
        session_id=c.session_id or "",
        canvas_id=canvas_id,
    )


@router.post("/{canvas_id}/refine-full", response_model=CanvasRefineResponse)
def canvas_refine_full(canvas_id: str, body: CanvasRefineRequest) -> CanvasRefineResponse:
    """对全文进行再次精炼，支持多轮迭代与回退。"""
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")

    source_md = (body.content_md or "").strip()
    if not source_md:
        source_md = (getattr(c, "refined_markdown", "") or "").strip()
    if not source_md:
        # fallback to exported markdown
        lines = []
        if c.working_title:
            lines.append(f"# {c.working_title}\n")
        if c.abstract:
            lines.append(f"## 摘要\n\n{c.abstract}\n")
        for _, block in c.drafts.items():
            if block.content_md:
                lines.append(block.content_md)
                lines.append("")
        source_md = "\n".join(lines).strip()
    if not source_md:
        raise HTTPException(status_code=400, detail="empty canvas content, nothing to refine")

    # Merge persistent + per-run directives
    directives: list[str] = []
    directives.extend(getattr(c, "user_directives", []) or [])
    directives.extend([d.strip() for d in (body.directives or []) if str(d or "").strip()])
    directives = list(dict.fromkeys(directives))
    directives_block = "\n".join(f"- {d}" for d in directives[:20]) if directives else "(none)"

    prompt_doc, locked_placeholders, locked_applied, locked_skipped = _prepare_locked_placeholders(
        source_md,
        body.locked_ranges or [],
    )
    locked_rule_line = "No locked segments in this run."
    if locked_placeholders:
        locked_rule_line = (
            "Keep every [[[LOCKED_SEGMENT_n]]] token exactly unchanged and in-place; "
            "these are protected sections and must not be edited."
        )

    cjk_n = len(re.findall(r"[\u4e00-\u9fff]", source_md))
    latin_n = len(re.findall(r"[A-Za-z]", source_md))
    if cjk_n >= latin_n:
        lang_hint = "Write in Chinese (中文)."
    else:
        lang_hint = "Write in English."

    from src.llm.llm_manager import get_manager
    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client()

    prompt = f"""You are a senior academic editor.
Refine the full markdown document for better coherence and readability.

Language:
- {lang_hint}

Hard constraints:
- Keep all citation/evidence tags unchanged (e.g., [cite_key], [evidence limited], [123abc])
- Keep factual meaning unchanged; do not invent claims
- Preserve heading hierarchy and section boundaries
- Apply user directives when compatible with evidence
- Only edit where necessary; keep unaffected sentences unchanged
- {locked_rule_line}
- Output markdown only

User directives:
{directives_block}

Document:
{prompt_doc}"""

    try:
        resp = client.chat(
            messages=[
                {"role": "system", "content": "You are precise and conservative in scientific editing."},
                {"role": "user", "content": prompt},
            ],
            max_tokens=5000,
            timeout_seconds=300,
        )
        edited = (resp.get("final_text") or "").strip()
    except requests.exceptions.Timeout:
        raise HTTPException(
            status_code=504,
            detail="LLM refine timeout: document may be too long or provider is slow; please retry with narrower directives or shorter content.",
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM refine failed: {e}")

    if not edited:
        edited = source_md
    else:
        edited, lock_guard_triggered, lock_guard_message = _restore_locked_placeholders(
            edited,
            source_md,
            locked_placeholders,
        )
        if lock_guard_triggered:
            return CanvasRefineResponse(
                edited_markdown=source_md,
                snapshot_version=None,
                locked_applied=locked_applied,
                locked_skipped=locked_skipped,
                lock_guard_triggered=True,
                lock_guard_message=lock_guard_message,
            )

        # Full-refine fallback guard: never accept output that drops existing citation/evidence tags.
        edited, guard_triggered, _ = _citation_guard(source_md, edited)
        if guard_triggered:
            edited = source_md

    snapshot_ver = None
    if body.save_snapshot_before:
        try:
            snapshot_ver = create_snapshot(canvas_id)
        except Exception:
            snapshot_ver = None

    update_canvas(
        canvas_id,
        refined_markdown=edited,
        stage="refine",
        version=int(getattr(c, "version", 1) or 1) + 1,
    )

    return CanvasRefineResponse(
        edited_markdown=edited,
        snapshot_version=snapshot_ver,
        locked_applied=locked_applied,
        locked_skipped=locked_skipped,
        lock_guard_triggered=False,
        lock_guard_message="",
    )


@router.post("/{canvas_id}/citations/filter", response_model=CitationFilterResponse)
def canvas_filter_citations(canvas_id: str, body: CitationFilterRequest) -> CitationFilterResponse:
    """筛选引用池：keep_keys 仅保留指定引用；remove_keys 删除指定引用。"""
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    if body.keep_keys is not None:
        removed = filter_canvas_citations(canvas_id, body.keep_keys)
        remaining = body.keep_keys
    elif body.remove_keys is not None:
        existing_keys = [cit.cite_key or cit.id for cit in get_canvas_citations(canvas_id)]
        keep_keys = [k for k in existing_keys if k not in body.remove_keys]
        removed = filter_canvas_citations(canvas_id, keep_keys)
        remaining = [cit.cite_key or cit.id for cit in get_canvas_citations(canvas_id)]
    else:
        raise HTTPException(status_code=400, detail="keep_keys or remove_keys is required")
    return CitationFilterResponse(removed_count=removed, remaining_keys=remaining)


@router.delete("/{canvas_id}/citations/{cite_key}")
def canvas_delete_citation(canvas_id: str, cite_key: str) -> dict:
    """删除指定 cite_key 的引用。"""
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    if not delete_canvas_citation(canvas_id, cite_key):
        raise HTTPException(status_code=404, detail="citation not found")
    return {"ok": True, "canvas_id": canvas_id, "removed_cite_key": cite_key}


@router.get("/{canvas_id}/citations")
def canvas_citations(
    canvas_id: str,
    format: str = Query("both", description="bibtex | text | ris | both"),
) -> dict:
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")
    citations = get_canvas_citations(canvas_id)
    if format == "bibtex":
        return {"format": "bibtex", "content": format_bibtex(citations)}
    if format == "text":
        return {"format": "text", "content": format_reference_list(citations)}
    if format == "ris":
        return {"format": "ris", "content": format_ris(citations)}
    return {
        "format": "both",
        "bibtex": format_bibtex(citations),
        "reference_list": format_reference_list(citations),
        "citations": [
            CitationResponse(
                cite_key=c.cite_key or c.id,
                title=c.title,
                authors=c.authors or [],
                year=c.year,
                doi=c.doi,
                url=c.url,
                bibtex=c.bibtex,
            )
            for c in citations
        ],
    }


# ---- AI Edit ----

_CONFIG_PATH = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"


def _extract_bracket_tags(text: str) -> list[str]:
    # Keep simple and conservative: treat bracketed tokens as evidence/citation-like tags.
    tags = re.findall(r"\[[^\[\]\n]{1,80}\]", text or "")
    seen: set[str] = set()
    out: list[str] = []
    for t in tags:
        if t not in seen:
            seen.add(t)
            out.append(t)
    return out


def _citation_guard(source_text: str, edited_text: str) -> tuple[str, bool, str]:
    source_tags = _extract_bracket_tags(source_text)
    if not source_tags:
        return edited_text, False, ""
    missing = [t for t in source_tags if t not in edited_text]
    if missing:
        preview = ", ".join(missing[:5])
        return (
            source_text,
            True,
            f"Detected citation/evidence loss, rolled back this edit. Missing tags: {preview}",
        )
    return edited_text, False, ""


def _prepare_locked_placeholders(
    source_text: str,
    locked_ranges: list[dict],
) -> tuple[str, dict[str, str], int, int]:
    if not locked_ranges:
        return source_text, {}, 0, 0

    normalized: list[tuple[int, int, str]] = []
    skipped = 0
    total_len = len(source_text)

    for raw in locked_ranges:
        if not isinstance(raw, dict):
            skipped += 1
            continue
        try:
            start = int(raw.get("start", -1))
            end = int(raw.get("end", -1))
            expected = str(raw.get("text", ""))
        except Exception:
            skipped += 1
            continue
        if start < 0 or end <= start or end > total_len:
            skipped += 1
            continue
        seg = source_text[start:end]
        if expected and seg != expected:
            skipped += 1
            continue
        normalized.append((start, end, seg))

    if not normalized:
        return source_text, {}, 0, skipped

    normalized.sort(key=lambda x: x[0])
    non_overlap: list[tuple[int, int, str]] = []
    last_end = -1
    for item in normalized:
        if item[0] < last_end:
            skipped += 1
            continue
        non_overlap.append(item)
        last_end = item[1]

    if not non_overlap:
        return source_text, {}, 0, skipped

    placeholders: dict[str, str] = {}
    parts: list[str] = []
    cursor = 0
    for i, (start, end, seg) in enumerate(non_overlap):
        token = f"[[[LOCKED_SEGMENT_{i}]]]"
        parts.append(source_text[cursor:start])
        parts.append(token)
        placeholders[token] = seg
        cursor = end
    parts.append(source_text[cursor:])
    return "".join(parts), placeholders, len(non_overlap), skipped


def _restore_locked_placeholders(
    edited_text: str,
    source_text: str,
    placeholders: dict[str, str],
) -> tuple[str, bool, str]:
    if not placeholders:
        return edited_text, False, ""
    missing = [t for t in placeholders.keys() if t not in edited_text]
    if missing:
        preview = ", ".join(missing[:3])
        return (
            source_text,
            True,
            f"Locked segments were modified/removed by model, rolled back this full refine. Missing tokens: {preview}",
        )
    restored = edited_text
    for token, seg in placeholders.items():
        restored = restored.replace(token, seg)
    return restored, False, ""


_AI_EDIT_PROMPTS = {
    "rewrite": (
        "请重写以下学术段落，使其更加清晰、简洁且符合学术写作规范。"
        "保持原始含义不变，仅改善表达。"
        "必须保留原文中的所有 [..] 引用/证据标签，禁止删除或改写标签。"
    ),
    "expand": (
        "请扩展以下学术段落，添加更多细节、解释和过渡句。"
        "保持与原文一致的学术风格和语气。"
        "必须保留原文中的所有 [..] 引用/证据标签，禁止删除或改写标签。"
    ),
    "condense": (
        "请精简以下学术段落，去除冗余表达，保留核心信息。"
        "目标长度约为原文的 50-70%。"
        "必须保留原文中的所有 [..] 引用/证据标签，禁止删除或改写标签。"
    ),
    "add_citations": (
        "请为以下段落中的关键论断添加引用标注（使用 [cite_key] 格式）。"
        "基于提供的参考资料匹配合适的引用。"
    ),
    "targeted_refine": (
        "请仅对指定段落做定向精炼，严格按用户指令修改。"
        "不要改动与指令无关的信息，不要扩散修改范围。"
        "必须保留原文中的所有 [..] 引用/证据标签，禁止删除或改写标签。"
        "输出仅包含修改后的该段落。"
    ),
}


@router.post("/{canvas_id}/ai-edit", response_model=CanvasAIEditResponse)
def canvas_ai_edit(canvas_id: str, body: CanvasAIEditRequest) -> CanvasAIEditResponse:
    """AI 段落级编辑：重写/扩展/精简/添加引用"""
    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")

    action = body.action
    if action not in _AI_EDIT_PROMPTS:
        raise HTTPException(status_code=400, detail=f"unknown action: {action}, expected: {list(_AI_EDIT_PROMPTS.keys())}")

    # 获取 LLM 客户端
    from src.llm.llm_manager import get_manager
    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client()

    # 可选：检索补充资料（用于 add_citations）
    retrieval_context = ""
    citations_added: list[str] = []
    if body.search_mode != "none" and action == "add_citations":
        try:
            from src.retrieval.service import get_retrieval_service
            svc = get_retrieval_service()
            pack = svc.search(query=body.section_text[:200], mode=body.search_mode, top_k=5)
            if pack.chunks:
                refs = []
                for ch in pack.chunks:
                    cite_key = ch.chunk_id or ch.doc_id
                    refs.append(f"[{cite_key}] {ch.text[:200]}")
                    citations_added.append(cite_key)
                retrieval_context = "\n\n可用参考资料：\n" + "\n".join(refs)
        except Exception:
            pass

    # 构建 prompt
    system = _AI_EDIT_PROMPTS[action]
    if retrieval_context:
        system += retrieval_context

    directive_block = ""
    if (body.directive or "").strip():
        directive_block = f"\n\n用户定向指令：\n{body.directive.strip()}"

    user_msg = body.section_text
    if body.context:
        user_msg = f"上下文：\n{body.context}\n\n需要编辑的段落：\n{body.section_text}{directive_block}"
    elif directive_block:
        user_msg = f"需要编辑的段落：\n{body.section_text}{directive_block}"

    messages = [
        {"role": "system", "content": system},
        {"role": "user", "content": user_msg},
    ]

    try:
        resp = client.chat(messages, max_tokens=2000)
        edited_text = (resp.get("final_text") or "").strip()
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM 调用失败: {e}")

    guard_triggered = False
    guard_message = ""
    if body.preserve_citations:
        edited_text, guard_triggered, guard_message = _citation_guard(body.section_text, edited_text)

    # 自动创建快照
    try:
        create_snapshot(canvas_id)
    except Exception:
        pass

    return CanvasAIEditResponse(
        edited_text=edited_text,
        citations_added=citations_added,
        citation_guard_triggered=guard_triggered,
        citation_guard_message=guard_message,
    )
</file>

<file path="src/api/routes_compare.py">
"""
多文档比较 API：选择多篇论文，生成结构化对比。
支持从会话引文聚合候选（GET /compare/candidates）与本地文库分页（GET /compare/papers）。
"""

import json
from pathlib import Path
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, HTTPException, Query
from pydantic import BaseModel, Field

from config.settings import settings
from src.collaboration.memory.session_memory import get_session_store
from src.log import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/compare", tags=["compare"])

_CONFIG_PATH = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"


class CompareRequest(BaseModel):
    paper_ids: List[str] = Field(..., min_length=2, max_length=5, description="要比较的 paper_id 列表 (2-5)")
    aspects: List[str] = Field(
        default_factory=lambda: ["objective", "methodology", "key_findings", "limitations"],
        description="比较维度",
    )
    llm_provider: Optional[str] = Field(None, description="LLM 提供商")
    model_override: Optional[str] = Field(None, description="覆盖默认模型")


class PaperSummary(BaseModel):
    paper_id: str = ""
    title: str = ""
    year: Optional[int] = None
    abstract: str = ""


class CompareResponse(BaseModel):
    papers: List[PaperSummary] = Field(default_factory=list)
    comparison_matrix: Dict[str, Dict[str, str]] = Field(
        default_factory=dict,
        description="对比矩阵: {aspect: {paper_id: description}}",
    )
    narrative: str = Field("", description="LLM 生成的对比叙述")


class _CompareLLMResponse(BaseModel):
    matrix: Dict[str, Dict[str, str]] = Field(default_factory=dict)
    narrative: str = ""


def _load_paper_data(paper_id: str) -> Optional[Dict[str, Any]]:
    """加载 enriched.json"""
    parsed_dir = settings.path.data / "parsed"
    # 尝试精确匹配和模糊匹配
    candidates = list(parsed_dir.glob(f"{paper_id}/enriched.json"))
    if not candidates:
        candidates = list(parsed_dir.glob(f"*{paper_id}*/enriched.json"))
    if not candidates:
        return None
    try:
        with open(candidates[0], "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return None


def _extract_paper_summary(paper_id: str, data: Dict[str, Any]) -> PaperSummary:
    """从 enriched.json 提取摘要信息"""
    if not isinstance(data, dict):
        data = {}

    # 尝试从 content_flow 找 abstract
    abstract = ""
    content_flow = data.get("content_flow")
    if not isinstance(content_flow, list):
        content_flow = []
    for block in content_flow:
        if not isinstance(block, dict):
            continue
        hp = block.get("heading_path", [])
        hp_lower = " ".join(str(h) for h in hp).lower()
        if "abstract" in hp_lower and block.get("text"):
            abstract = str(block.get("text") or "")[:500]
            break

    # 尝试从 global_summary 获取
    if not abstract:
        abstract = str(data.get("global_summary") or "")[:500]

    # 尝试从文件名解析年份
    year = None
    import re
    for part in paper_id.replace("-", "_").split("_"):
        if re.match(r"^(19|20)\d{2}$", part):
            year = int(part)
            break

    return PaperSummary(
        paper_id=paper_id,
        title=str(data.get("doc_id") or paper_id),
        year=year,
        abstract=abstract,
    )


def _extract_sections_text(data: Dict[str, Any], max_chars: int = 3000) -> str:
    """提取关键章节文本用于 LLM 比较"""
    if not isinstance(data, dict):
        return ""

    parts = []
    total = 0
    target_sections = ["abstract", "method", "result", "conclusion", "discussion", "finding"]

    content_flow = data.get("content_flow")
    if not isinstance(content_flow, list):
        content_flow = []
    for block in content_flow:
        if not isinstance(block, dict):
            continue
        hp = block.get("heading_path", [])
        hp_lower = " ".join(str(h) for h in hp).lower()
        text = str(block.get("text") or "")
        if not text.strip():
            continue
        if any(s in hp_lower for s in target_sections):
            if total + len(text) > max_chars:
                text = text[: max_chars - total]
            parts.append(text)
            total += len(text)
            if total >= max_chars:
                break

    return "\n\n".join(parts) if parts else ""


_COMPARE_PROMPT = """You are a scientific literature comparison expert.

Compare the following {n} papers across these aspects: {aspects}

For each paper, I provide an ID and key text excerpts.

{paper_blocks}

Return a JSON object with TWO fields:
1. "matrix": an object where keys are aspect names, and values are objects mapping paper_id to a brief description (1-2 sentences) for that aspect.
2. "narrative": a 3-5 sentence comparative analysis summarizing key similarities and differences.

Return ONLY valid JSON:
```json
{{
  "matrix": {{
    "objective": {{"paper1": "...", "paper2": "..."}},
    ...
  }},
  "narrative": "..."
}}
```"""


@router.post("", response_model=CompareResponse)
def compare_papers(body: CompareRequest) -> CompareResponse:
    """多文档对比：加载论文数据，LLM 生成结构化对比"""
    papers_data: List[tuple] = []
    summaries: List[PaperSummary] = []

    for pid in body.paper_ids:
        data = _load_paper_data(pid)
        if data is None:
            raise HTTPException(status_code=404, detail=f"论文 '{pid}' 未找到（请确认已解析）")
        papers_data.append((pid, data))
        summaries.append(_extract_paper_summary(pid, data))

    # 构建 LLM prompt
    paper_blocks = []
    for pid, data in papers_data:
        text = _extract_sections_text(data, max_chars=2000)
        if not text:
            text = "(No key sections extracted)"
        paper_blocks.append(f"--- Paper: {pid} ---\n{text}")

    aspects_str = ", ".join(body.aspects)
    prompt = _COMPARE_PROMPT.format(
        n=len(body.paper_ids),
        aspects=aspects_str,
        paper_blocks="\n\n".join(paper_blocks),
    )

    # 调用 LLM
    from src.llm.llm_manager import get_manager
    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client(body.llm_provider or None)

    try:
        resp = client.chat(
            messages=[
                {"role": "system", "content": "You are a scientific comparison expert. Return ONLY valid JSON."},
                {"role": "user", "content": prompt},
            ],
            model=body.model_override or None,
            max_tokens=3000,
            response_model=_CompareLLMResponse,
        )
        parsed: Optional[_CompareLLMResponse] = resp.get("parsed_object")
        if parsed is None:
            raw = (resp.get("final_text") or "").strip()
            if raw:
                parsed = _CompareLLMResponse.model_validate_json(raw)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"LLM 调用失败: {e}")

    matrix = parsed.matrix if parsed is not None else {}
    narrative = parsed.narrative if parsed is not None else (resp.get("final_text") or "")

    return CompareResponse(
        papers=summaries,
        comparison_matrix=matrix,
        narrative=narrative,
    )


def _citation_key(c: Dict[str, Any]) -> str:
    """Stable key for deduping: prefer doc_id/paper_id, then doi, url, title."""
    pid = (c.get("doc_id") or c.get("paper_id") or "").strip()
    if pid:
        return pid
    doi = (c.get("doi") or "").strip()
    if doi:
        return f"doi:{doi}"
    url = (c.get("url") or "").strip()
    if url:
        return f"url:{url}"
    title = (c.get("title") or "").strip() or "unknown"
    return f"title:{title}"


@router.get("/candidates")
def list_compare_candidates(
    session_id: str = Query(..., description="会话 ID"),
    scope: str = Query("session", description="session | current_turn"),
    limit: int = Query(50, ge=1, le=200),
    offset: int = Query(0, ge=0),
) -> Dict[str, Any]:
    """从会话引文中聚合对比候选：按 doc_id/doi/url/title 去重，带引用次数与最近引用轮次。"""
    store = get_session_store()
    meta = store.get_session_meta(session_id)
    if meta is None:
        raise HTTPException(status_code=404, detail="session not found")

    turns = store.get_turns(session_id)
    # key -> { paper_id, title, year, abstract, citation_count, last_cited_turn_index, is_local_ready }
    agg: Dict[str, Dict[str, Any]] = {}

    for turn_index, t in enumerate(turns):
        for c in (t.citations or []):
            key = _citation_key(c)
            if not key or key.startswith("title:unknown"):
                continue
            paper_id = (c.get("doc_id") or c.get("paper_id") or key).strip()
            if key.startswith("doi:"):
                paper_id = key
            elif key.startswith("url:"):
                paper_id = key
            elif key.startswith("title:"):
                paper_id = key

            if key not in agg:
                year = c.get("year")
                if isinstance(year, str) and year.isdigit():
                    year = int(year)
                agg[key] = {
                    "paper_id": paper_id,
                    "title": (c.get("title") or paper_id),
                    "year": year,
                    "abstract": "",
                    "citation_count": 0,
                    "last_cited_turn_index": turn_index,
                    "is_local_ready": False,
                }
            agg[key]["citation_count"] += 1
            agg[key]["last_cited_turn_index"] = max(
                agg[key]["last_cited_turn_index"], turn_index
            )

    # Resolve paper_id for local check: use doc_id if it matches a parsed dir
    parsed_dir = settings.path.data / "parsed"
    for key, row in agg.items():
        pid = row["paper_id"]
        if pid.startswith(("doi:", "url:", "title:")):
            row["is_local_ready"] = False
            continue
        data = _load_paper_data(pid)
        if data is not None:
            row["is_local_ready"] = True
            if not row.get("abstract"):
                row["abstract"] = (
                    _extract_paper_summary(pid, data).abstract or ""
                )[:500]

    # Sort by citation_count desc, then last_cited_turn_index desc; then paginate
    items = sorted(
        agg.values(),
        key=lambda x: (-x["citation_count"], -x["last_cited_turn_index"]),
    )
    total = len(items)
    page = items[offset : offset + limit]

    return {"candidates": page, "total": total}


@router.get("/papers")
def list_available_papers(
    limit: int = Query(50, ge=1, le=200),
    offset: int = Query(0, ge=0),
    q: Optional[str] = Query(None, description="搜索标题/paper_id"),
) -> Dict[str, Any]:
    """列出所有可用于比较的论文，支持分页与搜索。"""
    parsed_dir = settings.path.data / "parsed"
    if not parsed_dir.exists():
        return {"papers": [], "total": 0}

    papers = []
    for enriched_path in sorted(parsed_dir.glob("*/enriched.json")):
        pid = enriched_path.parent.name
        try:
            with open(enriched_path, "r", encoding="utf-8") as f:
                data = json.load(f)
            summary = _extract_paper_summary(pid, data)
            papers.append(summary.model_dump())
        except Exception:
            papers.append({"paper_id": pid, "title": pid, "year": None, "abstract": ""})

    if q and q.strip():
        ql = q.strip().lower()
        papers = [
            p
            for p in papers
            if ql in (p.get("paper_id") or "").lower()
            or ql in (p.get("title") or "").lower()
        ]

    total = len(papers)
    page = papers[offset : offset + limit]
    return {"papers": page, "total": total}
</file>

<file path="src/api/routes_export.py">
"""
导出 API：按 canvas 输出 Markdown / Word。

支持通过 cite_key_format 参数控制引用格式：
- numeric: [1], [2], [3]
- hash: [a3f7b2c91e04]
- author_date: [Smith2023]
"""

import io
import re

from fastapi import APIRouter, HTTPException, Response

from src.api.schemas import ExportRequest, ExportResponse
from src.collaboration.canvas.canvas_manager import get_canvas
from src.collaboration.export.formatter import export_canvas_markdown
from src.collaboration.memory.session_memory import get_session_store

router = APIRouter(prefix="/export", tags=["export"])


def _add_runs_from_html_node(
    paragraph,
    node,
    bold: bool = False,
    italic: bool = False,
    underline: bool = False,
    monospace: bool = False,
) -> None:
    """将 HTML 行内节点写入到 docx paragraph（基础格式）。"""
    from bs4 import NavigableString, Tag

    if isinstance(node, NavigableString):
        text = str(node)
        if text:
            run = paragraph.add_run(text)
            run.bold = bold
            run.italic = italic
            run.underline = underline
            if monospace:
                run.font.name = "Courier New"
        return
    if not isinstance(node, Tag):
        return

    tag = (node.name or "").lower()
    if tag in {"br"}:
        paragraph.add_run("\n")
        return
    next_bold = bold or tag in {"strong", "b"}
    next_italic = italic or tag in {"em", "i"}
    next_underline = underline or tag in {"u"}
    next_monospace = monospace or tag in {"code"}
    for child in node.contents:
        _add_runs_from_html_node(
            paragraph,
            child,
            bold=next_bold,
            italic=next_italic,
            underline=next_underline,
            monospace=next_monospace,
        )


def _markdown_to_docx_bytes(markdown_content: str) -> bytes:
    """将 Markdown 转换为基础 docx 字节流。"""
    try:
        import markdown2
        from bs4 import BeautifulSoup
        from docx import Document
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"docx export dependency missing: {e}")

    html = markdown2.markdown(markdown_content or "", extras=["fenced-code-blocks", "tables"])
    soup = BeautifulSoup(html, "html.parser")
    doc = Document()

    root = soup.body or soup
    for elem in root.children:
        tag = getattr(elem, "name", None)
        if not tag:
            text = str(elem).strip()
            if text:
                doc.add_paragraph(text)
            continue

        tag = tag.lower()
        if tag in {"h1", "h2", "h3", "h4", "h5", "h6"}:
            level = max(0, min(int(tag[1]) - 1, 5))
            p = doc.add_heading(level=level)
            for child in elem.contents:
                _add_runs_from_html_node(p, child)
        elif tag == "p":
            p = doc.add_paragraph()
            for child in elem.contents:
                _add_runs_from_html_node(p, child)
        elif tag in {"ul", "ol"}:
            list_style = "List Number" if tag == "ol" else "List Bullet"
            for li in elem.find_all("li", recursive=False):
                p = doc.add_paragraph(style=list_style)
                for child in li.contents:
                    _add_runs_from_html_node(p, child)
        elif tag == "pre":
            doc.add_paragraph(elem.get_text())
        else:
            text = elem.get_text(" ", strip=True)
            if text:
                doc.add_paragraph(text)

    out = io.BytesIO()
    doc.save(out)
    return out.getvalue()


def _safe_filename(name: str) -> str:
    s = re.sub(r"[^A-Za-z0-9._-]+", "_", (name or "").strip()).strip("._")
    return s or "research_draft"


@router.post("", response_model=ExportResponse)
def export_canvas(body: ExportRequest) -> ExportResponse | Response:
    canvas_id = body.canvas_id or ""
    if not canvas_id and body.session_id:
        meta = get_session_store().get_session_meta(body.session_id)
        if meta:
            canvas_id = meta.get("canvas_id") or ""
    if not canvas_id:
        raise HTTPException(status_code=400, detail="canvas_id or session_id is required")

    c = get_canvas(canvas_id)
    if c is None:
        raise HTTPException(status_code=404, detail="canvas not found")

    fmt = (body.format or "markdown").lower()
    if fmt not in {"markdown", "docx"}:
        raise HTTPException(status_code=400, detail="only markdown and docx format are supported")

    # 支持 cite_key_format 参数覆盖配置
    cite_key_format = body.cite_key_format
    if cite_key_format and cite_key_format not in ("numeric", "hash", "author_date"):
        raise HTTPException(
            status_code=400,
            detail=f"invalid cite_key_format: {cite_key_format}, must be one of: numeric, hash, author_date"
        )

    content = export_canvas_markdown(c, cite_key_format=cite_key_format)
    if fmt == "docx":
        docx_bytes = _markdown_to_docx_bytes(content)
        filename = _safe_filename(c.working_title or c.topic or canvas_id) + ".docx"
        return Response(
            content=docx_bytes,
            media_type="application/vnd.openxmlformats-officedocument.wordprocessingml.document",
            headers={"Content-Disposition": f'attachment; filename="{filename}"'},
        )

    return ExportResponse(format="markdown", content=content, canvas_id=canvas_id, session_id=body.session_id or "")
</file>

<file path="src/api/routes_graph.py">
"""
知识图谱 API：统计、实体列表、邻居查询。
"""

import json
from pathlib import Path
from typing import Any, Dict, List, Optional

from fastapi import APIRouter, HTTPException, Query
from fastapi.responses import FileResponse

from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/graph", tags=["graph"])


def _get_hippo():
    """获取 HippoRAG 实例"""
    try:
        from src.graph.hippo_rag import get_hippo_rag
        graph_path = settings.path.data / "hippo_graph.json"
        if not graph_path.exists():
            return None
        return get_hippo_rag(graph_path)
    except Exception as e:
        logger.warning(f"HippoRAG 加载失败: {e}")
        return None


def _query_chunk_in_collection(collection_name: str, chunk_id: str) -> Optional[Dict[str, Any]]:
    from src.indexing.milvus_ops import milvus
    escaped_chunk_id = chunk_id.replace('"', '\\"')

    try:
        rows = milvus.query(
            collection_name,
            filter=f'chunk_id == "{escaped_chunk_id}"',
            output_fields=["chunk_id", "paper_id", "content", "raw_content", "section_path", "page", "content_type", "chunk_type", "bbox"],
            limit=1,
        )
    except Exception as e:
        logger.warning("query chunk failed collection=%s chunk=%s err=%s", collection_name, chunk_id, e)
        return None

    if not rows:
        return None
    row = rows[0] if isinstance(rows, list) else rows
    if not isinstance(row, dict):
        return None
    return {
        "collection": collection_name,
        "chunk_id": row.get("chunk_id") or chunk_id,
        "paper_id": row.get("paper_id") or "",
        "content": (row.get("raw_content") or row.get("content") or ""),
        "section_path": row.get("section_path") or "",
        "page": row.get("page"),
        "content_type": row.get("content_type") or "",
        "chunk_type": row.get("chunk_type") or "",
        "bbox": row.get("bbox"),
    }


def _find_enriched_json_by_paper_id(paper_id: str) -> Optional[Path]:
    parsed_dir = settings.path.data / "parsed"
    direct = parsed_dir / paper_id / "enriched.json"
    if direct.exists():
        return direct
    candidates = list(parsed_dir.glob(f"*{paper_id}*/enriched.json"))
    if candidates:
        return candidates[0]
    return None


def _query_chunk_from_parsed(paper_id: str, chunk_id: str) -> Optional[Dict[str, Any]]:
    json_path = _find_enriched_json_by_paper_id(paper_id)
    if not json_path:
        return None
    try:
        from src.chunking.chunker import ChunkConfig, chunk_blocks

        with open(json_path, "r", encoding="utf-8") as f:
            doc = json.load(f)
        doc_id = str(doc.get("doc_id") or paper_id)
        content_flow = doc.get("content_flow") or []
        claims = doc.get("claims") or []

        # 先按 settings 配置切一次，再按默认配置切一次，兼容历史构建批次差异
        cfg_candidates = [
            ChunkConfig(
                target_chars=settings.chunk.target_chars,
                min_chars=settings.chunk.min_chars,
                max_chars=settings.chunk.max_chars,
                overlap_sentences=settings.chunk.overlap_sentences,
                table_rows_per_chunk=settings.chunk.table_rows_per_chunk,
            ),
            ChunkConfig(),
        ]
        for cfg in cfg_candidates:
            chunks = chunk_blocks(content_flow, doc_id=doc_id, config=cfg, claims=claims)
            for c in chunks:
                if str(c.chunk_id) != chunk_id:
                    continue
                meta = c.meta or {}
                page_range = meta.get("page_range") or [None, None]
                page = page_range[0] if isinstance(page_range, (list, tuple)) and page_range else None
                return {
                    "collection": "parsed_fallback",
                    "chunk_id": str(c.chunk_id),
                    "paper_id": doc_id,
                    "content": c.text or "",
                    "section_path": meta.get("section_path") or "",
                    "page": page,
                    "content_type": c.content_type or "",
                    "chunk_type": ",".join(meta.get("block_types", [])) or "",
                    "bbox": meta.get("bbox"),
                }
    except Exception as e:
        logger.warning("query chunk from parsed failed paper=%s chunk=%s err=%s", paper_id, chunk_id, e)
        return None
    return None


@router.get("/stats")
def graph_stats() -> Dict[str, Any]:
    """获取图谱统计信息"""
    hippo = _get_hippo()
    if hippo is None:
        return {
            "available": False,
            "total_nodes": 0,
            "total_edges": 0,
            "entity_count": 0,
            "chunk_count": 0,
            "entity_types": {},
        }
    stats = hippo.stats()
    return {"available": True, **stats}


@router.get("/entities")
def graph_entities(
    entity_type: Optional[str] = Query(None, description="按类型过滤: SPECIES|LOCATION|PHENOMENON|METHOD|SUBSTANCE"),
    limit: int = Query(200, ge=1, le=1000, description="最大返回数量"),
    offset: int = Query(0, ge=0, description="偏移量"),
    q: Optional[str] = Query(None, description="按名称搜索（子串匹配）"),
) -> Dict[str, Any]:
    """获取实体列表"""
    hippo = _get_hippo()
    if hippo is None:
        raise HTTPException(status_code=404, detail="图谱未加载")

    entities = []
    for name, entity in hippo.entities.items():
        if entity_type and entity.type != entity_type:
            continue
        if q and q.lower() not in name.lower():
            continue
        entities.append({
            "name": name,
            "type": entity.type,
            "mention_count": len(entity.mentions),
        })

    # 按 mention_count 降序排序
    entities.sort(key=lambda e: -e["mention_count"])
    total = len(entities)
    page = entities[offset: offset + limit]

    return {"total": total, "offset": offset, "limit": limit, "entities": page}


@router.get("/neighbors/{entity_name}")
def graph_neighbors(
    entity_name: str,
    depth: int = Query(1, ge=1, le=3, description="扩展深度 (1-3)"),
) -> Dict[str, Any]:
    """
    获取指定实体的邻居子图。

    返回以 entity_name 为中心的 nodes + edges 数据（可直接用于前端图谱渲染）。
    """
    hippo = _get_hippo()
    if hippo is None:
        raise HTTPException(status_code=404, detail="图谱未加载")

    G = hippo.G
    if entity_name not in G:
        raise HTTPException(status_code=404, detail=f"实体 '{entity_name}' 不存在")

    # BFS 收集 n-hop 邻居
    visited: set = set()
    frontier = {entity_name}
    edges_set: set = set()

    for _ in range(depth):
        next_frontier: set = set()
        for node in frontier:
            if node in visited:
                continue
            visited.add(node)
            for _, neighbor, data in G.edges(node, data=True):
                edges_set.add((node, neighbor, data.get("relation", ""), data.get("weight", 1)))
                next_frontier.add(neighbor)
            # 也查入边
            for predecessor, _, data in G.in_edges(node, data=True):
                edges_set.add((predecessor, node, data.get("relation", ""), data.get("weight", 1)))
                next_frontier.add(predecessor)
        frontier = next_frontier - visited

    # 所有涉及的节点
    all_node_ids = set()
    for s, t, *_ in edges_set:
        all_node_ids.add(s)
        all_node_ids.add(t)
    all_node_ids.add(entity_name)

    # 构建 nodes
    nodes = []
    for nid in all_node_ids:
        node_data = G.nodes.get(nid, {})
        ntype = node_data.get("type", "ENTITY")
        # 从 entities 表获取更多信息
        entity_info = hippo.entities.get(nid)
        if entity_info:
            ntype = entity_info.type
        nodes.append({
            "id": nid,
            "type": ntype,
            "paper_id": node_data.get("paper_id", ""),
            "is_center": nid == entity_name,
        })

    # 构建 edges
    edges = [
        {"source": s, "target": t, "relation": rel, "weight": w}
        for s, t, rel, w in edges_set
    ]

    return {
        "center": entity_name,
        "depth": depth,
        "nodes": nodes,
        "edges": edges,
    }


@router.get("/chunk/{chunk_id}")
def graph_chunk_detail(
    chunk_id: str,
    collection: Optional[str] = Query(None, description="优先查询的集合名"),
    paper_id: Optional[str] = Query(None, description="可选，前端透传用于展示"),
) -> Dict[str, Any]:
    """获取 chunk 详情文本与元数据（用于图谱节点点击弹窗）。"""
    from src.indexing.milvus_ops import milvus

    candidates: List[str] = []
    if collection:
        candidates.append(collection)

    try:
        for name in milvus.client.list_collections():
            if name not in candidates:
                candidates.append(name)
    except Exception:
        # 无法列集合时至少尝试默认集合配置
        for name in settings.collection.all():
            if name not in candidates:
                candidates.append(name)

    for cname in candidates:
        found = _query_chunk_in_collection(cname, chunk_id)
        if found:
            # 追加图上相邻实体，便于解释该 chunk 的关系来源
            related_entities: List[str] = []
            hippo = _get_hippo()
            if hippo is not None and chunk_id in hippo.G:
                for predecessor, _, _ in hippo.G.in_edges(chunk_id, data=True):
                    ptype = hippo.G.nodes.get(predecessor, {}).get("type", "ENTITY")
                    if ptype != "CHUNK":
                        related_entities.append(str(predecessor))
            found["related_entities"] = sorted(set(related_entities))[:30]
            if paper_id and not found.get("paper_id"):
                found["paper_id"] = paper_id
            return found

    # 回退：Milvus 不存在时，尝试从 parsed/enriched.json 重建并匹配 chunk_id
    if paper_id:
        fallback = _query_chunk_from_parsed(paper_id, chunk_id)
        if fallback:
            related_entities: List[str] = []
            hippo = _get_hippo()
            if hippo is not None and chunk_id in hippo.G:
                for predecessor, _, _ in hippo.G.in_edges(chunk_id, data=True):
                    ptype = hippo.G.nodes.get(predecessor, {}).get("type", "ENTITY")
                    if ptype != "CHUNK":
                        related_entities.append(str(predecessor))
            fallback["related_entities"] = sorted(set(related_entities))[:30]
            return fallback

    raise HTTPException(status_code=404, detail=f"chunk '{chunk_id}' 未找到")


@router.get("/pdf/{paper_id}")
def graph_pdf_stream(paper_id: str):
    """返回原始 PDF 文件流，供前端 PDF 高亮溯源使用。"""
    pdf_path = settings.path.raw_papers / f"{paper_id}.pdf"
    if not pdf_path.exists():
        raise HTTPException(status_code=404, detail=f"PDF '{paper_id}.pdf' 未找到")
    return FileResponse(
        path=str(pdf_path),
        media_type="application/pdf",
        filename=f"{paper_id}.pdf",
    )
</file>

<file path="src/api/server.py">
"""
FastAPI 应用入口 - 多轮对话 API
"""

import asyncio
from contextlib import asynccontextmanager

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from config.settings import settings
from src.api.routes_auth import router as auth_router, admin_router
from src.api.routes_canvas import router as canvas_router
from src.api.routes_chat import router as chat_router
from src.api.routes_export import router as export_router
from src.api.routes_auto import router as auto_router
from src.api.routes_project import router as project_router
from src.api.routes_models import router as models_router
from src.api.routes_ingest import router as ingest_router
from src.api.routes_graph import router as graph_router
from src.api.routes_compare import router as compare_router
from src.log import get_logger
from src.utils.storage_cleaner import run_cleanup, get_storage_stats
from src.utils.task_runner import cleanup_stale_jobs, run_background_worker
from src.observability import setup_observability

logger = get_logger(__name__)


@asynccontextmanager
async def lifespan(app: FastAPI):
    """应用生命周期：清理残留任务 → 存储清理 → 启动后台 Worker"""

    # 1. 将上次进程中断时残留的 running/cancelling 任务重置为 error
    cleanup_stale_jobs()

    # 2. 常规存储清理
    if settings.storage.cleanup_on_startup:
        try:
            result = run_cleanup(
                max_age_days=settings.storage.max_age_days,
                max_size_gb=settings.storage.max_size_gb,
                batch_size=settings.storage.cleanup_batch_size,
            )
            stats = get_storage_stats()
            logger.info(f"[startup] storage cleanup done: {result}, current={stats}")
        except Exception as e:
            logger.warning(f"[startup] storage cleanup failed: {e}")

    # 3. 启动后台任务轮询 Worker
    worker_task = asyncio.create_task(run_background_worker())

    yield

    # Shutdown: 取消 Worker
    worker_task.cancel()
    try:
        await worker_task
    except asyncio.CancelledError:
        pass


app = FastAPI(
    title="DeepSea RAG Chat API",
    description="多轮对话与综述协作 API",
    version="0.1.0",
    lifespan=lifespan,
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(auth_router)
app.include_router(admin_router)
app.include_router(project_router)
app.include_router(chat_router)
app.include_router(canvas_router)
app.include_router(export_router)
app.include_router(auto_router)
app.include_router(models_router)
app.include_router(ingest_router)
app.include_router(graph_router)
app.include_router(compare_router)

# Observability: 中间件 + /metrics + /health/detailed
setup_observability(app)


@app.get("/health")
def health() -> dict:
    return {"status": "ok"}


@app.get("/storage/stats")
def storage_stats() -> dict:
    """获取存储统计信息"""
    return get_storage_stats()
</file>

<file path="src/collaboration/canvas/models.py">
"""
综述画布 - 数据模型（含大纲与草稿）

Citation / KnowledgeFragment / OutlineSection / DraftBlock / SurveyCanvas。
"""

import uuid
from dataclasses import dataclass, field
from datetime import datetime
from typing import Dict, List, Literal, Optional


@dataclass
class OutlineSection:
    """大纲章节"""

    id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    title: str = ""
    level: int = 1  # 1=章, 2=节, 3=小节
    order: int = 0
    parent_id: Optional[str] = None
    status: Literal["todo", "drafting", "done"] = "todo"
    guidance: Optional[str] = None


@dataclass
class DraftBlock:
    """草稿块（对应 OutlineSection）"""

    section_id: str = ""
    content_md: str = ""
    version: int = 1
    used_fragment_ids: List[str] = field(default_factory=list)
    used_citation_ids: List[str] = field(default_factory=list)
    updated_at: datetime = field(default_factory=datetime.now)


@dataclass
class Citation:
    """引文记录"""

    id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    title: str = ""
    authors: List[str] = field(default_factory=list)
    year: Optional[int] = None
    doc_id: Optional[str] = None
    url: Optional[str] = None
    doi: Optional[str] = None
    bibtex: Optional[str] = None
    cite_key: Optional[str] = None
    bbox: Optional[list[float]] = None
    page_num: Optional[int] = None
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class KnowledgeFragment:
    """原子化知识片段（含来源追溯）"""

    id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    text: str = ""
    source_chunk_id: str = ""
    citation_id: str = ""
    linked_section_id: Optional[str] = None
    confidence: Literal["high", "medium", "low"] = "medium"
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class Annotation:
    """行内批注（用户在 Refine 阶段对具体段落提出的修改意见）"""

    id: str = field(default_factory=lambda: str(uuid.uuid4())[:8])
    section_id: str = ""           # 关联章节（可为空表示全局批注）
    target_text: str = ""          # 选中的原文片段
    directive: str = ""            # 用户的修改意见
    status: Literal["pending", "applied", "rejected"] = "pending"
    created_at: datetime = field(default_factory=datetime.now)


@dataclass
class ResearchBrief:
    """研究简报 - Explore 阶段的结构化输出（对应 Survey Canvas 的 Goal/Hypothesis/Questions 等板块）"""

    scope: str = ""                          # 研究范围/边界
    success_criteria: List[str] = field(default_factory=list)  # 完成标准 / 假设
    key_questions: List[str] = field(default_factory=list)     # 核心问题
    exclusions: List[str] = field(default_factory=list)        # 明确排除的内容
    time_range: str = ""                     # 文献时间范围
    source_priority: List[str] = field(default_factory=list)   # 优先来源类型
    action_plan: str = ""                    # 行动计划：拿到数据后怎么用


@dataclass
class SurveyCanvas:
    """综述画布 - 含 topic / outline / drafts / citations / fragments"""

    id: str = field(default_factory=lambda: str(uuid.uuid4()))
    session_id: str = ""
    topic: str = ""
    working_title: str = ""
    abstract: str = ""
    keywords: List[str] = field(default_factory=list)
    stage: Literal["explore", "outline", "drafting", "refine"] = "explore"
    refined_markdown: str = ""  # 全文精炼稿（Refine 阶段可反复迭代）
    outline: List[OutlineSection] = field(default_factory=list)
    drafts: Dict[str, DraftBlock] = field(default_factory=dict)  # key: section_id
    citation_pool: Dict[str, Citation] = field(default_factory=dict)
    knowledge_pool: Dict[str, KnowledgeFragment] = field(default_factory=dict)
    identified_gaps: List[str] = field(default_factory=list)
    user_directives: List[str] = field(default_factory=list)
    annotations: List[Annotation] = field(default_factory=list)
    research_brief: Optional[ResearchBrief] = None
    # 研究洞察：来自 Research Insights Ledger 的人类可读摘要
    research_insights: List[str] = field(default_factory=list)
    # 阶段跳过控制（Deep Research 流程中用户选择跳过的阶段）
    skip_draft_review: bool = False
    skip_refine_review: bool = False
    version: int = 1
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)

    def get_section_by_id(self, section_id: str) -> Optional[OutlineSection]:
        for s in self.outline:
            if s.id == section_id:
                return s
        return None

    def get_fragments_for_section(self, section_id: str) -> List[KnowledgeFragment]:
        return [f for f in self.knowledge_pool.values() if f.linked_section_id == section_id]

    def add_citation(self, citation: Citation) -> str:
        self.citation_pool[citation.id] = citation
        return citation.id

    def add_fragment(self, fragment: KnowledgeFragment) -> str:
        self.knowledge_pool[fragment.id] = fragment
        return fragment.id
</file>

<file path="src/collaboration/citation/manager.py">
"""
引用管理：EvidencePack → Citation，cite_key 多格式生成，与 Canvas citation_pool 同步。

支持三种 cite_key 格式：
- numeric: 数字序号 [1], [2], [3]
- hash: 12位哈希 [a3f7b2c91e04]
- author_date: 作者年份 [Smith2023]，重复时加后缀 [Smith2023a]
"""

import hashlib
import re
import unicodedata
from typing import Dict, List, Literal, Optional, Set

from config.settings import settings
from src.collaboration.canvas.models import Citation
from src.retrieval.evidence import EvidenceChunk, EvidencePack


CiteKeyFormat = Literal["numeric", "hash", "author_date"]


class CiteKeyGenerator:
    """统一的引用键生成器，支持 numeric / hash / author_date 三种格式。"""

    def __init__(
        self,
        format: Optional[CiteKeyFormat] = None,
        existing_keys: Optional[Set[str]] = None,
        hash_length: Optional[int] = None,
        max_authors: Optional[int] = None,
    ):
        """
        初始化生成器。

        Args:
            format: 引用键格式，默认从配置读取
            existing_keys: 已存在的引用键集合（用于去重和编号）
            hash_length: hash 模式下的长度
            max_authors: author_date 模式下最多显示几个作者
        """
        self.format: CiteKeyFormat = format or settings.citation.key_format
        self.existing_keys: Set[str] = set(existing_keys) if existing_keys else set()
        self.hash_length = hash_length or settings.citation.hash_length
        self.max_authors = max_authors or settings.citation.author_date_max_authors
        self._numeric_counter = len(self.existing_keys) + 1

    def generate(self, citation: Citation) -> str:
        """根据配置的格式生成 cite_key。"""
        if self.format == "numeric":
            return self._numeric_key()
        elif self.format == "hash":
            return self._hash_key(citation)
        else:  # author_date
            return self._author_date_key(citation)

    def _numeric_key(self) -> str:
        """生成数字序号：1, 2, 3..."""
        key = str(self._numeric_counter)
        self._numeric_counter += 1
        self.existing_keys.add(key)
        return key

    def _hash_key(self, c: Citation) -> str:
        """生成哈希键：基于内容的 SHA256 前 N 位。"""
        raw = f"{c.title or ''}|{','.join(c.authors or [])}|{c.year or ''}|{c.doi or ''}|{c.url or ''}|{c.doc_id or ''}"
        key = hashlib.sha256(raw.encode("utf-8")).hexdigest()[: self.hash_length]
        self.existing_keys.add(key)
        return key

    def _author_date_key(self, c: Citation) -> str:
        """生成作者年份键：Smith2023，重复时 Smith2023a, Smith2023b..."""
        base = self._extract_author_year(c)
        if base not in self.existing_keys:
            self.existing_keys.add(base)
            return base
        # 添加后缀 a-z
        for suffix in "abcdefghijklmnopqrstuvwxyz":
            key = f"{base}{suffix}"
            if key not in self.existing_keys:
                self.existing_keys.add(key)
                return key
        # 用尽26个字母，fallback 到 base_xxxx
        fallback = f"{base}_{hashlib.sha256(c.id.encode()).hexdigest()[:4]}"
        self.existing_keys.add(fallback)
        return fallback

    def _extract_author_year(self, c: Citation) -> str:
        """提取作者姓氏 + 年份，如 Smith2023 或 SmithJones2023。"""
        authors = c.authors or []
        year_part = str(c.year) if c.year else ""

        if not authors:
            # 无作者信息，使用标题首词
            if c.title:
                first_word = re.split(r"\s+", c.title.strip())[0]
                first_word = self._normalize_name(first_word)[:10]
                return f"{first_word}{year_part}" if first_word else f"Anon{year_part}"
            return f"Anon{year_part}"

        # 提取作者姓氏
        surnames = []
        for author in authors[: self.max_authors]:
            surname = self._extract_surname(author)
            if surname:
                surnames.append(surname)

        if len(authors) > self.max_authors:
            author_part = "".join(surnames) + "EtAl"
        else:
            author_part = "".join(surnames)

        return f"{author_part}{year_part}" if author_part else f"Anon{year_part}"

    def _extract_surname(self, author: str) -> str:
        """从作者全名提取姓氏（支持中英文）。"""
        author = author.strip()
        if not author:
            return ""

        # 检测是否为中文名（无空格且含汉字）
        if " " not in author and any("\u4e00" <= ch <= "\u9fff" for ch in author):
            # 中文名：取第一个字作为姓
            return self._normalize_name(author[0])

        # 西文名：常见格式 "First Last" 或 "Last, First"
        if "," in author:
            # "Smith, John" 格式
            parts = author.split(",")
            surname = parts[0].strip()
        else:
            # "John Smith" 格式
            parts = author.split()
            surname = parts[-1] if parts else ""

        return self._normalize_name(surname)

    def _normalize_name(self, name: str) -> str:
        """规范化名字：移除特殊字符，首字母大写。"""
        # 移除重音符号
        name = unicodedata.normalize("NFD", name)
        name = "".join(ch for ch in name if unicodedata.category(ch) != "Mn")
        # 只保留字母和数字
        name = re.sub(r"[^a-zA-Z0-9\u4e00-\u9fff]", "", name)
        # 首字母大写
        return name.capitalize() if name else ""


# 模块级生成器（懒初始化）
_generator: Optional[CiteKeyGenerator] = None


def get_generator(
    format: Optional[CiteKeyFormat] = None,
    existing_keys: Optional[Set[str]] = None,
) -> CiteKeyGenerator:
    """获取或创建 cite_key 生成器。"""
    global _generator
    if _generator is None or format is not None or existing_keys is not None:
        _generator = CiteKeyGenerator(format=format, existing_keys=existing_keys)
    return _generator


def reset_generator():
    """重置生成器（测试或切换格式时使用）。"""
    global _generator
    _generator = None


# ============================================================
# 兼容原有接口
# ============================================================


def _make_cite_key(
    title: str,
    authors: List[str],
    year: int | None,
    doi: str | None,
    url: str | None,
    doc_id: str | None,
    format: Optional[CiteKeyFormat] = None,
) -> str:
    """生成 cite_key（兼容旧接口，推荐使用 CiteKeyGenerator）。"""
    # 创建临时 Citation 对象
    c = Citation(
        title=title,
        authors=authors,
        year=year,
        doi=doi,
        url=url,
        doc_id=doc_id,
    )
    gen = get_generator(format=format)
    return gen.generate(c)


def chunk_to_citation(
    chunk: EvidenceChunk,
    format: Optional[CiteKeyFormat] = None,
    generator: Optional[CiteKeyGenerator] = None,
) -> Citation:
    """将 EvidenceChunk 转换为 Citation。"""
    title = chunk.doc_title or chunk.chunk_id or ""
    authors = list(chunk.authors) if chunk.authors else []
    year = getattr(chunk, "year", None)
    doi = getattr(chunk, "doi", None) or None
    url = getattr(chunk, "url", None) or None
    doc_id = chunk.doc_id or None

    bbox = getattr(chunk, "bbox", None) or None
    page_num = getattr(chunk, "page_num", None) or None

    # 先创建不带 cite_key 的 Citation
    citation = Citation(
        id=chunk.chunk_id[:16] if chunk.chunk_id else "",
        title=title,
        authors=authors,
        year=year,
        doc_id=doc_id,
        url=url,
        doi=doi,
        cite_key="",
        bbox=bbox,
        page_num=page_num,
    )

    # 使用生成器生成 cite_key
    gen = generator or get_generator(format=format)
    citation.cite_key = gen.generate(citation)

    # 如果 id 为空，使用 cite_key
    if not citation.id:
        citation.id = citation.cite_key

    return citation


def _dedupe_citations(citations: List[Citation]) -> List[Citation]:
    """去重：基于 cite_key 或 id。"""
    seen_keys: Set[str] = set()
    out: List[Citation] = []
    for c in citations:
        key = c.cite_key or c.id
        if key in seen_keys:
            continue
        seen_keys.add(key)
        out.append(c)
    return out


def merge_citations_by_document(citations: List[Citation]) -> List[Citation]:
    """
    按文档级别合并引文：同一文档/同一 URL 只保留一条。
    - 本地文献：按 doc_id 合并
    - Web 结果：按 URL 合并
    保留首次出现的条目（保持原有顺序）。
    """
    merged: Dict[str, Citation] = {}
    for c in citations:
        if c.url:
            key = (c.url or "").strip()
        else:
            key = (c.doc_id or c.cite_key or c.id or "").strip()
        if not key:
            key = c.cite_key or c.id
        if key not in merged:
            merged[key] = c
    return list(merged.values())


def resolve_response_citations(
    response_text: str,
    chunks: List[EvidenceChunk],
    format: Optional[CiteKeyFormat] = None,
    doc_key_to_cite_key: Optional[Dict[str, str]] = None,
    existing_cite_keys: Optional[Set[str]] = None,
    include_unreferenced_documents: bool = True,
) -> tuple[str, List[Citation], Dict[str, str]]:
    """
    对 LLM 回答做引文后处理：将 [ref_hash] 替换为正式 cite_key，并输出文档级引文列表。

    流程：
      1. 扫描 response_text 中的 [hex_hash] 模式
      2. hash → chunk → doc_group_key，按文档去重
      3. 按首次出现顺序为每个文档生成正式 cite_key
      4. 替换文本中所有 hash 为对应 cite_key
      5. 构建文档级 Citation 列表

    Args:
        response_text: LLM 原始回答（包含 [ref_hash] 引用）
        chunks:        本轮检索返回的所有 EvidenceChunk
        format:        cite_key 格式，默认读配置

    Returns:
        (resolved_text, citations, ref_map):
        - resolved_text: 替换后的回答文本
        - citations:     文档级 Citation 列表（按首次引用顺序）
        - ref_map:       ref_hash → cite_key 映射（供前端 / 调试）

    Notes:
        - 当传入 doc_key_to_cite_key / existing_cite_keys 时，会在原有映射基础上复用并增量生成，
          适用于跨阶段（write/refine/synthesize）保持稳定引用键。
        - include_unreferenced_documents=True 时，会把未在文本中出现但存在于 chunks 的文档也纳入 citations。
    """
    from collections import OrderedDict
    from src.retrieval.evidence import REF_HASH_LENGTH

    # ── 1. 建立 hash → chunk 查找表 ──
    hash_to_chunk: Dict[str, EvidenceChunk] = {}
    for c in chunks:
        hash_to_chunk[c.ref_hash] = c

    # ── 2. 扫描回答中出现的 hash ──
    pattern = re.compile(r"\[([0-9a-fA-F]{" + str(REF_HASH_LENGTH) + r"})\]")
    cited_hashes: List[str] = []
    seen_hashes: Set[str] = set()
    for m in pattern.finditer(response_text):
        h = m.group(1).lower()
        if h in hash_to_chunk and h not in seen_hashes:
            cited_hashes.append(h)
            seen_hashes.add(h)

    # 复用外部映射（可跨阶段保持稳定 cite_key）
    shared_doc_map = doc_key_to_cite_key if doc_key_to_cite_key is not None else {}
    shared_keys = existing_cite_keys if existing_cite_keys is not None else set()

    # ── 3. 按文档分组（保持首次引用顺序）──
    doc_groups: OrderedDict[str, List[EvidenceChunk]] = OrderedDict()
    for h in cited_hashes:
        chunk = hash_to_chunk[h]
        key = chunk.doc_group_key
        doc_groups.setdefault(key, []).append(chunk)

    # 未被引用的文档也追加到末尾（可选）
    if include_unreferenced_documents:
        for c in chunks:
            key = c.doc_group_key
            if key not in doc_groups:
                doc_groups.setdefault(key, []).append(c)

    # ── 4. 为每个文档生成 cite_key ──
    seed_keys = set(shared_keys) if shared_keys else set()
    if shared_doc_map:
        seed_keys.update(shared_doc_map.values())
    gen = CiteKeyGenerator(format=format, existing_keys=seed_keys)
    citations: List[Citation] = []
    for doc_key, group in doc_groups.items():
        if doc_key in shared_doc_map:
            cite_key = shared_doc_map[doc_key]
        else:
            # 先生成 cite_key，再写入共享映射
            temp = _pick_best_metadata(group)
            temp_citation = Citation(
                id=doc_key[:16],
                title=temp.doc_title or "",
                authors=list(temp.authors) if temp.authors else [],
                year=temp.year,
                doc_id=temp.doc_id,
                url=temp.url,
                doi=getattr(temp, "doi", None),
                cite_key="",
            )
            cite_key = gen.generate(temp_citation)
            shared_doc_map[doc_key] = cite_key
            shared_keys.add(cite_key)

        best = _pick_best_metadata(group)
        citation = Citation(
            id=doc_key[:16],
            title=best.doc_title or "",
            authors=list(best.authors) if best.authors else [],
            year=best.year,
            doc_id=best.doc_id,
            url=best.url,
            doi=getattr(best, "doi", None),
            cite_key=cite_key,
        )
        citations.append(citation)

    # ── 5. 构建 ref_hash → cite_key 映射 ──
    ref_map: Dict[str, str] = {}
    for h, chunk in hash_to_chunk.items():
        key = chunk.doc_group_key
        if key in shared_doc_map:
            ref_map[h] = shared_doc_map[key]

    # ── 6. 替换文本中的 hash ──
    def _replace_hash(m: re.Match) -> str:
        h = m.group(1).lower()
        cite_key = ref_map.get(h)
        if cite_key:
            return f"[{cite_key}]"
        return m.group(0)  # 未识别的 hash 保持原样

    resolved_text = pattern.sub(_replace_hash, response_text)

    return resolved_text, citations, ref_map


def _pick_best_metadata(chunks: List[EvidenceChunk]) -> EvidenceChunk:
    """从同一文档的多个 chunk 中选择元数据最完整的一个。"""
    best = chunks[0]
    best_score = 0
    for c in chunks:
        score = 0
        if c.doc_title:
            score += 2
        if c.authors:
            score += 2
        if c.year is not None:
            score += 1
        if getattr(c, "doi", None):
            score += 1
        if c.url:
            score += 1
        if score > best_score:
            best = c
            best_score = score
    return best


def sync_evidence_to_canvas(
    canvas_id: str,
    evidence_pack: EvidencePack,
    format: Optional[CiteKeyFormat] = None,
) -> List[str]:
    """
    将 EvidencePack 中的 chunk 转为 Citation，去重后合并写入 Canvas 引用池。

    Args:
        canvas_id: Canvas ID
        evidence_pack: 检索结果包
        format: 可选，覆盖配置中的 cite_key 格式

    Returns:
        本轮新增的 cite_key 列表
    """
    if not canvas_id or not evidence_pack or not evidence_pack.chunks:
        return []

    from src.collaboration.canvas.canvas_manager import get_canvas_store

    store = get_canvas_store()
    existing = store.get_citations(canvas_id)

    # 收集已有的 cite_key
    existing_keys = {c.cite_key or c.id for c in existing}

    # 创建生成器，传入已有 keys 以确保不重复
    gen = CiteKeyGenerator(format=format, existing_keys=existing_keys)

    # 转换新的 citations
    new_citations = _dedupe_citations(
        [chunk_to_citation(c, generator=gen) for c in evidence_pack.chunks]
    )
    if getattr(settings.citation, "merge_level", "document") == "document":
        new_citations = merge_citations_by_document(new_citations)

    # 合并
    by_key: Dict[str, Citation] = {c.cite_key or c.id: c for c in existing}
    for c in new_citations:
        by_key[c.cite_key or c.id] = c

    store.upsert_citations(canvas_id, list(by_key.values()))
    return [c.cite_key or c.id for c in new_citations]
</file>

<file path="src/collaboration/research/job_store.py">
"""
Deep Research 任务状态持久化（支持前端断连后恢复查看）。
存储位置：src/data/deep_research_jobs.db
"""

from __future__ import annotations

import json
import sqlite3
import time
import uuid
from pathlib import Path
from typing import Any, Dict, List, Optional


_DB_PATH = Path(__file__).resolve().parents[2] / "data" / "deep_research_jobs.db"


def _db() -> sqlite3.Connection:
    _DB_PATH.parent.mkdir(parents=True, exist_ok=True)
    conn = sqlite3.connect(str(_DB_PATH), timeout=30)
    conn.row_factory = sqlite3.Row
    conn.execute("PRAGMA journal_mode=WAL")
    _init_schema(conn)
    return conn


def _init_schema(conn: sqlite3.Connection) -> None:
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS deep_research_jobs (
            job_id              TEXT PRIMARY KEY,
            topic               TEXT NOT NULL,
            session_id          TEXT NOT NULL DEFAULT '',
            canvas_id           TEXT NOT NULL DEFAULT '',
            status              TEXT NOT NULL DEFAULT 'pending',
            current_stage       TEXT NOT NULL DEFAULT '',
            message             TEXT NOT NULL DEFAULT '',
            error_message       TEXT NOT NULL DEFAULT '',
            request_json        TEXT NOT NULL DEFAULT '{}',
            result_markdown     TEXT NOT NULL DEFAULT '',
            result_citations    TEXT NOT NULL DEFAULT '[]',
            result_dashboard    TEXT NOT NULL DEFAULT '{}',
            total_time_ms       REAL NOT NULL DEFAULT 0,
            created_at          REAL NOT NULL,
            updated_at          REAL NOT NULL,
            finished_at         REAL
        )
        """
    )
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS deep_research_job_events (
            id         INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id     TEXT NOT NULL,
            event      TEXT NOT NULL,
            data_json  TEXT NOT NULL DEFAULT '{}',
            created_at REAL NOT NULL
        )
        """
    )
    conn.execute(
        """
        CREATE INDEX IF NOT EXISTS idx_dr_job_events_job_id_id
        ON deep_research_job_events(job_id, id)
        """
    )
    conn.execute(
        """
        CREATE INDEX IF NOT EXISTS idx_dr_jobs_created_at
        ON deep_research_jobs(created_at DESC)
        """
    )
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS deep_research_section_reviews (
            id         INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id     TEXT NOT NULL,
            section_id TEXT NOT NULL,
            action     TEXT NOT NULL DEFAULT 'approve',
            feedback   TEXT NOT NULL DEFAULT '',
            created_at REAL NOT NULL,
            UNIQUE(job_id, section_id)
        )
        """
    )
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS deep_research_resume_queue (
            id              INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id          TEXT NOT NULL,
            owner_instance  TEXT NOT NULL DEFAULT '',
            source          TEXT NOT NULL DEFAULT 'review',
            status          TEXT NOT NULL DEFAULT 'pending',
            message         TEXT NOT NULL DEFAULT '',
            created_at      REAL NOT NULL,
            updated_at      REAL NOT NULL
        )
        """
    )
    conn.execute(
        """
        CREATE INDEX IF NOT EXISTS idx_dr_resume_queue_status_created
        ON deep_research_resume_queue(status, created_at)
        """
    )
    conn.execute(
        """
        CREATE INDEX IF NOT EXISTS idx_dr_resume_queue_owner_status
        ON deep_research_resume_queue(owner_instance, status, created_at)
        """
    )
    conn.execute(
        """
        CREATE UNIQUE INDEX IF NOT EXISTS idx_dr_resume_queue_job_status_unique
        ON deep_research_resume_queue(job_id, status)
        """
    )
    # ── Gap Supplements (section-scoped user supplements for information gaps) ──
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS deep_research_gap_supplements (
            id              INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id          TEXT NOT NULL,
            section_id      TEXT NOT NULL,
            gap_text        TEXT NOT NULL DEFAULT '',
            supplement_type TEXT NOT NULL DEFAULT 'material',
            content_json    TEXT NOT NULL DEFAULT '{}',
            status          TEXT NOT NULL DEFAULT 'pending',
            created_at      REAL NOT NULL,
            consumed_at     REAL
        )
        """
    )
    conn.execute(
        """
        CREATE INDEX IF NOT EXISTS idx_dr_gap_supplements_job_section
        ON deep_research_gap_supplements(job_id, section_id)
        """
    )
    # ── Research Insights Ledger (accumulated gaps/conflicts/limitations for future directions) ──
    conn.execute(
        """
        CREATE TABLE IF NOT EXISTS deep_research_insights (
            id              INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id          TEXT NOT NULL,
            section_id      TEXT NOT NULL DEFAULT '',
            insight_type    TEXT NOT NULL DEFAULT 'gap',
            text            TEXT NOT NULL DEFAULT '',
            source_context  TEXT NOT NULL DEFAULT '',
            status          TEXT NOT NULL DEFAULT 'open',
            created_at      REAL NOT NULL
        )
        """
    )
    conn.execute(
        """
        CREATE INDEX IF NOT EXISTS idx_dr_insights_job_id
        ON deep_research_insights(job_id)
        """
    )
    conn.commit()


def create_job(
    *,
    topic: str,
    session_id: str = "",
    canvas_id: str = "",
    request_payload: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    now = time.time()
    job_id = uuid.uuid4().hex
    with _db() as conn:
        conn.execute(
            """
            INSERT INTO deep_research_jobs (
                job_id, topic, session_id, canvas_id, status, request_json, created_at, updated_at
            ) VALUES (?, ?, ?, ?, 'pending', ?, ?, ?)
            """,
            (
                job_id,
                topic,
                session_id,
                canvas_id,
                json.dumps(request_payload or {}, ensure_ascii=False),
                now,
                now,
            ),
        )
        conn.commit()
    return get_job(job_id) or {}


def update_job(job_id: str, **fields: Any) -> Optional[Dict[str, Any]]:
    if not fields:
        return get_job(job_id)
    fields["updated_at"] = time.time()
    keys = list(fields.keys())
    set_expr = ", ".join([f"{k} = ?" for k in keys])
    vals = [fields[k] for k in keys]
    vals.append(job_id)
    with _db() as conn:
        conn.execute(f"UPDATE deep_research_jobs SET {set_expr} WHERE job_id = ?", vals)
        conn.commit()
    return get_job(job_id)


def append_event(job_id: str, event: str, data: Dict[str, Any]) -> int:
    now = time.time()
    with _db() as conn:
        cur = conn.execute(
            """
            INSERT INTO deep_research_job_events (job_id, event, data_json, created_at)
            VALUES (?, ?, ?, ?)
            """,
            (job_id, event, json.dumps(data, ensure_ascii=False, default=str), now),
        )
        conn.execute("UPDATE deep_research_jobs SET updated_at = ? WHERE job_id = ?", (now, job_id))
        conn.commit()
        return int(cur.lastrowid or 0)


def get_job(job_id: str) -> Optional[Dict[str, Any]]:
    with _db() as conn:
        row = conn.execute("SELECT * FROM deep_research_jobs WHERE job_id = ?", (job_id,)).fetchone()
        if not row:
            return None
    out = dict(row)
    try:
        out["request"] = json.loads(out.get("request_json") or "{}")
    except Exception:
        out["request"] = {}
    try:
        out["result_citations"] = json.loads(out.get("result_citations") or "[]")
    except Exception:
        out["result_citations"] = []
    try:
        out["result_dashboard"] = json.loads(out.get("result_dashboard") or "{}")
    except Exception:
        out["result_dashboard"] = {}
    out.pop("request_json", None)
    return out


def list_jobs(limit: int = 20, status: Optional[str] = None) -> List[Dict[str, Any]]:
    limit = max(1, min(int(limit), 200))
    with _db() as conn:
        if status:
            rows = conn.execute(
                """
                SELECT * FROM deep_research_jobs
                WHERE status = ?
                ORDER BY created_at DESC
                LIMIT ?
                """,
                (status, limit),
            ).fetchall()
        else:
            rows = conn.execute(
                """
                SELECT * FROM deep_research_jobs
                ORDER BY created_at DESC
                LIMIT ?
                """,
                (limit,),
            ).fetchall()
    result: List[Dict[str, Any]] = []
    for row in rows:
        item = dict(row)
        try:
            item["request"] = json.loads(item.get("request_json") or "{}")
        except Exception:
            item["request"] = {}
        try:
            item["result_citations"] = json.loads(item.get("result_citations") or "[]")
        except Exception:
            item["result_citations"] = []
        try:
            item["result_dashboard"] = json.loads(item.get("result_dashboard") or "{}")
        except Exception:
            item["result_dashboard"] = {}
        item.pop("request_json", None)
        result.append(item)
    return result


def list_events(job_id: str, after_id: int = 0, limit: int = 500) -> List[Dict[str, Any]]:
    after_id = max(0, int(after_id))
    limit = max(1, min(int(limit), 2000))
    with _db() as conn:
        rows = conn.execute(
            """
            SELECT id, event, data_json, created_at
            FROM deep_research_job_events
            WHERE job_id = ? AND id > ?
            ORDER BY id ASC
            LIMIT ?
            """,
            (job_id, after_id, limit),
        ).fetchall()
    out: List[Dict[str, Any]] = []
    for row in rows:
        try:
            data = json.loads(row["data_json"] or "{}")
        except Exception:
            data = {}
        data["event_id"] = int(row["id"])
        out.append(
            {
                "event_id": int(row["id"]),
                "event": str(row["event"]),
                "created_at": float(row["created_at"]),
                "data": data,
            }
        )
    return out


def submit_review(job_id: str, section_id: str, action: str = "approve", feedback: str = "") -> Dict[str, Any]:
    now = time.time()
    with _db() as conn:
        conn.execute(
            """
            INSERT INTO deep_research_section_reviews (job_id, section_id, action, feedback, created_at)
            VALUES (?, ?, ?, ?, ?)
            ON CONFLICT(job_id, section_id) DO UPDATE SET
                action = excluded.action,
                feedback = excluded.feedback,
                created_at = excluded.created_at
            """,
            (job_id, section_id, action, feedback, now),
        )
        conn.commit()
    return {"job_id": job_id, "section_id": section_id, "action": action}


def get_pending_review(job_id: str, section_id: str) -> Optional[Dict[str, Any]]:
    with _db() as conn:
        row = conn.execute(
            """
            SELECT job_id, section_id, action, feedback, created_at
            FROM deep_research_section_reviews
            WHERE job_id = ? AND section_id = ?
            """,
            (job_id, section_id),
        ).fetchone()
    return dict(row) if row else None


def list_reviews(job_id: str) -> List[Dict[str, Any]]:
    with _db() as conn:
        rows = conn.execute(
            """
            SELECT job_id, section_id, action, feedback, created_at
            FROM deep_research_section_reviews
            WHERE job_id = ?
            ORDER BY created_at ASC
            """,
            (job_id,),
        ).fetchall()
    return [dict(r) for r in rows]


def enqueue_resume_request(
    job_id: str,
    owner_instance: str,
    source: str = "review",
    message: str = "",
) -> Dict[str, Any]:
    """
    Enqueue one resume request for a job.
    Idempotent when an existing pending/running row already exists.
    """
    now = time.time()
    with _db() as conn:
        existing = conn.execute(
            """
            SELECT id, job_id, owner_instance, source, status, message, created_at, updated_at
            FROM deep_research_resume_queue
            WHERE job_id = ? AND status IN ('pending', 'running')
            ORDER BY id DESC
            LIMIT 1
            """,
            (job_id,),
        ).fetchone()
        if existing:
            row = dict(existing)
            if row.get("owner_instance") != owner_instance:
                conn.execute(
                    """
                    UPDATE deep_research_resume_queue
                    SET owner_instance = ?, source = ?, message = ?, updated_at = ?
                    WHERE id = ?
                    """,
                    (owner_instance, source, message, now, row["id"]),
                )
                conn.commit()
                row["owner_instance"] = owner_instance
                row["source"] = source
                row["message"] = message
                row["updated_at"] = now
            return row

        cur = conn.execute(
            """
            INSERT INTO deep_research_resume_queue
                (job_id, owner_instance, source, status, message, created_at, updated_at)
            VALUES (?, ?, ?, 'pending', ?, ?, ?)
            """,
            (job_id, owner_instance, source, message, now, now),
        )
        conn.commit()
        rid = int(cur.lastrowid or 0)
        row = conn.execute(
            """
            SELECT id, job_id, owner_instance, source, status, message, created_at, updated_at
            FROM deep_research_resume_queue
            WHERE id = ?
            """,
            (rid,),
        ).fetchone()
    return dict(row) if row else {}


def claim_resume_requests(owner_instance: str, limit: int = 10) -> List[Dict[str, Any]]:
    """Atomically claim pending resume requests for this instance."""
    limit = max(1, min(int(limit), 100))
    now = time.time()
    claimed: List[Dict[str, Any]] = []
    with _db() as conn:
        rows = conn.execute(
            """
            SELECT id, job_id, owner_instance, source, status, message, created_at, updated_at
            FROM deep_research_resume_queue
            WHERE status = 'pending' AND owner_instance = ?
            ORDER BY created_at ASC
            LIMIT ?
            """,
            (owner_instance, limit),
        ).fetchall()
        for row in rows:
            rid = int(row["id"])
            cur = conn.execute(
                """
                UPDATE deep_research_resume_queue
                SET status = 'running', updated_at = ?
                WHERE id = ? AND status = 'pending'
                """,
                (now, rid),
            )
            if cur.rowcount <= 0:
                continue
            updated = conn.execute(
                """
                SELECT id, job_id, owner_instance, source, status, message, created_at, updated_at
                FROM deep_research_resume_queue
                WHERE id = ?
                """,
                (rid,),
            ).fetchone()
            if updated:
                claimed.append(dict(updated))
        conn.commit()
    return claimed


def complete_resume_request(resume_id: int, status: str, message: str = "") -> None:
    """Mark a running resume request as done/error/cancelled."""
    if status not in {"done", "error", "cancelled"}:
        status = "error"
    now = time.time()
    with _db() as conn:
        conn.execute(
            """
            UPDATE deep_research_resume_queue
            SET status = ?, message = ?, updated_at = ?
            WHERE id = ?
            """,
            (status, message, now, int(resume_id)),
        )
        conn.commit()


def list_resume_requests(
    limit: int = 50,
    status: Optional[str] = None,
    owner_instance: Optional[str] = None,
    job_id: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """List resume queue rows with optional filters."""
    limit = max(1, min(int(limit), 500))
    sql = """
        SELECT id, job_id, owner_instance, source, status, message, created_at, updated_at
        FROM deep_research_resume_queue
        WHERE 1 = 1
    """
    params: List[Any] = []
    if status:
        sql += " AND status = ?"
        params.append(status)
    if owner_instance:
        sql += " AND owner_instance = ?"
        params.append(owner_instance)
    if job_id:
        sql += " AND job_id = ?"
        params.append(job_id)
    sql += " ORDER BY created_at DESC LIMIT ?"
    params.append(limit)
    with _db() as conn:
        rows = conn.execute(sql, params).fetchall()
    return [dict(r) for r in rows]


def cleanup_resume_requests(
    *,
    statuses: Optional[List[str]] = None,
    before_ts: Optional[float] = None,
    owner_instance: Optional[str] = None,
    job_id: Optional[str] = None,
) -> int:
    """
    Delete resume queue rows by filters, defaulting to terminal statuses only.
    Returns deleted row count.
    """
    effective_statuses = statuses or ["done", "error", "cancelled"]
    allowed = {"pending", "running", "done", "error", "cancelled"}
    effective_statuses = [s for s in effective_statuses if s in allowed]
    if not effective_statuses:
        return 0

    placeholders = ", ".join(["?"] * len(effective_statuses))
    sql = f"DELETE FROM deep_research_resume_queue WHERE status IN ({placeholders})"
    params: List[Any] = list(effective_statuses)
    if before_ts is not None:
        sql += " AND updated_at <= ?"
        params.append(float(before_ts))
    if owner_instance:
        sql += " AND owner_instance = ?"
        params.append(owner_instance)
    if job_id:
        sql += " AND job_id = ?"
        params.append(job_id)

    with _db() as conn:
        cur = conn.execute(sql, params)
        conn.commit()
        return int(cur.rowcount or 0)


def retry_resume_request(
    *,
    resume_id: int,
    owner_instance: str,
    message: str = "manual retry",
) -> Optional[Dict[str, Any]]:
    """
    Retry a terminal resume request by setting it back to pending.
    Raises ValueError when retry is unsafe/conflicting.
    """
    now = time.time()
    with _db() as conn:
        row = conn.execute(
            """
            SELECT id, job_id, owner_instance, source, status, message, created_at, updated_at
            FROM deep_research_resume_queue
            WHERE id = ?
            """,
            (int(resume_id),),
        ).fetchone()
        if not row:
            return None
        item = dict(row)
        status = str(item.get("status") or "")
        if status in {"pending", "running"}:
            raise ValueError("resume request is already active")

        conflict = conn.execute(
            """
            SELECT id
            FROM deep_research_resume_queue
            WHERE job_id = ? AND status IN ('pending', 'running') AND id != ?
            LIMIT 1
            """,
            (item["job_id"], int(resume_id)),
        ).fetchone()
        if conflict:
            raise ValueError("job already has active resume request")

        conn.execute(
            """
            UPDATE deep_research_resume_queue
            SET owner_instance = ?,
                source = 'manual_retry',
                status = 'pending',
                message = ?,
                updated_at = ?
            WHERE id = ?
            """,
            (owner_instance, message, now, int(resume_id)),
        )
        conn.commit()
        updated = conn.execute(
            """
            SELECT id, job_id, owner_instance, source, status, message, created_at, updated_at
            FROM deep_research_resume_queue
            WHERE id = ?
            """,
            (int(resume_id),),
        ).fetchone()
    return dict(updated) if updated else None


# ────────────────────────────────────────────────
# Gap Supplements CRUD
# ────────────────────────────────────────────────

def submit_gap_supplement(
    job_id: str,
    section_id: str,
    gap_text: str,
    supplement_type: str = "material",
    content: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """Submit a section-scoped supplement for a specific information gap."""
    now = time.time()
    content_json = json.dumps(content or {}, ensure_ascii=False)
    with _db() as conn:
        cur = conn.execute(
            """
            INSERT INTO deep_research_gap_supplements
                (job_id, section_id, gap_text, supplement_type, content_json, status, created_at)
            VALUES (?, ?, ?, ?, ?, 'pending', ?)
            """,
            (job_id, section_id, gap_text, supplement_type, content_json, now),
        )
        conn.commit()
        return {
            "id": int(cur.lastrowid or 0),
            "job_id": job_id,
            "section_id": section_id,
            "gap_text": gap_text,
            "supplement_type": supplement_type,
            "status": "pending",
        }


def list_gap_supplements(
    job_id: str,
    section_id: Optional[str] = None,
    status: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """List gap supplements, optionally filtered by section_id and/or status."""
    with _db() as conn:
        sql = "SELECT * FROM deep_research_gap_supplements WHERE job_id = ?"
        params: List[Any] = [job_id]
        if section_id:
            sql += " AND section_id = ?"
            params.append(section_id)
        if status:
            sql += " AND status = ?"
            params.append(status)
        sql += " ORDER BY created_at ASC"
        rows = conn.execute(sql, params).fetchall()
    out: List[Dict[str, Any]] = []
    for row in rows:
        item = dict(row)
        try:
            item["content"] = json.loads(item.get("content_json") or "{}")
        except Exception:
            item["content"] = {}
        item.pop("content_json", None)
        out.append(item)
    return out


def mark_gap_supplement_consumed(supplement_id: int) -> None:
    """Mark a gap supplement as consumed (used in research/write)."""
    now = time.time()
    with _db() as conn:
        conn.execute(
            "UPDATE deep_research_gap_supplements SET status = 'consumed', consumed_at = ? WHERE id = ?",
            (now, supplement_id),
        )
        conn.commit()


# ────────────────────────────────────────────────
# Research Insights Ledger CRUD
# ────────────────────────────────────────────────

def append_insight(
    job_id: str,
    insight_type: str,
    text: str,
    section_id: str = "",
    source_context: str = "",
) -> int:
    """Append a research insight (gap, conflict, limitation, future_direction)."""
    now = time.time()
    with _db() as conn:
        cur = conn.execute(
            """
            INSERT INTO deep_research_insights
                (job_id, section_id, insight_type, text, source_context, status, created_at)
            VALUES (?, ?, ?, ?, ?, 'open', ?)
            """,
            (job_id, section_id, insight_type, text, source_context, now),
        )
        conn.commit()
        return int(cur.lastrowid or 0)


def list_insights(
    job_id: str,
    insight_type: Optional[str] = None,
    status: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """List research insights, optionally filtered by type and/or status."""
    with _db() as conn:
        sql = "SELECT * FROM deep_research_insights WHERE job_id = ?"
        params: List[Any] = [job_id]
        if insight_type:
            sql += " AND insight_type = ?"
            params.append(insight_type)
        if status:
            sql += " AND status = ?"
            params.append(status)
        sql += " ORDER BY created_at ASC"
        rows = conn.execute(sql, params).fetchall()
    return [dict(r) for r in rows]


def update_insight_status(insight_id: int, status: str) -> None:
    """Update the status of a research insight (open, addressed, deferred)."""
    with _db() as conn:
        conn.execute(
            "UPDATE deep_research_insights SET status = ? WHERE id = ?",
            (status, insight_id),
        )
        conn.commit()


def bulk_mark_insights_addressed(job_id: str, insight_type: Optional[str] = None) -> int:
    """Mark all open insights for a job as addressed. Returns count of updated rows."""
    with _db() as conn:
        sql = "UPDATE deep_research_insights SET status = 'addressed' WHERE job_id = ? AND status = 'open'"
        params: List[Any] = [job_id]
        if insight_type:
            sql += " AND insight_type = ?"
            params.append(insight_type)
        cur = conn.execute(sql, params)
        conn.commit()
        return cur.rowcount


def get_latest_job_by_session(session_id: str) -> Optional[Dict[str, Any]]:
    """按 session_id 获取最近一条 Deep Research 任务（用于刷新后恢复 dashboard）。"""
    if not session_id:
        return None
    with _db() as conn:
        row = conn.execute(
            """
            SELECT * FROM deep_research_jobs
            WHERE session_id = ?
            ORDER BY updated_at DESC
            LIMIT 1
            """,
            (session_id,),
        ).fetchone()
    if not row:
        return None
    out = dict(row)
    try:
        out["request"] = json.loads(out.get("request_json") or "{}")
    except Exception:
        out["request"] = {}
    try:
        out["result_citations"] = json.loads(out.get("result_citations") or "[]")
    except Exception:
        out["result_citations"] = []
    try:
        out["result_dashboard"] = json.loads(out.get("result_dashboard") or "{}")
    except Exception:
        out["result_dashboard"] = {}
    out.pop("request_json", None)
    return out
</file>

<file path="src/indexing/milvus_ops.py">
"""
Milvus 操作封装
Mac 和服务器使用完全相同的代码
"""

from typing import Optional

from pymilvus import MilvusClient, DataType
from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)


class MilvusOps:
    """Milvus 操作类"""

    _instance = None
    _client = None

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    @property
    def client(self) -> MilvusClient:
        if self._client is None:
            self._client = MilvusClient(uri=settings.milvus.uri)
        return self._client

    def create_collection(self, name: str, recreate: bool = False, schema_version: str = "v1"):
        """
        创建 Collection。
        schema_version: "v1" = auto_id + id 主键（兼容旧脚本）；"v2" = chunk_id 主键，支持 upsert。
        """
        if self.client.has_collection(name):
            if recreate:
                self.client.drop_collection(name)
            else:
                logger.info(f"  Collection '{name}' 已存在")
                return

        if schema_version == "v2":
            self._create_collection_v2(name)
        else:
            self._create_collection_v1(name)

    def _create_collection_v1(self, name: str):
        """v1: auto_id + id 主键（兼容原有 insert）"""
        schema = self.client.create_schema(auto_id=True, enable_dynamic_field=True)
        schema.add_field("id", DataType.INT64, is_primary=True)
        schema.add_field("content", DataType.VARCHAR, max_length=65535)
        schema.add_field("raw_content", DataType.VARCHAR, max_length=65535)
        schema.add_field("dense_vector", DataType.FLOAT_VECTOR, dim=1024)
        schema.add_field("sparse_vector", DataType.SPARSE_FLOAT_VECTOR)
        schema.add_field("paper_id", DataType.VARCHAR, max_length=512)
        schema.add_field("chunk_id", DataType.VARCHAR, max_length=256)
        schema.add_field("domain", DataType.VARCHAR, max_length=64)
        schema.add_field("content_type", DataType.VARCHAR, max_length=64)
        schema.add_field("chunk_type", DataType.VARCHAR, max_length=128)
        schema.add_field("section_path", DataType.VARCHAR, max_length=65535)
        schema.add_field("page", DataType.INT32)
        index_params = self.client.prepare_index_params()
        index_params.add_index(field_name="dense_vector", **settings.index.params)
        index_params.add_index(
            field_name="sparse_vector",
            index_type="SPARSE_INVERTED_INDEX",
            metric_type="IP",
        )
        self.client.create_collection(collection_name=name, schema=schema, index_params=index_params)
        logger.info(f"  [OK] Collection '{name}' 创建成功 (v1)")

    def _create_collection_v2(self, name: str):
        """v2: chunk_id 主键，支持 upsert，重复跑不产生重复数据"""
        schema = self.client.create_schema(auto_id=False, enable_dynamic_field=True)
        schema.add_field("chunk_id", DataType.VARCHAR, is_primary=True, max_length=256)
        schema.add_field("content", DataType.VARCHAR, max_length=65535)
        schema.add_field("raw_content", DataType.VARCHAR, max_length=65535)
        schema.add_field("dense_vector", DataType.FLOAT_VECTOR, dim=1024)
        schema.add_field("sparse_vector", DataType.SPARSE_FLOAT_VECTOR)
        schema.add_field("paper_id", DataType.VARCHAR, max_length=512)
        schema.add_field("domain", DataType.VARCHAR, max_length=64)
        schema.add_field("content_type", DataType.VARCHAR, max_length=64)
        schema.add_field("chunk_type", DataType.VARCHAR, max_length=128)
        schema.add_field("section_path", DataType.VARCHAR, max_length=65535)
        schema.add_field("page", DataType.INT32)
        index_params = self.client.prepare_index_params()
        index_params.add_index(field_name="dense_vector", **settings.index.params)
        index_params.add_index(
            field_name="sparse_vector",
            index_type="SPARSE_INVERTED_INDEX",
            metric_type="IP",
        )
        self.client.create_collection(collection_name=name, schema=schema, index_params=index_params)
        logger.info(f"  [OK] Collection '{name}' 创建成功 (v2, chunk_id PK)")

    def init_all_collections(self, recreate: bool = False):
        """初始化所有子库"""
        logger.info("初始化 Collections...")
        for name in settings.collection.all():
            self.create_collection(name, recreate)

    def insert(self, collection: str, data: list):
        return self.client.insert(collection_name=collection, data=data)

    def upsert(self, collection: str, data: list):
        """Upsert 数据（需 collection 为 chunk_id 主键，即 schema_version='v2'）"""
        if not data:
            return
        return self.client.upsert(collection_name=collection, data=data)

    def search(self, collection: str, **kwargs):
        return self.client.search(collection_name=collection, **kwargs)

    def hybrid_search(self, collection: str, **kwargs):
        year_start = kwargs.pop("year_start", None)
        year_end = kwargs.pop("year_end", None)
        year_expr = self.build_year_expr(year_start=year_start, year_end=year_end)
        if year_expr:
            existing_expr = (kwargs.get("expr") or "").strip()
            if existing_expr:
                kwargs["expr"] = f"({existing_expr}) and ({year_expr})"
            else:
                kwargs["expr"] = year_expr
        return self.client.hybrid_search(collection_name=collection, **kwargs)

    def query(self, collection: str, **kwargs):
        return self.client.query(collection_name=collection, **kwargs)

    def count(self, collection: str) -> int:
        stats = self.client.get_collection_stats(collection)
        return stats.get("row_count", 0)

    @staticmethod
    def build_year_expr(year_start: Optional[int] = None, year_end: Optional[int] = None) -> str:
        """
        构造 Milvus 年份过滤表达式（硬过滤）。

        规则：
        - year_start / year_end 均为空 -> ""
        - 仅 start -> "year >= start"
        - 仅 end -> "year <= end"
        - 同时存在 -> "year >= start and year <= end"
        """
        if year_start is None and year_end is None:
            return ""
        clauses: list[str] = []
        if year_start is not None:
            clauses.append(f"year >= {int(year_start)}")
        if year_end is not None:
            clauses.append(f"year <= {int(year_end)}")
        return " and ".join(clauses)


# 全局实例
milvus = MilvusOps()
</file>

<file path="src/llm/react_loop.py">
"""
ReAct 执行循环：LLM → tool_call → execute → feed back → repeat。

支持 OpenAI / Anthropic 两种 tool calling 协议，自动降级为 prompt-based。
"""

from __future__ import annotations

import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from src.llm.tools import (
    ToolDef,
    ToolCall,
    ToolResult,
    execute_tool_call,
    parse_tool_calls,
    has_tool_calls,
    tool_result_to_openai_message,
    tool_result_to_anthropic_content,
    tools_to_prompt,
)
from src.log import get_logger
from src.observability.tracing import traceable

logger = get_logger(__name__)


@dataclass
class ReactResult:
    """ReAct 循环的最终结果"""
    final_text: str = ""
    reasoning_text: str = ""
    tool_trace: List[Dict[str, Any]] = field(default_factory=list)  # 工具调用记录
    iterations: int = 0
    total_time_ms: float = 0.0
    raw_response: Optional[Dict[str, Any]] = None


@traceable(run_type="agent", name="react_loop")
def react_loop(
    messages: List[Dict[str, Any]],
    tools: List[ToolDef],
    llm_client: Any,
    max_iterations: int = 10,
    model: Optional[str] = None,
    **llm_kwargs,
) -> ReactResult:
    """
    ReAct 循环核心。

    Args:
        messages: 初始消息列表（含 system + user）
        tools: 可用工具列表
        llm_client: BaseChatClient 实例
        max_iterations: 最大迭代次数（防止无限循环）
        model: 可选模型覆盖
        **llm_kwargs: 传递给 llm_client.chat() 的额外参数

    Returns:
        ReactResult
    """
    t0 = time.perf_counter()
    result = ReactResult()

    # 检测 provider 类型
    is_anthropic = getattr(getattr(llm_client, "config", None), "is_anthropic", lambda: False)()

    # 决定是否使用原生 FC
    supports_fc = True  # 当前所有 provider 都支持
    working_messages = list(messages)

    # 如果不支持 FC，注入 prompt-based tool 描述到 system message
    if not supports_fc:
        prompt_desc = tools_to_prompt(tools)
        if working_messages and working_messages[0].get("role") == "system":
            working_messages[0] = {
                **working_messages[0],
                "content": working_messages[0]["content"] + "\n\n" + prompt_desc,
            }
        else:
            working_messages.insert(0, {"role": "system", "content": prompt_desc})

    for iteration in range(max_iterations):
        result.iterations = iteration + 1

        # 调用 LLM
        try:
            resp = llm_client.chat(
                messages=working_messages,
                model=model,
                tools=tools if supports_fc else None,
                **llm_kwargs,
            )
        except Exception as e:
            logger.error(f"ReAct LLM call failed at iteration {iteration}: {e}")
            result.final_text = f"[LLM 调用失败: {e}]"
            break

        raw = resp.get("raw", {})
        result.raw_response = resp

        # 检查是否有 tool calls
        tool_calls = resp.get("tool_calls") or []
        if not tool_calls and supports_fc:
            # 原生 FC 没返回 tool_calls → 模型给出了最终回复
            result.final_text = resp.get("final_text", "")
            result.reasoning_text = resp.get("reasoning_text", "")
            break

        if not tool_calls and not supports_fc:
            # prompt-based 降级模式：从文本中解析
            text = resp.get("final_text", "")
            tool_calls = parse_tool_calls(raw, is_anthropic)
            if not tool_calls:
                # 无 tool call → 最终回复
                result.final_text = text
                result.reasoning_text = resp.get("reasoning_text", "")
                break

        # 执行 tool calls
        tool_results: List[ToolResult] = []
        for tc in tool_calls:
            logger.info(f"ReAct [{iteration}] calling tool: {tc.name}({tc.arguments})")
            tr = execute_tool_call(tc, tools)
            tool_results.append(tr)

            # 记录到 trace
            result.tool_trace.append({
                "iteration": iteration,
                "tool": tc.name,
                "arguments": tc.arguments,
                "result": tr.content[:500],
                "is_error": tr.is_error,
            })

        # 将 assistant 的 tool call + tool results 追加到消息历史
        if is_anthropic:
            # Anthropic: assistant message 的 content 包含 text + tool_use blocks
            assistant_content = []
            # 保留 text 部分
            final_text = resp.get("final_text", "")
            if final_text:
                assistant_content.append({"type": "text", "text": final_text})
            # 添加 tool_use blocks
            for tc in tool_calls:
                assistant_content.append({
                    "type": "tool_use",
                    "id": tc.id,
                    "name": tc.name,
                    "input": tc.arguments,
                })
            working_messages.append({"role": "assistant", "content": assistant_content})

            # tool results 作为 user message
            tool_result_content = [tool_result_to_anthropic_content(tr) for tr in tool_results]
            working_messages.append({"role": "user", "content": tool_result_content})
        else:
            # OpenAI: assistant message 包含 tool_calls 字段
            assistant_msg: Dict[str, Any] = {"role": "assistant", "content": resp.get("final_text") or None}
            assistant_msg["tool_calls"] = [
                {
                    "id": tc.id,
                    "type": "function",
                    "function": {
                        "name": tc.name,
                        "arguments": __import__("json").dumps(tc.arguments, ensure_ascii=False),
                    },
                }
                for tc in tool_calls
            ]
            working_messages.append(assistant_msg)

            # 每个 tool result 作为独立的 tool message
            for tr in tool_results:
                working_messages.append(tool_result_to_openai_message(tr))
    else:
        # 达到最大迭代次数
        logger.warning(f"ReAct loop reached max iterations ({max_iterations})")
        result.final_text = resp.get("final_text", "") if 'resp' in dir() else ""

    result.total_time_ms = (time.perf_counter() - t0) * 1000
    return result
</file>

<file path="src/mcp/server.py">
"""
MCP Server — 将 RAG 系统能力暴露给 MCP 客户端。

使用 FastMCP 高层 API，将 P0 定义的 8 个核心 Tool 暴露为 MCP Tools，
Canvas/论文列表作为 MCP Resources。

运行方式:
    python -m src.mcp.server
    # 或
    uvicorn src.mcp.server:app --host 0.0.0.0 --port 8100

连接测试:
    npx -y @modelcontextprotocol/inspector
    → http://localhost:8100/mcp
"""

from __future__ import annotations

import json
import sys
from pathlib import Path
from typing import Any, Dict, List, Optional

from mcp.server.fastmcp import FastMCP

# 确保项目根目录在 sys.path 中
_project_root = str(Path(__file__).resolve().parent.parent.parent)
if _project_root not in sys.path:
    sys.path.insert(0, _project_root)


# ────────────────────────────────────────────────
# MCP Server 实例
# ────────────────────────────────────────────────

mcp = FastMCP(
    "DeepSea RAG Research System",
    json_response=True,
)


# ────────────────────────────────────────────────
# Tools — 对应 src/llm/tools.py 中的 8 个核心 Tool
# ────────────────────────────────────────────────

@mcp.tool()
def search_local(query: str, top_k: int = 10) -> str:
    """检索本地知识库（向量数据库 + 图谱融合检索），适用于查找已入库的论文和文档内容。"""
    from src.retrieval.service import get_retrieval_service
    svc = get_retrieval_service()
    pack = svc.search(query=query, mode="local", top_k=top_k)
    return pack.to_context_string(max_chunks=min(top_k, 15))


@mcp.tool()
def search_web(query: str, top_k: int = 10) -> str:
    """网络搜索（Tavily/Google），获取最新的在线信息和网页内容。"""
    from src.retrieval.service import get_retrieval_service
    svc = get_retrieval_service()
    pack = svc.search(query=query, mode="web", top_k=top_k)
    return pack.to_context_string(max_chunks=min(top_k, 15))


@mcp.tool()
def search_scholar(query: str, year_from: Optional[int] = None, limit: int = 5) -> str:
    """学术论文搜索（Semantic Scholar），查找特定领域的学术文献，获取标题、摘要、DOI。"""
    try:
        from src.retrieval.semantic_scholar import SemanticScholarSearch
        ss = SemanticScholarSearch()
        results = ss.search(query, year_from=year_from, limit=limit)
        if not results:
            return "未找到相关学术论文。"
        lines = []
        for r in results[:limit]:
            title = r.get("title", "")
            year = r.get("year", "")
            abstract = (r.get("abstract") or "")[:300]
            doi = r.get("externalIds", {}).get("DOI", "")
            lines.append(f"- **{title}** ({year}) DOI:{doi}\n  {abstract}")
        return "\n".join(lines)
    except Exception as e:
        return f"学术搜索失败: {e}"


@mcp.tool()
def explore_graph(entity_name: str, depth: int = 1) -> str:
    """知识图谱探索，查看指定实体的关联实体和关系，发现跨文档的知识连接。"""
    try:
        from src.graph.hippo_rag import get_hipporag_instance
        hippo = get_hipporag_instance()
        if hippo is None or hippo.graph is None:
            return "知识图谱未加载。"
        G = hippo.graph
        if entity_name not in G:
            return f"实体 '{entity_name}' 不在图谱中。"
        # BFS 获取邻域
        visited = {entity_name}
        frontier = [entity_name]
        edges_out = []
        for _ in range(depth):
            next_frontier = []
            for n in frontier:
                for nb in G.neighbors(n):
                    edge_data = G.edges[n, nb]
                    edges_out.append(f"  {n} --[{edge_data.get('relation', '?')}]--> {nb}")
                    if nb not in visited:
                        visited.add(nb)
                        next_frontier.append(nb)
            frontier = next_frontier
        lines = [f"实体 '{entity_name}' 的知识图谱（深度={depth}）:"]
        lines.append(f"节点数: {len(visited)}, 边数: {len(edges_out)}")
        lines.extend(edges_out[:30])
        return "\n".join(lines)
    except Exception as e:
        return f"图谱查询失败: {e}"


@mcp.tool()
def canvas(action: str, canvas_id: str = "", topic: str = "", content: str = "") -> str:
    """操作研究画布：创建(create)、获取(get)、更新(update)画布内容。"""
    from src.collaboration.canvas.canvas_manager import create_canvas, get_canvas, update_canvas
    if action == "create":
        canvas = create_canvas(topic=topic or "Untitled")
        return json.dumps({"canvas_id": canvas.id, "topic": canvas.topic}, ensure_ascii=False)
    elif action == "get":
        canvas = get_canvas(canvas_id)
        if canvas is None:
            return f"画布 '{canvas_id}' 不存在"
        return json.dumps({
            "canvas_id": canvas.id,
            "topic": canvas.topic,
            "markdown": canvas.markdown[:3000],
        }, ensure_ascii=False)
    elif action == "update":
        update_canvas(canvas_id, markdown=content)
        return f"画布 '{canvas_id}' 已更新"
    return f"未知操作: {action}"


@mcp.tool()
def get_citations(canvas_id: str, format: str = "text") -> str:
    """获取画布的引文列表，支持 text 和 bibtex 格式。"""
    from src.collaboration.citation.formatter import format_reference_list
    from src.collaboration.canvas.canvas_manager import get_canvas_citations
    citations = get_canvas_citations(canvas_id)
    if not citations:
        return "该画布暂无引文。"
    style_map = {
        "text": "custom",
        "bibtex": "apa",
        "apa": "apa",
        "ieee": "ieee",
        "numeric": "numeric",
        "custom": "custom",
    }
    style = style_map.get(format, "custom")
    return format_reference_list(citations, style=style)


@mcp.tool()
def compare_papers(paper_ids: list, aspects: Optional[list] = None) -> str:
    """多文档对比：选择 2-5 篇论文，自动生成结构化对比矩阵和分析。"""
    try:
        import importlib
        mod = importlib.import_module("src.api.routes_compare")
        req = mod.CompareRequest(
            paper_ids=paper_ids,
            aspects=aspects or ["objective", "methodology", "key_findings", "limitations"],
        )
        resp = mod.compare_papers(req)
        parts = []
        if resp.narrative:
            parts.append(f"综合分析: {resp.narrative}")
        for aspect, cells in resp.comparison_matrix.items():
            parts.append(f"\n[{aspect}]")
            for pid, desc in cells.items():
                parts.append(f"  {pid}: {desc}")
        return "\n".join(parts) if parts else "对比结果为空"
    except Exception as e:
        return f"论文对比失败: {e}"


@mcp.tool()
def run_code(code: str) -> str:
    """执行简单的 Python 代码进行数据计算、统计验证或格式转换。"""
    import io
    import contextlib
    forbidden = ["import os", "import sys", "import subprocess", "exec(", "eval(",
                 "__import__", "open(", "shutil", "pathlib"]
    for f in forbidden:
        if f in code:
            return f"安全限制: 不允许使用 '{f}'"
    stdout = io.StringIO()
    local_ns: Dict[str, Any] = {}
    try:
        with contextlib.redirect_stdout(stdout):
            exec(code, {"__builtins__": {
                "print": print, "range": range, "len": len,
                "sum": sum, "min": min, "max": max, "abs": abs,
                "round": round, "sorted": sorted, "enumerate": enumerate,
                "zip": zip, "map": map, "filter": filter,
                "int": int, "float": float, "str": str, "list": list,
                "dict": dict, "set": set, "tuple": tuple, "bool": bool,
                "True": True, "False": False, "None": None,
            }}, local_ns)
        output = stdout.getvalue()
        return output.strip() if output.strip() else "(代码执行完毕，无输出)"
    except Exception as e:
        return f"执行错误: {e}"


# ────────────────────────────────────────────────
# Resources — Canvas 列表 + 论文列表
# ────────────────────────────────────────────────

@mcp.resource("rag://canvases")
def list_canvases() -> str:
    """获取所有研究画布列表。"""
    try:
        from src.collaboration.canvas.canvas_manager import list_canvases as _list
        canvases = _list()
        items = []
        for c in canvases[:50]:
            items.append({
                "id": c.id,
                "topic": c.topic,
                "updated_at": str(getattr(c, "updated_at", "")),
            })
        return json.dumps(items, ensure_ascii=False, default=str)
    except Exception as e:
        return json.dumps({"error": str(e)})


@mcp.resource("rag://papers")
def list_papers() -> str:
    """获取已入库的论文列表。"""
    try:
        from pathlib import Path as P
        data_dir = P(_project_root) / "data" / "parsed"
        if not data_dir.exists():
            return json.dumps([])
        papers = []
        for d in sorted(data_dir.iterdir()):
            enriched = d / "enriched.json"
            if enriched.exists():
                try:
                    meta = json.loads(enriched.read_text("utf-8"))
                    papers.append({
                        "id": d.name,
                        "title": meta.get("title", d.name),
                        "authors": meta.get("authors", [])[:3],
                    })
                except Exception:
                    papers.append({"id": d.name, "title": d.name})
        return json.dumps(papers[:100], ensure_ascii=False, default=str)
    except Exception as e:
        return json.dumps({"error": str(e)})


@mcp.resource("rag://canvas/{canvas_id}")
def get_canvas_content(canvas_id: str) -> str:
    """获取指定画布的完整内容。"""
    try:
        from src.collaboration.canvas.canvas_manager import get_canvas
        canvas = get_canvas(canvas_id)
        if canvas is None:
            return json.dumps({"error": f"Canvas '{canvas_id}' not found"})
        return json.dumps({
            "id": canvas.id,
            "topic": canvas.topic,
            "markdown": canvas.markdown,
        }, ensure_ascii=False)
    except Exception as e:
        return json.dumps({"error": str(e)})


# ────────────────────────────────────────────────
# 入口
# ────────────────────────────────────────────────

app = mcp.streamable_http_app()

if __name__ == "__main__":
    mcp.run(transport="streamable-http")
</file>

<file path="src/observability/__init__.py">
"""
Observability 模块：OpenTelemetry tracing + Prometheus metrics。

用法：
    from src.observability import setup_observability, metrics, tracer

    # 在 FastAPI lifespan 中初始化
    setup_observability(app)

    # 业务代码中手动埋点
    with tracer.start_as_current_span("my_operation"):
        ...

    metrics.retrieval_latency.observe(elapsed_seconds)
"""

from src.observability.setup import setup_observability
from src.observability.metrics import metrics
from src.observability.tracing import tracer, traceable, langsmith_enabled

__all__ = ["setup_observability", "metrics", "tracer", "traceable", "langsmith_enabled"]
</file>

<file path="src/observability/tracing.py">
"""
OpenTelemetry tracing 配置 + LangSmith / Langfuse Agentic Tracing。

提供全局 tracer 供业务代码使用：
    from src.observability import tracer
    with tracer.start_as_current_span("retrieval.vector"):
        ...

LangSmith 追踪通过环境变量控制，未配置时静默跳过：
    LANGCHAIN_TRACING_V2=true
    LANGCHAIN_API_KEY=<your-key>
    LANGCHAIN_PROJECT=deepsea-rag  # 可选
"""

import os

from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import SimpleSpanProcessor, ConsoleSpanExporter
from opentelemetry.sdk.resources import Resource

_resource = Resource.create({"service.name": "deepsea-rag", "service.version": "0.1.0"})

_provider = TracerProvider(resource=_resource)

# 默认导出到控制台（生产环境可替换为 OTLP exporter）
# 仅在 RAG_TRACE_CONSOLE=1 时启用 console export，避免日志噪音
if os.getenv("RAG_TRACE_CONSOLE", "0") == "1":
    _provider.add_span_processor(SimpleSpanProcessor(ConsoleSpanExporter()))

trace.set_tracer_provider(_provider)

tracer = trace.get_tracer("deepsea-rag", "0.1.0")

# ── LangSmith / Langfuse Agentic Tracing ──
# 当 LANGCHAIN_TRACING_V2=true 时启用，否则静默跳过，不报错。
langsmith_enabled: bool = os.getenv("LANGCHAIN_TRACING_V2", "").lower() in ("1", "true")

try:
    if not langsmith_enabled:
        raise ImportError("LANGCHAIN_TRACING_V2 not set")
    from langsmith import traceable as _langsmith_traceable

    def traceable(*args, **kwargs):  # type: ignore[misc]
        """
        LangSmith traceable 兼容层。
        兼容 run_type="agent" 的调用习惯，内部映射到受支持的 "chain"。
        """
        if kwargs.get("run_type") == "agent":
            kwargs = dict(kwargs)
            kwargs["run_type"] = "chain"
            kwargs.setdefault("name", "agent")
        return _langsmith_traceable(*args, **kwargs)
except ImportError:
    langsmith_enabled = False

    def traceable(*args, **kwargs):  # type: ignore[misc]
        """Dummy @traceable — langsmith 未安装或追踪未启用时使用。"""
        if args and callable(args[0]):
            return args[0]

        def _decorator(fn):
            return fn

        return _decorator
</file>

<file path="src/parser/claim_extractor.py">
"""
Claim Extractor — 从解析后的论文中提取核心科学声明

从 Abstract / Results / Conclusion 等关键章节提取 3-5 个核心 claims。
每个 claim 包含：声明文本、支持证据、方法、置信度、局限性。

Claims 存入 enriched.json 的 `claims` 字段，chunking 时注入 chunk.meta。

使用方法:
---------
from src.parser.claim_extractor import ClaimExtractor

extractor = ClaimExtractor()
claims = extractor.extract(enriched_doc, llm_client)
"""

from __future__ import annotations

import re
import uuid
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, model_validator

from src.log import get_logger

logger = get_logger(__name__)


# ============================================================
# Pydantic Response Models (结构化输出)
# ============================================================

class _ClaimItem(BaseModel):
    text: str = ""
    evidence: str = ""
    methodology: str = ""
    confidence: str = "medium"
    limitations: str = ""
    source_section: str = ""


class _ClaimListResponse(BaseModel):
    claims: List[_ClaimItem] = Field(default_factory=list)

    @model_validator(mode="before")
    @classmethod
    def _accept_legacy_array(cls, data: Any) -> Any:
        if isinstance(data, list):
            return {"claims": data}
        return data


# ============================================================
# Claim 数据结构
# ============================================================

@dataclass
class Claim:
    """从论文中提取的核心科学声明"""

    claim_id: str
    text: str  # 核心声明（1-2 句）
    evidence: str  # 支持证据摘要
    methodology: str  # 使用的方法
    confidence: str  # high | medium | low
    limitations: str  # 局限性
    source_section: str  # 来源章节
    source_block_ids: List[str] = field(default_factory=list)  # 关联的 block_id

    def to_dict(self) -> Dict[str, Any]:
        return {
            "claim_id": self.claim_id,
            "text": self.text,
            "evidence": self.evidence,
            "methodology": self.methodology,
            "confidence": self.confidence,
            "limitations": self.limitations,
            "source_section": self.source_section,
            "source_block_ids": self.source_block_ids,
        }

    @classmethod
    def from_dict(cls, d: Dict[str, Any]) -> "Claim":
        return cls(
            claim_id=d.get("claim_id", str(uuid.uuid4())[:8]),
            text=d.get("text", ""),
            evidence=d.get("evidence", ""),
            methodology=d.get("methodology", ""),
            confidence=d.get("confidence", "medium"),
            limitations=d.get("limitations", ""),
            source_section=d.get("source_section", ""),
            source_block_ids=d.get("source_block_ids", []),
        )


# ============================================================
# 关键章节识别
# ============================================================

_KEY_SECTION_PATTERNS = {
    "abstract": [r"abstract", r"摘要"],
    "results": [r"result", r"finding", r"outcome", r"data", r"observation"],
    "conclusion": [r"conclusion", r"summary", r"concluding"],
    "discussion": [r"discussion", r"implication"],
}


def _is_key_section(heading_path: List[str], target_sections: List[str]) -> Optional[str]:
    """判断 heading_path 是否属于关键章节，返回匹配的章节类型"""
    path_str = " > ".join(str(h) for h in heading_path).lower()
    for section_type in target_sections:
        for pattern in _KEY_SECTION_PATTERNS.get(section_type, []):
            if re.search(pattern, path_str, re.IGNORECASE):
                return section_type
    return None


# ============================================================
# LLM Prompt
# ============================================================

_CLAIM_EXTRACTION_PROMPT = """You are a scientific claim extraction expert.

Given the following text from a research paper, extract 3-5 **core scientific claims**.

For each claim, provide:
- **text**: The core claim statement (1-2 sentences, precise and falsifiable)
- **evidence**: Brief summary of supporting evidence from the paper
- **methodology**: What method/approach was used
- **confidence**: "high" (directly supported by data), "medium" (inferred), or "low" (speculative)
- **limitations**: Any caveats or limitations mentioned
- **source_section**: Which section this claim comes from (abstract/results/conclusion/discussion)

Return ONLY a JSON object:
{"claims": [{"text": "...", "evidence": "...", "methodology": "...", "confidence": "high|medium|low", "limitations": "...", "source_section": "..."}]}

Paper text:
{text}"""


# ============================================================
# ClaimExtractor
# ============================================================

class ClaimExtractor:
    """
    从 EnrichedDoc 提取核心 claims。

    使用 LLM 从 Abstract + Results + Conclusion 提取 3-5 个核心 claims。
    """

    def __init__(
        self,
        max_text_chars: int = 6000,
        target_sections: Optional[List[str]] = None,
        max_retries: int = 2,
    ):
        self.max_text_chars = max_text_chars
        self.target_sections = target_sections or ["abstract", "results", "conclusion", "discussion"]
        self.max_retries = max_retries

    def extract(self, doc: Any, llm_client: Any) -> List[Claim]:
        """
        从 EnrichedDoc 提取 claims。

        Args:
            doc: EnrichedDoc 实例（需要 content_flow 和 doc_id）
            llm_client: LLM 客户端（需要 .chat() 方法）

        Returns:
            List[Claim]
        """
        # 1. 提取关键章节文本
        section_texts, block_ids_map = self._extract_key_sections(doc)

        if not section_texts:
            logger.warning(f"[{getattr(doc, 'doc_id', '?')}] 未找到关键章节，跳过 claim 提取")
            return []

        # 2. 组装文本（截断到 max_text_chars）
        combined = self._combine_sections(section_texts)

        # 3. 调用 LLM
        raw_claims = self._call_llm(combined, llm_client)

        if not raw_claims:
            logger.warning(f"[{getattr(doc, 'doc_id', '?')}] LLM 未返回有效 claims")
            return []

        # 4. 构建 Claim 对象
        claims = []
        for i, raw in enumerate(raw_claims):
            section = raw.get("source_section", "")
            claim = Claim(
                claim_id=f"{getattr(doc, 'doc_id', 'doc')}_{i:02d}",
                text=raw.get("text", ""),
                evidence=raw.get("evidence", ""),
                methodology=raw.get("methodology", ""),
                confidence=raw.get("confidence", "medium"),
                limitations=raw.get("limitations", ""),
                source_section=section,
                source_block_ids=block_ids_map.get(section, []),
            )
            if claim.text.strip():
                claims.append(claim)

        logger.info(f"[{getattr(doc, 'doc_id', '?')}] 提取 {len(claims)} 个 claims")
        return claims

    def _extract_key_sections(self, doc: Any) -> tuple:
        """
        从 content_flow 提取关键章节文本。

        Returns:
            (section_texts: dict[str, str], block_ids_map: dict[str, list[str]])
        """
        section_texts: Dict[str, List[str]] = {}
        block_ids_map: Dict[str, List[str]] = {}

        blocks = getattr(doc, "content_flow", [])
        for block in blocks:
            hp = getattr(block, "heading_path", None) or []
            text = getattr(block, "text", None) or ""
            block_id = getattr(block, "block_id", None) or ""

            if not text.strip():
                continue

            matched = _is_key_section(hp, self.target_sections)
            if matched:
                section_texts.setdefault(matched, []).append(text)
                block_ids_map.setdefault(matched, []).append(block_id)

        # 转换为字符串
        return {k: "\n\n".join(v) for k, v in section_texts.items()}, block_ids_map

    def _combine_sections(self, section_texts: Dict[str, str]) -> str:
        """组装各章节文本，带标签，截断到 max_text_chars"""
        parts = []
        # 按优先级排序
        priority = ["abstract", "results", "conclusion", "discussion"]
        total = 0
        for section in priority:
            if section in section_texts:
                text = section_texts[section]
                label = f"[{section.upper()}]"
                remaining = self.max_text_chars - total
                if remaining <= 0:
                    break
                if len(text) > remaining:
                    text = text[:remaining] + "..."
                parts.append(f"{label}\n{text}")
                total += len(text)

        return "\n\n".join(parts)

    def _call_llm(self, text: str, llm_client: Any) -> Optional[List[Dict]]:
        """调用 LLM 提取 claims，使用 Pydantic 结构化输出保障解析稳定性"""
        prompt = _CLAIM_EXTRACTION_PROMPT.replace("{text}", text)

        for attempt in range(self.max_retries + 1):
            try:
                resp = llm_client.chat(
                    messages=[
                        {"role": "system", "content": "Extract scientific claims. Return ONLY valid JSON."},
                        {"role": "user", "content": prompt},
                    ],
                    max_tokens=2000,
                    response_model=_ClaimListResponse,
                )
                parsed: Optional[_ClaimListResponse] = resp.get("parsed_object")
                if parsed is None:
                    raw = (resp.get("final_text") or "").strip()
                    if raw:
                        parsed = _ClaimListResponse.model_validate_json(raw)
                if parsed is not None:
                    return [item.model_dump() for item in parsed.claims]
            except Exception as e:
                logger.warning(f"Claim extraction LLM call failed (attempt {attempt + 1}): {e}")
                if attempt == self.max_retries:
                    return None

        return None
</file>

<file path="src/retrieval/dedup.py">
"""
去重与多样性模块
- 指纹去重（避免相邻 chunk 高度相似）
- 单文档上限（per_doc_cap，防止单 doc 垄断 top N）
- 跨源去重（拦截网络搜索中与本地文库重叠的文献）
"""

from __future__ import annotations

import json
import logging
import re
from collections import OrderedDict, defaultdict
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from typing import Any, Dict, List, Optional, Set, Tuple
from urllib.parse import urlencode
from urllib.request import Request, urlopen

logger = logging.getLogger(__name__)

_LRU_MAX = 512


class _LRUCache:
    """Minimal thread-unsafe LRU; fine for per-request hot path."""
    __slots__ = ("_data", "_max")

    def __init__(self, maxsize: int = _LRU_MAX):
        self._data: OrderedDict[str, Any] = OrderedDict()
        self._max = maxsize

    def __contains__(self, key: str) -> bool:
        return key in self._data

    def __getitem__(self, key: str) -> Any:
        self._data.move_to_end(key)
        return self._data[key]

    def __setitem__(self, key: str, value: Any) -> None:
        if key in self._data:
            self._data.move_to_end(key)
        self._data[key] = value
        if len(self._data) > self._max:
            self._data.popitem(last=False)


_crossref_lru = _LRUCache()

_DOI_RE = re.compile(
    r"(?:doi[:\s]*|(?:https?://)?(?:dx\.)?doi\.org/)?"
    r"(10\.\d{4,9}/[^\s,;)\]\"'<>]+[^\s,;)\]\"'<>.:])",
    re.IGNORECASE,
)
_CROSSREF_API = "https://api.crossref.org/works"
_CROSSREF_TIMEOUT_SECONDS = 4


def _get_paper_meta_store():
    """延迟加载 PaperMetadataStore，避免循环导入。"""
    from src.indexing.paper_metadata_store import paper_meta_store
    return paper_meta_store


def normalize_doi(doi: Optional[str]) -> str:
    """
    归一化 DOI：小写、去空白、去 URL 前缀。
    '10.1038/ismej.2016.124' / 'https://doi.org/10.1038/ISMEJ.2016.124' → '10.1038/ismej.2016.124'
    """
    if not doi or not isinstance(doi, str):
        return ""
    d = doi.strip().lower()
    d = re.sub(r"^https?://(?:dx\.)?doi\.org/", "", d)
    return d.rstrip("/.")


def normalize_title(title: str) -> str:
    """标题归一化：小写 + 去除非字母数字字符。"""
    if not title or not isinstance(title, str):
        return ""
    return re.sub(r"[^a-z0-9]+", "", title.lower())


def _fingerprint(text: str) -> str:
    """简单 hash 指纹（用于去重）"""
    if not text or not isinstance(text, str):
        return ""
    t = text.strip()[:512]
    return str(hash(t))


# ── 跨源去重 ──────────────────────────────────────────

def _extract_local_fields(chunk: Any) -> Tuple[str, str, str]:
    """兼容 EvidenceChunk / dict，提取 (doi, title, doc_id)。"""
    if isinstance(chunk, dict):
        meta = chunk.get("metadata") or {}
        doi = meta.get("doi") or chunk.get("doi") or ""
        title = meta.get("doc_title") or meta.get("title") or chunk.get("doc_title") or chunk.get("title") or ""
        doc_id = meta.get("doc_id") or meta.get("paper_id") or chunk.get("doc_id") or chunk.get("paper_id") or ""
        return str(doi), str(title), str(doc_id)

    doi = getattr(chunk, "doi", "") or ""
    title = getattr(chunk, "doc_title", "") or getattr(chunk, "title", "") or ""
    doc_id = getattr(chunk, "doc_id", "") or getattr(chunk, "paper_id", "") or ""
    return str(doi), str(title), str(doc_id)


def _extract_web_title(hit: Dict[str, Any]) -> str:
    meta = hit.get("metadata") or {}
    return str(meta.get("title") or hit.get("title") or "")


def _extract_web_url(hit: Dict[str, Any]) -> str:
    meta = hit.get("metadata") or {}
    return str(meta.get("url") or hit.get("url") or "")


def _extract_doi_from_text(text: str) -> str:
    if not text:
        return ""
    m = _DOI_RE.search(text)
    return normalize_doi(m.group(1)) if m else ""


def _token_set(text: str) -> Set[str]:
    if not text:
        return set()
    t = re.sub(r"[^a-z0-9\s]", " ", text.lower())
    return {w for w in t.split() if len(w) >= 3}


def _title_overlap(a: str, b: str) -> float:
    sa, sb = _token_set(a), _token_set(b)
    if not sa or not sb:
        return 0.0
    return len(sa & sb) / max(len(sa), 1)


def _crossref_lookup_by_title(title: str) -> Optional[Dict[str, Any]]:
    """
    用标题查 CrossRef，返回 {doi,title,authors,year,venue} 或 None。
    两级缓存：SQLite 持久化 + 进程内 LRU（热路径零 I/O）。
    """
    key = normalize_title(title)
    if not key:
        return None

    # L1: 进程内 LRU（capacity=512，覆盖一次检索的热词）
    if key in _crossref_lru:
        return _crossref_lru[key]

    # L2: SQLite 持久化缓存
    try:
        store = _get_paper_meta_store()
        cached = store.crossref_get(title)
        if cached is not None:
            _crossref_lru[key] = cached
            return cached
        if store.crossref_has(title):
            _crossref_lru[key] = None
            return None
    except Exception:
        pass

    # L3: 实际 API 请求
    def _request() -> Dict[str, Any]:
        qs = urlencode({"query.bibliographic": title, "rows": 3})
        req = Request(
            f"{_CROSSREF_API}?{qs}",
            headers={"User-Agent": "DeepSea-RAG/1.0", "Accept": "application/json"},
            method="GET",
        )
        with urlopen(req, timeout=_CROSSREF_TIMEOUT_SECONDS) as resp:
            if getattr(resp, "status", 200) != 200:
                return {}
            return json.loads(resp.read().decode("utf-8"))

    try:
        with ThreadPoolExecutor(max_workers=1) as ex:
            fut = ex.submit(_request)
            data = fut.result(timeout=float(_CROSSREF_TIMEOUT_SECONDS + 1))
        items = (data.get("message") or {}).get("items") or []
    except (FuturesTimeoutError, Exception):
        _crossref_lru[key] = None
        try:
            _get_paper_meta_store().crossref_put(title, None)
        except Exception:
            pass
        return None

    best: Optional[Dict[str, Any]] = None
    best_score = 0.0
    for it in items:
        cr_title = ((it.get("title") or [""])[0] or "").strip()
        score = _title_overlap(title, cr_title)
        if score > best_score:
            best_score = score
            best = it

    if not best or best_score < 0.45:
        _crossref_lru[key] = None
        try:
            _get_paper_meta_store().crossref_put(title, None)
        except Exception:
            pass
        return None

    doi = normalize_doi(best.get("DOI"))
    if not doi:
        _crossref_lru[key] = None
        try:
            _get_paper_meta_store().crossref_put(title, None)
        except Exception:
            pass
        return None

    authors = []
    for a in best.get("author") or []:
        name = f"{a.get('given', '')} {a.get('family', '')}".strip()
        if name:
            authors.append(name)

    year = None
    for df in ("published-print", "published-online", "issued", "created"):
        dp = (best.get(df) or {}).get("date-parts", [[]])
        if dp and dp[0] and dp[0][0]:
            year = dp[0][0]
            break

    venue = ((best.get("container-title") or [""])[0] or "").strip()
    parsed = {
        "doi": doi,
        "title": ((best.get("title") or [""])[0] or "").strip() or title,
        "authors": authors or None,
        "year": year,
        "venue": venue or None,
    }
    _crossref_lru[key] = parsed
    try:
        _get_paper_meta_store().crossref_put(title, parsed)
    except Exception:
        pass
    return parsed


def _enrich_web_hits_missing_doi(
    web_hits: List[Dict[str, Any]],
    local_title_blacklist: Set[str],
) -> Tuple[int, int]:
    """
    尝试为无 DOI 的 web hit 回填 DOI（先 URL 正则，再 CrossRef 标题检索）。
    返回 (crossref_lookups, resolved_with_crossref)。
    """
    lookups = 0
    resolved = 0
    for hit in web_hits:
        meta = hit.get("metadata") or {}
        if normalize_doi(meta.get("doi")):
            continue

        # 先试 URL 直接抽 DOI（零成本）
        url_doi = _extract_doi_from_text(_extract_web_url(hit))
        if url_doi:
            meta["doi"] = url_doi
            hit["metadata"] = meta
            continue

        # 再试标题走 CrossRef
        title = _extract_web_title(hit)
        nt = normalize_title(title)
        if not nt:
            continue
        # 标题已命中本地黑名单，无需再打 CrossRef
        if nt in local_title_blacklist:
            continue

        lookups += 1
        crossref = _crossref_lookup_by_title(title)
        if not crossref:
            continue

        meta["doi"] = crossref["doi"]
        # 补充 semantic / ncbi 等来源常缺的字段
        if not meta.get("title") and crossref.get("title"):
            meta["title"] = crossref["title"]
        if not meta.get("authors") and crossref.get("authors"):
            meta["authors"] = crossref["authors"]
        if not meta.get("year") and crossref.get("year"):
            meta["year"] = crossref["year"]
        if not meta.get("venue") and crossref.get("venue"):
            meta["venue"] = crossref["venue"]
        meta["doi_source"] = "crossref"
        hit["metadata"] = meta
        resolved += 1

    return lookups, resolved

def cross_source_dedup(
    local_chunks: List[Any],
    web_hits: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    拦截网络搜索中与本地文库重叠的文献。

    通过 DOI 匹配构建"本地黑名单"，过滤掉 web_hits 中已存在于本地的结果，
    把上下文 Token 留给真正的增量信息。

    Args:
        local_chunks: 本地检索结果（通常是 EvidenceChunk）
        web_hits: 网络搜索结果（raw dicts from UnifiedWebSearch）

    Returns:
        过滤后的 web_hits（仅保留本地不存在的文献）
    """
    if not local_chunks or not web_hits:
        return web_hits

    store = _get_paper_meta_store()

    # 构建本地 DOI + Title 黑名单
    # 基础集合：从 SQLite 全量拉取（已有索引，毫秒级）
    local_dois: Set[str] = store.all_dois()
    local_titles: Set[str] = store.all_normalized_titles()

    # 再从当前检索结果中补充（可能有新入库但还未写 SQLite 的）
    for c in local_chunks:
        doi_raw, title_raw, doc_id = _extract_local_fields(c)
        doi = normalize_doi(doi_raw)
        if doi:
            local_dois.add(doi)
        nt = normalize_title(title_raw)
        if nt:
            local_titles.add(nt)

    if not local_dois and not local_titles:
        return web_hits

    # 先为无 DOI 的 web hit 尝试补 DOI（URL / CrossRef）
    crossref_lookups, crossref_resolved = _enrich_web_hits_missing_doi(web_hits, local_titles)

    # 过滤 web_hits
    before = len(web_hits)
    filtered: List[Dict[str, Any]] = []
    for hit in web_hits:
        meta = hit.get("metadata") or {}
        web_doi = normalize_doi(meta.get("doi"))
        web_title = normalize_title(meta.get("title") or hit.get("title") or "")
        if (web_doi and web_doi in local_dois) or (web_title and web_title in local_titles):
            continue
        filtered.append(hit)

    removed = before - len(filtered)
    if removed > 0:
        logger.info(
            "cross_source_dedup: removed=%d, local_doi=%d, local_title=%d, crossref_lookups=%d, crossref_resolved=%d",
            removed,
            len(local_dois),
            len(local_titles),
            crossref_lookups,
            crossref_resolved,
        )
    return filtered


# ── 指纹去重 + 单文档上限 ─────────────────────────────

def dedup_and_diversify(
    candidates: List[Dict],
    per_doc_cap: int = 3,
) -> List[Dict]:
    """
    去重 + 单文档上限
    
    Args:
        candidates: 已按 rerank score 排序的候选列表
        per_doc_cap: 单文档最多保留条数
    
    Returns:
        去重且满足 per_doc_cap 的结果
    """
    seen_fingerprints: set = set()
    per_doc_count: Dict[str, int] = defaultdict(int)
    output: List[Dict] = []

    for c in candidates:
        text = c.get("content") or c.get("raw_content") or ""
        fp = _fingerprint(text)
        if fp and fp in seen_fingerprints:
            continue

        meta = c.get("metadata", {}) or {}
        doc_id = meta.get("doc_id") or meta.get("paper_id") or ""
        if per_doc_cap > 0 and doc_id and per_doc_count[doc_id] >= per_doc_cap:
            continue

        output.append(c)
        if fp:
            seen_fingerprints.add(fp)
        if doc_id:
            per_doc_count[doc_id] += 1

    return output
</file>

<file path="src/retrieval/evidence.py">
"""
检索证据包 - 统一检索层输出格式

EvidenceChunk / EvidencePack 确保上层可追溯，支持 to_context_string() 生成 LLM 上下文。
"""

import hashlib
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Literal, Optional

# chunk 引用哈希的长度（SHA-256 前 N 位十六进制）
REF_HASH_LENGTH = 8


@dataclass
class EvidenceChunk:
    """单个证据块"""

    chunk_id: str
    doc_id: str
    text: str
    score: float
    source_type: Literal["dense", "sparse", "graph", "web"]

    doc_title: Optional[str] = None
    authors: Optional[List[str]] = None
    year: Optional[int] = None
    url: Optional[str] = None
    doi: Optional[str] = None
    page_num: Optional[int] = None
    section_title: Optional[str] = None
    evidence_type: Optional[str] = None  # finding | method | interpretation | background | summary
    bbox: Optional[List[float]] = None  # Docling 物理坐标 [x0,y0,x1,y1]

    @property
    def ref_hash(self) -> str:
        """
        8 位 SHA-256 哈希，用作 LLM 上下文中的稳定引用标记。
        后续 resolve_response_citations() 会将其替换为正式的 cite_key。
        """
        return hashlib.sha256(self.chunk_id.encode("utf-8")).hexdigest()[:REF_HASH_LENGTH]

    @property
    def doc_group_key(self) -> str:
        """用于按文档分组的 key：Web 来源用 url，本地来源用 doc_id。"""
        if self.url:
            return self.url.strip()
        return (self.doc_id or self.chunk_id or "").strip()


@dataclass
class EvidencePack:
    """检索结果的统一封装"""

    query: str
    chunks: List[EvidenceChunk]

    total_candidates: int = 0
    retrieval_time_ms: float = 0.0
    sources_used: List[str] = field(default_factory=list)
    timestamp: datetime = field(default_factory=datetime.now)
    synthesis_meta: Optional[dict] = None  # 由 EvidenceSynthesizer 填充
    diagnostics: Optional[dict] = None  # 检索诊断信息（各阶段计数+耗时）

    def get_chunks_by_source(self, source_type: str) -> List[EvidenceChunk]:
        return [c for c in self.chunks if c.source_type == source_type]

    def to_context_string(self, max_chunks: int = 10) -> str:
        """生成 LLM 可用的上下文字符串（使用 ref_hash 作为引用标记）"""
        lines = []
        for chunk in self.chunks[:max_chunks]:
            ref = f"[{chunk.ref_hash}]"
            lines.append(f"{ref} {chunk.text}")
        return "\n\n".join(lines)
</file>

<file path="src/retrieval/google_search.py">
"""
Google Scholar / Google 搜索模块

基于 Playwright 的浏览器自动化搜索，输出与 hybrid_retriever 兼容的 RAG 格式。
支持 capsolver 扩展自动处理验证码。

使用方法:
---------
from src.retrieval.google_search import google_searcher

# 异步搜索 Scholar
results = await google_searcher.search_scholar("deep learning", limit=5)

# 异步搜索 Google
results = await google_searcher.search_google("machine learning tutorial", limit=5)

输出格式:
--------
[
    {
        "content": "摘要/片段",
        "score": 0.8,
        "metadata": {
            "source": "scholar" | "google",
            "doc_id": "标题",
            "title": "标题",
            "url": "https://...",
            "domain": "scholar.google.com",
            "search_query": "检索词",
            ...
        }
    }
]
"""

import asyncio
import json
import os
import platform
import random
import re
import time
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple
from urllib.parse import quote_plus, urlparse

from bs4 import BeautifulSoup

from config.settings import settings
from src.log import get_logger
from src.utils.cache import TTLCache, _make_key, get_cache

logger = get_logger(__name__)

# 浏览器复用：全局单例与空闲超时
_shared_browser_manager: Optional["_BrowserManager"] = None
_shared_browser_last_used: float = 0.0
_shared_browser_lock = asyncio.Lock()


async def _get_or_create_shared_browser_manager(
    timeout: int,
    user_data_dir: str,
    headless: Optional[bool],
    proxy: Optional[str],
    extension_path: Optional[str],
) -> Tuple[Optional["_BrowserManager"], Any]:
    """当 performance.google_search.browser_reuse 为 True 时返回复用的 (manager, context)。"""
    perf = getattr(settings, "perf_google_search", None)
    if not perf or not getattr(perf, "browser_reuse", True):
        return None, None
    
    max_idle = getattr(perf, "max_idle_seconds", 300) or 300
    async with _shared_browser_lock:
        global _shared_browser_manager, _shared_browser_last_used
        now = time.monotonic()
        if _shared_browser_manager is not None:
            if (now - _shared_browser_last_used) > max_idle:
                try:
                    await _shared_browser_manager.close()
                except Exception:
                    pass
                _shared_browser_manager = None
            else:
                _shared_browser_last_used = now
                if _shared_browser_manager.browsers:
                    return _shared_browser_manager, _shared_browser_manager.browsers[0]
                try:
                    await _shared_browser_manager.close()
                except Exception:
                    pass
                _shared_browser_manager = None
        manager = _BrowserManager(timeout=timeout)
        context = await manager.launch_persistent_browser(
            user_data_dir=user_data_dir,
            headless=headless,
            proxy=proxy,
            stealth_mode=True,
            extension_path=extension_path,
        )
        _shared_browser_manager = manager
        _shared_browser_last_used = time.monotonic()
        return manager, context


# ============================================================
# 配置读取
# ============================================================
def _get_google_search_config() -> Dict[str, Any]:
    """从 config 或 settings 读取 google_search 配置"""
    try:
        from config.settings import settings
        gs = getattr(settings, "google_search", None)
        if gs is not None:
            return {
                "enabled": getattr(gs, "enabled", True),
                "scholar_enabled": getattr(gs, "scholar_enabled", True),
                "google_enabled": getattr(gs, "google_enabled", False),
                "extension_path": getattr(gs, "extension_path", "extra_tools/CapSolverExtension"),
                "headless": getattr(gs, "headless", None),
                "proxy": getattr(gs, "proxy", None),
                "timeout": getattr(gs, "timeout", 60000),
                "max_results": getattr(gs, "max_results", 5),
                "user_data_dir": getattr(gs, "user_data_dir", None),
            }
    except Exception:
        pass
    
    config_path = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"
    raw: Dict[str, Any] = {}
    if config_path.exists():
        with open(config_path, "r", encoding="utf-8") as f:
            raw = json.load(f)
    gs = raw.get("google_search") or {}
    return {
        "enabled": gs.get("enabled", True),
        "scholar_enabled": gs.get("scholar_enabled", True),
        "google_enabled": gs.get("google_enabled", False),
        "extension_path": gs.get("extension_path", "extra_tools/CapSolverExtension"),
        "headless": gs.get("headless"),
        "proxy": gs.get("proxy"),
        "timeout": gs.get("timeout", 60000),
        "max_results": min(int(gs.get("max_results", 5)), 20),
        "user_data_dir": gs.get("user_data_dir"),
    }


# ============================================================
# 显示管理（虚拟显示器支持 - Linux 服务器）
# ============================================================
_virtual_display = None
_display_mode = None


def _is_display_available() -> bool:
    """检测是否有可用的显示器"""
    if platform.system() in ("Darwin", "Windows"):
        return True
    
    display = os.environ.get("DISPLAY")
    if display:
        try:
            import subprocess
            result = subprocess.run(["xdpyinfo"], capture_output=True, timeout=5)
            return result.returncode == 0
        except Exception:
            return True
    return False


def _start_virtual_display(width: int = 1920, height: int = 1080) -> Optional[object]:
    """启动虚拟显示器（仅 Linux）"""
    global _virtual_display
    
    if _virtual_display is not None:
        return _virtual_display
    
    if platform.system() != "Linux":
        return None
    
    try:
        from pyvirtualdisplay import Display
        logger.info(f"启动虚拟显示器 ({width}x{height})...")
        _virtual_display = Display(visible=False, size=(width, height), backend="xvfb")
        _virtual_display.start()
        logger.info(f"虚拟显示器已启动: DISPLAY={os.environ.get('DISPLAY')}")
        return _virtual_display
    except ImportError:
        logger.warning("pyvirtualdisplay 未安装，无法启动虚拟显示器")
        return None
    except Exception as e:
        logger.error(f"启动虚拟显示器失败: {e}")
        return None


def _stop_virtual_display():
    """停止虚拟显示器"""
    global _virtual_display
    if _virtual_display is not None:
        try:
            _virtual_display.stop()
        except Exception:
            pass
        _virtual_display = None


def _ensure_display(force_mode: Optional[str] = None) -> Tuple[bool, str]:
    """确保有可用的显示器"""
    global _display_mode
    
    mode = force_mode or os.environ.get("DISPLAY_MODE", "auto").lower()
    
    if mode == "headless":
        _display_mode = "headless"
        return False, "headless"
    
    if platform.system() in ("Darwin", "Windows"):
        _display_mode = "real"
        return True, "real"
    
    # Linux
    if _is_display_available():
        _display_mode = "real"
        return True, "real"
    else:
        display = _start_virtual_display()
        if display:
            _display_mode = "virtual"
            return True, "virtual"
        else:
            _display_mode = "headless"
            return False, "headless"


def _get_display_mode() -> str:
    """获取当前显示模式"""
    return _display_mode or "unknown"


# ============================================================
# Stealth 模式支持
# ============================================================
_STEALTH_AVAILABLE = "none"
_Stealth = None

try:
    from playwright_stealth import Stealth as _StealthClass
    _Stealth = _StealthClass
    _STEALTH_AVAILABLE = "v2"
except Exception:
    try:
        from playwright_stealth import stealth_async
        _STEALTH_AVAILABLE = "async"
    except Exception:
        _STEALTH_AVAILABLE = "none"


# ============================================================
# 随机化配置
# ============================================================
_USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0",
]


def _get_random_user_agent() -> str:
    return random.choice(_USER_AGENTS)


def _get_random_viewport() -> dict:
    return {
        "width": 1280 + random.randint(-50, 50),
        "height": 720 + random.randint(-50, 50)
    }


# ============================================================
# 人类行为模拟
# ============================================================
async def _simulate_human_behavior(page):
    """模拟人类用户行为"""
    try:
        await page.evaluate("""
            () => {
                const scrollAmount = Math.floor(Math.random() * window.innerHeight * 0.8);
                window.scrollBy(0, scrollAmount);
                setTimeout(() => {
                    window.scrollBy(0, -scrollAmount * 0.7);
                }, 500 + Math.random() * 1000);
            }
        """)
        
        viewport = await page.evaluate("() => ({ width: window.innerWidth, height: window.innerHeight })")
        if viewport:
            for _ in range(3):
                x = random.randint(0, viewport['width'] - 100)
                y = random.randint(0, viewport['height'] - 100)
                await page.mouse.move(x, y)
                await asyncio.sleep(random.uniform(0.1, 0.3))
        
        await asyncio.sleep(random.uniform(0.5, 2.0))
    except Exception as e:
        logger.debug(f"模拟人类行为时出错: {e}")


async def _simulate_human_scroll_to_bottom(page):
    """模拟滚动到页面底部"""
    try:
        await page.evaluate("""
            () => {
                const scrollHeight = document.body.scrollHeight;
                const duration = 1000 + Math.random() * 1500;
                const startTime = performance.now();
                const startPos = window.pageYOffset;
                
                function scrollStep(timestamp) {
                    const elapsed = timestamp - startTime;
                    const progress = Math.min(elapsed / duration, 1);
                    const ease = progress < 0.5 
                        ? 2 * progress * progress 
                        : 1 - Math.pow(-2 * progress + 2, 2) / 2;
                    window.scrollTo(0, startPos + (scrollHeight - startPos) * ease);
                    if (progress < 1) window.requestAnimationFrame(scrollStep);
                }
                window.requestAnimationFrame(scrollStep);
            }
        """)
        await asyncio.sleep(random.uniform(0.5, 1.5))
    except Exception as e:
        logger.debug(f"滚动到底部时出错: {e}")


# ============================================================
# 浏览器管理器
# ============================================================
class _BrowserManager:
    """浏览器管理器，负责创建和管理浏览器实例"""
    
    def __init__(self, timeout: int = 120000):
        self.playwright = None
        self.browsers = []
        self.browser_pids = []  # 记录 chromium 进程 PID
        self.timeout = timeout
        self._playwright_lock = asyncio.Lock()
    
    async def close(self):
        """关闭所有浏览器实例：先关所有页面，再关 context，最后 stop playwright"""
        import signal
        
        # 保存 PID 列表副本（关闭时使用）
        pids_to_check = list(self.browser_pids)
        
        # 1. 关闭所有页面
        for ctx in self.browsers:
            try:
                pages = getattr(ctx, "pages", []) or []
                for p in pages:
                    try:
                        if not p.is_closed():
                            await asyncio.wait_for(p.close(), timeout=5.0)
                    except Exception:
                        pass
            except Exception:
                pass
        
        # 2. 关闭 context
        for ctx in self.browsers:
            try:
                await asyncio.wait_for(ctx.close(), timeout=10.0)
                logger.debug("BrowserContext closed successfully")
            except Exception as e:
                logger.warning(f"关闭 BrowserContext 时出错: {e}")
        self.browsers = []
        await asyncio.sleep(0.5)
        
        # 3. 停止 playwright（这会清理底层进程）
        if self.playwright:
            try:
                await asyncio.wait_for(self.playwright.stop(), timeout=10.0)
                logger.debug("Playwright stopped successfully")
            except Exception as e:
                logger.warning(f"停止 Playwright 时出错: {e}")
            self.playwright = None
        
        # 4. 最后手段：如果还有残留进程，强制 kill
        await asyncio.sleep(0.5)
        for pid in pids_to_check:
            try:
                os.kill(pid, 0)  # 检查进程是否还存在
                logger.warning(f"检测到残留 chromium 进程 {pid}，尝试强制终止")
                os.kill(pid, signal.SIGTERM)
                await asyncio.sleep(0.5)
                try:
                    os.kill(pid, 0)
                    os.kill(pid, signal.SIGKILL)
                    logger.info(f"已强制终止 chromium 进程 {pid}")
                except OSError:
                    logger.info(f"chromium 进程 {pid} 已通过 SIGTERM 退出")
            except OSError:
                pass  # 进程已不存在，正常
        
        self.browser_pids = []
    
    async def _ensure_playwright(self):
        """确保 Playwright 已初始化"""
        async with self._playwright_lock:
            if not self.playwright:
                from playwright.async_api import async_playwright
                self.playwright = await async_playwright().start()
    
    async def launch_persistent_browser(
        self,
        user_data_dir: str,
        headless: Optional[bool] = None,
        proxy: Optional[str] = None,
        stealth_mode: bool = True,
        extension_path: Optional[str] = None,
        **kwargs
    ):
        """启动持久化浏览器实例"""
        await self._ensure_playwright()
        
        # 自动检测显示模式
        if headless is None:
            use_headed, display_mode = _ensure_display()
            headless = not use_headed
            logger.info(f"显示模式: {display_mode}, headless={headless}")
        
        os.makedirs(user_data_dir, exist_ok=True)
        
        context_options = {
            "headless": headless,
            "accept_downloads": True,
            "user_agent": _get_random_user_agent(),
            "viewport": _get_random_viewport(),
            "locale": "en-US",
            "timezone_id": "America/New_York",
            **kwargs
        }
        
        # 代理配置
        if proxy:
            if proxy.startswith("socks5h://"):
                proxy = proxy.replace("socks5h://", "socks5://")
            
            proxy_config = {"server": proxy}
            if '@' in proxy and '://' in proxy:
                try:
                    protocol, rest = proxy.split('://', 1)
                    if '@' in rest:
                        auth, server = rest.split('@', 1)
                        if ':' in auth:
                            username, password = auth.split(':', 1)
                            proxy_config = {
                                "server": f"{protocol}://{server}",
                                "username": username,
                                "password": password
                            }
                except Exception:
                    pass
            context_options["proxy"] = proxy_config
            logger.info(f"使用代理: {proxy}")
        
        # 隐身模式参数
        args = []
        if stealth_mode:
            args.extend([
                "--disable-blink-features=AutomationControlled",
                "--disable-features=AutomationControlled",
                "--no-sandbox"
            ])
        
        # 扩展加载
        if extension_path and os.path.exists(extension_path):
            manifest_path = os.path.join(extension_path, 'manifest.json')
            if os.path.exists(manifest_path):
                logger.info(f"加载 capsolver 扩展: {extension_path}")
                args.extend([
                    f'--disable-extensions-except={extension_path}',
                    f'--load-extension={extension_path}',
                ])
                context_options["ignore_default_args"] = ["--disable-extensions"]
        
        if args:
            context_options["args"] = args
        
        browser = await self.playwright.chromium.launch_persistent_context(
            user_data_dir,
            **context_options
        )
        
        # 尝试获取并记录 chromium 进程 PID
        pid = None
        try:
            # Playwright 内部属性路径（可能因版本而异）
            impl = getattr(browser, "_impl_obj", None)
            if impl:
                br = getattr(impl, "_browser", None)
                if br:
                    conn = getattr(br, "_connection", None)
                    if conn:
                        transport = getattr(conn, "_transport", None)
                        if transport:
                            proc = getattr(transport, "_proc", None)
                            if proc and hasattr(proc, "pid"):
                                pid = proc.pid
        except Exception:
            pass
        
        if pid:
            self.browser_pids.append(pid)
            logger.debug(f"记录 chromium 进程 PID: {pid}")
        
        # 应用隐身模式
        if stealth_mode and browser.pages:
            for page in browser.pages:
                await self._apply_stealth_mode(page)
        
        self.browsers.append(browser)
        return browser
    
    async def _apply_stealth_mode(self, page):
        """应用隐身模式脚本"""
        if _STEALTH_AVAILABLE == "v2" and _Stealth is not None:
            try:
                s = _Stealth()
                await s.apply_stealth_async(page)
                return
            except Exception:
                pass
        
        # 备用方案
        await page.add_init_script("""
            Object.defineProperty(navigator, 'webdriver', { get: () => undefined });
            Object.defineProperty(navigator, 'plugins', { get: () => [1, 2, 3, 4, 5] });
            Object.defineProperty(navigator, 'languages', { get: () => ['en-US', 'en'] });
        """)


# ============================================================
# Scholar HTML 解析器
# ============================================================
class _ScholarParser:
    """Google Scholar 页面解析器"""
    
    @staticmethod
    def extract_results(html_content: str) -> List[Dict[str, Any]]:
        """从 HTML 内容中提取搜索结果"""
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []
        
        for result in soup.find_all('div', class_='gs_r'):
            try:
                ri = result.find('div', class_='gs_ri')
                if not ri:
                    continue
                
                title, link, doi = _ScholarParser._extract_title_link_doi(ri)
                if not title:
                    continue
                
                pdf_link = _ScholarParser._extract_pdf_link(result)
                if not doi and pdf_link:
                    doi = _ScholarParser._extract_doi_from_url(pdf_link)
                
                cited_count = _ScholarParser._extract_cited_count(result)
                authors, pub_info = _ScholarParser._extract_author_publisher(ri)
                snippet = _ScholarParser._extract_snippet(ri)
                year = _ScholarParser._extract_year(pub_info)
                
                results.append({
                    'title': title,
                    'link': link,
                    'pdf_link': pdf_link,
                    'authors': authors,
                    'year': year,
                    'snippet': snippet,
                    'cited_count': cited_count,
                    'publication_info': pub_info,
                    'doi': doi
                })
            except Exception as e:
                logger.debug(f"解析结果时出错: {e}")
                continue
        
        return results
    
    @staticmethod
    def _extract_title_link_doi(ri) -> Tuple[Optional[str], Optional[str], Optional[str]]:
        """提取标题、链接和 DOI"""
        title_tag = ri.find('h3', class_='gs_rt')
        if not title_tag:
            return None, None, None
        
        a_tag = title_tag.find('a')
        if a_tag:
            title = ' '.join(a_tag.get_text(separator=' ').split())
            link = a_tag.get('href')
            doi = _ScholarParser._extract_doi_from_url(link)
        else:
            title = ' '.join(title_tag.get_text(separator=' ').split())
            link = None
            doi = None
        
        return title, link, doi
    
    @staticmethod
    def _extract_pdf_link(result) -> Optional[str]:
        """提取 PDF 链接"""
        pdf_div = result.find('div', class_='gs_or_ggsm')
        if pdf_div:
            a_pdf = pdf_div.find('a', href=True)
            if a_pdf:
                if re.search(r'\[PDF\]', a_pdf.get_text(), re.I):
                    return a_pdf.get('href')
        return None
    
    @staticmethod
    def _extract_cited_count(result) -> int:
        """提取引用次数"""
        cited_link = result.find('a', string=re.compile(r'(被引用次数：|Cited by)', re.I))
        if cited_link:
            m = re.search(r'(?:被引用次数：|Cited by)\s*(\d+)', cited_link.get_text(), re.I)
            if m:
                return int(m.group(1))
        return 0
    
    @staticmethod
    def _extract_author_publisher(ri) -> Tuple[List[str], Optional[str]]:
        """提取作者和出版信息"""
        gs_a = ri.find('div', class_='gs_a')
        if not gs_a:
            return [], None
        
        text = ' '.join(gs_a.get_text(separator=' ').split())
        parts = [p.strip() for p in text.split(" - ")]
        
        if not parts:
            return [], None
        
        authors = [a.strip() for a in parts[0].split(',') if a.strip()]
        pub_info = " - ".join(parts[1:]) if len(parts) > 1 else None
        
        return authors, pub_info
    
    @staticmethod
    def _extract_snippet(ri) -> Optional[str]:
        """提取摘要"""
        snippet = ri.find('div', class_='gs_rs')
        if snippet:
            return ' '.join(snippet.get_text(separator=' ').split())
        return None
    
    @staticmethod
    def _extract_year(pub_info: Optional[str]) -> Optional[int]:
        """从出版信息中提取年份"""
        if not pub_info:
            return None
        match = re.search(r'\b(19|20)\d{2}\b', pub_info)
        if match:
            return int(match.group(0))
        return None
    
    @staticmethod
    def _extract_doi_from_url(url: Optional[str]) -> Optional[str]:
        """从 URL 中提取 DOI"""
        if not url:
            return None
        
        match = re.search(r'(?:doi\.org|dx\.doi\.org)/+(10\.\d{4,}/[^\s&?#]+)', url, re.I)
        if match:
            return match.group(1).rstrip('/.')
        
        match = re.search(r'/(10\.\d{4,}/[a-zA-Z0-9._\-/()]+)', url)
        if match:
            doi = match.group(1).rstrip('/.')
            if '/' in doi and 10 < len(doi) < 100:
                return re.sub(r'[/.\-]+$', '', doi)
        
        return None
    
    @staticmethod
    def extract_total_results(html_content: str) -> Optional[int]:
        """提取总结果数"""
        try:
            soup = BeautifulSoup(html_content, 'html.parser')
            results_div = soup.find('div', {'class': 'gs_ab_mdw'})
            if results_div:
                text = results_div.get_text()
                match = re.search(r'(?:About|约)\s*([\d,]+)', text)
                if match:
                    return int(match.group(1).replace(',', ''))
        except Exception:
            pass
        return None


# ============================================================
# Google 搜索 HTML 解析器
# ============================================================
class _GoogleParser:
    """Google 搜索页面解析器"""
    
    @staticmethod
    def extract_results(html_content: str) -> List[Dict[str, Any]]:
        """从 Google 搜索结果 HTML 中提取结果"""
        soup = BeautifulSoup(html_content, 'html.parser')
        results = []
        
        # Google 搜索结果通常在 div.g 中
        for result in soup.find_all('div', class_='g'):
            try:
                # 提取标题和链接
                title_tag = result.find('h3')
                link_tag = result.find('a', href=True)
                
                if not title_tag or not link_tag:
                    continue
                
                title = title_tag.get_text(strip=True)
                link = link_tag.get('href', '')
                
                # 过滤无效链接
                if not link or link.startswith('/search') or 'google.com' in link:
                    continue
                
                # 提取摘要
                snippet = ""
                snippet_div = result.find('div', {'data-sncf': True}) or result.find('span', class_='aCOpRe')
                if snippet_div:
                    snippet = snippet_div.get_text(strip=True)
                else:
                    # 备用方案
                    for div in result.find_all('div'):
                        text = div.get_text(strip=True)
                        if len(text) > 50 and title not in text:
                            snippet = text
                            break
                
                results.append({
                    'title': title,
                    'link': link,
                    'snippet': snippet,
                })
            except Exception as e:
                logger.debug(f"解析 Google 结果时出错: {e}")
                continue
        
        return results


# ============================================================
# Google 搜索器主类
# ============================================================
@dataclass
class GoogleSearcher:
    """
    Google/Scholar 搜索器，输出与 RAG 检索结果统一格式。
    """
    
    _config: Dict[str, Any] = field(default_factory=dict)
    _browser_manager: Optional[_BrowserManager] = field(default=None, repr=False)
    _cache: Optional[TTLCache] = field(default=None, repr=False)
    
    # 默认分数
    SCHOLAR_DEFAULT_SCORE = 0.8
    GOOGLE_DEFAULT_SCORE = 0.6
    
    def __post_init__(self):
        if not self._config:
            self._config = _get_google_search_config()
        perf = getattr(settings, "perf_google_search", None)
        self._cache = (
            get_cache(
                getattr(perf, "cache_enabled", False),
                getattr(perf, "cache_ttl_seconds", 1800),
                prefix="google_search",
            )
            if perf else None
        )
    
    @property
    def enabled(self) -> bool:
        return bool(self._config.get("enabled", True))
    
    @property
    def scholar_enabled(self) -> bool:
        return bool(self._config.get("scholar_enabled", True))
    
    @property
    def google_enabled(self) -> bool:
        return bool(self._config.get("google_enabled", False))
    
    def _get_extension_path(self) -> Optional[str]:
        """获取扩展路径"""
        ext_path = self._config.get("extension_path", "extra_tools/CapSolverExtension")
        if ext_path:
            # 支持相对路径
            if not os.path.isabs(ext_path):
                base_path = Path(__file__).resolve().parents[2]
                ext_path = str(base_path / ext_path)
            if os.path.exists(ext_path):
                return ext_path
        return None
    
    def _get_user_data_dir(self) -> str:
        """获取浏览器用户数据目录"""
        user_data_dir = self._config.get("user_data_dir")
        if user_data_dir:
            return user_data_dir
        return os.path.join(os.path.expanduser("~"), ".google_search_browser")
    
    async def search_scholar(
        self,
        query: str,
        limit: Optional[int] = None,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        搜索 Google Scholar
        
        Args:
            query: 搜索查询
            limit: 最大结果数
            year_start: 开始年份
            year_end: 结束年份
        
        Returns:
            RAG 格式的结果列表
        """
        if not self.enabled or not self.scholar_enabled:
            logger.warning("Google Scholar search skipped: disabled in config")
            return []

        limit = limit or self._config.get("max_results", 5)
        cache_key = _make_key("google_scholar", query, limit, year_start, year_end)
        if self._cache:
            cached = self._cache.get(cache_key)
            if cached is not None:
                return cached
        
        timeout = self._config.get("timeout", 60000)
        headless = self._config.get("headless")
        proxy = self._config.get("proxy")
        user_data_dir = self._get_user_data_dir()
        
        results = []
        browser_manager = None
        context = None
        page = None
        reuse_browser = getattr(getattr(settings, "perf_google_search", None), "browser_reuse", True)

        try:
            shared_mgr, shared_ctx = await _get_or_create_shared_browser_manager(
                timeout=timeout,
                user_data_dir=user_data_dir,
                headless=headless,
                proxy=proxy,
                extension_path=self._get_extension_path(),
            )
            if shared_mgr is not None and shared_ctx is not None:
                browser_manager = shared_mgr
                context = shared_ctx
            else:
                browser_manager = _BrowserManager(timeout=timeout)
                context = await browser_manager.launch_persistent_browser(
                    user_data_dir=self._get_user_data_dir(),
                    headless=headless,
                    proxy=proxy,
                    stealth_mode=True,
                    extension_path=self._get_extension_path()
                )

            page = await context.new_page()
            page.set_default_timeout(timeout)
            await browser_manager._apply_stealth_mode(page)

            # 通过搜索框输入方式搜索（避免 URL 携带搜索词触发封控）
            await self._navigate_scholar_via_searchbox(
                page, query, year_start, year_end, timeout
            )
            
            # 检查验证码
            if await self._check_captcha(page):
                logger.warning("检测到验证码")
                display_mode = _get_display_mode()
                is_headed = display_mode in ("real", "virtual") or headless == False
                
                if is_headed:
                    logger.info("有头模式，等待验证码解决...")
                    await self._inject_capsolver_attributes(page)
                    
                    for i in range(90):
                        await asyncio.sleep(1)
                        if not await self._check_captcha(page):
                            logger.info("验证码已解决")
                            try:
                                await page.wait_for_load_state("domcontentloaded", timeout=15000)
                                await page.wait_for_selector('.gs_r', timeout=10000)
                            except Exception:
                                pass
                            break
                        if i > 0 and i % 15 == 0:
                            await self._inject_capsolver_attributes(page)
                    
                    if not await self._check_results(page, '.gs_r'):
                        logger.error("验证码处理后仍无结果")
                        return results
                else:
                    logger.error("无头模式下无法处理验证码")
                    return results
            
            # 获取总结果数
            html_content = await page.content()
            total = _ScholarParser.extract_total_results(html_content)
            if total:
                logger.info(f"总共约 {total} 条结果")
            
            # 多页抓取
            current_page = 1
            needed_pages = (limit + 9) // 10
            
            while current_page <= needed_pages and len(results) < limit:
                logger.info(f"处理第 {current_page} 页 (已收集 {len(results)}/{limit})")
                
                try:
                    await page.wait_for_selector('div.gs_r.gs_or.gs_scl', timeout=timeout/4)
                except Exception:
                    pass
                
                html = await page.content()
                page_results = _ScholarParser.extract_results(html)
                
                if not page_results:
                    logger.warning(f"第 {current_page} 页无结果")
                    break
                
                for r in page_results[:limit - len(results)]:
                    rag_item = self._to_scholar_rag_format(r, query)
                    results.append(rag_item)
                
                logger.info(f"第 {current_page} 页提取 {len(page_results)} 条")
                
                if len(results) >= limit or current_page >= needed_pages:
                    break
                
                # 翻页
                await self._random_delay(1.0, 3.0)
                await _simulate_human_scroll_to_bottom(page)
                
                if not await self._click_next_page(page, timeout):
                    break
                
                current_page += 1
                await self._random_delay()
                await _simulate_human_behavior(page)
            
            logger.info(f"Scholar 搜索完成，共 {len(results)} 条结果")
            if self._cache:
                self._cache.set(cache_key, results)
            return results

        except Exception as e:
            logger.error(f"Scholar 搜索出错: {e}")
            return results
        finally:
            if page:
                try:
                    await page.close()
                except Exception:
                    pass
            if browser_manager and not reuse_browser:
                await browser_manager.close()
            elif browser_manager and reuse_browser:
                global _shared_browser_last_used
                _shared_browser_last_used = time.monotonic()

    async def search_scholar_batch(
        self,
        queries: List[str],
        limit_per_query: int = 5,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        批量搜索 Google Scholar（串行执行）
        
        在同一个浏览器实例中依次执行多个查询，每个查询返回 limit_per_query 条结果。
        搜索完成后关闭浏览器。
        
        Args:
            queries: 查询列表
            limit_per_query: 每个查询的最大结果数
            year_start: 开始年份
            year_end: 结束年份
        
        Returns:
            合并后的 RAG 格式结果列表（len = queries数 × limit_per_query）
        """
        if not self.enabled or not self.scholar_enabled:
            logger.warning("Google Scholar batch search skipped: disabled in config")
            return []
        
        if not queries:
            return []
        
        # 检查缓存：如果所有查询都有缓存，直接返回
        all_results = []
        queries_to_search = []
        for q in queries:
            cache_key = _make_key("google_scholar", q, limit_per_query, year_start, year_end)
            if self._cache:
                cached = self._cache.get(cache_key)
                if cached is not None:
                    all_results.extend(cached)
                    continue
            queries_to_search.append(q)
        
        if not queries_to_search:
            logger.info(f"Scholar 批量搜索：所有 {len(queries)} 个查询都命中缓存")
            return all_results
        
        logger.info(f"Scholar 批量搜索：{len(queries_to_search)} 个查询待执行，{len(queries) - len(queries_to_search)} 个命中缓存")
        
        timeout = self._config.get("timeout", 60000)
        headless = self._config.get("headless")
        proxy = self._config.get("proxy")
        user_data_dir = self._get_user_data_dir()
        
        browser_manager = None
        context = None
        page = None
        
        try:
            # 启动浏览器（只启动一次）
            browser_manager = _BrowserManager(timeout=timeout)
            context = await browser_manager.launch_persistent_browser(
                user_data_dir=user_data_dir,
                headless=headless,
                proxy=proxy,
                stealth_mode=True,
                extension_path=self._get_extension_path()
            )
            
            page = await context.new_page()
            page.set_default_timeout(timeout)
            await browser_manager._apply_stealth_mode(page)
            
            # 串行执行每个查询
            for idx, query in enumerate(queries_to_search):
                logger.info(f"Scholar 批量搜索 [{idx+1}/{len(queries_to_search)}]: {query!r}")
                
                query_results = []
                try:
                    # 通过搜索框输入方式搜索（避免 URL 携带搜索词触发封控）
                    await self._navigate_scholar_via_searchbox(
                        page, query, year_start, year_end, timeout
                    )
                    
                    # 检查验证码
                    if await self._check_captcha(page):
                        logger.warning(f"检测到验证码 (query={query!r})")
                        display_mode = _get_display_mode()
                        is_headed = display_mode in ("real", "virtual") or headless == False
                        
                        if is_headed:
                            await self._inject_capsolver_attributes(page)
                            for i in range(90):
                                await asyncio.sleep(1)
                                if not await self._check_captcha(page):
                                    logger.info("验证码已解决")
                                    try:
                                        await page.wait_for_load_state("domcontentloaded", timeout=15000)
                                        await page.wait_for_selector('.gs_r', timeout=10000)
                                    except Exception:
                                        pass
                                    break
                                if i > 0 and i % 15 == 0:
                                    await self._inject_capsolver_attributes(page)
                            
                            if not await self._check_results(page, '.gs_r'):
                                logger.error(f"验证码处理后仍无结果 (query={query!r})")
                                continue
                        else:
                            logger.error(f"无头模式下无法处理验证码 (query={query!r})")
                            continue
                    
                    # 提取结果（支持多页抓取，每页 10 条）
                    needed_pages = max(1, (limit_per_query + 9) // 10)
                    current_page = 1

                    while current_page <= needed_pages and len(query_results) < limit_per_query:
                        try:
                            await page.wait_for_selector('div.gs_r.gs_or.gs_scl', timeout=timeout/4)
                        except Exception:
                            pass

                        html = await page.content()
                        page_results = _ScholarParser.extract_results(html)

                        if not page_results:
                            logger.warning(f"  Scholar batch: 第 {current_page} 页无结果 (query={query!r})")
                            break

                        for r in page_results[:limit_per_query - len(query_results)]:
                            rag_item = self._to_scholar_rag_format(r, query)
                            query_results.append(rag_item)

                        logger.info(f"  Scholar batch: 第 {current_page} 页提取 {len(page_results)} 条 (累计 {len(query_results)}/{limit_per_query})")

                        if len(query_results) >= limit_per_query or current_page >= needed_pages:
                            break

                        # 翻页
                        await self._random_delay(1.0, 3.0)
                        await _simulate_human_scroll_to_bottom(page)
                        if not await self._click_next_page(page, timeout):
                            break
                        current_page += 1
                        await self._random_delay()

                    logger.info(f"  -> 获取 {len(query_results)} 条结果 (query={query!r})")

                    # 缓存单个查询的结果
                    if self._cache and query_results:
                        cache_key = _make_key("google_scholar", query, limit_per_query, year_start, year_end)
                        self._cache.set(cache_key, query_results)

                    all_results.extend(query_results)
                    
                    # 查询间延迟（避免被封）
                    if idx < len(queries_to_search) - 1:
                        await self._random_delay(2.0, 4.0)
                
                except Exception as e:
                    logger.error(f"Scholar 批量搜索单个查询出错 (query={query!r}): {e}")
                    continue
            
            logger.info(f"Scholar 批量搜索完成，共 {len(all_results)} 条结果")
            return all_results
        
        except Exception as e:
            logger.error(f"Scholar 批量搜索出错: {e}")
            return all_results
        finally:
            # 关闭页面
            if page:
                try:
                    await page.close()
                except Exception:
                    pass
            # 关闭浏览器（批量搜索完成后始终关闭）
            if browser_manager:
                try:
                    await browser_manager.close()
                    logger.info("Scholar 批量搜索：浏览器已关闭")
                except Exception as e:
                    logger.warning(f"关闭浏览器时出错: {e}")

    async def search_google(
        self,
        query: str,
        limit: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        搜索 Google
        
        Args:
            query: 搜索查询
            limit: 最大结果数
        
        Returns:
            RAG 格式的结果列表
        """
        if not self.enabled or not self.google_enabled:
            logger.warning("Google search skipped: disabled in config")
            return []

        limit = limit or self._config.get("max_results", 5)
        cache_key = _make_key("google_web", query, limit)
        if self._cache:
            cached = self._cache.get(cache_key)
            if cached is not None:
                return cached
        
        timeout = self._config.get("timeout", 60000)
        headless = self._config.get("headless")
        proxy = self._config.get("proxy")
        user_data_dir = self._get_user_data_dir()

        results = []
        browser_manager = None
        context = None
        page = None
        reuse_browser = getattr(getattr(settings, "perf_google_search", None), "browser_reuse", True)

        try:
            shared_mgr, shared_ctx = await _get_or_create_shared_browser_manager(
                timeout=timeout,
                user_data_dir=user_data_dir,
                headless=headless,
                proxy=proxy,
                extension_path=self._get_extension_path(),
            )
            if shared_mgr is not None and shared_ctx is not None:
                browser_manager = shared_mgr
                context = shared_ctx
            else:
                browser_manager = _BrowserManager(timeout=timeout)
                context = await browser_manager.launch_persistent_browser(
                    user_data_dir=self._get_user_data_dir(),
                    headless=headless,
                    proxy=proxy,
                    stealth_mode=True,
                    extension_path=self._get_extension_path()
                )

            page = await context.new_page()
            page.set_default_timeout(timeout)
            await browser_manager._apply_stealth_mode(page)

            # 构建搜索 URL
            encoded = quote_plus(query)
            search_url = f"https://www.google.com/search?q={encoded}&hl=en"
            logger.info(f"Google 搜索 URL: {search_url}")
            
            await page.goto(search_url, wait_until="domcontentloaded")
            await self._random_delay()
            await _simulate_human_behavior(page)
            
            try:
                await page.wait_for_load_state("networkidle", timeout=timeout/2)
            except Exception:
                logger.warning("等待页面加载超时，继续处理...")
            
            # 检查验证码
            if await self._check_captcha(page):
                logger.warning("检测到验证码")
                display_mode = _get_display_mode()
                is_headed = display_mode in ("real", "virtual") or headless == False
                
                if is_headed:
                    logger.info("有头模式，等待验证码解决...")
                    for i in range(90):
                        await asyncio.sleep(1)
                        if not await self._check_captcha(page):
                            logger.info("验证码已解决")
                            break
                    
                    if not await self._check_results(page, 'div.g'):
                        logger.error("验证码处理后仍无结果")
                        return results
                else:
                    logger.error("无头模式下无法处理验证码")
                    return results
            
            # 提取结果
            html = await page.content()
            page_results = _GoogleParser.extract_results(html)
            
            for r in page_results[:limit]:
                rag_item = self._to_google_rag_format(r, query)
                results.append(rag_item)
            
            logger.info(f"Google 搜索完成，共 {len(results)} 条结果")
            if self._cache:
                self._cache.set(cache_key, results)
            return results

        except Exception as e:
            logger.error(f"Google 搜索出错: {e}")
            return results
        finally:
            if page:
                try:
                    await page.close()
                except Exception:
                    pass
            if browser_manager and not reuse_browser:
                await browser_manager.close()
            elif browser_manager and reuse_browser:
                global _shared_browser_last_used
                _shared_browser_last_used = time.monotonic()

    async def search_google_batch(
        self,
        queries: List[str],
        limit_per_query: int = 5,
    ) -> List[Dict[str, Any]]:
        """
        批量搜索 Google（串行执行）
        
        在同一个浏览器实例中依次执行多个查询，每个查询返回 limit_per_query 条结果。
        搜索完成后关闭浏览器。
        
        Args:
            queries: 查询列表
            limit_per_query: 每个查询的最大结果数
        
        Returns:
            合并后的 RAG 格式结果列表（len = queries数 × limit_per_query）
        """
        if not self.enabled or not self.google_enabled:
            logger.warning("Google batch search skipped: disabled in config")
            return []
        
        if not queries:
            return []
        
        # 检查缓存
        all_results = []
        queries_to_search = []
        for q in queries:
            cache_key = _make_key("google_web", q, limit_per_query)
            if self._cache:
                cached = self._cache.get(cache_key)
                if cached is not None:
                    all_results.extend(cached)
                    continue
            queries_to_search.append(q)
        
        if not queries_to_search:
            logger.info(f"Google 批量搜索：所有 {len(queries)} 个查询都命中缓存")
            return all_results
        
        logger.info(f"Google 批量搜索：{len(queries_to_search)} 个查询待执行，{len(queries) - len(queries_to_search)} 个命中缓存")
        
        timeout = self._config.get("timeout", 60000)
        headless = self._config.get("headless")
        proxy = self._config.get("proxy")
        user_data_dir = self._get_user_data_dir()
        
        browser_manager = None
        context = None
        page = None
        
        try:
            # 启动浏览器（只启动一次）
            browser_manager = _BrowserManager(timeout=timeout)
            context = await browser_manager.launch_persistent_browser(
                user_data_dir=user_data_dir,
                headless=headless,
                proxy=proxy,
                stealth_mode=True,
                extension_path=self._get_extension_path()
            )
            
            page = await context.new_page()
            page.set_default_timeout(timeout)
            await browser_manager._apply_stealth_mode(page)
            
            # 串行执行每个查询
            for idx, query in enumerate(queries_to_search):
                logger.info(f"Google 批量搜索 [{idx+1}/{len(queries_to_search)}]: {query!r}")
                
                query_results = []
                try:
                    # 构建搜索 URL
                    encoded = quote_plus(query)
                    search_url = f"https://www.google.com/search?q={encoded}&hl=en"
                    
                    await page.goto(search_url, wait_until="domcontentloaded")
                    await self._random_delay()
                    await _simulate_human_behavior(page)
                    
                    try:
                        await page.wait_for_load_state("networkidle", timeout=timeout/2)
                    except Exception:
                        pass
                    
                    # 检查验证码
                    if await self._check_captcha(page):
                        logger.warning(f"检测到验证码 (query={query!r})")
                        display_mode = _get_display_mode()
                        is_headed = display_mode in ("real", "virtual") or headless == False
                        
                        if is_headed:
                            for i in range(90):
                                await asyncio.sleep(1)
                                if not await self._check_captcha(page):
                                    logger.info("验证码已解决")
                                    break
                            
                            if not await self._check_results(page, 'div.g'):
                                logger.error(f"验证码处理后仍无结果 (query={query!r})")
                                continue
                        else:
                            logger.error(f"无头模式下无法处理验证码 (query={query!r})")
                            continue
                    
                    # 提取结果
                    html = await page.content()
                    page_results = _GoogleParser.extract_results(html)
                    
                    for r in page_results[:limit_per_query]:
                        rag_item = self._to_google_rag_format(r, query)
                        query_results.append(rag_item)
                    
                    logger.info(f"  -> 获取 {len(query_results)} 条结果")
                    
                    # 缓存单个查询的结果
                    if self._cache and query_results:
                        cache_key = _make_key("google_web", query, limit_per_query)
                        self._cache.set(cache_key, query_results)
                    
                    all_results.extend(query_results)
                    
                    # 查询间延迟（避免被封）
                    if idx < len(queries_to_search) - 1:
                        await self._random_delay(2.0, 4.0)
                
                except Exception as e:
                    logger.error(f"Google 批量搜索单个查询出错 (query={query!r}): {e}")
                    continue
            
            logger.info(f"Google 批量搜索完成，共 {len(all_results)} 条结果")
            return all_results
        
        except Exception as e:
            logger.error(f"Google 批量搜索出错: {e}")
            return all_results
        finally:
            # 关闭页面
            if page:
                try:
                    await page.close()
                except Exception:
                    pass
            # 关闭浏览器（批量搜索完成后始终关闭）
            if browser_manager:
                try:
                    await browser_manager.close()
                    logger.info("Google 批量搜索：浏览器已关闭")
                except Exception as e:
                    logger.warning(f"关闭浏览器时出错: {e}")

    def _build_scholar_url(
        self, 
        query: str, 
        year_start: Optional[int] = None, 
        year_end: Optional[int] = None
    ) -> str:
        """构建 Scholar 搜索 URL（仅作为搜索框方式的 fallback）"""
        encoded = quote_plus(query)
        url = f"https://scholar.google.com/scholar?q={encoded}&hl=en&start=0"
        
        if year_start:
            url += f"&as_ylo={year_start}"
        if year_end:
            url += f"&as_yhi={year_end}"
        
        return url
    
    async def _human_type(self, page, text: str):
        """模拟人类逐字输入，带随机延迟"""
        for char in text:
            await page.keyboard.type(char)
            delay = random.uniform(0.05, 0.18)
            # 空格和标点后略长暂停
            if char in ' ,.;:':
                delay += random.uniform(0.05, 0.15)
            # 偶尔更长暂停（模拟思考）
            if random.random() < 0.05:
                delay += random.uniform(0.3, 0.6)
            await asyncio.sleep(delay)
    
    async def _navigate_scholar_via_searchbox(
        self,
        page,
        query: str,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
        timeout: int = 60000,
    ) -> None:
        """
        通过搜索框输入方式执行 Scholar 搜索，避免 URL 携带搜索词触发反爬封控。
        
        流程: 加载首页(或复用已有结果页) → 搜索框输入 → 提交 → (可选)年份过滤
        若搜索框不可用（如验证码页面），自动回退到 URL 方式。
        """
        current_url = page.url or ""
        on_scholar = "scholar.google.com" in current_url
        
        if not on_scholar:
            # 加载 Scholar 首页（不含搜索词，不易触发封控）
            logger.info("加载 Scholar 首页...")
            await page.goto("https://scholar.google.com/", wait_until="domcontentloaded")
            await self._random_delay(1.5, 3.0)
            await _simulate_human_behavior(page)
        
        # 查找搜索框（首页和结果页通用选择器）
        search_input = None
        for sel in ['input#gs_hdr_tsi', 'input[name="q"]', 'textarea[name="q"]']:
            search_input = await page.query_selector(sel)
            if search_input:
                break
        
        if not search_input:
            # 搜索框未找到（可能是验证码页面），回退到 URL 方式
            logger.warning("Scholar 搜索框未找到，回退到 URL 直接跳转")
            url = self._build_scholar_url(query, year_start, year_end)
            await page.goto(url, wait_until="domcontentloaded")
            await self._random_delay()
            await _simulate_human_behavior(page)
            try:
                await page.wait_for_load_state("networkidle", timeout=timeout / 2)
            except Exception:
                pass
            return
        
        # 点击搜索框
        await search_input.click()
        await asyncio.sleep(random.uniform(0.3, 0.6))
        
        # 全选并清空已有内容
        modifier = "Meta" if platform.system() == "Darwin" else "Control"
        await page.keyboard.press(f"{modifier}+KeyA")
        await asyncio.sleep(random.uniform(0.1, 0.2))
        await page.keyboard.press("Backspace")
        await asyncio.sleep(random.uniform(0.2, 0.5))
        
        # 逐字输入查询词（模拟人类打字）
        await self._human_type(page, query)
        await asyncio.sleep(random.uniform(0.5, 1.0))
        
        # 提交搜索
        await page.keyboard.press("Enter")
        logger.info(f"Scholar 搜索框提交: {query!r}")
        
        # 等待结果加载
        try:
            await page.wait_for_load_state("domcontentloaded", timeout=timeout / 2)
        except Exception:
            pass
        await self._random_delay(1.0, 2.0)
        try:
            await page.wait_for_load_state("networkidle", timeout=timeout / 2)
        except Exception:
            logger.debug("等待 networkidle 超时，继续处理")
        
        # 年份过滤（在已建立 session 的基础上追加参数，不触发封控）
        if year_start or year_end:
            await self._apply_scholar_year_filter(page, year_start, year_end, timeout)
    
    async def _apply_scholar_year_filter(
        self, page, year_start: Optional[int], year_end: Optional[int], timeout: int
    ) -> None:
        """在已有搜索结果页上追加年份过滤（session 已建立，URL 参数修改安全）"""
        current_url = page.url
        params = ""
        if year_start:
            params += f"&as_ylo={year_start}"
        if year_end:
            params += f"&as_yhi={year_end}"
        if not params:
            return
        
        new_url = current_url + params
        logger.info(f"应用年份过滤: as_ylo={year_start}, as_yhi={year_end}")
        await page.goto(new_url, wait_until="domcontentloaded")
        await self._random_delay(1.0, 2.0)
        try:
            await page.wait_for_load_state("networkidle", timeout=timeout / 2)
        except Exception:
            pass
    
    def _to_scholar_rag_format(self, result: Dict, query: str) -> Dict[str, Any]:
        """转换为 RAG 格式（Scholar）"""
        url = result.get('link') or ''
        domain = urlparse(url).netloc if url else 'scholar.google.com'
        
        metadata = {
            "source": "scholar",
            "doc_id": result.get('title', ''),
            "title": result.get('title', ''),
            "url": url,
            "domain": domain,
            "search_query": query
        }
        
        if result.get('authors'):
            metadata["authors"] = result['authors']
        if result.get('year'):
            metadata["year"] = result['year']
        if result.get('publication_info'):
            metadata["venue"] = result['publication_info']
        if result.get('cited_count'):
            metadata["cited_by"] = result['cited_count']
        if result.get('pdf_link'):
            metadata["pdf_url"] = result['pdf_link']
        if result.get('doi'):
            metadata["doi"] = result['doi']
        
        return {
            "content": result.get('snippet') or result.get('title', ''),
            "score": self.SCHOLAR_DEFAULT_SCORE,
            "metadata": metadata
        }
    
    def _to_google_rag_format(self, result: Dict, query: str) -> Dict[str, Any]:
        """转换为 RAG 格式（Google）"""
        url = result.get('link') or ''
        domain = urlparse(url).netloc if url else 'google.com'
        if domain.startswith('www.'):
            domain = domain[4:]
        
        return {
            "content": result.get('snippet') or result.get('title', ''),
            "score": self.GOOGLE_DEFAULT_SCORE,
            "metadata": {
                "source": "google",
                "doc_id": result.get('title', ''),
                "title": result.get('title', ''),
                "url": url,
                "domain": domain,
                "search_query": query
            }
        }
    
    async def _check_captcha(self, page) -> bool:
        """检查是否有验证码"""
        try:
            selector = "form#captcha-form, #gs_captcha_ccl, div.g-recaptcha, #recaptcha"
            return await page.query_selector(selector) is not None
        except Exception:
            return False
    
    async def _check_results(self, page, selector: str) -> bool:
        """检查是否有搜索结果"""
        try:
            return await page.query_selector(selector) is not None
        except Exception:
            return False
    
    async def _inject_capsolver_attributes(self, page):
        """注入 capsolver 属性"""
        try:
            await page.evaluate("""
                () => {
                    const recaptcha = document.querySelector('iframe[src*="recaptcha"]') ||
                                     document.querySelector('.g-recaptcha');
                    if (recaptcha) return { success: true, isRecaptcha: true };
                    
                    const img = document.querySelector('img[src*="sorry/image"]') ||
                               document.querySelector('form img');
                    const input = document.querySelector('input[name="captcha"]') ||
                                 document.querySelector('form input[type="text"]');
                    
                    if (img && input) {
                        const id = 'gs-captcha-' + Date.now();
                        img.setAttribute('capsolver-image-to-text-source', id);
                        input.setAttribute('capsolver-image-to-text-result', id);
                        return { success: true };
                    }
                    return { success: false };
                }
            """)
        except Exception as e:
            logger.debug(f"注入 capsolver 属性失败: {e}")
    
    async def _click_next_page(self, page, timeout: int) -> bool:
        """点击下一页按钮"""
        try:
            selectors = [
                '#gs_n td:last-child a',
                'a.gs_btnPR',
                '.gs_btnPR'
            ]
            
            for selector in selectors:
                btn = await page.query_selector(selector)
                if btn:
                    url_before = page.url
                    await btn.scroll_into_view_if_needed()
                    await asyncio.sleep(0.5)
                    await btn.click()
                    
                    try:
                        await page.wait_for_load_state("networkidle", timeout=timeout/2)
                    except Exception:
                        pass
                    
                    if page.url != url_before:
                        return True
            
            return False
        except Exception as e:
            logger.debug(f"点击下一页失败: {e}")
            return False
    
    async def _random_delay(self, min_s: float = 1.0, max_s: float = 2.0):
        """随机延迟"""
        await asyncio.sleep(random.uniform(min_s, max_s))


async def cleanup_shared_browser():
    """清理全局共享的浏览器实例。在程序退出或测试完成后调用。"""
    global _shared_browser_manager, _shared_browser_last_used
    async with _shared_browser_lock:
        if _shared_browser_manager is not None:
            try:
                await _shared_browser_manager.close()
                logger.info("共享浏览器实例已关闭")
            except Exception as e:
                logger.warning(f"关闭共享浏览器时出错: {e}")
            finally:
                _shared_browser_manager = None
                _shared_browser_last_used = 0


def cleanup_shared_browser_sync():
    """同步版本的清理函数（在非异步上下文中使用）"""
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            asyncio.ensure_future(cleanup_shared_browser())
        else:
            loop.run_until_complete(cleanup_shared_browser())
    except RuntimeError:
        # 没有事件循环，创建新的
        asyncio.run(cleanup_shared_browser())


# 注册退出时自动清理
import atexit
atexit.register(cleanup_shared_browser_sync)


# 全局单例
google_searcher = GoogleSearcher()
</file>

<file path="src/retrieval/hybrid_retriever.py">
"""
混合检索模块（整合 HippoRAG）

支持三种检索模式：
1. vector: 纯向量检索（Dense + Sparse + Weighted RRF）
2. graph: 纯图检索（PPR）
3. hybrid: 向量 + 图融合（默认）

三阶段架构：
Stage 1: Recall (dense/sparse 各 80)
Stage 2: Weighted RRF Fusion (dense=0.6, sparse=0.4)
Stage 3: Rerank (输入 80-120 -> 输出 10)
"""

import os
import time
from collections import defaultdict
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError  # noqa: A001
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple
from dataclasses import dataclass

from pymilvus import AnnSearchRequest, RRFRanker

from config.settings import settings
from src.indexing.milvus_ops import milvus
from src.indexing.embedder import embedder
from src.graph.hippo_rag import HippoRAG, get_hippo_rag
from src.log import get_logger
from src.utils.cache import TTLCache, _make_key, get_cache

try:
    from src.retrieval.dedup import dedup_and_diversify
except ImportError:
    def dedup_and_diversify(candidates: List[Dict], per_doc_cap: int = 3) -> List[Dict]:
        return candidates

try:
    from src.retrieval.colbert_reranker import colbert_reranker
except ImportError:
    colbert_reranker = None


def _count_entities(query: str) -> int:
    """简单实体计数：2+ 中文字符短语或首字母大写的英文词"""
    import re
    zh = len(re.findall(r"[\u4e00-\u9fff]{2,}", query or ""))
    en = len(re.findall(r"\b[A-Z][a-z]+\b", query or ""))
    return zh + en


def should_use_hipporag(query: str) -> bool:
    """是否触发 HippoRAG：多实体 + 关系类关键词"""
    relation_keywords = ["关系", "关联", "因果", "影响", "链", "比较", "对比", "相互作用", "联系"]
    multi_entity = _count_entities(query) >= 2
    has_relation_word = any(kw in (query or "") for kw in relation_keywords)
    return bool(multi_entity and has_relation_word)


def _get_search_params():
    return getattr(settings.search, "dense_recall_k", 80), getattr(settings.search, "sparse_recall_k", 80)


def weighted_rrf(
    dense_list: List[Dict],
    sparse_list: List[Dict],
    w_dense: float = 0.6,
    w_sparse: float = 0.4,
    k: int = 60,
) -> List[tuple[str, float]]:
    """应用层加权 RRF 融合"""
    scores: Dict[str, float] = defaultdict(float)
    for rank, doc in enumerate(dense_list):
        cid = doc.get("chunk_id") or doc.get("metadata", {}).get("chunk_id") or str(id(doc))
        scores[cid] += w_dense / (k + rank + 1)
    for rank, doc in enumerate(sparse_list):
        cid = doc.get("chunk_id") or doc.get("metadata", {}).get("chunk_id") or str(id(doc))
        scores[cid] += w_sparse / (k + rank + 1)
    return sorted(scores.items(), key=lambda x: -x[1])


def _rerank_candidates(
    query: str,
    candidates: List[Dict],
    top_k: int,
) -> List[Dict]:
    """
    按 settings.search.reranker_mode 做精排，返回带 score 的 hit 列表。
    支持 bge_only | colbert_only | cascade。需 use_colbert_reranker=true 时 ColBERT 才生效。
    """
    if not candidates:
        return []
    docs = [c.get("content") or "" for c in candidates]
    use_colbert = getattr(settings.search, "use_colbert_reranker", False)
    mode = getattr(settings.search, "reranker_mode", "bge_only") if use_colbert else "bge_only"
    colbert_top_k = getattr(settings.search, "colbert_top_k", 30)

    if mode == "colbert_only" and colbert_reranker is not None:
        try:
            reranked = colbert_reranker.rerank(query, docs, top_k=min(top_k * 2, len(docs)))
            return [
                {**candidates[r.index], "score": r.score}
                for r in reranked
            ][:top_k]
        except Exception:
            # ColBERT 不可用/加载失败时，回退到 BGE
            pass

    if mode == "cascade" and colbert_reranker is not None:
        bge_k = min(colbert_top_k, len(docs))
        bge_out = embedder.rerank(query, docs, top_k=bge_k)
        bge_candidates = [candidates[r.index] for r in bge_out]
        bge_docs = [c.get("content") or "" for c in bge_candidates]
        try:
            colbert_out = colbert_reranker.rerank(query, bge_docs, top_k=min(top_k * 2, len(bge_docs)))
            return [
                {**bge_candidates[r.index], "score": r.score}
                for r in colbert_out
            ][:top_k]
        except Exception:
            # ColBERT 不可用/加载失败时，回退到 BGE
            pass

    # bge_only 或 ColBERT 不可用时
    reranked = embedder.rerank(query, docs, top_k=min(top_k * 2, len(docs)))
    return [
        {**candidates[r.index], "score": r.score}
        for r in reranked
    ][:top_k]


@dataclass
class RetrievalConfig:
    """检索配置"""
    mode: str = "hybrid"  # vector, graph, hybrid
    top_k: int = 10
    rerank: bool = True
    graph_weight: float = 0.3  # hybrid 模式下图检索权重
    year_start: Optional[int] = None  # 年份窗口起始（硬过滤）
    year_end: Optional[int] = None  # 年份窗口结束（硬过滤）


class HybridRetriever:
    """
    混合检索器

    整合向量检索和 HippoRAG 图检索
    """

    def __init__(self, graph_path: Optional[Path] = None):
        self.hippo: Optional[HippoRAG] = None
        self._graph_path = graph_path or (settings.path.data / "hippo_graph.json")
        self.logger = get_logger(__name__)
        perf = getattr(settings, "perf_retrieval", None)
        self._vector_cache: Optional[TTLCache] = (
            get_cache(
                getattr(perf, "cache_enabled", False),
                getattr(perf, "cache_ttl_seconds", 3600),
                prefix="retrieval_vector",
            )
            if perf else None
        )

    def _ensure_graph(self):
        """确保图谱已加载"""
        if self.hippo is None:
            if self._graph_path.exists():
                self.hippo = get_hippo_rag(self._graph_path)
            else:
                self.logger.warning(f"图谱文件不存在: {self._graph_path}")
                self.logger.warning("请先运行: python scripts/03b_build_graph.py")

    def retrieve_vector(
        self,
        query: str,
        collection: str,
        top_k: int = 10,
        rerank: bool = True,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
        diagnostics: Optional[Dict] = None,
    ) -> List[Dict]:
        """
        纯向量检索（Dense + Sparse + Weighted RRF + Rerank）
        Stage 1: 分离 dense/sparse 召回
        Stage 2: 应用层加权 RRF 融合
        Stage 3: Rerank (输入 80-120 -> 输出 10)

        Args:
            diagnostics: 可选字典，传入时填充各阶段 count + time_ms
        """
        dense_recall_k = getattr(settings.search, "dense_recall_k", 80)
        sparse_recall_k = getattr(settings.search, "sparse_recall_k", 80)
        rerank_input_k = getattr(settings.search, "rerank_input_k", 100)
        rerank_output_k = getattr(settings.search, "rerank_output_k", 10)
        w_dense = getattr(settings.search, "rrf_dense_weight", 0.6)
        w_sparse = getattr(settings.search, "rrf_sparse_weight", 0.4)
        per_doc_cap = getattr(settings.search, "per_doc_cap", 3)

        cache_key = _make_key("retrieval_vector", query, collection, top_k, rerank, year_start, year_end)
        if self._vector_cache:
            cached = self._vector_cache.get(cache_key)
            if cached is not None:
                if diagnostics is not None:
                    diagnostics["cache_hit"] = True
                return cached

        emb = embedder.encode([query])
        dense_vec = emb["dense"][0]
        sparse_vec = emb["sparse"]._getrow(0)
        sparse_coo = sparse_vec.tocoo()
        sparse_dict = {int(col): float(val) for col, val in zip(sparse_coo.col, sparse_coo.data)}

        output_fields = [
            "content", "raw_content", "paper_id", "chunk_id",
            "domain", "content_type", "chunk_type", "section_path", "page"
        ]

        # Stage 1: 分离召回（可选并行 + 超时）
        dense_req = AnnSearchRequest(
            data=[dense_vec.tolist()],
            anns_field="dense_vector",
            param={"metric_type": "COSINE", "params": {"nprobe": 16}},
            limit=dense_recall_k,
        )
        sparse_req = AnnSearchRequest(
            data=[sparse_dict],
            anns_field="sparse_vector",
            param={"metric_type": "IP"},
            limit=sparse_recall_k,
        )
        timeout_s = getattr(settings, "perf_retrieval", None)
        timeout_s = getattr(timeout_s, "timeout_seconds", 60) if timeout_s else 60
        parallel = getattr(getattr(settings, "perf_retrieval", None), "parallel_dense_sparse", True)
        max_workers = getattr(getattr(settings, "perf_retrieval", None), "max_workers", 4) or 4

        def _do_dense():
            return milvus.hybrid_search(
                collection=collection,
                reqs=[dense_req],
                ranker=RRFRanker(k=settings.search.rrf_k),
                limit=dense_recall_k,
                output_fields=output_fields,
                year_start=year_start,
                year_end=year_end,
            )

        def _do_sparse():
            return milvus.hybrid_search(
                collection=collection,
                reqs=[sparse_req],
                ranker=RRFRanker(k=settings.search.rrf_k),
                limit=sparse_recall_k,
                output_fields=output_fields,
                year_start=year_start,
                year_end=year_end,
            )

        dense_res, sparse_res = None, None
        t_recall = time.perf_counter()
        if parallel and max_workers >= 2:
            with ThreadPoolExecutor(max_workers=2) as ex:
                fd = ex.submit(_do_dense)
                fs = ex.submit(_do_sparse)
                try:
                    dense_res = fd.result(timeout=timeout_s)
                except (FuturesTimeoutError, Exception) as e:
                    self.logger.warning("dense search timeout or error: %s", e)
                try:
                    sparse_res = fs.result(timeout=timeout_s)
                except (FuturesTimeoutError, Exception) as e:
                    self.logger.warning("sparse search timeout or error: %s", e)
        else:
            dense_res = _do_dense()
            sparse_res = _do_sparse()
        recall_ms = (time.perf_counter() - t_recall) * 1000

        def _hit_to_doc(hit: Any) -> Dict:
            e = getattr(hit, "entity", hit) if hasattr(hit, "entity") else (hit.get("entity", hit) if isinstance(hit, dict) else hit)
            cid = e.get("chunk_id", "") if isinstance(e, dict) else getattr(e, "chunk_id", "")
            return {
                "chunk_id": cid,
                "content": e.get("content") if isinstance(e, dict) else getattr(e, "content", None),
                "raw_content": e.get("raw_content") if isinstance(e, dict) else getattr(e, "raw_content", None),
                "metadata": {
                    "paper_id": e.get("paper_id", "") if isinstance(e, dict) else getattr(e, "paper_id", ""),
                    "chunk_id": cid,
                    "domain": e.get("domain", "") if isinstance(e, dict) else getattr(e, "domain", ""),
                    "content_type": e.get("content_type", "") if isinstance(e, dict) else getattr(e, "content_type", ""),
                    "chunk_type": e.get("chunk_type", "") if isinstance(e, dict) else getattr(e, "chunk_type", ""),
                    "section_path": e.get("section_path", "") if isinstance(e, dict) else getattr(e, "section_path", ""),
                    "page": e.get("page", 0) if isinstance(e, dict) else getattr(e, "page", 0),
                }
            }

        dense_hits = [_hit_to_doc(h) for h in (dense_res[0] if dense_res else [])]
        sparse_hits = [_hit_to_doc(h) for h in (sparse_res[0] if sparse_res else [])]

        # Stage 2: Weighted RRF
        t_fusion = time.perf_counter()
        fused = weighted_rrf(
            dense_hits, sparse_hits,
            w_dense=w_dense, w_sparse=w_sparse, k=settings.search.rrf_k
        )
        cid_to_doc = {d["chunk_id"]: d for d in dense_hits + sparse_hits}
        candidates = []
        for cid, _ in fused[:rerank_input_k]:
            if cid in cid_to_doc:
                candidates.append(cid_to_doc[cid])
        fusion_ms = (time.perf_counter() - t_fusion) * 1000

        # Stage 3: Rerank（bge_only | colbert_only | cascade）
        t_rerank = time.perf_counter()
        reranked = False
        if rerank and candidates:
            docs = [c["content"] for c in candidates if c.get("content")]
            if docs:
                hits = _rerank_candidates(query, candidates, top_k=min(rerank_output_k * 2, len(candidates)))
                hits = dedup_and_diversify(hits, per_doc_cap=per_doc_cap)
                out = hits[:top_k]
                reranked = True
        rerank_ms = (time.perf_counter() - t_rerank) * 1000

        if not reranked:
            out = candidates[:top_k]

        # 填充诊断信息
        if diagnostics is not None:
            diagnostics["stages"] = {
                "dense_recall": {"count": len(dense_hits), "time_ms": round(recall_ms, 1)},
                "sparse_recall": {"count": len(sparse_hits), "time_ms": round(recall_ms, 1)},
                "fusion": {"count": len(candidates), "time_ms": round(fusion_ms, 1)},
                "rerank": {"count": len(out), "time_ms": round(rerank_ms, 1)},
            }

        if self._vector_cache:
            self._vector_cache.set(cache_key, out)
        return out

    def retrieve_graph(
        self,
        query: str,
        top_k: int = 10
    ) -> List[Dict]:
        """
        纯图检索（PPR）
        """
        self._ensure_graph()
        if self.hippo is None:
            return []

        # 从查询抽取种子实体
        seed_entities = self.hippo.get_seed_entities(query)
        if not seed_entities:
            return []

        # PPR 扩展
        ppr_results = self.hippo.personalized_pagerank(seed_entities, top_k=top_k)

        # 转换为标准格式
        hits = []
        for chunk_id, score in ppr_results:
            paper_id = self.hippo.G.nodes[chunk_id].get("paper_id", "")
            hits.append({
                "content": "",  # 需要从 Milvus 补充
                "score": score,
                "metadata": {
                    "chunk_id": chunk_id,
                    "paper_id": paper_id,
                },
                "source": "graph"
            })

        return hits

    def retrieve_hybrid(
        self,
        query: str,
        collection: str,
        top_k: int = 10,
        rerank: bool = True,
        graph_weight: float = 0.3,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
        diagnostics: Optional[Dict] = None,
    ) -> List[Dict]:
        """
        混合检索（向量 + 图融合）
        条件触发 HippoRAG：多实体 + 关系类关键词时合并图检索结果
        """
        # 1. 向量检索
        vector_hits = self.retrieve_vector(
            query,
            collection,
            top_k=top_k * 2,
            rerank=False,
            year_start=year_start,
            year_end=year_end,
            diagnostics=diagnostics,
        )

        # 2. 条件触发 HippoRAG（多实体+关系词时融合图检索）
        fused_hits = vector_hits
        year_window_enabled = year_start is not None or year_end is not None
        if should_use_hipporag(query) and not year_window_enabled:
            self._ensure_graph()
            if self.hippo is not None:
                fused_hits = self.hippo.retrieve_with_graph(
                    query,
                    vector_hits,
                    top_k=top_k * 2,
                    graph_weight=graph_weight
                )

        # 3. Rerank（bge_only | colbert_only | cascade）
        if rerank and fused_hits:
            hits_with_content = [h for h in fused_hits if h.get("content")]
            if hits_with_content:
                rerank_output_k = getattr(settings.search, "rerank_output_k", 10)
                fused_hits = _rerank_candidates(query, hits_with_content, top_k=min(rerank_output_k * 2, len(hits_with_content)))
                per_doc_cap = getattr(settings.search, "per_doc_cap", 3)
                fused_hits = dedup_and_diversify(fused_hits, per_doc_cap=per_doc_cap)

        return fused_hits[:top_k]

    def retrieve(
        self,
        query: str,
        collection: str = None,
        config: RetrievalConfig = None,
        diagnostics: Optional[Dict] = None,
    ) -> List[Dict]:
        """
        统一检索入口

        Args:
            query: 查询文本
            collection: Milvus collection 名（默认 global）
            config: 检索配置
            diagnostics: 可选字典，传入时填充检索诊断信息
        """
        if config is None:
            config = RetrievalConfig()

        if collection is None:
            collection = settings.collection.global_

        if config.mode == "vector":
            return self.retrieve_vector(
                query,
                collection,
                config.top_k,
                config.rerank,
                config.year_start,
                config.year_end,
                diagnostics=diagnostics,
            )
        elif config.mode == "graph":
            return self.retrieve_graph(query, config.top_k)
        else:  # hybrid
            return self.retrieve_hybrid(
                query,
                collection,
                config.top_k,
                config.rerank,
                config.graph_weight,
                config.year_start,
                config.year_end,
                diagnostics=diagnostics,
            )


# 全局实例
retriever = HybridRetriever()
</file>

<file path="src/retrieval/ncbi_search.py">
"""
NCBI PubMed 生物医学文献搜索（E-Utilities 免费 API）。

使用 esearch 获取 PubMed ID 列表，再用 esummary 获取元数据并通过 efetch 拉取摘要，
输出与 unified_web_search 兼容的 RAG 格式（content / score / metadata）。
"""

from __future__ import annotations

import asyncio
import xml.etree.ElementTree as ET
from typing import Any, Dict, List, Optional

import aiohttp

from src.log import get_logger
from src.utils.cache import TTLCache, _make_key

logger = get_logger(__name__)

ESEARCH_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
ESUMMARY_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esummary.fcgi"
EFETCH_URL = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi"


class NCBISearcher:
    """
    PubMed E-Utilities 搜索器。

    先调 esearch 获取相关 PubMed ID，
    再调 esummary 批量拉取元数据（标题、作者、年份、DOI），
    最后用 efetch 拉取摘要文本。
    结果带 TTL 缓存，避免重复请求。
    """

    def __init__(
        self,
        api_key: str = "",
        timeout_seconds: int = 20,
        cache_ttl: int = 3600,
        cache_maxsize: int = 256,
    ):
        self.api_key = api_key
        self.timeout_seconds = timeout_seconds
        self._cache = TTLCache(maxsize=cache_maxsize, ttl_seconds=cache_ttl)

    async def close(self) -> None:
        # 保持兼容：当前实现为短生命周期 session，无需显式关闭。
        return None

    # ── 内部请求 ─────────────────────────────────────────────────────────────

    def _base_params(self) -> Dict[str, str]:
        params: Dict[str, str] = {"retmode": "json"}
        if self.api_key:
            params["api_key"] = self.api_key
        return params

    async def _esearch(
        self,
        session: aiohttp.ClientSession,
        query: str,
        limit: int,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[str]:
        """返回相关度排序的 PubMed ID 列表。"""
        term = query
        if year_start is not None or year_end is not None:
            if year_start is not None and year_end is not None:
                y0, y1 = sorted((int(year_start), int(year_end)))
                term += f' AND ("{y0}"[Date - Publication] : "{y1}"[Date - Publication])'
            elif year_start is not None:
                term += f' AND ("{int(year_start)}"[Date - Publication] : "3000"[Date - Publication])'
            elif year_end is not None:
                term += f' AND ("1000"[Date - Publication] : "{int(year_end)}"[Date - Publication])'
        params = self._base_params()
        params.update(
            {
                "db": "pubmed",
                "term": term,
                "retmax": str(limit),
                "sort": "relevance",
            }
        )
        async with session.get(ESEARCH_URL, params=params) as resp:
            resp.raise_for_status()
            data = await resp.json(content_type=None)
        return data.get("esearchresult", {}).get("idlist", [])

    async def _esummary(
        self,
        session: aiohttp.ClientSession,
        ids: List[str],
    ) -> Dict[str, Any]:
        """批量获取 PubMed 元数据记录，返回 {pmid: doc_dict}。"""
        if not ids:
            return {}
        params = self._base_params()
        params.update(
            {
                "db": "pubmed",
                "id": ",".join(ids),
                "version": "2.0",
            }
        )
        async with session.get(ESUMMARY_URL, params=params) as resp:
            resp.raise_for_status()
            data = await resp.json(content_type=None)
        return data.get("result", {})

    async def _efetch_abstracts(
        self,
        session: aiohttp.ClientSession,
        ids: List[str],
    ) -> Dict[str, str]:
        """批量获取摘要文本，返回 {pmid: abstract}。"""
        if not ids:
            return {}
        params = self._base_params()
        params.update(
            {
                "db": "pubmed",
                "id": ",".join(ids),
                "retmode": "xml",
            }
        )
        async with session.get(EFETCH_URL, params=params) as resp:
            resp.raise_for_status()
            xml_text = await resp.text()

        out: Dict[str, str] = {}
        try:
            root = ET.fromstring(xml_text)
        except ET.ParseError:
            return out

        for article in root.findall(".//PubmedArticle"):
            pmid = (
                article.findtext(".//MedlineCitation/PMID")
                or article.findtext(".//PMID")
                or ""
            ).strip()
            if not pmid:
                continue

            parts: List[str] = []
            for node in article.findall(".//Abstract/AbstractText"):
                text = "".join(node.itertext()).strip()
                if not text:
                    continue
                label = (node.attrib.get("Label") or "").strip()
                if label:
                    parts.append(f"{label}: {text}")
                else:
                    parts.append(text)

            abstract = " ".join(parts).strip()
            if abstract:
                out[pmid] = abstract
        return out

    # ── 格式转换 ─────────────────────────────────────────────────────────────

    @staticmethod
    def _to_rag_hit(pmid: str, doc: Dict[str, Any], abstract: str = "") -> Dict[str, Any]:
        """将 esummary 记录转换为 RAG-compatible hit。"""
        title = (doc.get("title") or "").strip().rstrip(".")

        authors_raw = doc.get("authors") or []
        authors = [a.get("name", "") for a in authors_raw if a.get("name")]

        pub_date = doc.get("pubdate") or doc.get("epubdate") or ""
        year = pub_date[:4] if pub_date else ""

        doi = ""
        for aid in doc.get("articleids") or []:
            if aid.get("idtype") == "doi":
                doi = (aid.get("value") or "").strip()
                break

        url = f"https://pubmed.ncbi.nlm.nih.gov/{pmid}/"

        content_parts = [title] if title else [f"PubMed:{pmid}"]
        if authors:
            content_parts.append(f"Authors: {', '.join(authors[:5])}")
        if year:
            content_parts.append(f"Year: {year}")
        if doi:
            content_parts.append(f"DOI: {doi}")
        if abstract:
            content_parts.append(f"Abstract: {abstract[:900]}")

        return {
            "content": " | ".join(content_parts),
            "score": 0.98,
            "metadata": {
                "source": "ncbi",
                "title": title,
                "url": url,
                "doi": doi,
                "authors": authors,
                "year": year,
                "pmid": pmid,
                "abstract": abstract,
            },
        }

    # ── 公开接口 ─────────────────────────────────────────────────────────────

    async def search(
        self,
        query: str,
        limit: int = 5,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        异步搜索 PubMed，返回最多 `limit` 条 RAG-compatible 结果。
        相同 query+limit 命中缓存时直接返回，不重复请求。
        """
        cache_key = _make_key("ncbi", query, limit, year_start, year_end)
        cached = self._cache.get(cache_key)
        if cached is not None:
            logger.debug(f"NCBI cache hit: {query!r}")
            return cached

        try:
            timeout = aiohttp.ClientTimeout(total=self.timeout_seconds)
            connector = aiohttp.TCPConnector(limit=5)
            async with aiohttp.ClientSession(timeout=timeout, connector=connector) as session:
                ids = await self._esearch(
                    session,
                    query,
                    limit,
                    year_start=year_start,
                    year_end=year_end,
                )
                if not ids:
                    logger.info(f"NCBI esearch 无结果: {query!r}")
                    self._cache.set(cache_key, [])
                    return []

                result_map = await self._esummary(session, ids)
                abstracts = await self._efetch_abstracts(session, ids)

                hits: List[Dict[str, Any]] = []
                for pmid in ids:
                    doc = result_map.get(pmid) or result_map.get(str(pmid))
                    if not isinstance(doc, dict):
                        continue
                    hits.append(self._to_rag_hit(pmid, doc, abstracts.get(str(pmid), "")))

            self._cache.set(cache_key, hits)
            logger.info(f"NCBI 搜索完成: query={query!r}, 返回 {len(hits)} 条")
            return hits

        except asyncio.TimeoutError:
            logger.warning(f"NCBI 搜索超时: {query!r}")
            return []
        except Exception as e:
            logger.error(f"NCBI 搜索失败: {e}")
            return []


# ── 全局单例（无需 API key，免费端点）─────────────────────────────────────────

_ncbi_searcher_instance: Optional[NCBISearcher] = None


def get_ncbi_searcher() -> NCBISearcher:
    global _ncbi_searcher_instance
    if _ncbi_searcher_instance is None:
        try:
            from config.settings import settings
            cfg = getattr(settings, "ncbi", None)
            _ncbi_searcher_instance = NCBISearcher(
                api_key=getattr(cfg, "api_key", "") or "",
                timeout_seconds=int(getattr(cfg, "timeout_seconds", 20)),
                cache_ttl=int(getattr(cfg, "cache_ttl_seconds", 3600)),
                cache_maxsize=int(getattr(cfg, "cache_maxsize", 256)),
            )
        except Exception:
            _ncbi_searcher_instance = NCBISearcher()
    return _ncbi_searcher_instance
</file>

<file path="src/retrieval/web_search.py">
"""
Tavily 网络搜索模块

与 hybrid_retriever 输出格式兼容，可作为 RAG 的补充素材提交给 LangGraph/LLM。
支持同步/异步搜索、可选的 LLM 多查询扩展。
"""

import asyncio
import json
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

from pydantic import BaseModel, Field, model_validator

from config.settings import settings
from src.log import get_logger
from src.utils.cache import TTLCache, _make_key, get_cache

logger = get_logger(__name__)


class _QueryExpansionResponse(BaseModel):
    queries: List[str] = Field(default_factory=list)

    @model_validator(mode="before")
    @classmethod
    def _accept_legacy_array(cls, data: Any) -> Any:
        if isinstance(data, list):
            return {"queries": data}
        return data


def _get_web_search_config() -> Dict[str, Any]:
    """从 config 或 settings 读取 web_search 配置"""
    try:
        from config.settings import settings
        ws = getattr(settings, "web_search", None)
        if ws is not None:
            return {
                "enabled": getattr(ws, "enabled", True),
                "api_key": getattr(ws, "api_key", "") or "",
                "search_depth": getattr(ws, "search_depth", "advanced"),
                "max_results": getattr(ws, "max_results", 5),
                "include_answer": getattr(ws, "include_answer", True),
                "include_domains": getattr(ws, "include_domains", []) or [],
                "exclude_domains": getattr(ws, "exclude_domains", []) or [],
                "enable_query_optimizer": getattr(ws, "enable_query_optimizer", True),
                "enable_query_expansion": getattr(ws, "enable_query_expansion", False),
                "query_expansion_llm": getattr(ws, "query_expansion_llm", "deepseek"),
                "max_queries": getattr(ws, "max_queries", 4),
            }
    except Exception:
        pass
    config_path = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"
    raw: Dict[str, Any] = {}
    if config_path.exists():
        with open(config_path, "r", encoding="utf-8") as f:
            raw = json.load(f)
    ws = raw.get("web_search") or {}
    include = ws.get("include_domains")
    exclude = ws.get("exclude_domains")
    if isinstance(include, str):
        include = [x.strip() for x in include.split(",") if x.strip()]
    if isinstance(exclude, str):
        exclude = [x.strip() for x in exclude.split(",") if x.strip()]
    return {
        "enabled": ws.get("enabled", True),
        "api_key": (ws.get("api_key") or "").strip(),
        "search_depth": ws.get("search_depth", "advanced"),
        "max_results": min(int(ws.get("max_results", 5)), 10),
        "include_answer": ws.get("include_answer", True),
        "include_domains": include or [],
        "exclude_domains": exclude or [],
        "enable_query_optimizer": ws.get("enable_query_optimizer", True),
        "enable_query_expansion": ws.get("enable_query_expansion", False),
        "query_expansion_llm": (ws.get("query_expansion_llm") or "deepseek").strip(),
        "max_queries": min(int(ws.get("max_queries", 4)), 8),
    }


def _domain_from_url(url: str) -> str:
    try:
        parsed = urlparse(url or "")
        netloc = (parsed.netloc or "").strip()
        if netloc.startswith("www."):
            netloc = netloc[4:]
        return netloc or ""
    except Exception:
        return ""


def _normalize_hit(
    title: str,
    url: str,
    content: str,
    score: float,
    search_query: str,
) -> Dict[str, Any]:
    """转为与 hybrid_retriever 兼容的 hit 格式"""
    domain = _domain_from_url(url)
    # content 用于 context_packer，保留摘要
    text = (content or "").strip() or title
    return {
        "content": text,
        "score": float(score),
        "metadata": {
            "source": "web",
            "doc_id": title or url or "web",
            "title": title or "无标题",
            "url": url or "",
            "domain": domain,
            "search_query": search_query or "",
        },
    }


@dataclass
class TavilySearcher:
    """
    Tavily 搜索器，输出与 RAG 检索结果统一格式，便于与 context_packer / LangGraph 集成。
    """

    _config: Dict[str, Any] = field(default_factory=dict)
    _cache: Optional[TTLCache] = field(default=None, repr=False)

    def __post_init__(self):
        if not self._config:
            self._config = _get_web_search_config()
        perf = getattr(settings, "perf_web_search", None)
        self._cache = (
            get_cache(
                getattr(perf, "cache_enabled", False),
                getattr(perf, "cache_ttl_seconds", 3600),
                prefix="tavily",
            )
            if perf else None
        )

    @property
    def enabled(self) -> bool:
        return bool(self._config.get("enabled") and (self._config.get("api_key") or "").strip())

    def _generate_queries_sync(self, user_query: str) -> List[str]:
        """同步：使用 LLM 生成多查询（可选）"""
        if not self._config.get("enable_query_expansion"):
            return [user_query]
        try:
            from src.llm import LLMManager
            cfg_path = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"
            manager = LLMManager.from_json(str(cfg_path))
            provider = (self._config.get("query_expansion_llm") or "deepseek").strip()
            if not manager.is_available(provider):
                return [user_query]
            client = manager.get_client(provider)
            from datetime import datetime
            year = datetime.now().strftime("%Y")
            prompt = f"""
Act as a search query optimizer for the Tavily API. 
Generate 3-5 distinct search queries based on the user's input: "{user_query}"

**Optimization Rules:**
1.  **Refine vs. Convert:**
    - If the input is a **question**, refine it to be more specific or technical (e.g., add "benefits," "comparison," or "technical specs").
    - If the input is **keywords**, convert them into a natural language question to capture intent.
2.  **Global Knowledge:** If the input is not in English, you MUST provide at least 2 queries in English to access a broader index.
3.  **Search Angles:** - Query 1: A deep-dive "How" or "Why" question.
    - Query 2: A specific keyword string (3-6 words) focused on technical terms or entities.
    - Query 3: A trend-focused query including the year {year} (if relevant).
4.  **Format:** Output ONLY a raw JSON array of strings. No Markdown, no code blocks.

Example Output: ["How does X affect Y in {year}?", "X technical architecture overview", "latest developments in X"]
            """
            resp = client.chat(
                messages=[
                    {"role": "system", "content": "You are a search query generator. Output ONLY a valid JSON array of query strings."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=600,
                response_model=_QueryExpansionResponse,
            )
            parsed: Optional[_QueryExpansionResponse] = resp.get("parsed_object")
            if parsed is None:
                raw_text = (resp.get("final_text") or "").strip()
                if raw_text:
                    parsed = _QueryExpansionResponse.model_validate_json(raw_text)
            queries = [str(q).strip() for q in (parsed.queries if parsed else []) if str(q).strip()]
            if queries:
                return queries[: self._config.get("max_queries", 4)]
        except Exception:
            pass
        return [user_query]

    async def _generate_queries_async(self, user_query: str) -> List[str]:
        """异步：LLM 生成多查询（在 executor 中跑同步）"""
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._generate_queries_sync, user_query)

    def _search_tavily_sync(self, queries: List[str]) -> List[Dict[str, Any]]:
        """同步调用 Tavily API，返回标准化 hit 列表"""
        try:
            from tavily import TavilyClient
        except ImportError as e:
            logger.warning(f"Tavily search skipped: tavily-python not installed ({e}).")
            return []
        api_key = (self._config.get("api_key") or "").strip()
        if not api_key:
            logger.warning("Tavily search skipped: api_key empty in config.")
            return []
        try:
            client = TavilyClient(api_key=api_key)
        except Exception as e:
            logger.error(f"Tavily search failed (client init): {type(e).__name__}: {e}")
            return []
        max_results = min(self._config.get("max_results", 5), 10)
        search_depth = self._config.get("search_depth", "advanced")
        include_answer = self._config.get("include_answer", True)
        include_domains = self._config.get("include_domains") or []
        exclude_domains = self._config.get("exclude_domains") or []
        seen_urls: set = set()
        out: List[Dict[str, Any]] = []
        for q in queries:
            try:
                params = {
                    "query": q,
                    "max_results": max_results,
                    "search_depth": search_depth,
                    "include_answer": include_answer,
                }
                if include_domains:
                    params["include_domains"] = include_domains
                if exclude_domains:
                    params["exclude_domains"] = exclude_domains
                response = client.search(**params)
                for item in response.get("results") or []:
                    url = item.get("url") or ""
                    if url and url not in seen_urls:
                        seen_urls.add(url)
                        hit = _normalize_hit(
                            title=item.get("title") or "无标题",
                            url=url,
                            content=item.get("content") or "",
                            score=float(item.get("score", 0)),
                            search_query=q,
                        )
                        out.append(hit)
            except Exception as e:
                logger.error(f"Tavily search failed (query={q!r}): {type(e).__name__}: {e}")
                continue
        if not out:
            logger.warning("Tavily search returned 0 results (all queries failed or no hits).")
        out.sort(key=lambda x: x.get("score", 0), reverse=True)
        return out

    def search(
        self,
        query: str,
        *,
        use_query_expansion: Optional[bool] = None,
    ) -> List[Dict[str, Any]]:
        """
        同步搜索。返回与 hybrid_retriever 兼容的 hit 列表。
        """
        if not self.enabled:
            logger.warning("Tavily search skipped: disabled or api_key not set.")
            return []
        expand = use_query_expansion if use_query_expansion is not None else self._config.get("enable_query_expansion", False)
        cache_key = _make_key("tavily", query, expand)
        if self._cache:
            cached = self._cache.get(cache_key)
            if cached is not None:
                return cached
        queries = self._generate_queries_sync(query) if expand else [query]
        if not queries:
            logger.warning("Tavily search skipped: no queries (query expansion may have failed).")
            return []
        timeout_s = getattr(getattr(settings, "perf_web_search", None), "timeout_seconds", 30) or 30
        try:
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:
                future = ex.submit(self._search_tavily_sync, queries)
                out = future.result(timeout=timeout_s)
        except concurrent.futures.TimeoutError:
            logger.warning("Tavily search timeout: %ss", timeout_s)
            out = []
        if self._cache and out is not None:
            self._cache.set(cache_key, out)
        return out

    async def async_search(
        self,
        query: str,
        *,
        use_query_expansion: Optional[bool] = None,
    ) -> List[Dict[str, Any]]:
        """异步搜索。"""
        if not self.enabled:
            return []
        expand = use_query_expansion if use_query_expansion is not None else self._config.get("enable_query_expansion", False)
        cache_key = _make_key("tavily", query, expand)
        if self._cache:
            cached = self._cache.get(cache_key)
            if cached is not None:
                return cached
        queries = await self._generate_queries_async(query) if expand else [query]
        timeout_s = getattr(getattr(settings, "perf_web_search", None), "timeout_seconds", 30) or 30
        loop = asyncio.get_event_loop()
        try:
            out = await asyncio.wait_for(
                loop.run_in_executor(None, self._search_tavily_sync, queries),
                timeout=float(timeout_s),
            )
        except asyncio.TimeoutError:
            logger.warning("Tavily async search timeout: %ss", timeout_s)
            out = []
        if self._cache and out is not None:
            self._cache.set(cache_key, out)
        return out


# 全局单例（懒加载配置）
web_searcher = TavilySearcher()
</file>

<file path="src/utils/__init__.py">
# Utils: cache, limiter, storage_cleaner, prompt_manager
from src.utils.cache import TTLCache, _make_key, get_cache
from src.utils.limiter import ConcurrencyLimiter, get_global_limiter
from src.utils.prompt_manager import PromptManager
from src.utils.storage_cleaner import (
    run_cleanup,
    cleanup_by_age,
    cleanup_by_size,
    vacuum_databases,
    get_storage_stats,
)

__all__ = [
    "TTLCache",
    "_make_key",
    "get_cache",
    "ConcurrencyLimiter",
    "get_global_limiter",
    "PromptManager",
    "run_cleanup",
    "cleanup_by_age",
    "cleanup_by_size",
    "vacuum_databases",
    "get_storage_stats",
]
</file>

<file path="tests/test_research_meta_analysis_guards.py">
"""
Regression tests for debate/meta-analysis enhancements:
- verifier CoV conflict attribution extraction
- write_node quantitative run_code enforcement path
- synthesize_node Debate & Divergence prompt enrichment
"""

import json
import sys
from pathlib import Path
from types import SimpleNamespace

# Support direct execution: `python tests/test_research_meta_analysis_guards.py`
PROJECT_ROOT = Path(__file__).resolve().parents[1]
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from src.collaboration.research import agent as research_agent
from src.collaboration.research.dashboard import ResearchBrief, ResearchDashboard
from src.collaboration.research.trajectory import ResearchTrajectory
from src.collaboration.research.verifier import verify_claims


class _SequentialLLM:
    """Simple mock client returning pre-seeded chat responses in order."""

    def __init__(self, payloads):
        self._payloads = list(payloads)

    def chat(self, **_kwargs):
        if not self._payloads:
            raise AssertionError("No more mocked LLM payloads available")
        return self._payloads.pop(0)


def test_verify_claims_extracts_attribution_into_conflict_notes():
    claims_json = json.dumps(
        [
            {
                "claim": "Study A reports higher diversity than Study B.",
                "has_citation": True,
                "citation_keys": ["refA", "refB"],
            }
        ],
        ensure_ascii=False,
    )
    verification_json = json.dumps(
        [
            {
                "claim_index": 0,
                "confidence": "medium",
                "evidence_found": "Both studies examine similar ecosystems but report opposite trends.",
                "needs_revision": True,
                "revision_note": "结论存在冲突，需要解释。",
                "attribution_analysis": "差异可能来自采样深度和测序仪器（NovaSeq vs MiSeq）不同。",
                "supplementary_query": "",
            }
        ],
        ensure_ascii=False,
    )
    llm = _SequentialLLM(
        [
            {"final_text": claims_json},
            {"final_text": verification_json},
        ]
    )

    result = verify_claims(
        section_text="A paragraph containing a conflict claim.",
        citations=[],
        llm_client=llm,
        model=None,
    )

    assert len(result.claims) == 1
    claim = result.claims[0]
    assert "采样深度" in claim.attribution_analysis
    assert "Attribution Analysis:" in claim.revision_note
    assert result.conflict_notes, "Expected conflict attribution notes to be populated"
    assert "测序仪器" in result.conflict_notes[0]


def test_effective_write_k_respects_floor_scaling_and_cap():
    preset = {"search_top_k_write": 12, "search_top_k_write_max": 40}

    # No UI override -> preset floor.
    assert research_agent._compute_effective_write_k(preset, {}) == 12
    # Moderate UI top_k -> 1.5x scaling.
    assert research_agent._compute_effective_write_k(preset, {"final_top_k": 10}) == 15
    # Large UI top_k -> capped.
    assert research_agent._compute_effective_write_k(preset, {"final_top_k": 100}) == 40

    # Cap below floor is normalized to floor.
    preset_bad_cap = {"search_top_k_write": 20, "search_top_k_write_max": 10}
    assert research_agent._compute_effective_write_k(preset_bad_cap, {"final_top_k": 200}) == 20

    # Invalid UI top_k falls back to preset floor.
    assert research_agent._compute_effective_write_k(preset, {"final_top_k": "not-a-number"}) == 12


def test_write_node_uses_run_code_path_when_structured_numeric_data_present(monkeypatch):
    dashboard = ResearchDashboard(brief=ResearchBrief(topic="DeepSea microbiome"))
    dashboard.add_section("Quantitative Comparison")
    trajectory = ResearchTrajectory(topic="DeepSea microbiome")

    class _DummyPack:
        def __init__(self, text):
            self._text = text
            self.chunks = []

        def to_context_string(self, max_chunks=8):
            _ = max_chunks
            return self._text

    class _DummyRetrievalSvc:
        def search(self, query, mode, top_k, filters):
            _ = mode, top_k, filters
            if "verification" in query.lower():
                return _DummyPack("verification table computed_stats mean=10 std=2")
            return _DummyPack("primary table computed_stats mean=12 std=3")

    # Keep LLM client simple; in this branch content comes from react_loop.
    dummy_client = SimpleNamespace(chat=lambda **_kwargs: {"final_text": "fallback text"})
    monkeypatch.setattr(
        research_agent,
        "_resolve_step_client_and_model",
        lambda state, step: (dummy_client, None),
    )
    monkeypatch.setattr(research_agent, "_get_retrieval_svc", lambda state: _DummyRetrievalSvc())

    captured = {"called": False, "tool_names": []}

    def _fake_react_loop(messages, tools, llm_client, max_iterations=10, model=None, **llm_kwargs):
        _ = messages, llm_client, max_iterations, model, llm_kwargs
        captured["called"] = True
        captured["tool_names"] = [getattr(t, "name", "") for t in tools]
        return SimpleNamespace(final_text="Computed differences via run_code.")

    import src.llm.react_loop as react_loop_module

    monkeypatch.setattr(react_loop_module, "react_loop", _fake_react_loop)

    state = {
        "dashboard": dashboard,
        "trajectory": trajectory,
        "current_section": "Quantitative Comparison",
        "markdown_parts": [],
        "output_language": "en",
        "search_mode": "hybrid",
        "filters": {},
    }

    out = research_agent.write_node(state)
    assert captured["called"], "Expected write_node to enter the react_loop path"
    assert captured["tool_names"] == ["run_code"], "Expected run_code-only toolset in quantitative mode"
    assert any("Computed differences via run_code." in part for part in out.get("markdown_parts", []))


def test_synthesize_node_injects_debate_divergence_requirements(monkeypatch):
    dashboard = ResearchDashboard(brief=ResearchBrief(topic="DeepSea symbiosis"))
    sec = dashboard.add_section("Findings")
    sec.status = "done"
    dashboard.conflict_notes = [
        "Study A vs Study B disagree; potential drivers include sampling depth (2000m vs 3500m) and sequencing platform.",
    ]

    captured = {"limitations_prompt": ""}

    class _CaptureLLM:
        def chat(self, messages, model=None, max_tokens=None, **kwargs):
            _ = model, max_tokens, kwargs
            user_prompt = str((messages or [{}])[-1].get("content", ""))
            if "Generate a 150-250 word abstract" in user_prompt:
                return {"final_text": "Abstract body."}
            if "write the BODY content for section" in user_prompt:
                captured["limitations_prompt"] = user_prompt
                return {"final_text": "观点交锋与实验条件差异 (Debate & Divergence): analysis paragraph."}
            if "Rewrite the full review to improve global coherence" in user_prompt:
                body = user_prompt.split("Document:\n", 1)[-1]
                return {"final_text": body}
            return {"final_text": ""}

    monkeypatch.setattr(
        research_agent,
        "_resolve_step_client_and_model",
        lambda state, step: (_CaptureLLM(), None),
    )

    state = {
        "dashboard": dashboard,
        "markdown_parts": ["# DeepSea symbiosis\n\n## Findings\n\nSome evidence text."],
        "sections_completed": ["Findings"],
        "output_language": "en",
        "citations": [],
    }

    research_agent.synthesize_node(state)
    lim_prompt = captured["limitations_prompt"]
    assert lim_prompt, "Expected limitations prompt to be generated"
    assert "观点交锋与实验条件差异 (Debate & Divergence)" in lim_prompt
    assert "sampling depth" in lim_prompt
    assert "Conflicts/Contradictions with Attribution Clues" in lim_prompt
    assert "Study A vs Study B disagree" in lim_prompt


if __name__ == "__main__":
    import pytest

    raise SystemExit(pytest.main([__file__]))
</file>

<file path="frontend/src/components/chat/ChatInput.tsx">
import { useState, useRef, useCallback, type KeyboardEvent } from 'react';
import { useTranslation } from 'react-i18next';
import { ArrowRight, Loader2, Telescope, Settings } from 'lucide-react';
import { useChatStore, useConfigStore, useToastStore, useCanvasStore, useUIStore } from '../../stores';
import { chatStream, clarifyForDeepResearch } from '../../api/chat';
import { exportCanvas, getCanvas } from '../../api/canvas';
import { CommandPalette, DeepResearchSettingsPopover } from '../workflow';
import { COMMAND_LIST } from '../../types';
import type { ChatRequest, Source, EvidenceSummary, IntentInfo, CommandDefinition } from '../../types';

export function ChatInput() {
  const [inputValue, setInputValue] = useState('');
  const [showDRSettings, setShowDRSettings] = useState(false);
  const inputRef = useRef<HTMLInputElement>(null);
  const toggleDRSettings = useCallback(() => setShowDRSettings((v) => !v), []);
  const closeDRSettings = useCallback(() => setShowDRSettings(false), []);
  const {
    sessionId,
    canvasId,
    isStreaming,
    addMessage,
    appendToLastMessage,
    setLastMessageSources,
    setSessionId,
    setWorkflowStep,
    setIsStreaming,
    setLastEvidenceSummary,
    setShowDeepResearchDialog,
    setDeepResearchTopic,
    setClarificationQuestions,
  } = useChatStore();
  const {
    webSearchConfig,
    ragConfig,
    selectedProvider,
    selectedModel,
    currentCollection,
    deepResearchDefaults,
  } = useConfigStore();
  const addToast = useToastStore((s) => s.addToast);
  const { setCanvas, setCanvasContent, setIsLoading: setCanvasLoading } = useCanvasStore();
  const setCanvasOpen = useUIStore((s) => s.setCanvasOpen);
  const { t } = useTranslation();

  /**
   * 触发 Deep Research 流程（打开澄清对话框）
   * 使用 ⚙ 弹窗中持久化的 scope 模型来生成澄清问题。
   */
  const handleDeepResearch = async (topic?: string) => {
    const researchTopic = topic || inputValue.trim();
    if (!researchTopic) {
      addToast(t('chatInput.enterTopic'), 'error');
      return;
    }
    setInputValue('');
    closeDRSettings(); // 关闭设置弹窗（如果打开的话）
    setDeepResearchTopic(researchTopic);

    // Resolve clarify model from Deep Research persistent defaults
    const { deepResearchDefaults } = useConfigStore.getState();
    const scopeModel = (deepResearchDefaults.stepModels.scope || '').trim();
    let llmProvider: string | undefined;
    let modelOverride: string | undefined;

    if (scopeModel && scopeModel.includes('::')) {
      const [p, m] = scopeModel.split('::', 2);
      llmProvider = p || undefined;
      modelOverride = m || undefined;
    } else if (scopeModel) {
      llmProvider = selectedProvider || undefined;
      modelOverride = scopeModel || undefined;
    } else {
      llmProvider = selectedProvider || undefined;
      modelOverride = selectedModel || undefined;
    }

    // 调用澄清问题生成 API
    try {
      addToast(t('chatInput.generatingQuestions'), 'info');
      const result = await clarifyForDeepResearch({
        message: researchTopic,
        session_id: sessionId || undefined,
        search_mode: 'hybrid',
        llm_provider: llmProvider,
        model_override: modelOverride,
      });
      const questions = result.questions || [];
      setClarificationQuestions(questions);
      if (result.suggested_topic) {
        setDeepResearchTopic(result.suggested_topic);
      }
      if (questions.length === 0) {
        addToast(t('chatInput.topicClear'), 'info');
      }
      setShowDeepResearchDialog(true);
    } catch (err) {
      console.error('[ChatInput] Clarify failed:', err);
      setClarificationQuestions([
        {
          id: 'q1',
          text: t('chatInput.defaultQuestion'),
          question_type: 'text',
          options: [],
          default: researchTopic,
        },
      ]);
      addToast(t('chatInput.clarifyFailed'), 'info');
      setShowDeepResearchDialog(true);
    }
  };

  /**
   * 发送 Chat 消息（检索由 UI 开关决定，不再做意图检测）
   */
  const handleSend = async (messageOverride?: string, modeOverride?: 'chat' | 'deep_research') => {
    const messageToSend = messageOverride || inputValue.trim();
    if (!messageToSend || isStreaming) return;

    // /auto 命令 → 触发 Deep Research 流程
    if (messageToSend.startsWith('/auto')) {
      const topic = messageToSend.replace(/^\/auto\s*/, '').trim();
      handleDeepResearch(topic || undefined);
      return;
    }

    const userMessage = messageToSend;
    setInputValue('');

    addMessage({ role: 'user', content: userMessage });
    addMessage({ role: 'assistant', content: '' });
    setIsStreaming(true);
    setWorkflowStep('explore');

    // 构建检索参数
    const enabledProviders = webSearchConfig.sources
      .filter((s) => s.enabled)
      .map((s) => s.id);

    const webSourceConfigs: Record<string, { topK: number; threshold: number }> = {};
    webSearchConfig.sources.forEach((source) => {
      if (source.enabled) {
        webSourceConfigs[source.id] = { topK: source.topK, threshold: source.threshold };
      }
    });

    const localEnabled = ragConfig.enabled ?? true;
    const webEnabled = webSearchConfig.enabled && enabledProviders.length > 0;

    // search_mode 纯由 UI 开关决定
    let searchMode: 'local' | 'web' | 'hybrid' | 'none';
    if (localEnabled && webEnabled) {
      searchMode = 'hybrid';
    } else if (webEnabled) {
      searchMode = 'web';
    } else if (localEnabled) {
      searchMode = 'local';
    } else {
      searchMode = 'none';
    }

    const mode = modeOverride || 'chat';
    const queryOptimizerEnabled = Boolean(webSearchConfig.queryOptimizer ?? true);
    const maxQueries = Math.min(5, Math.max(1, Number(webSearchConfig.maxQueriesPerProvider ?? 3)));

    const request: ChatRequest = {
      session_id: sessionId || undefined,
      canvas_id: canvasId || undefined,
      message: userMessage,
      collection: currentCollection || undefined,
      search_mode: searchMode,
      mode,
      llm_provider: selectedProvider || undefined,
      model_override: selectedModel || undefined,
      web_providers: (searchMode !== 'none' && webEnabled) ? enabledProviders : undefined,
      web_source_configs: (searchMode !== 'none' && webEnabled && Object.keys(webSourceConfigs).length > 0) ? webSourceConfigs : undefined,
      use_query_optimizer: (searchMode !== 'none' && webEnabled) ? queryOptimizerEnabled : undefined,
      query_optimizer_max_queries: (searchMode !== 'none' && webEnabled) ? maxQueries : undefined,
      local_top_k: (searchMode !== 'none' && localEnabled) ? ragConfig.localTopK : undefined,
      local_threshold: (searchMode !== 'none' && localEnabled) ? (ragConfig.localThreshold ?? undefined) : undefined,
      year_start: deepResearchDefaults.yearStart ?? undefined,
      year_end: deepResearchDefaults.yearEnd ?? undefined,
      final_top_k: (searchMode !== 'none') ? (ragConfig.finalTopK ?? 10) : undefined,
      use_content_fetcher: (searchMode !== 'none' && webEnabled) ? (webSearchConfig.enableContentFetcher ?? false) : undefined,
      use_agent: ragConfig.enableAgent || undefined,
    };

    if (import.meta.env.DEV) {
      const base = import.meta.env.VITE_API_BASE_URL || '/api';
      console.log('[ChatInput] POST', `${base}/chat/stream`, JSON.stringify(request, null, 2));
    }

    try {
      const stream = chatStream(request);

      for await (const { event, data } of stream) {
        if (event === 'meta') {
          interface CitationData {
            cite_key: string;
            title: string;
            authors: string[];
            year?: number | null;
            doc_id?: string | null;
            url?: string | null;
            doi?: string | null;
            bbox?: number[];
            page_num?: number | null;
          }
          const meta = data as {
            session_id: string;
            canvas_id?: string;
            citations: CitationData[];
            evidence_summary: EvidenceSummary | null;
            intent?: IntentInfo;
            current_stage?: string;
          };

          if (meta.session_id) setSessionId(meta.session_id);
          if (meta.current_stage) setWorkflowStep(meta.current_stage as any);
          if (meta.evidence_summary) {
            setLastEvidenceSummary(meta.evidence_summary);
            if (!meta.current_stage) setWorkflowStep('outline');
          }

          if (meta.citations && meta.citations.length > 0) {
            const sources: Source[] = meta.citations.map((cite, idx) => ({
              id: idx + 1,
              cite_key: cite.cite_key,
              title: cite.title || cite.cite_key,
              authors: cite.authors || [],
              year: cite.year,
              doc_id: cite.doc_id,
              url: cite.url,
              doi: cite.doi,
              bbox: cite.bbox,
              page_num: cite.page_num,
              type: cite.url ? 'web' : 'local',
            }));
            setLastMessageSources(sources);
          }

          if (meta.canvas_id) {
            setCanvasLoading(true);
            Promise.all([
              getCanvas(meta.canvas_id).catch((err) => {
                console.error('[ChatInput] Canvas data load failed:', err);
                return null;
              }),
              exportCanvas(meta.canvas_id, 'markdown').catch((err) => {
                console.error('[ChatInput] Canvas markdown load failed:', err);
                return null;
              }),
            ])
              .then(([canvasData, exportResp]) => {
                if (canvasData) setCanvas(canvasData);
                if (exportResp?.content) {
                  setCanvasContent(exportResp.content);
                  setCanvasOpen(true);
                }
              })
              .finally(() => setCanvasLoading(false));
          }
        } else if (event === 'dashboard') {
          // Deep Research 进度更新
          const dashboardData = data as import('../../types').ResearchDashboardData;
          useChatStore.getState().setResearchDashboard(dashboardData);
        } else if (event === 'tool_trace') {
          // Agent 工具调用轨迹
          const traceData = data as import('../../types').ToolTraceItem[];
          useChatStore.getState().setToolTrace(traceData);
        } else if (event === 'delta') {
          appendToLastMessage((data as { delta: string }).delta);
          setWorkflowStep('drafting');
        } else if (event === 'done') {
          setWorkflowStep('refine');
          setTimeout(() => setWorkflowStep('idle'), 1000);
        }
      }
    } catch (error) {
      console.error('Chat error:', error);
      addToast(t('chat.sendFailed'), 'error');
      appendToLastMessage('\n\n' + t('chat.requestError'));
    } finally {
      setIsStreaming(false);
    }
  };

  const filteredCommands = inputValue.startsWith('/')
    ? COMMAND_LIST.filter(cmd =>
        cmd.command.toLowerCase().includes(inputValue.slice(1).toLowerCase()) ||
        cmd.label.toLowerCase().includes(inputValue.slice(1).toLowerCase())
      )
    : [];

  const handleKeyDown = (e: KeyboardEvent<HTMLInputElement>) => {
    if (e.key === 'Enter' && !e.shiftKey) {
      e.preventDefault();
      handleSend();
    }
    if (e.key === 'Tab' && inputValue.startsWith('/')) {
      e.preventDefault();
      if (filteredCommands.length > 0) {
        handleSelectCommand(filteredCommands[0]);
      }
    }
  };

  const handleSelectCommand = (cmd: CommandDefinition) => {
    if (cmd.mode === 'deep_research') {
      // /auto 命令直接触发 Deep Research
      setInputValue('');
      handleDeepResearch();
      return;
    }
    setInputValue(cmd.command + ' ');
    inputRef.current?.focus();
  };

  return (
    <div className="p-4 glass-header border-t border-slate-700/50 z-30">
      <div className="max-w-3xl mx-auto relative">
        {/* 命令面板 */}
        <CommandPalette
          inputValue={inputValue}
          onSelectCommand={handleSelectCommand}
        />

        {/* 输入区域 */}
        <div className="flex items-center gap-2">
          {/* Deep Research 按钮组：⚙ 设置 + 🔭 启动 */}
          <div className="relative flex items-center">
            <button
              onClick={toggleDRSettings}
              className={`
                flex items-center px-2 py-2.5 rounded-l-lg border border-r-0 text-sm
                transition-all cursor-pointer
                ${showDRSettings
                  ? 'bg-indigo-900/30 text-indigo-400 border-indigo-500/30'
                  : 'bg-slate-800/60 text-slate-400 border-slate-700/60 hover:bg-slate-700 hover:text-slate-300'
                }
              `}
              title={t('chatInput.drSettingsTitle')}
            >
              <Settings size={14} />
            </button>
            <button
              onClick={() => handleDeepResearch()}
              disabled={isStreaming || !inputValue.trim()}
              className={`
                flex items-center gap-1.5 px-3 py-2.5 rounded-r-lg border text-sm font-medium
                transition-all cursor-pointer whitespace-nowrap
                ${isStreaming || !inputValue.trim()
                  ? 'bg-slate-800/60 text-slate-500 border-slate-700/60 cursor-not-allowed'
                  : 'bg-gradient-to-r from-indigo-900/30 to-purple-900/30 text-indigo-400 border-indigo-500/30 hover:border-indigo-400 hover:shadow-sm'
                }
              `}
              title="Deep Research - 多步深度研究"
            >
              <Telescope size={16} />
              <span className="hidden sm:inline">Deep Research</span>
            </button>

            {/* Settings popover (positioned above the button group) */}
            <DeepResearchSettingsPopover open={showDRSettings} onClose={closeDRSettings} />
          </div>

          {/* 输入框 */}
          <div className="flex-1 relative">
            <input
              ref={inputRef}
              type="text"
              value={inputValue}
              onChange={(e) => setInputValue(e.target.value)}
              onKeyDown={handleKeyDown}
              placeholder={t('chatInput.placeholder')}
              className="w-full bg-slate-900/60 border border-slate-700 text-slate-200 rounded-xl py-3 pl-4 pr-12 shadow-sm focus:ring-1 focus:ring-sky-500 focus:border-sky-500 placeholder-slate-500 outline-none transition-all"
              disabled={isStreaming}
            />
            <button
              onClick={() => handleSend()}
              disabled={isStreaming || !inputValue.trim()}
              className="absolute right-2 top-1/2 -translate-y-1/2 w-8 h-8 bg-sky-600 text-white rounded-lg flex items-center justify-center hover:bg-sky-500 transition-colors disabled:opacity-50 disabled:cursor-not-allowed cursor-pointer"
            >
              {isStreaming ? (
                <Loader2 size={18} className="animate-spin" />
              ) : (
                <ArrowRight size={18} />
              )}
            </button>
          </div>
        </div>

        {/* 快捷提示 */}
        <div className="mt-2 flex items-center justify-between text-xs text-slate-500">
          <span>
            {t('chatInput.inputCommand')} <kbd className="px-1 py-0.5 bg-slate-800 rounded text-slate-400">/</kbd> {t('chatInput.viewCommands')}
            {' | '}
            <kbd className="px-1 py-0.5 bg-slate-800 rounded text-slate-400">/auto</kbd> {t('chatInput.deepResearch')}
          </span>
          <span>
            {t('chatInput.pressEnter')} <kbd className="px-1 py-0.5 bg-slate-800 rounded text-slate-400">Enter</kbd> {t('chatInput.toSend')}
          </span>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/chat/ChatWindow.tsx">
import { useRef, useEffect, useState, useCallback } from 'react';
import { useTranslation } from 'react-i18next';
import { MessageSquare, FileSearch, Copy, Download, ExternalLink, FileText, User, Calendar, GitCompareArrows, BookOpen } from 'lucide-react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import { useChatStore, useToastStore, useCompareStore } from '../../stores';
import type { Source, DeepResearchJobInfo } from '../../types';
import { cancelDeepResearchJob, getDeepResearchJob, listDeepResearchJobEvents } from '../../api/chat';
import { getChunkDetail } from '../../api/graph';
import { RetrievalDebugPanel } from './RetrievalDebugPanel';
import { ToolTracePanel } from './ToolTracePanel';
import { ResearchProgressPanel } from '../research/ResearchProgressPanel';
import { PdfViewerModal } from '../ui/PdfViewerModal';

export function ChatWindow() {
  const { t } = useTranslation();
  const messages = useChatStore((s) => s.messages);
  const lastEvidenceSummary = useChatStore((s) => s.lastEvidenceSummary);
  const deepResearchActive = useChatStore((s) => s.deepResearchActive);
  const researchDashboard = useChatStore((s) => s.researchDashboard);
  const toolTrace = useChatStore((s) => s.toolTrace);
  const setShowDeepResearchDialog = useChatStore((s) => s.setShowDeepResearchDialog);
  const setDeepResearchTopic = useChatStore((s) => s.setDeepResearchTopic);
  const isStreaming = useChatStore((s) => s.isStreaming);
  const addToast = useToastStore((s) => s.addToast);
  const addComparePreselected = useCompareStore((s) => s.addComparePreselected);
  const scrollRef = useRef<HTMLDivElement>(null);
  const [backgroundJob, setBackgroundJob] = useState<DeepResearchJobInfo | null>(null);
  const [stoppingJobId, setStoppingJobId] = useState<string | null>(null);
  const [showBackgroundLogs, setShowBackgroundLogs] = useState(false);
  const [backgroundEventLines, setBackgroundEventLines] = useState<string[]>([]);
  const lastBackgroundEventIdRef = useRef(0);
  const trackedBackgroundJobIdRef = useRef<string | null>(null);

  // PDF 溯源 Modal 状态
  const [pdfModal, setPdfModal] = useState<{
    open: boolean;
    pdfUrl: string;
    pageNumber: number;
    bbox?: number[];
    title?: string;
  }>({ open: false, pdfUrl: '', pageNumber: 1 });

  // 自动滚动到底部
  useEffect(() => {
    if (scrollRef.current) {
      scrollRef.current.scrollTop = scrollRef.current.scrollHeight;
    }
  }, [messages]);

  // 新对话场景下：如果后台仍有 Deep Research 任务，显示窄条提示并允许停止。
  useEffect(() => {
    let cancelled = false;

    const toEventLine = (eventName: string, data: Record<string, unknown>): string => {
      const section = typeof data.section === 'string' ? data.section : '';
      const message = typeof data.message === 'string' ? data.message : '';
      const typ = typeof data.type === 'string' ? data.type : eventName;
      if (section) return `[${typ}] ${section}`;
      if (message) return `[${eventName}] ${message}`;
      return `[${eventName}] ${JSON.stringify(data)}`;
    };

    const refreshBackgroundJob = async () => {
      const activeJobId = localStorage.getItem('deep_research_active_job_id');
      if (!activeJobId) {
        if (!cancelled) {
          setBackgroundJob(null);
          setBackgroundEventLines([]);
          setShowBackgroundLogs(false);
        }
        trackedBackgroundJobIdRef.current = null;
        lastBackgroundEventIdRef.current = 0;
        return;
      }
      try {
        const job = await getDeepResearchJob(activeJobId);
        if (cancelled) return;
        if (trackedBackgroundJobIdRef.current !== activeJobId) {
          trackedBackgroundJobIdRef.current = activeJobId;
          lastBackgroundEventIdRef.current = 0;
          setBackgroundEventLines([]);
          setShowBackgroundLogs(false);
        }
        const running = job.status === 'pending' || job.status === 'running' || job.status === 'cancelling';
        if (running) {
          setBackgroundJob(job);
          const events = await listDeepResearchJobEvents(activeJobId, lastBackgroundEventIdRef.current, 30);
          if (events.length > 0) {
            const maxId = events[events.length - 1].event_id;
            lastBackgroundEventIdRef.current = Math.max(lastBackgroundEventIdRef.current, maxId);
            setBackgroundEventLines((prev) => {
              const next = [...prev];
              events.forEach((evt) => {
                if (evt.event === 'progress' || evt.event === 'warning' || evt.event === 'waiting_review') {
                  next.push(toEventLine(evt.event, evt.data || {}));
                }
              });
              return next.slice(-10);
            });
          }
        } else {
          setBackgroundJob(null);
          setBackgroundEventLines([]);
          setShowBackgroundLogs(false);
          localStorage.removeItem('deep_research_active_job_id');
        }
      } catch {
        if (!cancelled) setBackgroundJob(null);
      }
    };

    refreshBackgroundJob();
    const timer = window.setInterval(refreshBackgroundJob, 10000);
    return () => {
      cancelled = true;
      window.clearInterval(timer);
    };
  }, []);

  const handleCopy = (content: string) => {
    navigator.clipboard.writeText(content);
    addToast(t('chat.copied'), 'info');
  };

  const handleExport = (content: string) => {
    const blob = new Blob([content], { type: 'text/markdown' });
    const url = URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = 'chat_response.md';
    document.body.appendChild(a);
    a.click();
    document.body.removeChild(a);
    URL.revokeObjectURL(url);
    addToast(t('chat.exported'), 'success');
  };

  const handleOpenSource = (source: Source) => {
    if (source.url) {
      window.open(source.url, '_blank');
    } else if (source.doi) {
      window.open(`https://doi.org/${source.doi}`, '_blank');
    } else if (source.doc_id) {
      addToast(t('chat.localDoc') + ': ' + source.doc_id, 'info');
    } else {
      addToast(`引用: [${source.cite_key}] ${source.title}`, 'info');
    }
  };

  const handleForceStopBackgroundJob = async () => {
    if (!backgroundJob || stoppingJobId === backgroundJob.job_id) return;
    const ok = window.confirm(t('chat.confirmForceStop'));
    if (!ok) return;
    try {
      setStoppingJobId(backgroundJob.job_id);
      await cancelDeepResearchJob(backgroundJob.job_id);
      addToast(t('chat.stopRequested'), 'info');
    } catch {
      addToast(t('chat.stopFailed'), 'error');
    } finally {
      setStoppingJobId(null);
    }
  };

  const formatAuthors = (authors: string[] | undefined): string => {
    if (!authors || authors.length === 0) return '佚名';
    if (authors.length === 1) return authors[0];
    if (authors.length === 2) return authors.join(' & ');
    return `${authors[0]} et al.`;
  };

  const handleOpenPdfTrace = useCallback(async (src: Source) => {
    if (!src.doc_id) return;
    try {
      // 先尝试从 source 本身获取 bbox/page，如已有则直接使用
      let bbox = src.bbox;
      let page = src.page_num ?? undefined;

      // 若 source 上没有 bbox，通过 chunk API 补取
      if (!bbox || bbox.length < 4) {
        const chunkId = String(src.id);
        const detail = await getChunkDetail({
          chunk_id: chunkId,
          paper_id: src.doc_id,
        });
        const rawBbox = detail.bbox;
        if (Array.isArray(rawBbox) && rawBbox.length >= 4) {
          if (typeof rawBbox[0] === 'number') {
            bbox = rawBbox as number[];
          } else if (Array.isArray(rawBbox[0])) {
            bbox = rawBbox[0] as number[];
          }
        }
        page = detail.page ?? undefined;
      }

      const apiBase = import.meta.env.VITE_API_BASE_URL || '/api';
      const pdfUrl = `${apiBase}/graph/pdf/${encodeURIComponent(src.doc_id)}`;

      setPdfModal({
        open: true,
        pdfUrl,
        pageNumber: page || 1,
        bbox: bbox || undefined,
        title: src.title || src.doc_id,
      });
    } catch {
      addToast('PDF 溯源失败：无法获取 chunk 信息', 'error');
    }
  }, [addToast]);

  const showBackgroundBanner = Boolean(backgroundJob) && !deepResearchActive && !researchDashboard;

  const backgroundBanner = showBackgroundBanner ? (
    <div className="border rounded-lg bg-sky-900/30 border-sky-500/30 px-3 py-2 text-xs text-sky-300 shadow-[0_0_10px_rgba(14,165,233,0.1)]">
      <div className="flex items-center justify-between gap-3">
        <div className="min-w-0">
          <div className="font-medium truncate">
            {backgroundJob?.status === 'cancelling' ? t('chat.bgResearchStopping') : t('chat.bgResearchRunning')}
          </div>
          <div className="text-[11px] text-sky-400 truncate">
            {backgroundJob?.topic || t('chat.unnamed')} · {t('chat.stage')}: {backgroundJob?.current_stage || 'unknown'}
          </div>
        </div>
        <div className="flex items-center gap-2 shrink-0">
          <button
            onClick={() => setShowBackgroundLogs((v) => !v)}
            className="px-2 py-1 rounded-md border border-sky-500/30 text-sky-400 hover:bg-sky-500/10 transition-colors"
          >
            {showBackgroundLogs ? t('chat.collapseLogs') : t('chat.recentLogs')}
          </button>
          <button
            onClick={() => {
              if (!backgroundJob) return;
              setDeepResearchTopic(backgroundJob.topic || '');
              setShowDeepResearchDialog(true);
            }}
            className="px-2 py-1 rounded-md border border-sky-500/30 text-sky-400 hover:bg-sky-500/10 transition-colors"
          >
            {t('chat.viewTask')}
          </button>
          <button
            onClick={handleForceStopBackgroundJob}
            disabled={!backgroundJob || backgroundJob.status === 'cancelling' || stoppingJobId === backgroundJob?.job_id}
            className="px-2 py-1 rounded-md bg-red-900/40 border border-red-500/30 text-red-400 hover:bg-red-900/60 disabled:opacity-50 transition-colors"
          >
            {t('chat.forceStop')}
          </button>
        </div>
      </div>
      {showBackgroundLogs && (
        <div className="mt-2 border-t border-sky-500/30 pt-2 space-y-1">
          {(backgroundEventLines.length > 0 ? backgroundEventLines.slice(-3) : [t('chat.noNewLogs')]).map((line, idx) => (
            <div key={`${idx}-${line}`} className="text-[11px] text-sky-500 truncate font-mono">
              {line}
            </div>
          ))}
        </div>
      )}
    </div>
  ) : null;

  if (messages.length === 0) {
    return (
      <div className="max-w-3xl mx-auto space-y-4 pb-24">
        {backgroundBanner}
        {(deepResearchActive || researchDashboard) && (
          <ResearchProgressPanel dashboard={researchDashboard} isActive={deepResearchActive} />
        )}
        <div className="flex flex-col items-center justify-center h-64 text-slate-500">
          <div className="w-16 h-16 bg-slate-800/50 rounded-2xl flex items-center justify-center mb-4 shadow-[0_0_20px_rgba(56,189,248,0.1)] border border-slate-700/50 animate-float">
            <MessageSquare size={32} className="opacity-60 text-sky-400" />
          </div>
          <p className="font-medium text-slate-400">{t('chat.newConversation')}</p>
        </div>
      </div>
    );
  }

  return (
    <div
      ref={scrollRef}
      className="max-w-3xl mx-auto space-y-6 pb-24 overflow-y-auto px-4"
    >
      {backgroundBanner}
      {/* Agent 工具调用轨迹 */}
      {toolTrace && toolTrace.length > 0 && (
        <ToolTracePanel trace={toolTrace} />
      )}
      {/* Deep Research 进度面板 */}
      {(deepResearchActive || researchDashboard) && (
        <ResearchProgressPanel dashboard={researchDashboard} isActive={deepResearchActive} />
      )}
      {!deepResearchActive && researchDashboard && researchDashboard.coverage_gaps.length > 0 && (
        <div className="border rounded-lg bg-amber-900/20 border-amber-500/30 p-3 text-sm text-amber-400">
          <div className="font-medium mb-1 flex items-center gap-2">
            <span className="relative flex h-2 w-2">
              <span className="animate-ping absolute inline-flex h-full w-full rounded-full bg-amber-400 opacity-75"></span>
              <span className="relative inline-flex rounded-full h-2 w-2 bg-amber-500"></span>
            </span>
            {t('chat.coverageGaps')}
          </div>
          <div className="text-xs mb-2 text-amber-400/80">{t('chat.supplementHint')}</div>
          <button
            onClick={() => setShowDeepResearchDialog(true)}
            className="px-3 py-1.5 text-xs rounded-md bg-amber-600/20 border border-amber-500/30 text-amber-300 hover:bg-amber-600/40 transition-colors"
          >
            {t('chat.supplementAndContinue')}
          </button>
        </div>
      )}
      {/* 检索诊断面板 */}
      {lastEvidenceSummary && lastEvidenceSummary.total_chunks > 0 && (
        <RetrievalDebugPanel summary={lastEvidenceSummary} />
      )}
      {messages.map((msg, idx) => {
        const displayContent =
          isStreaming && idx === messages.length - 1
            ? msg.content.replace(/\[([a-fA-F0-9]{8})\]/g, '[⏳...]')
            : msg.content;
        return (
        <div
          key={idx}
          className={`flex ${
            msg.role === 'user' ? 'justify-end' : 'justify-start'
          } animate-in slide-in-from-bottom-2 duration-300`}
        >
          <div
            className={`max-w-[90%] rounded-2xl p-5 shadow-lg group backdrop-blur-sm ${
              msg.role === 'user'
                ? 'bg-[var(--bg-bubble-user)] text-white border-transparent shadow-sky-500/10'
                : 'bg-[var(--bg-bubble-ai)] text-slate-200 border border-slate-700/50 shadow-black/20'
            }`}
          >
            {/* 消息内容 - 使用 Markdown 渲染 */}
            {msg.role === 'assistant' ? (
              <div className="prose prose-invert prose-sm max-w-none 
                prose-headings:text-sky-300 
                prose-h1:text-xl prose-h2:text-lg prose-h3:text-base 
                prose-p:text-slate-300 prose-p:leading-relaxed 
                prose-li:text-slate-300 
                prose-strong:text-sky-200 
                prose-a:text-sky-400 prose-a:no-underline hover:prose-a:underline
                prose-code:bg-slate-900/60 prose-code:px-1.5 prose-code:py-0.5 prose-code:rounded prose-code:text-sky-300 prose-code:border prose-code:border-slate-700/50
                prose-pre:bg-slate-950/80 prose-pre:border prose-pre:border-slate-800 prose-pre:shadow-inner
                prose-blockquote:border-l-sky-500/50 prose-blockquote:bg-slate-800/20 prose-blockquote:py-1 prose-blockquote:px-4
                prose-table:border-collapse prose-th:border-b prose-th:border-slate-700 prose-td:border-b prose-td:border-slate-800/50">
                <ReactMarkdown remarkPlugins={[remarkGfm]}>
                  {displayContent}
                </ReactMarkdown>
              </div>
            ) : (
              <p className="text-[15px] leading-relaxed whitespace-pre-wrap">
                {msg.content}
              </p>
            )}

            {/* 助手消息工具栏 */}
            {msg.role === 'assistant' && msg.content && (
              <div className="mt-3 pt-2 border-t border-slate-700/30 flex items-center justify-end gap-2 opacity-0 group-hover:opacity-100 transition-opacity">
                <button
                  onClick={() => handleCopy(msg.content)}
                  className="p-1.5 text-slate-500 hover:text-sky-400 hover:bg-slate-800 rounded-lg transition-colors cursor-pointer"
                  title={t('chat.copyContent')}
                >
                  <Copy size={14} />
                </button>
                <button
                  onClick={() => handleExport(msg.content)}
                  className="p-1.5 text-slate-500 hover:text-sky-400 hover:bg-slate-800 rounded-lg transition-colors cursor-pointer"
                  title={t('chat.exportMarkdown')}
                >
                  <Download size={14} />
                </button>
              </div>
            )}

            {/* 引用来源 - 显示完整信息 */}
            {msg.sources && msg.sources.length > 0 && (
              <div className="mt-4 pt-3 border-t border-slate-700/30 space-y-2">
                <div className="text-[11px] font-bold uppercase tracking-wider text-slate-500 flex items-center gap-1.5 mb-3">
                  <FileSearch size={12} className="text-sky-500" /> {t('chat.references')} ({msg.sources.length})
                </div>
                <div className="space-y-2">
                  {msg.sources.map((src) => (
                    <div
                      key={src.id}
                      onClick={() => handleOpenSource(src)}
                      className="bg-slate-800/40 hover:bg-slate-700/60 border border-slate-700/50 hover:border-sky-500/30 rounded-lg p-3 transition-all cursor-pointer group/ref relative overflow-hidden"
                    >
                      {/* Glow effect on hover */}
                      <div className="absolute inset-0 bg-gradient-to-r from-transparent via-sky-500/5 to-transparent -translate-x-full group-hover/ref:translate-x-full transition-transform duration-1000 pointer-events-none"></div>

                      {/* 第一行：cite_key + 链接图标 */}
                      <div className="flex items-start justify-between gap-2 mb-1">
                        <span className="text-xs font-mono font-bold text-sky-400 bg-slate-900/50 border border-slate-700/50 px-1.5 py-0.5 rounded shadow-sm">
                          [{src.cite_key}]
                        </span>
                        {(src.url || src.doi) && (
                          <ExternalLink size={12} className="text-slate-500 group-hover/ref:text-sky-400 flex-shrink-0 mt-0.5 transition-colors" />
                        )}
                      </div>
                      
                      {/* 第二行：标题 */}
                      {src.title && (
                        <div className="text-sm font-medium text-slate-200 leading-snug mb-1.5 group-hover/ref:text-sky-100 transition-colors">
                          {src.title}
                        </div>
                      )}
                      
                      {/* 第三行：作者 + 年份 */}
                      <div className="flex items-center gap-3 text-xs text-slate-400">
                        {src.authors && src.authors.length > 0 && (
                          <span className="flex items-center gap-1">
                            <User size={10} className="opacity-60" />
                            {formatAuthors(src.authors)}
                          </span>
                        )}
                        {src.year && (
                          <span className="flex items-center gap-1">
                            <Calendar size={10} className="opacity-60" />
                            {src.year}
                          </span>
                        )}
                        {src.doc_id && !src.url && (
                          <span className="flex items-center gap-1 text-slate-500">
                            <FileText size={10} />
                            {t('chat.localDoc')}
                          </span>
                        )}
                      </div>
                      {/* 操作栏：溯源原文 + 加入对比 */}
                      <div className="mt-2 pt-2 border-t border-slate-700/30 flex justify-end gap-3">
                        {/* 溯源原文：仅本地文档且有 doc_id 时可用 */}
                        {src.type === 'local' && src.doc_id && (
                          <button
                            type="button"
                            onClick={(e) => {
                              e.stopPropagation();
                              handleOpenPdfTrace(src);
                            }}
                            className="text-xs text-amber-500 hover:text-amber-300 flex items-center gap-1 font-medium transition-colors"
                          >
                            <BookOpen size={10} />
                            📄 溯源原文
                          </button>
                        )}
                        <button
                          type="button"
                          onClick={(e) => {
                            e.stopPropagation();
                            if (src.doc_id) {
                              addComparePreselected(src.doc_id);
                              addToast(t('chat.addedToCompare'), 'info');
                            } else {
                              addToast(t('chat.onlyLocalCompare'), 'info');
                            }
                          }}
                          className="text-xs text-sky-500 hover:text-sky-300 flex items-center gap-1 font-medium transition-colors"
                        >
                          <GitCompareArrows size={10} />
                          {t('chat.addToCompare')}
                        </button>
                      </div>
                    </div>
                  ))}
                </div>
              </div>
            )}
          </div>
        </div>
      );
      })}

      {/* PDF 溯源 Modal */}
      <PdfViewerModal
        open={pdfModal.open}
        onClose={() => setPdfModal((prev) => ({ ...prev, open: false }))}
        pdfUrl={pdfModal.pdfUrl}
        pageNumber={pdfModal.pageNumber}
        bbox={pdfModal.bbox}
        title={pdfModal.title}
      />
    </div>
  );
}
</file>

<file path="frontend/src/components/workflow/DeepResearchDialog.tsx">
import { useState, useEffect, useMemo, useRef } from 'react';
import { Telescope, Loader2, X, ChevronRight, Plus, Trash2, GripVertical, Square, Paperclip, Copy } from 'lucide-react';
import { useChatStore, useConfigStore, useToastStore, useCanvasStore, useUIStore } from '../../stores';
import {
  clarifyForDeepResearch,
  deepResearchStart,
  deepResearchSubmit,
  getDeepResearchJob,
  listDeepResearchJobEvents,
  cancelDeepResearchJob,
  extractDeepResearchContextFiles,
} from '../../api/chat';
import { exportCanvas, getCanvas } from '../../api/canvas';
import type {
  DeepResearchConfirmRequest,
  DeepResearchStartRequest,
  DeepResearchJobEvent,
  Source,
  ResearchDashboardData,
  ChatCitation,
} from '../../types';

/**
 * Deep Research 澄清对话框
 * 在用户触发 Deep Research 时弹出，显示 LLM 生成的澄清问题，
 * 用户回答后发送 Deep Research 请求。
 */
export function DeepResearchDialog() {
  const DEEP_RESEARCH_JOB_KEY = 'deep_research_active_job_id';
  const DEEP_RESEARCH_PENDING_CONTEXT_KEY = 'deep_research_pending_user_context';
  const DEEP_RESEARCH_ARCHIVED_JOBS_KEY = 'deep_research_archived_job_ids';
  type ProgressPayload = {
    type?: string;
    section?: string;
    message?: string;
    [key: string]: unknown;
  };
  type BriefDraft = Record<string, unknown>;
  type InitialStats = {
    total_sources?: number;
    total_iterations?: number;
    [key: string]: unknown;
  };
  type ResearchMonitorState = {
    graphSteps: number;
    warnSteps: number | null;
    forceSteps: number | null;
    lastNode: string;
    costState: 'normal' | 'warn' | 'force';
    selfCorrectionCount: number;
    plateauEarlyStopCount: number;
    verificationContextCount: number;
    sectionCoverage: Record<string, number[]>;
    sectionSteps: Record<string, number[]>;
  };

  const createEmptyMonitor = (): ResearchMonitorState => ({
    graphSteps: 0,
    warnSteps: null,
    forceSteps: null,
    lastNode: '',
    costState: 'normal',
    selfCorrectionCount: 0,
    plateauEarlyStopCount: 0,
    verificationContextCount: 0,
    sectionCoverage: {},
    sectionSteps: {},
  });

  const {
    showDeepResearchDialog,
    setShowDeepResearchDialog,
    deepResearchTopic,
    setDeepResearchTopic,
    clarificationQuestions,
    setClarificationQuestions,
    sessionId,
    canvasId,
    isStreaming,
    addMessage,
    updateLastMessage,
    appendToLastMessage,
    setLastMessageSources,
    setSessionId,
    setCanvasId,
    setWorkflowStep,
    setIsStreaming,
    setDeepResearchActive,
  } = useChatStore();
  const setResearchDashboard = useChatStore((s) => s.setResearchDashboard);
  const {
    webSearchConfig,
    ragConfig,
    selectedProvider,
    selectedModel,
    currentCollection,
  } = useConfigStore();
  const addToast = useToastStore((s) => s.addToast);
  const { setCanvas, setCanvasContent, setIsLoading: setCanvasLoading, setActiveStage } = useCanvasStore();
  const setCanvasOpen = useUIStore((s) => s.setCanvasOpen);

  // 用户对每个问题的回答
  const [answers, setAnswers] = useState<Record<string, string>>({});
  const [phase, setPhase] = useState<'clarify' | 'confirm' | 'running'>('clarify');
  const [outlineDraft, setOutlineDraft] = useState<string[]>([]);
  const [briefDraft, setBriefDraft] = useState<BriefDraft | null>(null);
  const [initialStats, setInitialStats] = useState<InitialStats | null>(null);
  const [progressLogs, setProgressLogs] = useState<string[]>([]);
  const [researchMonitor, setResearchMonitor] = useState<ResearchMonitorState>(createEmptyMonitor);
  const [outputLanguage, setOutputLanguage] = useState<'auto' | 'en' | 'zh'>('auto');
  const [yearStart, setYearStart] = useState<number | null>(null);
  const [yearEnd, setYearEnd] = useState<number | null>(null);
  const [showAdvancedModels, setShowAdvancedModels] = useState(false);
  const [stepModelStrict, setStepModelStrict] = useState(false);
  const [draggingOutlineIndex, setDraggingOutlineIndex] = useState<number | null>(null);
  const [dragOverOutlineIndex, setDragOverOutlineIndex] = useState<number | null>(null);
  const [activeJobId, setActiveJobId] = useState<string | null>(null);
  const [isStopping, setIsStopping] = useState(false);
  const [userContext, setUserContext] = useState('');
  const [userContextMode, setUserContextMode] = useState<'supporting' | 'direct_injection'>('supporting');
  const [tempDocuments, setTempDocuments] = useState<Array<{ name: string; content: string }>>([]);
  const [isExtractingContextFiles, setIsExtractingContextFiles] = useState(false);
  const [depth, setDepth] = useState<'lite' | 'comprehensive'>('comprehensive');
  const [skipDraftReview, setSkipDraftReview] = useState(false);
  const [skipRefineReview, setSkipRefineReview] = useState(false);
  const [skipClaimGeneration, setSkipClaimGeneration] = useState(false);
  const [keepPreviousJobId, setKeepPreviousJobId] = useState(true);
  const [optimizationPromptDraft, setOptimizationPromptDraft] = useState('');
  const contextFileInputRef = useRef<HTMLInputElement | null>(null);
  const pollTimerRef = useRef<number | null>(null);
  const isPollingRef = useRef(false);
  const lastEventIdRef = useRef(0);
  const canvasRefreshCounterRef = useRef(0);
  const [stepModels, setStepModels] = useState<Record<string, string>>({
    scope: 'sonar::sonar-pro',
    plan: '',
    research: '',
    evaluate: '',
    write: '',
    verify: '',
    synthesize: '',
  });

  // 初始化默认回答
  useEffect(() => {
    if (clarificationQuestions.length > 0) {
      const defaults: Record<string, string> = {};
      clarificationQuestions.forEach((q) => {
        defaults[q.id] = q.default || '';
      });
      setAnswers(defaults);
    }
  }, [clarificationQuestions]);

  const [isClarifying, setIsClarifying] = useState(false);

  /** Resolve scope model from persistent defaults for clarify calls */
  const resolveScopeModel = () => {
    const defaults = useConfigStore.getState().deepResearchDefaults;
    const scopeValue = (defaults.stepModels.scope || '').trim();
    if (scopeValue && scopeValue.includes('::')) {
      const [provider, model] = scopeValue.split('::', 2);
      return { llm_provider: provider || undefined, model_override: model || undefined };
    }
    if (scopeValue) {
      return { llm_provider: selectedProvider || undefined, model_override: scopeValue || undefined };
    }
    return { llm_provider: selectedProvider || undefined, model_override: selectedModel || undefined };
  };

  const runClarify = async (topic: string) => {
    const trimTopic = (topic || '').trim();
    if (!trimTopic) return;
    const resolved = resolveScopeModel();
    setIsClarifying(true);
    try {
      const result = await clarifyForDeepResearch({
        message: trimTopic,
        session_id: useChatStore.getState().sessionId || undefined,
        search_mode: 'hybrid',
        llm_provider: resolved.llm_provider,
        model_override: resolved.model_override,
      });
      const qs = result.questions || [];
      setClarificationQuestions(qs);
      if (result.suggested_topic) {
        setDeepResearchTopic(result.suggested_topic);
      }
      if (result.used_fallback) {
        addToast(`澄清阶段触发回退：${result.fallback_reason || 'unknown'}`, 'warning');
      } else {
        addToast(
          `澄清模型：${result.llm_provider_used || resolved.llm_provider || 'default'}`
            + `${result.llm_model_used ? `::${result.llm_model_used}` : ''}`,
          'info',
        );
      }
    } catch (err) {
      console.error('[DeepResearchDialog] Clarify failed:', err);
      setClarificationQuestions([
        {
          id: 'q1',
          text: '请确认本次研究最关键的目标与范围边界',
          question_type: 'text',
          options: [],
          default: trimTopic,
        },
      ]);
      addToast('澄清问题生成失败，已回退到最小问题集', 'warning');
    } finally {
      setIsClarifying(false);
    }
  };

  useEffect(() => {
    if (showDeepResearchDialog) {
      if (activeJobId) {
        setPhase('running');
      } else {
        setPhase('clarify');
        setOutlineDraft([]);
        setBriefDraft(null);
        setInitialStats(null);
        setProgressLogs([]);
        setResearchMonitor(createEmptyMonitor());
        setOptimizationPromptDraft('');
        const pendingUserContext = localStorage.getItem(DEEP_RESEARCH_PENDING_CONTEXT_KEY) || '';
        if (pendingUserContext.trim()) {
          setUserContext(pendingUserContext.trim());
          setUserContextMode('direct_injection');
          localStorage.removeItem(DEEP_RESEARCH_PENDING_CONTEXT_KEY);
        } else {
          setUserContext('');
          setUserContextMode('supporting');
        }
        setTempDocuments([]);

        // Initialize local overrides from persistent defaults (configured via gear popover)
        const defaults = useConfigStore.getState().deepResearchDefaults;
        setDepth(defaults.depth);
        setOutputLanguage(defaults.outputLanguage);
        setYearStart(defaults.yearStart ?? null);
        setYearEnd(defaults.yearEnd ?? null);
        setStepModelStrict(defaults.stepModelStrict);
        setSkipClaimGeneration(defaults.skipClaimGeneration);
        setStepModels({ ...defaults.stepModels });
        setAnswers({});
      }
    }
  // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [showDeepResearchDialog, activeJobId]);

  const stopPolling = () => {
    if (pollTimerRef.current !== null) {
      window.clearInterval(pollTimerRef.current);
      pollTimerRef.current = null;
    }
    isPollingRef.current = false;
  };

  const mapCitationsToSources = (citations: ChatCitation[]): Source[] => (
    citations.map((cite, idx: number) => ({
      id: idx + 1,
      cite_key: cite.cite_key,
      title: cite.title || cite.cite_key,
      authors: cite.authors || [],
      year: cite.year,
      doc_id: cite.doc_id,
      url: cite.url,
      doi: cite.doi,
      type: cite.url ? 'web' : 'local',
    }))
  );

  const archiveJobId = (jobId: string) => {
    try {
      const raw = localStorage.getItem(DEEP_RESEARCH_ARCHIVED_JOBS_KEY);
      const parsed = raw ? JSON.parse(raw) : [];
      const ids: string[] = Array.isArray(parsed) ? parsed.filter((x) => typeof x === 'string') : [];
      const next = [jobId, ...ids.filter((id) => id !== jobId)].slice(0, 20);
      localStorage.setItem(DEEP_RESEARCH_ARCHIVED_JOBS_KEY, JSON.stringify(next));
    } catch (err) {
      console.debug('[DeepResearch] archive job id failed:', err);
    }
  };

  const appendProgressEvents = (events: DeepResearchJobEvent[]) => {
    if (!events.length) return;
    setResearchMonitor((prev) => {
      const next: ResearchMonitorState = {
        ...prev,
        sectionCoverage: { ...prev.sectionCoverage },
        sectionSteps: { ...prev.sectionSteps },
      };
      const toNum = (v: unknown): number | null => {
        if (v === null || v === undefined || v === '') return null;
        const n = Number(v);
        return Number.isFinite(n) ? n : null;
      };
      events.forEach((evt) => {
        const payload = (evt.data || {}) as ProgressPayload;
        if (evt.event !== 'progress') return;
        const pType = String(payload?.type || 'progress');
        if (pType === 'section_evaluate_done') {
          const section = String(payload?.section || '').trim();
          const cov = toNum(payload?.coverage);
          const graphSteps = toNum(payload?.graph_steps);
          if (section && cov !== null) {
            const hist = [...(next.sectionCoverage[section] || []), cov];
            next.sectionCoverage[section] = hist.slice(-8);
            if (graphSteps !== null) {
              const stepHist = [...(next.sectionSteps[section] || []), graphSteps];
              next.sectionSteps[section] = stepHist.slice(-8);
            }
          }
        } else if (pType === 'search_self_correction') {
          next.selfCorrectionCount += 1;
        } else if (pType === 'coverage_plateau_early_stop') {
          next.plateauEarlyStopCount += 1;
        } else if (pType === 'write_verification_context') {
          next.verificationContextCount += 1;
        } else if (pType === 'cost_monitor_tick' || pType === 'cost_monitor_warn' || pType === 'cost_monitor_force_summary') {
          const steps = toNum(payload?.steps);
          const warn = toNum(payload?.warn_steps);
          const force = toNum(payload?.force_steps);
          const node = String(payload?.node || '').trim();
          if (steps !== null) next.graphSteps = Math.max(next.graphSteps, steps);
          if (warn !== null) next.warnSteps = warn;
          if (force !== null) next.forceSteps = force;
          if (node) next.lastNode = node;
          if (pType === 'cost_monitor_warn' && next.costState === 'normal') next.costState = 'warn';
          if (pType === 'cost_monitor_force_summary') next.costState = 'force';
        }
      });
      return next;
    });

    setProgressLogs((prev) => {
      const next = [...prev];
      events.forEach((evt) => {
        const payload = (evt.data || {}) as ProgressPayload;
        if (evt.event === 'progress') {
          const pType = String(payload?.type || 'progress');
          let line = payload?.section
            ? `[${pType}] ${payload.section}`
            : `[${pType}] ${JSON.stringify(payload)}`;
          if (pType === 'section_evaluate_done') {
            const section = String(payload?.section || '');
            const parseNum = (v: unknown): number | null => {
              if (v === null || v === undefined || v === '') return null;
              const n = Number(v);
              return Number.isFinite(n) ? n : null;
            };
            const coverage = parseNum(payload?.coverage);
            const gain = parseNum(payload?.coverage_gain);
            const round = parseNum(payload?.research_round);
            const coverageText = coverage !== null ? coverage.toFixed(3) : '?';
            const gainText = gain !== null ? gain.toFixed(3) : '?';
            line = `[section_evaluate_done] ${section || 'section'}: coverage=${coverageText}, gain=${gainText}, round=${round !== null ? Math.trunc(round) : '?'}`;
          } else if (pType === 'evidence_insufficient') {
            line = payload?.section
              ? `[evidence_insufficient] ${payload.section}: ${payload.message || 'Evidence remains insufficient after fallback search.'}`
              : `[evidence_insufficient] ${payload.message || JSON.stringify(payload)}`;
            addToast(payload?.message ? String(payload.message) : 'Evidence insufficient for current section', 'warning');
          } else if (pType === 'section_degraded') {
            line = payload?.section
              ? `[section_degraded] ${payload.section}: ${payload.message || 'Section downgraded due to sparse evidence.'}`
              : `[section_degraded] ${payload.message || JSON.stringify(payload)}`;
            addToast(payload?.message ? String(payload.message) : 'Section downgraded due to sparse evidence', 'info');
          } else if (pType === 'search_self_correction') {
            line = payload?.section
              ? `[search_self_correction] ${payload.section}: top_k ${String(payload?.top_k_from || '?')} -> ${String(payload?.top_k_to || '?')}`
              : `[search_self_correction] ${payload?.message || JSON.stringify(payload)}`;
          } else if (pType === 'coverage_plateau_early_stop') {
            line = payload?.section
              ? `[coverage_plateau_early_stop] ${payload.section}: coverage ${String(payload?.coverage || '?')}`
              : `[coverage_plateau_early_stop] ${payload?.message || JSON.stringify(payload)}`;
            addToast(payload?.message ? String(payload.message) : 'Coverage gain flattened, early stop applied', 'info');
          } else if (pType === 'write_verification_context') {
            line = payload?.section
              ? `[write_verification_context] ${payload.section}: write_k=${String(payload?.write_top_k || '?')}, verify_k=${String(payload?.verification_k || '?')}`
              : `[write_verification_context] ${payload?.message || JSON.stringify(payload)}`;
          } else if (pType === 'cost_monitor_tick') {
            line = `[cost_monitor_tick] steps=${String(payload?.steps || '?')} (${String(payload?.node || 'node')})`;
          } else if (pType === 'step_model_fallback') {
            line = payload?.step
              ? `[step_model_fallback] ${payload.step}: ${payload.message || 'Fallback to default model'}`
              : `[step_model_fallback] ${payload.message || JSON.stringify(payload)}`;
            addToast(payload?.message ? String(payload.message) : 'Step model fallback occurred', 'warning');
          } else if (pType === 'step_model_resolved') {
            const step = String(payload?.step || '');
            const provider = String(payload?.actual_provider || 'default');
            const model = String(payload?.actual_model || '');
            line = `[step_model_resolved] ${step || 'step'} -> ${provider}${model ? `::${model}` : ''}`;
          } else if (pType === 'all_reviews_approved') {
            line = `[all_reviews_approved] approved ${String(payload?.approved || 0)}/${String(payload?.total || 0)}; entering synthesize`;
            addToast('所有章节已通过，开始最终整合...', 'success');
          } else if (pType === 'global_refine_done') {
            line = `[global_refine_done] ${payload?.message || 'Global coherence refinement completed.'}`;
            addToast('全文连贯性整合完成', 'success');
          } else if (pType === 'citation_guard_fallback') {
            line = `[citation_guard_fallback] ${payload?.message || 'Citation guard fallback to pre-refine version.'}`;
            addToast('检测到引用丢失风险，已自动回退到安全版本', 'warning');
          } else if (pType === 'cost_monitor_warn') {
            line = `[cost_monitor_warn] ${payload?.message || 'Cost monitor warning triggered.'}`;
            addToast(payload?.message ? String(payload.message) : '研究步数较高，建议人工介入', 'warning');
          } else if (pType === 'cost_monitor_force_summary') {
            line = `[cost_monitor_force_summary] ${payload?.message || 'Forced summary mode activated.'}`;
            addToast('已触发强制摘要模式以控制成本', 'warning');
          }
          next.push(line);
        } else if (evt.event === 'warning') {
          const line = payload?.message
            ? `[warning] ${payload.message}`
            : `[warning] ${JSON.stringify(payload)}`;
          next.push(line);
        } else if (evt.event === 'cancel_requested') {
          next.push('[info] 收到停止请求，任务正在终止...');
        }
      });
      return next.slice(-300);
    });
  };

  const finalizeRunningJob = async (jobId: string) => {
    try {
      const job = await getDeepResearchJob(jobId);
      if (job.session_id) setSessionId(job.session_id);
      if (job.canvas_id) setCanvasId(job.canvas_id);
      if (job.result_dashboard && Object.keys(job.result_dashboard).length > 0) {
        setResearchDashboard(job.result_dashboard as unknown as ResearchDashboardData);
      }
      const citations = (job.result_citations || []) as ChatCitation[];
      if (citations.length > 0) {
        setLastMessageSources(mapCitationsToSources(citations));
      }
      if (job.result_markdown) {
        updateLastMessage(job.result_markdown);
        // Prefer synthesized final markdown as refine content.
        setCanvasContent(job.result_markdown);
      }
      if (job.canvas_id) {
        setCanvasLoading(true);
        // 加载结构化 canvas 数据（用于 stage views）+ Markdown 内容（用于 Refine 预览）
        Promise.all([
          getCanvas(job.canvas_id).catch((err) => {
            console.error('[DeepResearch] Canvas data load failed:', err);
            return null;
          }),
          exportCanvas(job.canvas_id, 'markdown').catch((err) => {
            console.error('[DeepResearch] Canvas markdown load failed:', err);
            return null;
          }),
        ])
          .then(([canvasData, exportResp]) => {
            if (canvasData) setCanvas(canvasData);
            if (exportResp?.content && !job.result_markdown) {
              setCanvasContent(exportResp.content);
            }
            if (canvasData || exportResp?.content || job.result_markdown) setCanvasOpen(true);
          })
          .finally(() => setCanvasLoading(false));
      }
      if (job.status === 'done') {
        setActiveStage('refine');
        setWorkflowStep('refine');
        addToast('Deep Research 完成', 'success');
        setShowDeepResearchDialog(false);
        setTimeout(() => setWorkflowStep('idle'), 1000);
      } else if (job.status === 'cancelled') {
        appendToLastMessage('\n\n[提示] Deep Research 已停止。');
        addToast('Deep Research 已停止', 'info');
      } else {
        appendToLastMessage(`\n\n[错误] Deep Research 失败：${job.error_message || job.message || 'unknown error'}`);
        addToast('Deep Research 失败，请重试', 'error');
      }
    } finally {
      stopPolling();
      localStorage.removeItem(DEEP_RESEARCH_JOB_KEY);
      setActiveJobId(null);
      setIsStopping(false);
      setIsStreaming(false);
      setDeepResearchActive(false);
      setPhase('confirm');
    }
  };

  const pollJobOnce = async (jobId: string) => {
    if (isPollingRef.current) return;
    isPollingRef.current = true;
    try {
      const events = await listDeepResearchJobEvents(jobId, lastEventIdRef.current, 200);
      if (events.length > 0) {
        const maxId = events[events.length - 1].event_id;
        lastEventIdRef.current = Math.max(lastEventIdRef.current, maxId);
        appendProgressEvents(events);
      }
      const job = await getDeepResearchJob(jobId);
      if (job.result_dashboard && Object.keys(job.result_dashboard).length > 0) {
        setResearchDashboard(job.result_dashboard as unknown as ResearchDashboardData);
      }
      if (job.message) {
        setProgressLogs((prev) => (prev[prev.length - 1] === `[status] ${job.message}` ? prev : [...prev, `[status] ${job.message}`].slice(-300)));
      }
      canvasRefreshCounterRef.current += 1;
      const runningCanvasId = job.canvas_id || canvasId;
      if (runningCanvasId && canvasRefreshCounterRef.current % 3 === 0) {
        getCanvas(runningCanvasId)
          .then((canvasData) => {
            if (!canvasData) return;
            setCanvas(canvasData);
            if ((job.status === 'running' || job.status === 'cancelling') && (canvasData.stage === 'drafting' || canvasData.stage === 'refine')) {
              setCanvasOpen(true);
            }
          })
          .catch((err) => {
            console.debug('[DeepResearch] periodic canvas refresh failed:', err);
          });
      }
      if (job.status === 'done' || job.status === 'cancelled' || job.status === 'error') {
        await finalizeRunningJob(jobId);
      }
    } catch (err) {
      console.error('[DeepResearch] poll failed:', err);
    } finally {
      isPollingRef.current = false;
    }
  };

  const startPollingJob = (jobId: string, resetEvents: boolean) => {
    stopPolling();
    if (resetEvents) {
      lastEventIdRef.current = 0;
      canvasRefreshCounterRef.current = 0;
      setProgressLogs([]);
      setResearchMonitor(createEmptyMonitor());
      setOptimizationPromptDraft('');
    }
    setActiveJobId(jobId);
    localStorage.setItem(DEEP_RESEARCH_JOB_KEY, jobId);
    pollJobOnce(jobId);
    pollTimerRef.current = window.setInterval(() => {
      pollJobOnce(jobId);
    }, 2000);
  };

  useEffect(() => {
    if (!showDeepResearchDialog) return;
    const savedJobId = localStorage.getItem(DEEP_RESEARCH_JOB_KEY);
    if (!savedJobId || activeJobId) return;
    // 新对话未绑定 session 时，不自动恢复旧任务，避免“新对话无法启动新任务”
    if (!sessionId) return;

    let cancelled = false;
    (async () => {
      try {
        const job = await getDeepResearchJob(savedJobId);
        if (cancelled) return;
        const isRunnable = job.status === 'running' || job.status === 'cancelling';
        const sameSession = !job.session_id || job.session_id === sessionId;
        const requestedTopic = (useChatStore.getState().deepResearchTopic || '').trim();
        const runningTopic = String(job.topic || '').trim();
        const sameTopic = !requestedTopic || !runningTopic || requestedTopic === runningTopic;
        if (!isRunnable) {
          localStorage.removeItem(DEEP_RESEARCH_JOB_KEY);
          return;
        }
        if (!sameSession) return;
        // If user is starting a NEW topic, don't auto-resume an old running job.
        // This preserves clarify -> outline confirm flow for the new request.
        if (!sameTopic) {
          addToast('检测到其他进行中的 Deep Research 任务；当前按新主题进入大纲确认流程。', 'info');
          return;
        }
        setPhase('running');
        setIsStreaming(true);
        setDeepResearchActive(true);
        startPollingJob(savedJobId, false);
        addToast('已恢复 Deep Research 后台任务状态', 'info');
      } catch (err) {
        console.debug('[DeepResearch] skip stale saved job:', err);
        localStorage.removeItem(DEEP_RESEARCH_JOB_KEY);
      }
    })();

    return () => {
      cancelled = true;
    };
    // eslint-disable-next-line react-hooks/exhaustive-deps
  }, [showDeepResearchDialog, activeJobId, sessionId]);

  useEffect(() => () => {
    stopPolling();
  }, []);

  const modelOptions = useMemo(() => {
    const current = selectedModel
      ? `${selectedProvider}::${selectedModel}`
      : '';
    return [
      { value: '', label: 'Default (current global model)' },
      ...(current ? [{ value: current, label: `Current: ${current}` }] : []),
      { value: 'sonar::sonar', label: 'sonar (search)' },
      { value: 'sonar::sonar-pro', label: 'sonar-pro (search)' },
      { value: 'sonar::sonar-reasoning-pro', label: 'sonar-reasoning-pro' },
    ];
  }, [selectedProvider, selectedModel]);

  const monitorSectionEntries = useMemo(
    () => Object.entries(researchMonitor.sectionCoverage),
    [researchMonitor.sectionCoverage],
  );
  const sectionEfficiencyRows = useMemo(() => {
    const rows = monitorSectionEntries
      .map(([section, coverageValues]) => {
        if (!coverageValues || coverageValues.length < 2) return null;
        const stepValues = researchMonitor.sectionSteps[section] || [];
        const firstCoverage = coverageValues[0];
        const lastCoverage = coverageValues[coverageValues.length - 1];
        const rounds = coverageValues.length - 1;
        const deltaCoverage = lastCoverage - firstCoverage;
        const avgDelta = deltaCoverage / Math.max(1, rounds);
        const lastDelta = coverageValues[coverageValues.length - 1] - coverageValues[coverageValues.length - 2];
        let per10Steps: number | null = null;
        if (stepValues.length >= 2) {
          const stepSpan = stepValues[stepValues.length - 1] - stepValues[0];
          if (stepSpan > 0) {
            per10Steps = deltaCoverage / (stepSpan / 10);
          }
        }
        const score = avgDelta * 100;
        let level: 'high' | 'medium' | 'low' = 'low';
        if (avgDelta >= 0.08) level = 'high';
        else if (avgDelta >= 0.03) level = 'medium';
        return {
          section,
          firstCoverage,
          lastCoverage,
          rounds,
          avgDelta,
          lastDelta,
          per10Steps,
          score,
          level,
        };
      })
      .filter((row): row is NonNullable<typeof row> => Boolean(row));
    return rows.sort((a, b) => b.score - a.score);
  }, [monitorSectionEntries, researchMonitor.sectionSteps]);
  const targetCoverage = depth === 'lite' ? 0.6 : 0.8;
  const highEfficiencyRows = useMemo(
    () => sectionEfficiencyRows.filter((row) => row.level === 'high').slice(0, 3),
    [sectionEfficiencyRows],
  );
  const lowEfficiencyRows = useMemo(
    () => sectionEfficiencyRows.filter((row) => row.level === 'low').slice(0, 3),
    [sectionEfficiencyRows],
  );
  const handleGenerateOptimizationPrompt = () => {
    const targets = lowEfficiencyRows.length > 0 ? lowEfficiencyRows : sectionEfficiencyRows.slice(0, 3);
    if (!targets.length) {
      addToast('当前样本不足，请至少完成两轮 section evaluate', 'info');
      return;
    }
    const lines: string[] = [];
    lines.push(`# Deep Research Section Optimization Template`);
    lines.push(`Topic: ${deepResearchTopic || '(fill topic)'}`);
    lines.push(`Target coverage: ${targetCoverage.toFixed(2)}`);
    lines.push('');
    lines.push(`Usage: paste selected blocks into "Intervention" before next run.`);
    lines.push('');
    targets.forEach((row, idx) => {
      lines.push(`## ${idx + 1}. ${row.section}`);
      lines.push(`Current signal:`);
      lines.push(`- Coverage: ${row.lastCoverage.toFixed(2)} (target ${targetCoverage.toFixed(2)})`);
      lines.push(`- Avg gain/round: ${row.avgDelta.toFixed(3)}`);
      if (row.per10Steps !== null) {
        lines.push(`- Gain per 10 steps: ${row.per10Steps.toFixed(3)}`);
      }
      lines.push(`Optimization prompt skeleton:`);
      lines.push(`- Scope constraints:`);
      lines.push(`  - Focus only on "${row.section}"`);
      lines.push(`  - Exclude adjacent sections and generic narrative`);
      lines.push(`- Retrieval directives:`);
      lines.push(`  - Expand terminology variants, abbreviations, and mechanism synonyms`);
      lines.push(`  - Prioritize primary studies and data-bearing sources`);
      lines.push(`- Evidence directives:`);
      lines.push(`  - Provide explicit support for each major claim with citation tags`);
      lines.push(`  - Flag weak claims as evidence-limited instead of asserting`);
      lines.push(`- My supplemental evidence:`);
      lines.push(`  - [Paste your materials, notes, or constraints here]`);
      lines.push('');
    });
    const next = lines.join('\n');
    setOptimizationPromptDraft(next);
    addToast('已生成章节优化提示词模板，可复制后用于 Intervention', 'success');
  };
  const handleCopyOptimizationPrompt = async () => {
    if (!optimizationPromptDraft.trim()) return;
    try {
      await navigator.clipboard.writeText(optimizationPromptDraft);
      addToast('已复制优化提示词模板', 'success');
    } catch (err) {
      console.error('[DeepResearch] copy optimization prompt failed:', err);
      addToast('复制失败，请手动复制文本', 'warning');
    }
  };
  const handleInsertOptimizationPrompt = () => {
    const text = optimizationPromptDraft.trim();
    if (!text) return;
    setUserContext((prev) => {
      const current = (prev || '').trim();
      if (!current) return text;
      if (current.includes(text)) return current;
      return `${current}\n\n${text}`;
    });
    addToast('已写入 Intervention，可在下一轮直接使用', 'success');
  };

  if (!showDeepResearchDialog) return null;

  const handleClose = () => {
    setShowDeepResearchDialog(false);
    if (!activeJobId) {
      setDeepResearchActive(false);
    }
  };

  const handleAnswerChange = (questionId: string, value: string) => {
    setAnswers((prev) => ({ ...prev, [questionId]: value }));
  };

  const buildCommonRequestParams = () => {
    const enabledProviders = webSearchConfig.sources
      .filter((s) => s.enabled)
      .map((s) => s.id);

    const webSourceConfigs: Record<string, { topK: number; threshold: number }> = {};
    webSearchConfig.sources.forEach((source) => {
      if (source.enabled) {
        webSourceConfigs[source.id] = { topK: source.topK, threshold: source.threshold };
      }
    });

    const localEnabled = ragConfig.enabled ?? true;
    const webEnabled = webSearchConfig.enabled && enabledProviders.length > 0;

    let searchMode: 'local' | 'web' | 'hybrid';
    if (localEnabled && webEnabled) {
      searchMode = 'hybrid';
    } else if (webEnabled) {
      searchMode = 'web';
    } else {
      searchMode = 'local';
    }

    const queryOptimizerEnabled = Boolean(webSearchConfig.queryOptimizer ?? true);
    const maxQueries = Math.min(5, Math.max(1, Number(webSearchConfig.maxQueriesPerProvider ?? 3)));
    const deepFinalTopK = Math.max(ragConfig.finalTopK ?? 10, 30);

    return {
      searchMode,
      enabledProviders,
      webSourceConfigs,
      webEnabled,
      localEnabled,
      queryOptimizerEnabled,
      maxQueries,
      deepFinalTopK,
    };
  };

  const normalizeStepModels = () => {
    const out: Record<string, string | null> = {};
    Object.entries(stepModels).forEach(([k, v]) => {
      out[k] = (v || '').trim() || null;
    });
    return out;
  };

  const handleGeneratePlan = async () => {
    if (!deepResearchTopic.trim()) {
      addToast('请输入研究主题', 'error');
      return;
    }
    const {
      searchMode,
      enabledProviders,
      webSourceConfigs,
      webEnabled,
      localEnabled,
      queryOptimizerEnabled,
      maxQueries,
      deepFinalTopK,
    } = buildCommonRequestParams();
    const hasNonEmptyAnswers = Object.values(answers).some((v) => v.trim().length > 0);
    const startRequest: DeepResearchStartRequest = {
      topic: deepResearchTopic,
      session_id: sessionId || undefined,
      canvas_id: canvasId || undefined,
      collection: currentCollection || undefined,
      search_mode: searchMode,
      llm_provider: selectedProvider || undefined,
      model_override: selectedModel || undefined,
      web_providers: webEnabled ? enabledProviders : undefined,
      web_source_configs: (webEnabled && Object.keys(webSourceConfigs).length > 0) ? webSourceConfigs : undefined,
      use_query_optimizer: webEnabled ? queryOptimizerEnabled : undefined,
      query_optimizer_max_queries: webEnabled ? maxQueries : undefined,
      local_top_k: localEnabled ? Math.max(ragConfig.localTopK, 15) : undefined,
      local_threshold: localEnabled ? (ragConfig.localThreshold ?? undefined) : undefined,
      year_start: yearStart ?? undefined,
      year_end: yearEnd ?? undefined,
      final_top_k: deepFinalTopK,
      clarification_answers: hasNonEmptyAnswers ? answers : undefined,
      output_language: outputLanguage,
      step_models: normalizeStepModels(),
      step_model_strict: stepModelStrict,
    };
    setIsStreaming(true);
    try {
      const startResp = await deepResearchStart(startRequest);
      if (startResp.session_id) setSessionId(startResp.session_id);
      if (startResp.canvas_id) setCanvasId(startResp.canvas_id);
      setOutlineDraft(startResp.outline?.length ? startResp.outline : [deepResearchTopic]);
      setBriefDraft(startResp.brief || null);
      setInitialStats(startResp.initial_stats || null);
      if (startResp.used_fallback) {
        addToast(`澄清阶段触发回退：${startResp.fallback_reason || 'unknown'}`, 'warning');
      }
      setPhase('confirm');
    } catch (error) {
      console.error('[DeepResearch] Start error:', error);
      addToast('研究规划生成失败，请重试', 'error');
    } finally {
      setIsStreaming(false);
    }
  };

  const handleSkipClarificationAndGenerate = async () => {
    setAnswers({});
    await handleGeneratePlan();
  };

  const handleConfirmAndRun = async () => {
    if (!deepResearchTopic.trim()) return;
    const filteredOutline = outlineDraft.map((x) => x.trim()).filter(Boolean);
    if (!filteredOutline.length) {
      addToast('请至少保留一个大纲章节', 'error');
      return;
    }
    const {
      searchMode,
      enabledProviders,
      webSourceConfigs,
      webEnabled,
      localEnabled,
      queryOptimizerEnabled,
      maxQueries,
      deepFinalTopK,
    } = buildCommonRequestParams();

    setDeepResearchActive(true);
    setPhase('running');
    setProgressLogs([]);
    addMessage({ role: 'user', content: `[Deep Research] ${deepResearchTopic}` });
    addMessage({ role: 'assistant', content: '' });
    setWorkflowStep('explore');

    const confirmRequest: DeepResearchConfirmRequest = {
      topic: deepResearchTopic,
      session_id: sessionId || undefined,
      canvas_id: canvasId || undefined,
      collection: currentCollection || undefined,
      search_mode: searchMode,
      confirmed_outline: filteredOutline,
      confirmed_brief: briefDraft || undefined,
      output_language: outputLanguage,
      step_models: normalizeStepModels(),
      step_model_strict: stepModelStrict,
      llm_provider: selectedProvider || undefined,
      model_override: selectedModel || undefined,
      web_providers: webEnabled ? enabledProviders : undefined,
      web_source_configs: (webEnabled && Object.keys(webSourceConfigs).length > 0) ? webSourceConfigs : undefined,
      use_query_optimizer: webEnabled ? queryOptimizerEnabled : undefined,
      query_optimizer_max_queries: webEnabled ? maxQueries : undefined,
      local_top_k: localEnabled ? Math.max(ragConfig.localTopK, 15) : undefined,
      local_threshold: localEnabled ? (ragConfig.localThreshold ?? undefined) : undefined,
      year_start: yearStart ?? undefined,
      year_end: yearEnd ?? undefined,
      final_top_k: deepFinalTopK,
      user_context: userContext.trim() || undefined,
      user_context_mode: userContext.trim() ? userContextMode : undefined,
      user_documents: tempDocuments.length ? tempDocuments : undefined,
      depth,
      skip_draft_review: skipDraftReview,
      skip_refine_review: skipRefineReview,
      skip_claim_generation: skipClaimGeneration,
    };

    let submitted = false;
    const previousActiveJobId = localStorage.getItem(DEEP_RESEARCH_JOB_KEY);
    try {
      const submitResp = await deepResearchSubmit(confirmRequest);
      if (submitResp.session_id) setSessionId(submitResp.session_id);
      if (submitResp.canvas_id) setCanvasId(submitResp.canvas_id);
      if (
        keepPreviousJobId
        && previousActiveJobId
        && previousActiveJobId !== submitResp.job_id
      ) {
        archiveJobId(previousActiveJobId);
      }
      setWorkflowStep('drafting');
      startPollingJob(submitResp.job_id, true);
      setShowDeepResearchDialog(false);
      submitted = true;
      addToast('已转为后台执行，可安全关闭当前前端页面', 'info');
    } catch (error) {
      console.error('[DeepResearch] Error:', error);
      addToast('Deep Research 失败，请重试', 'error');
      appendToLastMessage('\n\n[错误] Deep Research 请求失败。');
      stopPolling();
      localStorage.removeItem(DEEP_RESEARCH_JOB_KEY);
      setActiveJobId(null);
      setDeepResearchActive(false);
      setPhase('confirm');
    } finally {
      if (!submitted) {
        setIsStreaming(false);
      }
    }
  };

  const handleStopRunningJob = async () => {
    if (!activeJobId || isStopping) return;
    setIsStopping(true);
    try {
      await cancelDeepResearchJob(activeJobId);
      addToast('已请求停止任务，正在终止...', 'info');
    } catch (err) {
      console.error('[DeepResearch] Cancel failed:', err);
      setIsStopping(false);
      addToast('停止任务失败，请重试', 'error');
    }
  };

  const handleSelectContextFiles = async (fileList: FileList | null) => {
    if (!fileList || fileList.length === 0) return;
    const files = Array.from(fileList);
    setIsExtractingContextFiles(true);
    try {
      const docs = await extractDeepResearchContextFiles(files);
      if (!docs.length) {
        addToast('未从文件提取到有效文本（支持 pdf/md/txt）', 'info');
        return;
      }
      setTempDocuments((prev) => {
        const merged = [...prev];
        docs.forEach((d) => {
          const exists = merged.some((x) => x.name === d.name && x.content === d.content);
          if (!exists) merged.push(d);
        });
        return merged.slice(0, 10);
      });
      addToast(`已添加 ${docs.length} 份临时材料`, 'success');
    } catch (err) {
      console.error('[DeepResearch] context extract failed:', err);
      addToast('临时材料提取失败，请重试', 'error');
    } finally {
      setIsExtractingContextFiles(false);
      if (contextFileInputRef.current) contextFileInputRef.current.value = '';
    }
  };

  const updateOutlineItem = (idx: number, value: string) => {
    setOutlineDraft((prev) => prev.map((item, i) => (i === idx ? value : item)));
  };

  const moveOutlineItem = (fromIdx: number, toIdx: number) => {
    if (fromIdx === toIdx) return;
    setOutlineDraft((prev) => {
      if (fromIdx < 0 || toIdx < 0 || fromIdx >= prev.length || toIdx >= prev.length) {
        return prev;
      }
      const next = [...prev];
      const [moved] = next.splice(fromIdx, 1);
      next.splice(toIdx, 0, moved);
      return next;
    });
  };

  const getQuestionRationale = (questionText: string) => {
    const text = questionText.toLowerCase();
    if (text.includes('范围') || text.includes('scope')) {
      return '用于锁定研究边界，避免大纲发散。';
    }
    if (text.includes('受众') || text.includes('风格') || text.includes('audience') || text.includes('style')) {
      return '用于匹配表达方式和写作深度。';
    }
    if (text.includes('篇幅') || text.includes('深度') || text.includes('字数') || text.includes('length')) {
      return '用于控制章节粒度和信息密度。';
    }
    if (text.includes('排除') || text.includes('exclude')) {
      return '用于减少无关检索和错误扩展。';
    }
    if (text.includes('语言') || text.includes('language')) {
      return '用于提前确定文献与输出语言策略。';
    }
    return '用于减少歧义，让后续大纲更贴合目标。';
  };

  return (
    <div className="fixed inset-0 bg-black/50 flex items-center justify-center z-50 animate-in fade-in duration-200">
      <div className="bg-white rounded-2xl shadow-2xl w-full max-w-lg mx-4 max-h-[80vh] flex flex-col animate-in slide-in-from-bottom-4 duration-300">
        {/* Header */}
        <div className="flex items-center justify-between px-6 py-4 border-b">
          <div className="flex items-center gap-3">
            <div className="p-2 bg-indigo-100 rounded-lg">
              <Telescope size={20} className="text-indigo-600" />
            </div>
            <div>
              <h2 className="text-lg font-semibold text-gray-900">Deep Research</h2>
              <p className="text-xs text-gray-500">多步深度研究 - 可确认大纲并跟踪进度</p>
            </div>
          </div>
          <button onClick={handleClose} className="p-1 hover:bg-gray-100 rounded-lg transition-colors">
            <X size={18} className="text-gray-400" />
          </button>
        </div>

        {/* Body */}
        <div className="flex-1 overflow-y-auto px-6 py-4 space-y-4">
          <div className="rounded-lg border border-gray-200 bg-gray-50 px-3 py-2">
            <div className="text-xs text-gray-600">
              <span className={phase === 'clarify' ? 'font-semibold text-indigo-600' : ''}>1. 澄清问题</span>
              <span className="mx-2 text-gray-300">→</span>
              <span className={phase === 'confirm' ? 'font-semibold text-indigo-600' : ''}>2. 确认大纲</span>
              <span className="mx-2 text-gray-300">→</span>
              <span className={phase === 'running' ? 'font-semibold text-indigo-600' : ''}>3. 执行研究</span>
            </div>
          </div>

          {/* 主题 */}
          <div>
            <label className="block text-sm font-medium text-gray-700 mb-1">研究主题</label>
            <input
              type="text"
              value={deepResearchTopic}
              onChange={(e) => setDeepResearchTopic(e.target.value)}
              className="w-full border border-gray-300 rounded-lg px-3 py-2 text-sm focus:ring-2 focus:ring-indigo-500 focus:border-indigo-500 outline-none"
              placeholder="输入综述主题..."
            />
          </div>

          {phase === 'clarify' && clarificationQuestions.length > 0 && (
            <div className="space-y-3">
              <h3 className="text-sm font-medium text-gray-600">
                请补充以下信息（可选，共 {clarificationQuestions.length} 题）
              </h3>
              {clarificationQuestions.map((q) => (
                <div key={q.id} className="bg-gray-50 rounded-lg p-3">
                  <label className="block text-sm text-gray-700 mb-1.5">{q.text}</label>
                  {q.question_type === 'choice' && q.options.length > 0 ? (
                    <select
                      value={answers[q.id] || ''}
                      onChange={(e) => handleAnswerChange(q.id, e.target.value)}
                      className="w-full border border-gray-200 rounded-md px-2.5 py-1.5 text-sm bg-white focus:ring-2 focus:ring-indigo-500 outline-none"
                    >
                      {q.options.map((opt) => (
                        <option key={opt} value={opt}>{opt}</option>
                      ))}
                    </select>
                  ) : (
                    <input
                      type="text"
                      value={answers[q.id] || ''}
                      onChange={(e) => handleAnswerChange(q.id, e.target.value)}
                      placeholder={q.default || '输入回答...'}
                      className="w-full border border-gray-200 rounded-md px-2.5 py-1.5 text-sm focus:ring-2 focus:ring-indigo-500 outline-none"
                    />
                  )}
                  <div className="mt-1.5 text-xs text-gray-500">
                    为什么问：{getQuestionRationale(q.text)}
                  </div>
                </div>
              ))}
            </div>
          )}

          {phase === 'clarify' && clarificationQuestions.length === 0 && (
            <div className="text-center py-4 text-gray-500 text-sm bg-gray-50 rounded-lg border border-gray-200">
              {isClarifying ? (
                <div className="flex items-center justify-center gap-2">
                  <Loader2 size={14} className="animate-spin text-indigo-500" />
                  <span>正在生成澄清问题...</span>
                </div>
              ) : (
                'Click "Regenerate" to generate clarification questions, or proceed directly to outline.'
              )}
            </div>
          )}

          {/* Clarify phase: compact settings summary + regenerate button */}
          {phase === 'clarify' && (
            <div className="flex items-center justify-between bg-gray-50 border border-gray-200 rounded-lg px-3 py-2.5">
              <div className="text-[11px] text-gray-500 flex items-center gap-1.5 flex-wrap">
                <span className="font-medium text-gray-700">{depth === 'lite' ? 'Lite' : 'Comprehensive'}</span>
                <span className="text-gray-300">|</span>
                <span>Scope: <span className="font-medium">{stepModels.scope || 'default'}</span></span>
                <span className="text-gray-300">|</span>
                <span>Lang: {outputLanguage === 'auto' ? 'Auto' : outputLanguage}</span>
                <span className="text-gray-300">|</span>
                <span className="text-[10px] text-gray-400">via input &#9881;</span>
              </div>
              <button
                onClick={() => runClarify(deepResearchTopic)}
                disabled={isClarifying || !deepResearchTopic.trim()}
                className="inline-flex items-center gap-1 px-2 py-1 border rounded-md text-[11px] text-indigo-600 hover:bg-indigo-50 disabled:opacity-50 shrink-0 ml-2"
              >
                {isClarifying ? <Loader2 size={10} className="animate-spin" /> : <ChevronRight size={10} />}
                Regenerate
              </button>
            </div>
          )}

          {/* Confirm phase: output language (can override per-run) */}
          {phase === 'confirm' && (
            <div>
              <label className="block text-sm font-medium text-gray-700 mb-1">Output Language</label>
              <select
                value={outputLanguage}
                onChange={(e) => setOutputLanguage(e.target.value as 'auto' | 'en' | 'zh')}
                className="w-full border border-gray-300 rounded-lg px-3 py-2 text-sm focus:ring-2 focus:ring-indigo-500 outline-none"
              >
                <option value="auto">Auto (follow topic language)</option>
                <option value="en">English</option>
                <option value="zh">中文</option>
              </select>
            </div>
          )}

          {/* Confirm phase: collapsed per-step model override */}
          {phase === 'confirm' && (
            <div className="space-y-2">
              <button
                onClick={() => setShowAdvancedModels((v) => !v)}
                className="text-xs text-indigo-600 hover:text-indigo-700"
              >
                {showAdvancedModels ? '\u25BE Hide' : '\u25B8 Override'} Per-step Models
              </button>
              {showAdvancedModels && (
                <div className="space-y-2 border border-gray-200 rounded-lg p-3">
                  <div className="text-[10px] text-gray-400 pb-1.5 border-b border-gray-100">
                    Loaded from &#9881; defaults. Changes here apply to this run only.
                  </div>
                  <label className="flex items-center justify-between text-xs text-gray-600">
                    <span className="flex items-center gap-1">
                      Strict step model resolution
                      <span className="text-gray-400 cursor-help" title="OFF: model failure falls back to default silently. ON: model failure aborts the research immediately.">?</span>
                    </span>
                    <input
                      type="checkbox"
                      checked={stepModelStrict}
                      onChange={(e) => setStepModelStrict(e.target.checked)}
                      className="accent-indigo-500"
                    />
                  </label>
                  {['scope', 'plan', 'research', 'evaluate', 'write', 'verify', 'synthesize'].map((step) => (
                    <div key={step} className="grid grid-cols-3 items-center gap-2">
                      <div className="text-xs font-medium text-gray-600 uppercase">{step}</div>
                      <select
                        value={stepModels[step] || ''}
                        onChange={(e) => setStepModels((prev) => ({ ...prev, [step]: e.target.value }))}
                        className="col-span-2 border border-gray-200 rounded-md px-2 py-1 text-xs"
                      >
                        {modelOptions.map((opt) => (
                          <option key={`${step}-${opt.value || 'default'}`} value={opt.value}>{opt.label}</option>
                        ))}
                      </select>
                    </div>
                  ))}
                </div>
              )}
            </div>
          )}

          {phase === 'confirm' && (
            <>
              <div className="bg-indigo-50 border border-indigo-100 rounded-lg p-3 text-xs text-indigo-800">
                <div>Initial sources: {initialStats?.total_sources ?? 0}</div>
                <div>Iterations: {initialStats?.total_iterations ?? 0}</div>
                <div>Tip: edit and reorder outline before execution.</div>
              </div>
              <div className="space-y-2">
                <div className="text-sm font-medium text-gray-700">Outline Confirmation</div>
                {outlineDraft.map((item, idx) => (
                  <div key={`outline-${idx}`} className="space-y-1">
                    {dragOverOutlineIndex === idx && draggingOutlineIndex !== null && (
                      <div className="h-0.5 bg-indigo-500 rounded-full" />
                    )}
                    <div
                      className={`flex gap-2 ${draggingOutlineIndex === idx ? 'opacity-60' : ''}`}
                      draggable
                      onDragStart={() => setDraggingOutlineIndex(idx)}
                      onDragEnd={() => {
                        setDraggingOutlineIndex(null);
                        setDragOverOutlineIndex(null);
                      }}
                      onDragOver={(e) => {
                        e.preventDefault();
                        setDragOverOutlineIndex(idx);
                      }}
                      onDrop={() => {
                        if (draggingOutlineIndex === null) return;
                        moveOutlineItem(draggingOutlineIndex, idx);
                        setDraggingOutlineIndex(null);
                        setDragOverOutlineIndex(null);
                      }}
                    >
                    <button
                      type="button"
                      className="px-2 py-1 border rounded-md text-gray-400 hover:bg-gray-50 cursor-grab active:cursor-grabbing"
                      title="拖拽排序"
                    >
                      <GripVertical size={14} />
                    </button>
                    <input
                      type="text"
                      value={item}
                      onChange={(e) => updateOutlineItem(idx, e.target.value)}
                      placeholder="New section title..."
                      className="flex-1 border border-gray-200 rounded-md px-2.5 py-1.5 text-sm"
                    />
                    <button
                      onClick={() => setOutlineDraft((prev) => prev.filter((_, i) => i !== idx))}
                      className="px-2 py-1 border rounded-md text-gray-500 hover:bg-gray-50"
                    >
                      <Trash2 size={14} />
                    </button>
                    </div>
                  </div>
                ))}
                <button
                  onClick={() => setOutlineDraft((prev) => [...prev, ''])}
                  className="inline-flex items-center gap-1 px-2.5 py-1 border rounded-md text-xs text-gray-600 hover:bg-gray-50"
                >
                  <Plus size={12} /> Add section
                </button>
                <div className="text-xs text-gray-500">可拖拽左侧图标调整章节顺序。</div>
              </div>

              {/* 研究深度选择 */}
              <div className="bg-gray-50 border border-gray-200 rounded-lg p-3 space-y-2">
                <div className="text-sm font-medium text-gray-700">Research Depth (研究深度)</div>
                <div className="grid grid-cols-2 gap-2">
                  <button
                    type="button"
                    onClick={() => setDepth('lite')}
                    className={`flex flex-col items-start p-2.5 rounded-lg border text-left transition-all ${
                      depth === 'lite'
                        ? 'border-indigo-400 bg-indigo-50 ring-1 ring-indigo-300'
                        : 'border-gray-200 bg-white hover:border-gray-300'
                    }`}
                  >
                    <span className="text-xs font-semibold text-gray-800">Lite</span>
                    <span className="text-[10px] text-gray-500 leading-tight mt-0.5">
                      Quick but academically usable, ~5-15 min. 4 queries/section (recall+precision), tiered top_k 18/10/10, coverage &ge; 60%.
                    </span>
                  </button>
                  <button
                    type="button"
                    onClick={() => setDepth('comprehensive')}
                    className={`flex flex-col items-start p-2.5 rounded-lg border text-left transition-all ${
                      depth === 'comprehensive'
                        ? 'border-indigo-400 bg-indigo-50 ring-1 ring-indigo-300'
                        : 'border-gray-200 bg-white hover:border-gray-300'
                    }`}
                  >
                    <span className="text-xs font-semibold text-gray-800">Comprehensive</span>
                    <span className="text-[10px] text-gray-500 leading-tight mt-0.5">
                      Thorough academic review, ~20-60 min. 8 queries/section (recall+precision), tiered top_k 30/15/12, coverage &ge; 80%.
                    </span>
                  </button>
                </div>
              </div>

              {/* 阶段介入控制 — 放在大纲确认之后，位置更显眼 */}
              <div className="bg-gray-50 border border-gray-200 rounded-lg p-3 space-y-2.5">
                <div className="text-sm font-medium text-gray-700">Stage Intervention (阶段介入)</div>
                <div className="space-y-1.5">
                  <label className="flex items-center gap-2 text-xs text-gray-600 cursor-not-allowed opacity-70">
                    <input type="checkbox" checked disabled className="accent-indigo-500" />
                    <span>澄清意图 (Clarify)</span>
                    <span className="ml-auto text-[10px] text-indigo-500 font-medium">必须</span>
                  </label>
                  <label className="flex items-center gap-2 text-xs text-gray-600 cursor-not-allowed opacity-70">
                    <input type="checkbox" checked disabled className="accent-indigo-500" />
                    <span>确认大纲 (Confirm Outline)</span>
                    <span className="ml-auto text-[10px] text-indigo-500 font-medium">必须</span>
                  </label>
                  <label className="flex items-center gap-2 text-xs text-gray-600 cursor-pointer">
                    <input
                      type="checkbox"
                      checked={!skipDraftReview}
                      onChange={(e) => setSkipDraftReview(!e.target.checked)}
                      className="accent-indigo-500"
                    />
                    <span>逐章审阅 (Review Each Section)</span>
                    <span className="ml-auto text-[10px] text-gray-400">{skipDraftReview ? '已跳过' : '可选'}</span>
                  </label>
                  <label className="flex items-center gap-2 text-xs text-gray-600 cursor-pointer">
                    <input
                      type="checkbox"
                      checked={!skipRefineReview}
                      onChange={(e) => setSkipRefineReview(!e.target.checked)}
                      className="accent-indigo-500"
                    />
                    <span>精炼修改 (Refine with Directives)</span>
                    <span className="ml-auto text-[10px] text-gray-400">{skipRefineReview ? '已跳过' : '可选'}</span>
                  </label>
                  <label className="flex items-center gap-2 text-xs text-gray-600 cursor-pointer">
                    <input
                      type="checkbox"
                      checked={skipClaimGeneration}
                      onChange={(e) => setSkipClaimGeneration(e.target.checked)}
                      className="accent-indigo-500"
                    />
                    <span>跳过前置论点提炼 (Skip Claim Generation)</span>
                    <span className="ml-auto text-[10px] text-gray-400">{skipClaimGeneration ? '已跳过' : '可选'}</span>
                  </label>
                </div>
                <label className="flex items-center gap-2 text-xs text-gray-500 pt-1 border-t border-gray-200 cursor-pointer">
                  <input
                    type="checkbox"
                    checked={skipDraftReview && skipRefineReview}
                    onChange={(e) => {
                      setSkipDraftReview(e.target.checked);
                      setSkipRefineReview(e.target.checked);
                      setSkipClaimGeneration(e.target.checked);
                    }}
                    className="accent-gray-400"
                  />
                  <span>最小化人工介入（仅保留必须步骤）</span>
                </label>
              </div>

              <div className="bg-gray-50 border border-gray-200 rounded-lg p-3 space-y-1.5">
                <div className="text-sm font-medium text-gray-700">任务 ID 策略</div>
                <label className="flex items-center gap-2 text-xs text-gray-600 cursor-pointer">
                  <input
                    type="checkbox"
                    checked={keepPreviousJobId}
                    onChange={(e) => setKeepPreviousJobId(e.target.checked)}
                    className="accent-indigo-500"
                  />
                  <span>开始新任务时保留旧任务 ID（便于后续恢复）</span>
                </label>
                <div className="text-[11px] text-gray-500">
                  当前任务将继续在后台运行；新任务会使用新的 job id。
                </div>
              </div>

              <div className="space-y-2">
                <div className="text-sm font-medium text-gray-700">Intervention (补充上下文，可选)</div>
                <div>
                  <label className="block text-xs text-gray-600 mb-1">文本介入模式</label>
                  <select
                    value={userContextMode}
                    onChange={(e) => setUserContextMode(e.target.value as 'supporting' | 'direct_injection')}
                    className="w-full border border-gray-200 rounded-md px-2.5 py-1.5 text-xs"
                  >
                    <option value="supporting">作为补充上下文（默认）</option>
                    <option value="direct_injection">作为强提示直接注入（我对内容非常自信）</option>
                  </select>
                </div>
                <textarea
                  value={userContext}
                  onChange={(e) => setUserContext(e.target.value)}
                  placeholder={userContextMode === 'direct_injection'
                    ? '输入高置信观点/约束，系统会作为高优先级提示并要求显式验证...'
                    : '可补充新观点、反例、约束条件、重点文献线索...'}
                  className="w-full min-h-20 border border-gray-200 rounded-md px-2.5 py-2 text-sm"
                />
                <input
                  ref={contextFileInputRef}
                  type="file"
                  accept=".pdf,.md,.txt"
                  multiple
                  className="hidden"
                  onChange={(e) => handleSelectContextFiles(e.target.files)}
                />
                <button
                  onClick={() => contextFileInputRef.current?.click()}
                  disabled={isExtractingContextFiles}
                  className="inline-flex items-center gap-1 px-2.5 py-1 border rounded-md text-xs text-gray-600 hover:bg-gray-50 disabled:opacity-50"
                >
                  {isExtractingContextFiles ? <Loader2 size={12} className="animate-spin" /> : <Paperclip size={12} />}
                  上传临时材料 (pdf/md/txt)
                </button>
                {tempDocuments.length > 0 && (
                  <div className="space-y-1">
                    {tempDocuments.map((doc, idx) => (
                      <div key={`${doc.name}-${idx}`} className="flex items-center justify-between text-xs bg-gray-50 border border-gray-200 rounded px-2 py-1.5">
                        <span className="truncate pr-2">{doc.name}</span>
                        <button
                          onClick={() => setTempDocuments((prev) => prev.filter((_, i) => i !== idx))}
                          className="text-gray-500 hover:text-red-500"
                        >
                          移除
                        </button>
                      </div>
                    ))}
                  </div>
                )}
                <div className="text-xs text-gray-500">这些材料仅用于本次任务，不写入持久本地知识库。</div>
              </div>

            </>
          )}

          {phase === 'running' && (
            <div className="space-y-2">
              <div className="text-sm font-medium text-gray-700">Research Progress</div>
              <div className="text-xs text-gray-500">Executing confirmed plan...</div>
              <div className="border border-gray-200 rounded-lg p-2.5 bg-white space-y-2">
                <div className="text-xs font-medium text-gray-700">Research Monitor</div>
                <div className="grid grid-cols-2 gap-2 text-[11px]">
                  <div className="rounded border border-gray-200 bg-gray-50 px-2 py-1.5">
                    <div className="text-gray-500">Graph steps</div>
                    <div className={`font-medium ${
                      researchMonitor.costState === 'force'
                        ? 'text-red-600'
                        : researchMonitor.costState === 'warn'
                          ? 'text-amber-600'
                          : 'text-gray-800'
                    }`}
                    >
                      {researchMonitor.graphSteps > 0 ? researchMonitor.graphSteps : '--'}
                    </div>
                    <div className="text-[10px] text-gray-400">
                      warn {researchMonitor.warnSteps ?? '--'} / force {researchMonitor.forceSteps ?? '--'}
                    </div>
                  </div>
                  <div className="rounded border border-gray-200 bg-gray-50 px-2 py-1.5">
                    <div className="text-gray-500">Cost status</div>
                    <div className={`font-medium ${
                      researchMonitor.costState === 'force'
                        ? 'text-red-600'
                        : researchMonitor.costState === 'warn'
                          ? 'text-amber-600'
                          : 'text-emerald-600'
                    }`}
                    >
                      {researchMonitor.costState.toUpperCase()}
                    </div>
                    <div className="text-[10px] text-gray-400">node: {researchMonitor.lastNode || '--'}</div>
                  </div>
                  <div className="rounded border border-gray-200 bg-gray-50 px-2 py-1.5">
                    <div className="text-gray-500">Self-correction</div>
                    <div className="font-medium text-gray-800">{researchMonitor.selfCorrectionCount}</div>
                  </div>
                  <div className="rounded border border-gray-200 bg-gray-50 px-2 py-1.5">
                    <div className="text-gray-500">Plateau early-stop</div>
                    <div className="font-medium text-gray-800">{researchMonitor.plateauEarlyStopCount}</div>
                  </div>
                </div>
                <div className="rounded border border-gray-200 bg-gray-50 px-2 py-1.5 text-[11px]">
                  <span className="text-gray-500">Write verification passes: </span>
                  <span className="font-medium text-gray-800">{researchMonitor.verificationContextCount}</span>
                </div>
                {monitorSectionEntries.length > 0 && (
                  <div className="space-y-1">
                    <div className="text-[11px] text-gray-600">Section coverage curve</div>
                    {monitorSectionEntries.map(([section, values]) => (
                      <div key={`cov-${section}`} className="text-[11px] text-gray-700 border border-gray-200 rounded px-2 py-1 bg-gray-50">
                        <span className="font-medium">{section}</span>
                        <span className="text-gray-400"> : </span>
                        {values.map((v, idx) => (
                          <span key={`${section}-${idx}`} className="font-mono text-[10px]">
                            {idx > 0 ? ' -> ' : ''}
                            {v.toFixed(2)}
                          </span>
                        ))}
                      </div>
                    ))}
                  </div>
                )}
                {sectionEfficiencyRows.length > 0 && (
                  <div className="space-y-1.5">
                    <div className="text-[11px] text-gray-600">Efficiency insights</div>
                    {sectionEfficiencyRows.slice(0, 5).map((row) => (
                      <div key={`eff-${row.section}`} className="text-[11px] border border-gray-200 rounded px-2 py-1.5 bg-gray-50">
                        <div className="flex items-center justify-between">
                          <span className="font-medium text-gray-700">{row.section}</span>
                          <span className={`font-medium ${
                            row.level === 'high'
                              ? 'text-emerald-600'
                              : row.level === 'medium'
                                ? 'text-amber-600'
                                : 'text-red-600'
                          }`}
                          >
                            {row.level.toUpperCase()} ({row.score.toFixed(1)})
                          </span>
                        </div>
                        <div className="text-[10px] text-gray-500 mt-0.5">
                          cov {row.firstCoverage.toFixed(2)} {'->'} {row.lastCoverage.toFixed(2)}
                          {' | '}avg gain {row.avgDelta.toFixed(3)}/round
                          {row.per10Steps !== null ? ` | gain/10 steps ${row.per10Steps.toFixed(3)}` : ''}
                        </div>
                      </div>
                    ))}
                    <div className="rounded border border-gray-200 bg-white px-2 py-1.5 text-[11px] text-gray-600">
                      {highEfficiencyRows.length > 0 && (
                        <div>
                          Continue deepening: {highEfficiencyRows.map((r) => r.section).join(' / ')}.
                        </div>
                      )}
                      {lowEfficiencyRows.length > 0 && (
                        <div>
                          Optimize first (prompt/evidence): {lowEfficiencyRows.map((r) => r.section).join(' / ')}.
                        </div>
                      )}
                      <div>
                        Action hint: if low-efficiency section is below target coverage ({targetCoverage.toFixed(2)}), add section-specific constraints,
                        terminology variants, or temporary materials in Intervention before next run.
                      </div>
                    </div>
                    <div className="flex items-center gap-2">
                      <button
                        onClick={handleGenerateOptimizationPrompt}
                        className="px-2 py-1 border rounded-md text-[11px] text-indigo-600 hover:bg-indigo-50"
                      >
                        Generate optimization prompt template
                      </button>
                      {optimizationPromptDraft.trim() && (
                        <>
                          <button
                            onClick={handleInsertOptimizationPrompt}
                            className="px-2 py-1 border rounded-md text-[11px] text-emerald-700 hover:bg-emerald-50"
                          >
                            Insert to Intervention
                          </button>
                          <button
                            onClick={handleCopyOptimizationPrompt}
                            className="inline-flex items-center gap-1 px-2 py-1 border rounded-md text-[11px] text-gray-600 hover:bg-gray-50"
                          >
                            <Copy size={11} />
                            Copy
                          </button>
                        </>
                      )}
                    </div>
                    {optimizationPromptDraft.trim() && (
                      <textarea
                        readOnly
                        value={optimizationPromptDraft}
                        className="w-full min-h-32 border border-gray-200 rounded-md px-2 py-1.5 text-[11px] font-mono bg-gray-50"
                      />
                    )}
                  </div>
                )}
              </div>
              <div className="max-h-56 overflow-auto border border-gray-200 rounded-lg p-2 bg-gray-50 text-xs space-y-1">
                {progressLogs.length === 0 ? (
                  <div className="text-gray-400">Waiting for progress events...</div>
                ) : (
                  progressLogs.map((line, idx) => <div key={`log-${idx}`}>{line}</div>)
                )}
              </div>
            </div>
          )}
        </div>

        {/* Footer */}
        <div className="flex items-center justify-between px-6 py-4 border-t bg-gray-50 rounded-b-2xl">
          <button
            onClick={handleClose}
            className="px-4 py-2 text-sm text-gray-600 hover:bg-gray-200 rounded-lg transition-colors"
            disabled={phase === 'running'}
          >
            取消
          </button>
          <div className="flex items-center gap-2">
            {phase === 'clarify' && clarificationQuestions.length > 0 && (
              <button
                onClick={handleSkipClarificationAndGenerate}
                disabled={isStreaming || !deepResearchTopic.trim()}
                className="px-3 py-2 text-sm text-gray-600 hover:bg-gray-200 rounded-lg transition-colors disabled:opacity-50"
              >
                跳过澄清
              </button>
            )}
            {phase === 'clarify' && (
              <button
                onClick={handleGeneratePlan}
                disabled={isStreaming || !deepResearchTopic.trim()}
                className="flex items-center gap-2 px-5 py-2 bg-indigo-600 text-white text-sm font-medium rounded-lg hover:bg-indigo-700 transition-colors disabled:opacity-50 disabled:cursor-not-allowed"
              >
                {isStreaming ? <Loader2 size={16} className="animate-spin" /> : <ChevronRight size={16} />}
                生成大纲
              </button>
            )}
            {phase === 'confirm' && (
              <>
                <button
                  onClick={() => setPhase('clarify')}
                  disabled={isStreaming}
                  className="px-3 py-2 text-sm text-gray-600 hover:bg-gray-200 rounded-lg transition-colors disabled:opacity-50"
                >
                  返回澄清
                </button>
                <button
                  onClick={handleConfirmAndRun}
                  disabled={isStreaming || !deepResearchTopic.trim()}
                  className="flex items-center gap-2 px-5 py-2 bg-indigo-600 text-white text-sm font-medium rounded-lg hover:bg-indigo-700 transition-colors disabled:opacity-50 disabled:cursor-not-allowed"
                >
                  {isStreaming ? <Loader2 size={16} className="animate-spin" /> : <ChevronRight size={16} />}
                  确认并开始研究
                </button>
              </>
            )}
            {phase === 'running' && (
              <div className="flex items-center gap-2">
                <div className="flex items-center gap-2 px-4 py-2 bg-indigo-100 text-indigo-700 text-sm font-medium rounded-lg">
                  <Loader2 size={16} className="animate-spin" />
                  后台研究中
                </div>
                <button
                  onClick={handleStopRunningJob}
                  disabled={isStopping || !activeJobId}
                  className="flex items-center gap-2 px-4 py-2 bg-red-50 text-red-600 text-sm font-medium rounded-lg hover:bg-red-100 transition-colors disabled:opacity-50 disabled:cursor-not-allowed"
                >
                  <Square size={14} />
                  {isStopping ? '停止中...' : '停止任务'}
                </button>
              </div>
            )}
          </div>
        </div>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/components/workflow/DeepResearchSettingsPopover.tsx">
import { useState, useEffect, useRef } from 'react';
import { Settings, X, HelpCircle } from 'lucide-react';
import { useConfigStore } from '../../stores';

/** Hover tooltip with delay, matching Sidebar HelpTooltip pattern */
function Tip({ content, children }: { content: string; children: React.ReactNode }) {
  const [visible, setVisible] = useState(false);
  const showRef = useRef<number | null>(null);
  const hideRef = useRef<number | null>(null);

  const handleEnter = () => {
    if (hideRef.current) { window.clearTimeout(hideRef.current); hideRef.current = null; }
    showRef.current = window.setTimeout(() => setVisible(true), 600);
  };
  const handleLeave = () => {
    if (showRef.current) { window.clearTimeout(showRef.current); showRef.current = null; }
    hideRef.current = window.setTimeout(() => setVisible(false), 150);
  };

  return (
    <span className="relative inline-flex items-center text-gray-400 cursor-help shrink-0" onMouseEnter={handleEnter} onMouseLeave={handleLeave}>
      {children}
      {visible && (
        <span className="absolute left-1/2 -translate-x-1/2 bottom-full mb-1 px-2.5 py-2 text-[10px] leading-relaxed text-gray-100 bg-gray-800/95 rounded-lg shadow-lg max-w-[240px] whitespace-normal z-[100] border border-gray-600/80 pointer-events-none" role="tooltip">
          {content}
        </span>
      )}
    </span>
  );
}

/**
 * Per-step model options for Deep Research.
 * value = "provider::model" or "" for default.
 */
const STEP_MODEL_OPTIONS = [
  { value: '', label: 'Default (global model)' },
  // Sonar (search-optimized)
  { value: 'sonar::sonar', label: 'sonar (search)' },
  { value: 'sonar::sonar-pro', label: 'sonar-pro (search)' },
  { value: 'sonar::sonar-reasoning-pro', label: 'sonar-reasoning-pro' },
  // DeepSeek
  { value: 'deepseek::deepseek-chat', label: 'deepseek-chat' },
  { value: 'deepseek-thinking::deepseek-reasoner', label: 'deepseek-reasoner (thinking)' },
  // Claude
  { value: 'claude::claude-sonnet-4-5', label: 'claude-sonnet-4.5' },
  { value: 'claude::claude-haiku-4-5', label: 'claude-haiku-4.5' },
  { value: 'claude-thinking::claude-sonnet-4-5', label: 'claude-sonnet-4.5 (thinking)' },
  // Gemini
  { value: 'gemini::gemini-pro-latest', label: 'gemini-pro' },
  { value: 'gemini::gemini-flash-latest', label: 'gemini-flash' },
  // Kimi
  { value: 'kimi::kimi-k2.5', label: 'kimi-k2.5' },
];

const RESEARCH_STEPS = ['scope', 'plan', 'research', 'evaluate', 'write', 'verify', 'synthesize'] as const;

interface Props {
  open: boolean;
  onClose: () => void;
}

export function DeepResearchSettingsPopover({ open, onClose }: Props) {
  const { deepResearchDefaults, updateDeepResearchDefaults, setDeepResearchStepModel } = useConfigStore();
  const popoverRef = useRef<HTMLDivElement>(null);

  // Close on outside click
  useEffect(() => {
    if (!open) return;
    const handler = (e: MouseEvent) => {
      if (popoverRef.current && !popoverRef.current.contains(e.target as Node)) {
        onClose();
      }
    };
    // Use setTimeout to avoid the opening click triggering close
    const timer = setTimeout(() => {
      document.addEventListener('mousedown', handler);
    }, 0);
    return () => {
      clearTimeout(timer);
      document.removeEventListener('mousedown', handler);
    };
  }, [open, onClose]);

  // Close on Escape
  useEffect(() => {
    if (!open) return;
    const handler = (e: KeyboardEvent) => {
      if (e.key === 'Escape') onClose();
    };
    document.addEventListener('keydown', handler);
    return () => document.removeEventListener('keydown', handler);
  }, [open, onClose]);

  if (!open) return null;

  return (
    <div
      ref={popoverRef}
      className="absolute bottom-full mb-2 left-0 w-80 bg-white rounded-xl shadow-2xl border border-gray-200 z-50"
    >
      {/* Header */}
      <div className="flex items-center justify-between px-4 py-2.5 border-b border-gray-100">
        <div className="flex items-center gap-2">
          <Settings size={13} className="text-indigo-500" />
          <span className="text-xs font-semibold text-gray-800">Deep Research 默认设置</span>
        </div>
        <button onClick={onClose} className="p-1 hover:bg-gray-100 rounded-md transition-colors">
          <X size={12} className="text-gray-400" />
        </button>
      </div>

      {/* Body */}
      <div className="px-4 py-3 space-y-3.5 max-h-[60vh] overflow-y-auto">
        {/* Depth */}
        <div>
          <label className="flex items-center gap-1 text-[11px] font-medium text-gray-600 mb-1.5">
            Research Depth
            <Tip content="Lite: faster, fewer iterations, lower coverage threshold (~60%). Comprehensive: thorough academic review, more queries per section, higher coverage (~80%).">
              <HelpCircle size={11} />
            </Tip>
          </label>
          <div className="grid grid-cols-2 gap-2">
            {(['lite', 'comprehensive'] as const).map((d) => (
              <button
                key={d}
                type="button"
                onClick={() => updateDeepResearchDefaults({ depth: d })}
                className={`px-2.5 py-1.5 rounded-lg border text-left transition-all text-[11px] ${
                  deepResearchDefaults.depth === d
                    ? 'border-indigo-400 bg-indigo-50 ring-1 ring-indigo-200 font-semibold text-indigo-700'
                    : 'border-gray-200 bg-white hover:border-gray-300 text-gray-600'
                }`}
              >
                <div className="font-medium">{d === 'lite' ? 'Lite' : 'Comprehensive'}</div>
                <div className="text-[9px] text-gray-400 leading-tight mt-0.5">
                  {d === 'lite' ? '~5-15 min, coverage >= 60%' : '~20-60 min, coverage >= 80%'}
                </div>
              </button>
            ))}
          </div>
        </div>

        {/* Output Language */}
        <div>
          <label className="flex items-center gap-1 text-[11px] font-medium text-gray-600 mb-1">
            Output Language
            <Tip content="Auto: follow the topic's language. You can also force English or Chinese output regardless of the input language.">
              <HelpCircle size={11} />
            </Tip>
          </label>
          <select
            value={deepResearchDefaults.outputLanguage}
            onChange={(e) => updateDeepResearchDefaults({ outputLanguage: e.target.value as 'auto' | 'en' | 'zh' })}
            className="w-full border border-gray-200 rounded-lg px-2.5 py-1.5 text-[11px] focus:ring-2 focus:ring-indigo-500 outline-none"
          >
            <option value="auto">Auto (follow topic language)</option>
            <option value="en">English</option>
            <option value="zh">中文</option>
          </select>
        </div>

        {/* Year Window */}
        <div>
          <label className="flex items-center gap-1 text-[11px] font-medium text-gray-600 mb-1">
            Year Window (Hard Filter)
            <Tip content="Apply strict publication-year filtering during retrieval. Empty means no limit.">
              <HelpCircle size={11} />
            </Tip>
          </label>
          <div className="grid grid-cols-2 gap-2">
            <div>
              <div className="text-[10px] text-gray-500 mb-1">起始年份</div>
              <input
                type="number"
                min={1900}
                max={2100}
                value={deepResearchDefaults.yearStart ?? ''}
                onChange={(e) => {
                  const raw = e.target.value.trim();
                  if (raw === '') {
                    updateDeepResearchDefaults({ yearStart: null });
                    return;
                  }
                  const n = Number(raw);
                  updateDeepResearchDefaults({
                    yearStart: Number.isFinite(n) ? Math.max(1900, Math.min(2100, Math.trunc(n))) : null,
                  });
                }}
                className="w-full border border-gray-200 rounded-lg px-2.5 py-1.5 text-[11px] focus:ring-2 focus:ring-indigo-500 outline-none"
                placeholder="e.g. 2020"
              />
            </div>
            <div>
              <div className="text-[10px] text-gray-500 mb-1">结束年份</div>
              <input
                type="number"
                min={1900}
                max={2100}
                value={deepResearchDefaults.yearEnd ?? ''}
                onChange={(e) => {
                  const raw = e.target.value.trim();
                  if (raw === '') {
                    updateDeepResearchDefaults({ yearEnd: null });
                    return;
                  }
                  const n = Number(raw);
                  updateDeepResearchDefaults({
                    yearEnd: Number.isFinite(n) ? Math.max(1900, Math.min(2100, Math.trunc(n))) : null,
                  });
                }}
                className="w-full border border-gray-200 rounded-lg px-2.5 py-1.5 text-[11px] focus:ring-2 focus:ring-indigo-500 outline-none"
                placeholder="e.g. 2025"
              />
            </div>
          </div>
        </div>

        {/* Per-step Models */}
        <div>
          <label className="flex items-center gap-1 text-[11px] font-medium text-gray-600 mb-1.5">
            Per-step Models
            <Tip content="Assign different LLMs to each research step. E.g. use sonar-pro for scope/research (web search), claude for write/verify (quality). 'Default' uses the global model selected in the header.">
              <HelpCircle size={11} />
            </Tip>
          </label>
          <div className="space-y-1 border border-gray-100 rounded-lg p-2.5 bg-gray-50">
            {RESEARCH_STEPS.map((step) => (
              <div key={step} className="grid grid-cols-[72px_1fr] items-center gap-1.5">
                <span className="text-[10px] font-medium text-gray-500 uppercase">{step}</span>
                <select
                  value={deepResearchDefaults.stepModels[step] || ''}
                  onChange={(e) => setDeepResearchStepModel(step, e.target.value)}
                  className="border border-gray-200 rounded-md px-1.5 py-1 text-[10px] bg-white focus:ring-1 focus:ring-indigo-400 outline-none"
                >
                  {STEP_MODEL_OPTIONS.map((opt) => (
                    <option key={`${step}-${opt.value}`} value={opt.value}>{opt.label}</option>
                  ))}
                </select>
              </div>
            ))}
          </div>
        </div>

        {/* Strict Mode */}
        <label className="flex items-center justify-between text-[11px] text-gray-600 cursor-pointer px-0.5">
          <span className="flex items-center gap-1">
            Strict step model resolution
            <Tip content="OFF (default): if a step's designated model fails to load (API key missing, provider unavailable, etc.), the system silently falls back to the global default model and continues the research. ON: if any step's model fails, the entire research is aborted immediately. Use this when you need a specific model for correctness (e.g. sonar for web search) and a fallback would produce meaningless results.">
              <HelpCircle size={11} />
            </Tip>
          </span>
          <input
            type="checkbox"
            checked={deepResearchDefaults.stepModelStrict}
            onChange={(e) => updateDeepResearchDefaults({ stepModelStrict: e.target.checked })}
            className="accent-indigo-500"
          />
        </label>

        {/* Skip Claim Generation */}
        <label className="flex items-center justify-between text-[11px] text-gray-600 cursor-pointer px-0.5">
          <span className="flex items-center gap-1">
            跳过前置论点提炼 (Skip Claim Generation)
            <Tip content="When OFF (default): before writing each section, the system first extracts 3–5 core claims from evidence with [ref_hash] citations, then expands them into prose. When ON: skip claim extraction and write directly from evidence. Use ON for faster runs or when you prefer free-form section writing.">
              <HelpCircle size={11} />
            </Tip>
          </span>
          <input
            type="checkbox"
            checked={deepResearchDefaults.skipClaimGeneration}
            onChange={(e) => updateDeepResearchDefaults({ skipClaimGeneration: e.target.checked })}
            className="accent-indigo-500"
          />
        </label>
      </div>

      {/* Footer */}
      <div className="px-4 py-2 border-t border-gray-100 bg-gray-50 rounded-b-xl">
        <p className="text-[9px] text-gray-400">设置自动保存，跨会话持久化。在 Deep Research 对话内可临时覆盖。</p>
      </div>
    </div>
  );
}
</file>

<file path="frontend/src/stores/useConfigStore.ts">
import { create } from 'zustand';
import { persist } from 'zustand/middleware';
import type { RagConfig, WebSearchConfig, WebSource, DeepResearchDefaults } from '../types';

interface ConfigState {
  // 服务连接
  dbAddress: string;
  dbStatus: 'disconnected' | 'connecting' | 'connected';
  currentCollection: string;
  collections: string[];

  // RAG 配置
  ragConfig: RagConfig;

  // Web 搜索配置
  webSearchConfig: WebSearchConfig;

  // LLM 模型选择
  selectedProvider: string;  // 当前选中的 LLM provider
  selectedModel: string;     // 当前选中的具体模型（空字符串表示使用 provider 默认）

  // Deep Research 默认设置（通过 ⚙ 弹窗配置，跨会话持久化）
  deepResearchDefaults: DeepResearchDefaults;

  // Actions
  setDbAddress: (addr: string) => void;
  setDbStatus: (status: 'disconnected' | 'connecting' | 'connected') => void;
  setCurrentCollection: (name: string) => void;
  setCollections: (list: string[]) => void;
  addCollection: (name: string) => void;

  updateRagConfig: (update: Partial<RagConfig>) => void;

  setSelectedProvider: (provider: string) => void;
  setSelectedModel: (model: string) => void;

  updateDeepResearchDefaults: (update: Partial<DeepResearchDefaults>) => void;
  setDeepResearchStepModel: (step: string, value: string) => void;

  setWebSearchEnabled: (enabled: boolean) => void;
  toggleWebSource: (sourceId: string) => void;
  updateWebSourceParam: (
    sourceId: string,
    field: 'topK' | 'threshold',
    value: number
  ) => void;
  setQueryOptimizer: (enabled: boolean) => void;
  setMaxQueriesPerProvider: (value: number) => void;
  setContentFetcherEnabled: (enabled: boolean) => void;
  setAgentEnabled: (enabled: boolean) => void;
}

const defaultWebSources: WebSource[] = [
  { id: 'tavily', name: 'Tavily API', enabled: true, topK: 5, threshold: 0.5 },
  { id: 'google', name: 'Google Search', enabled: false, topK: 5, threshold: 0.4 },
  { id: 'scholar', name: 'Google Scholar', enabled: false, topK: 3, threshold: 0.6 },
  { id: 'semantic', name: 'Semantic Scholar', enabled: false, topK: 3, threshold: 0.7 },
];

const normalizeOptionalYear = (value: unknown): number | null => {
  if (value === null || value === undefined || value === '') return null;
  const n = Number(value);
  if (!Number.isInteger(n) || n < 1900 || n > 2100) return null;
  return n;
};

export const useConfigStore = create<ConfigState>()(
  persist(
    (set) => ({
      dbAddress: 'localhost:19530',
      dbStatus: 'disconnected',
      currentCollection: 'deepsea_research_v1',
      collections: ['deepsea_research_v1', 'general_ocean_v2'],

      ragConfig: {
        enabled: true,  // 默认启用本地 RAG
        localTopK: 5,
        localThreshold: 0.5,  // 默认相似度阈值
        finalTopK: 10,  // 默认最终保留10条
        enableHippoRAG: false,
        enableReranker: true,
        enableAgent: true,  // Agent 模式默认开启
      },

      webSearchConfig: {
        enabled: true,
        sources: defaultWebSources,
        queryOptimizer: true,   // 默认开启
        maxQueriesPerProvider: 3,
        enableContentFetcher: false,  // 全文抓取默认关闭
      },

      selectedProvider: 'deepseek',  // 默认 provider
      selectedModel: '',              // 空=使用 provider 默认模型

      deepResearchDefaults: {
        depth: 'comprehensive',
        outputLanguage: 'auto',
        yearStart: null,
        yearEnd: null,
        stepModelStrict: false,
        skipClaimGeneration: false,
        stepModels: {
          scope: 'sonar::sonar-pro',
          plan: '',
          research: '',
          evaluate: '',
          write: '',
          verify: '',
          synthesize: '',
        },
      },

      setDbAddress: (addr) => set({ dbAddress: addr }),
      setDbStatus: (status) => set({ dbStatus: status }),
      setCurrentCollection: (name) => set({ currentCollection: name }),
      setCollections: (list) => set({ collections: list }),
      addCollection: (name) =>
        set((state) => ({
          collections: [...state.collections, name],
          currentCollection: name,
        })),

      updateRagConfig: (update) =>
        set((state) => ({
          ragConfig: { ...state.ragConfig, ...update },
        })),

      setSelectedProvider: (provider) => set({ selectedProvider: provider, selectedModel: '' }),
      setSelectedModel: (model) => set({ selectedModel: model }),

      updateDeepResearchDefaults: (update) =>
        set((state) => ({
          deepResearchDefaults: { ...state.deepResearchDefaults, ...update },
        })),
      setDeepResearchStepModel: (step, value) =>
        set((state) => ({
          deepResearchDefaults: {
            ...state.deepResearchDefaults,
            stepModels: { ...state.deepResearchDefaults.stepModels, [step]: value },
          },
        })),

      setWebSearchEnabled: (enabled) =>
        set((state) => ({
          webSearchConfig: { ...state.webSearchConfig, enabled },
        })),

      toggleWebSource: (sourceId) =>
        set((state) => ({
          webSearchConfig: {
            ...state.webSearchConfig,
            sources: state.webSearchConfig.sources.map((s) =>
              s.id === sourceId ? { ...s, enabled: !s.enabled } : s
            ),
          },
        })),

      updateWebSourceParam: (sourceId, field, value) =>
        set((state) => ({
          webSearchConfig: {
            ...state.webSearchConfig,
            sources: state.webSearchConfig.sources.map((s) =>
              s.id === sourceId ? { ...s, [field]: value } : s
            ),
          },
        })),

      setQueryOptimizer: (enabled) =>
        set((state) => ({
          webSearchConfig: { ...state.webSearchConfig, queryOptimizer: enabled },
        })),

      setMaxQueriesPerProvider: (value) =>
        set((state) => ({
          webSearchConfig: {
            ...state.webSearchConfig,
            maxQueriesPerProvider: Math.min(Math.max(1, value), 5),
          },
        })),

      setContentFetcherEnabled: (enabled) =>
        set((state) => ({
          webSearchConfig: { ...state.webSearchConfig, enableContentFetcher: enabled },
        })),

      setAgentEnabled: (enabled) =>
        set((state) => ({
          ragConfig: { ...state.ragConfig, enableAgent: enabled },
        })),
    }),
    {
      name: 'config-storage',
      partialize: (state) => ({
        dbAddress: state.dbAddress,
        currentCollection: state.currentCollection,
        collections: state.collections,
        ragConfig: state.ragConfig,
        webSearchConfig: state.webSearchConfig,
        selectedProvider: state.selectedProvider,
        selectedModel: state.selectedModel,
        deepResearchDefaults: state.deepResearchDefaults,
      }),
      // 合并存储数据与默认值，确保新增字段有默认值
      merge: (persistedState, currentState) => {
        const persisted = persistedState as Partial<ConfigState>;
        return {
          ...currentState,
          ...persisted,
          ragConfig: {
            ...currentState.ragConfig,
            ...(persisted.ragConfig || {}),
            // 确保新字段有默认值
            enabled: persisted.ragConfig?.enabled ?? true,
            localThreshold: persisted.ragConfig?.localThreshold ?? 0.5,
            finalTopK: persisted.ragConfig?.finalTopK ?? 10,
            enableAgent: persisted.ragConfig?.enableAgent ?? true,
          },
          webSearchConfig: {
            ...currentState.webSearchConfig,
            ...(persisted.webSearchConfig || {}),
            // 确保新字段有默认值
            queryOptimizer: persisted.webSearchConfig?.queryOptimizer ?? true,
            maxQueriesPerProvider: persisted.webSearchConfig?.maxQueriesPerProvider ?? 3,
            enableContentFetcher: persisted.webSearchConfig?.enableContentFetcher ?? false,
          },
          deepResearchDefaults: {
            ...currentState.deepResearchDefaults,
            ...(persisted.deepResearchDefaults || {}),
            yearStart: normalizeOptionalYear(persisted.deepResearchDefaults?.yearStart),
            yearEnd: normalizeOptionalYear(persisted.deepResearchDefaults?.yearEnd),
            stepModels: {
              ...currentState.deepResearchDefaults.stepModels,
              ...(persisted.deepResearchDefaults?.stepModels || {}),
            },
          },
        };
      },
    }
  )
);
</file>

<file path="src/api/routes_ingest.py">
"""
数据入库 API：文件上传、解析、chunk、embedding、入库；集合管理。
"""

import hashlib
import json
import queue
import shutil
import threading
import time
import traceback
import uuid
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from pathlib import Path
from typing import List, Optional

from fastapi import APIRouter, File, Form, UploadFile, HTTPException
from fastapi.responses import StreamingResponse, JSONResponse

from config.settings import settings
from src.log import get_logger

logger = get_logger(__name__)

router = APIRouter(prefix="/ingest", tags=["ingest"])
_INGEST_CANCEL_EVENTS: dict[str, threading.Event] = {}
_INGEST_CANCEL_LOCK = threading.Lock()


def _request_cancel(job_id: str) -> None:
    with _INGEST_CANCEL_LOCK:
        ev = _INGEST_CANCEL_EVENTS.get(job_id)
        if ev is None:
            ev = threading.Event()
            _INGEST_CANCEL_EVENTS[job_id] = ev
        ev.set()


def _is_cancel_requested(job_id: str) -> bool:
    with _INGEST_CANCEL_LOCK:
        ev = _INGEST_CANCEL_EVENTS.get(job_id)
    return bool(ev and ev.is_set())


def _clear_cancel_event(job_id: str) -> None:
    with _INGEST_CANCEL_LOCK:
        _INGEST_CANCEL_EVENTS.pop(job_id, None)


# ============================================================
# Collections
# ============================================================

@router.get("/collections")
def list_collections() -> dict:
    """列出 Milvus 中已有的集合及行数"""
    from src.indexing.milvus_ops import milvus
    try:
        names: list = milvus.client.list_collections()
    except Exception as e:
        logger.error("list_collections failed: %s", e)
        raise HTTPException(status_code=503, detail=f"Milvus 不可用: {e}")
    result = []
    for name in sorted(names):
        try:
            cnt = milvus.count(name)
        except Exception:
            cnt = -1
        result.append({"name": name, "count": cnt})
    return {"collections": result}


@router.post("/collections")
def create_collection(body: dict) -> dict:
    """创建新集合（v2 schema: chunk_id 主键，支持 upsert）"""
    name = (body.get("name") or "").strip()
    if not name:
        raise HTTPException(status_code=400, detail="集合名称不能为空")
    recreate = bool(body.get("recreate", False))
    from src.indexing.milvus_ops import milvus
    try:
        milvus.create_collection(name, recreate=recreate, schema_version="v2")
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    return {"ok": True, "name": name}


@router.delete("/collections/{name}")
def delete_collection(name: str) -> dict:
    """删除指定集合（不可恢复）"""
    from src.indexing.milvus_ops import milvus
    try:
        if not milvus.client.has_collection(name):
            raise HTTPException(status_code=404, detail=f"集合 '{name}' 不存在")
        milvus.client.drop_collection(name)
        # 同时清理 paper 元数据
        try:
            from src.indexing.paper_store import delete_collection_papers
            delete_collection_papers(name)
        except Exception as pe:
            logger.warning("paper_store cleanup failed for '%s': %s", name, pe)
        logger.info("Collection '%s' deleted", name)
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
    return {"ok": True, "name": name}


# ============================================================
# Papers (文件级管理)
# ============================================================

@router.get("/collections/{name}/papers")
def list_papers_in_collection(name: str) -> dict:
    """列出指定集合中已入库的文件列表"""
    from src.indexing.paper_store import list_papers
    papers = list_papers(name)
    return {"collection": name, "papers": papers}


@router.delete("/collections/{name}/papers/{paper_id:path}")
def delete_paper_from_collection(name: str, paper_id: str) -> dict:
    """
    删除指定文件：从 Milvus 删除该 paper_id 的全部 chunks，并移除元数据记录。
    """
    from src.indexing.milvus_ops import milvus
    from src.indexing.paper_store import delete_paper

    # 1. 从 Milvus 删除该 paper_id 的所有 chunks
    deleted_count = 0
    try:
        if milvus.client.has_collection(name):
            # Milvus delete by filter
            result = milvus.client.delete(
                collection_name=name,
                filter=f'paper_id == "{paper_id}"',
            )
            deleted_count = result.get("delete_count", 0) if isinstance(result, dict) else 0
            logger.info("Deleted %s chunks for paper_id='%s' from '%s'", deleted_count, paper_id, name)
    except Exception as e:
        logger.error("Milvus delete failed for paper_id='%s': %s", paper_id, e)
        raise HTTPException(status_code=500, detail=f"Milvus 删除失败: {e}")

    # 2. 从 SQLite 删除元数据
    delete_paper(name, paper_id)

    return {"ok": True, "collection": name, "paper_id": paper_id, "deleted_chunks": deleted_count}


# ============================================================
# Upload
# ============================================================

@router.post("/upload")
async def upload_files(
    files: List[UploadFile] = File(...),
    collection: str = Form(""),
) -> dict:
    """
    上传文件（PDF 等），保存到 data/raw_papers/，返回文件信息列表。
    """
    raw_papers = settings.path.raw_papers
    raw_papers.mkdir(parents=True, exist_ok=True)

    saved = []
    for f in files:
        if not f.filename:
            continue
        # 安全文件名
        safe_name = Path(f.filename).name
        dest = raw_papers / safe_name
        # 如果重名，加 uuid 后缀
        if dest.exists():
            stem = dest.stem
            suffix = dest.suffix
            dest = raw_papers / f"{stem}_{uuid.uuid4().hex[:6]}{suffix}"
        with open(dest, "wb") as out:
            content = await f.read()
            out.write(content)
        content_hash = hashlib.sha256(content).hexdigest()
        saved.append({
            "filename": safe_name,
            "path": str(dest),
            "size": len(content),
            "content_hash": content_hash,
        })
        logger.info("Uploaded: %s -> %s (%d bytes)", safe_name, dest, len(content))

    return {"uploaded": saved, "count": len(saved)}


# ============================================================
# Process (Parse + Chunk + Embed + Upsert) with SSE progress
# ============================================================

def _normalize_process_body(body: dict) -> dict:
    file_paths = body.get("file_paths") or []
    if not file_paths:
        raise HTTPException(status_code=400, detail="file_paths 不能为空")
    collection_name = (body.get("collection") or "").strip() or settings.collection.global_
    content_hashes = body.get("content_hashes") or {}
    skip_enrichment = body.get("skip_enrichment", True)
    enrich_tables = body.get("enrich_tables", not skip_enrichment)
    enrich_figures = body.get("enrich_figures", not skip_enrichment)
    actual_skip = bool(skip_enrichment and (not enrich_tables) and (not enrich_figures))
    return {
        "file_paths": file_paths,
        "collection_name": collection_name,
        "content_hashes": content_hashes,
        "enrich_tables": bool(enrich_tables),
        "enrich_figures": bool(enrich_figures),
        "actual_skip": actual_skip,
        "llm_text_provider": (body.get("llm_text_provider") or "").strip() or None,
        "llm_text_model": (body.get("llm_text_model") or "").strip() or None,
        "llm_text_concurrency": body.get("llm_text_concurrency"),
        "llm_vision_provider": (body.get("llm_vision_provider") or "").strip() or None,
        "llm_vision_model": (body.get("llm_vision_model") or "").strip() or None,
        "llm_vision_concurrency": body.get("llm_vision_concurrency"),
    }


def _emit_job_event(job_id: str, event: str, data: dict) -> None:
    from src.indexing.ingest_job_store import append_event
    append_event(job_id, event, data)


def _finalize_cancelled_job(
    job_id: str,
    *,
    total: int,
    processed_files: int,
    failed_files: int,
    total_chunks: int,
    total_upserted: int,
    current_file: str = "",
) -> None:
    from src.indexing.ingest_job_store import get_job, update_job

    job = get_job(job_id)
    if job and job.get("status") == "cancelled":
        return
    _emit_job_event(
        job_id,
        "cancelled",
        {
            "message": "任务已取消",
            "current_file": current_file,
            "processed_files": processed_files,
            "failed_files": failed_files,
            "total_files": total,
        },
    )
    _emit_job_event(
        job_id,
        "done",
        {
            "cancelled": True,
            "total_files": total,
            "total_chunks": total_chunks,
            "total_upserted": total_upserted,
            "errors": [],
        },
    )
    update_job(
        job_id,
        status="cancelled",
        processed_files=processed_files,
        failed_files=failed_files,
        total_chunks=total_chunks,
        total_upserted=total_upserted,
        current_stage="cancelled",
        message="任务已取消",
        finished_at=time.time(),
    )


def _run_ingest_job(job_id: str, cfg: dict) -> None:
    from src.chunking.chunker import ChunkConfig, chunk_blocks
    from src.indexing.embedder import embedder
    from src.indexing.ingest_job_store import update_job
    from src.indexing.milvus_ops import milvus

    file_paths = cfg["file_paths"]
    collection_name = cfg["collection_name"]
    content_hashes = cfg["content_hashes"]
    enrich_tables = cfg["enrich_tables"]
    enrich_figures = cfg["enrich_figures"]
    actual_skip = cfg["actual_skip"]
    llm_text_provider = cfg["llm_text_provider"]
    llm_text_model = cfg["llm_text_model"]
    llm_text_concurrency = cfg["llm_text_concurrency"]
    llm_vision_provider = cfg["llm_vision_provider"]
    llm_vision_model = cfg["llm_vision_model"]
    llm_vision_concurrency = cfg["llm_vision_concurrency"]

    update_job(job_id, status="running", message="任务已启动")
    total = len(file_paths)
    total_chunks = 0
    total_upserted = 0
    errors = []

    try:
        milvus.create_collection(collection_name, recreate=False, schema_version="v2")
    except Exception as e:
        msg = f"集合创建失败: {e}"
        _emit_job_event(job_id, "error", {"message": msg})
        _emit_job_event(job_id, "done", {"total_files": total, "total_chunks": 0, "total_upserted": 0, "errors": [{"stage": "init", "error": str(e)}]})
        update_job(job_id, status="error", error_message=str(e), message=msg, finished_at=time.time())
        return

    config_path = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"
    try:
        from src.parser.pdf_parser import PDFProcessor, ParserConfig
        parser_cfg = ParserConfig.from_json(config_path) if config_path.exists() else ParserConfig()
        parser_cfg.enrich_tables = enrich_tables
        parser_cfg.enrich_figures = enrich_figures
        if llm_text_provider:
            parser_cfg.llm_text_provider = llm_text_provider
        parser_cfg.llm_text_model = llm_text_model
        if llm_text_concurrency is not None:
            try:
                parser_cfg.llm_text_concurrency = max(1, int(llm_text_concurrency))
            except Exception:
                pass
        if llm_vision_provider:
            parser_cfg.llm_vision_provider = llm_vision_provider
        parser_cfg.llm_vision_model = llm_vision_model
        if llm_vision_concurrency is not None:
            try:
                parser_cfg.llm_vision_concurrency = max(1, int(llm_vision_concurrency))
            except Exception:
                pass
        llm_manager = None
        if not actual_skip:
            try:
                from src.llm import LLMManager
                llm_manager = LLMManager.from_json(str(config_path))
            except Exception:
                pass
        processor = PDFProcessor(config=parser_cfg, llm_manager=llm_manager)
    except Exception as e:
        msg = f"处理器初始化失败: {e}"
        _emit_job_event(job_id, "error", {"message": msg})
        _emit_job_event(job_id, "done", {"total_files": total, "total_chunks": 0, "total_upserted": 0, "errors": [{"stage": "init", "error": str(e)}]})
        update_job(job_id, status="error", error_message=str(e), message=msg, finished_at=time.time())
        return

    chunk_cfg = ChunkConfig(
        target_chars=settings.chunk.target_chars,
        min_chars=settings.chunk.min_chars,
        max_chars=settings.chunk.max_chars,
        overlap_sentences=settings.chunk.overlap_sentences,
        table_rows_per_chunk=settings.chunk.table_rows_per_chunk,
    )
    parsed_dir = settings.path.parsed
    processed_files = 0
    failed_files = 0

    _emit_job_event(job_id, "start", {
        "total": total,
        "collection": collection_name,
        "enrich_tables": enrich_tables,
        "enrich_figures": enrich_figures,
    })

    if _is_cancel_requested(job_id):
        _finalize_cancelled_job(
            job_id,
            total=total,
            processed_files=0,
            failed_files=0,
            total_chunks=0,
            total_upserted=0,
        )
        return

    for idx, fpath in enumerate(file_paths):
        pdf_path = Path(fpath)
        paper_id = pdf_path.stem
        file_name = pdf_path.name
        if _is_cancel_requested(job_id):
            _finalize_cancelled_job(
                job_id,
                total=total,
                processed_files=processed_files,
                failed_files=failed_files,
                total_chunks=total_chunks,
                total_upserted=total_upserted,
                current_file=file_name,
            )
            return
        update_job(job_id, current_file=file_name, current_stage="parsing", message=f"解析 {file_name}...")
        _emit_job_event(job_id, "progress", {
            "file": file_name,
            "index": idx,
            "total": total,
            "stage": "parsing",
            "message": f"解析 {file_name}...",
        })

        output_dir = parsed_dir / paper_id
        try:
            enrich_queue = queue.Queue()

            def put_enrich_progress(kind_key: str, payload: dict) -> None:
                kind = "table" if kind_key == "enrich_table" else "figure"
                enrich_queue.put({
                    "file": file_name,
                    "kind": kind,
                    "index": payload.get("index"),
                    "total": payload.get("total"),
                    "status": payload.get("status", ""),
                    "message": payload.get("message"),
                })

            parse_timeout = 600
            with ThreadPoolExecutor(max_workers=1) as pool:
                future = pool.submit(
                    processor.process,
                    pdf_path,
                    output_dir=output_dir,
                    skip_enrichment=actual_skip,
                    progress_callback=put_enrich_progress if not actual_skip else None,
                )
                start_ts = time.time()
                while True:
                    if _is_cancel_requested(job_id):
                        future.cancel()
                        _finalize_cancelled_job(
                            job_id,
                            total=total,
                            processed_files=processed_files,
                            failed_files=failed_files,
                            total_chunks=total_chunks,
                            total_upserted=total_upserted,
                            current_file=file_name,
                        )
                        return
                    try:
                        while True:
                            ev = enrich_queue.get_nowait()
                            _emit_job_event(job_id, "enrich_progress", ev)
                    except queue.Empty:
                        pass
                    try:
                        future.result(timeout=5)
                        break
                    except FuturesTimeoutError:
                        elapsed = int(time.time() - start_ts)
                        if elapsed > parse_timeout:
                            future.cancel()
                            raise TimeoutError(f"PDF 解析超时 ({elapsed}s > {parse_timeout}s)")
                        _emit_job_event(job_id, "heartbeat", {
                            "file": file_name,
                            "stage": "parsing",
                            "elapsed": elapsed,
                            "message": f"解析中 {file_name}... ({elapsed}s)",
                        })
            try:
                while True:
                    ev = enrich_queue.get_nowait()
                    _emit_job_event(job_id, "enrich_progress", ev)
            except queue.Empty:
                pass
            logger.info("Parsed: %s -> %s", file_name, output_dir)
        except Exception as e:
            logger.error("Parse failed for %s: %s", file_name, e)
            errors.append({"file": file_name, "stage": "parse", "error": str(e)})
            _emit_job_event(job_id, "file_error", {"file": file_name, "stage": "parse", "error": str(e)})
            processed_files += 1
            failed_files += 1
            update_job(job_id, processed_files=processed_files, failed_files=failed_files, current_stage="parse_error")
            continue

        update_job(job_id, current_stage="chunking", message=f"切块 {file_name}...")
        _emit_job_event(job_id, "progress", {
            "file": file_name,
            "index": idx,
            "total": total,
            "stage": "chunking",
            "message": f"切块 {file_name}...",
        })

        json_path = output_dir / "enriched.json"
        if not json_path.exists():
            errors.append({"file": file_name, "stage": "chunk", "error": "enriched.json 不存在"})
            _emit_job_event(job_id, "file_error", {"file": file_name, "stage": "chunk", "error": "enriched.json 不存在"})
            processed_files += 1
            failed_files += 1
            update_job(job_id, processed_files=processed_files, failed_files=failed_files, current_stage="chunk_error")
            continue

        try:
            with open(json_path, "r", encoding="utf-8") as f:
                doc = json.load(f)
            doc_id = doc.get("doc_id", paper_id)
            enrich_meta = doc.get("enrichment_meta") or {}
            table_count = int(enrich_meta.get("table_count", 0) or 0)
            figure_count = int(enrich_meta.get("figure_count", 0) or 0)
            table_success = int(enrich_meta.get("table_success", 0) or 0)
            figure_success = int(enrich_meta.get("figure_success", 0) or 0)
            content_flow = doc.get("content_flow", [])
            doc_claims = doc.get("claims") or []
            doc_metadata = doc.get("doc_metadata") or {}
            chunks = chunk_blocks(content_flow, doc_id=doc_id, config=chunk_cfg, claims=doc_claims)
        except Exception as e:
            errors.append({"file": file_name, "stage": "chunk", "error": str(e)})
            _emit_job_event(job_id, "file_error", {"file": file_name, "stage": "chunk", "error": str(e)})
            processed_files += 1
            failed_files += 1
            update_job(job_id, processed_files=processed_files, failed_files=failed_files, current_stage="chunk_error")
            continue

        if not chunks:
            _emit_job_event(job_id, "file_done", {"file": file_name, "chunks": 0, "upserted": 0})
            processed_files += 1
            update_job(job_id, processed_files=processed_files, current_stage="done")
            continue

        update_job(job_id, current_stage="embedding", message=f"向量化 {file_name} ({len(chunks)} chunks)...")
        _emit_job_event(job_id, "progress", {
            "file": file_name,
            "index": idx,
            "total": total,
            "stage": "embedding",
            "message": f"向量化 {file_name} ({len(chunks)} chunks)...",
        })

        try:
            rows = _build_rows(chunks, doc_id, collection_name, doc_metadata=doc_metadata)
            # 写入 DOI/Title 到 SQLite 持久化存储（供跨源去重使用）
            if doc_metadata.get("doi") or doc_metadata.get("title"):
                _update_paper_metadata(doc_id, doc_metadata)
            texts = [r.pop("_text_for_embed") for r in rows]
            batch_size = 32
            total_batches = (len(texts) + batch_size - 1) // batch_size
            for bi, i in enumerate(range(0, len(texts), batch_size)):
                if _is_cancel_requested(job_id):
                    _finalize_cancelled_job(
                        job_id,
                        total=total,
                        processed_files=processed_files,
                        failed_files=failed_files,
                        total_chunks=total_chunks,
                        total_upserted=total_upserted,
                        current_file=file_name,
                    )
                    return
                batch = texts[i:i + batch_size]
                logger.info("Embedding %s batch %d/%d (%d texts)", file_name, bi + 1, total_batches, len(batch))
                emb = embedder.encode(batch)
                for k, j in enumerate(range(i, min(i + batch_size, len(rows)))):
                    rows[j]["dense_vector"] = emb["dense"][k].tolist()
                    sp = emb["sparse"]._getrow(k).tocoo()
                    rows[j]["sparse_vector"] = {int(col): float(val) for col, val in zip(sp.col, sp.data)}
        except Exception as e:
            logger.error("Embed failed for %s: %s\n%s", file_name, e, traceback.format_exc())
            errors.append({"file": file_name, "stage": "embed", "error": str(e)})
            _emit_job_event(job_id, "file_error", {"file": file_name, "stage": "embed", "error": str(e)})
            processed_files += 1
            failed_files += 1
            update_job(job_id, processed_files=processed_files, failed_files=failed_files, current_stage="embed_error")
            continue

        update_job(job_id, current_stage="indexing", message=f"入库 {file_name} ({len(rows)} rows)...")
        _emit_job_event(job_id, "progress", {
            "file": file_name,
            "index": idx,
            "total": total,
            "stage": "indexing",
            "message": f"入库 {file_name} ({len(rows)} rows)...",
        })

        try:
            upsert_batch = 100
            total_ubatches = (len(rows) + upsert_batch - 1) // upsert_batch
            for ui, start in enumerate(range(0, len(rows), upsert_batch)):
                if _is_cancel_requested(job_id):
                    _finalize_cancelled_job(
                        job_id,
                        total=total,
                        processed_files=processed_files,
                        failed_files=failed_files,
                        total_chunks=total_chunks,
                        total_upserted=total_upserted,
                        current_file=file_name,
                    )
                    return
                batch = rows[start:start + upsert_batch]
                logger.info("Upsert %s batch %d/%d (%d rows)", file_name, ui + 1, total_ubatches, len(batch))
                milvus.upsert(collection_name, batch)
            total_chunks += len(chunks)
            total_upserted += len(rows)
            logger.info("Upsert done for %s: %d chunks, %d rows", file_name, len(chunks), len(rows))
            try:
                from src.indexing.paper_store import upsert_paper
                upsert_paper(
                    collection=collection_name,
                    paper_id=doc_id,
                    filename=file_name,
                    file_path=str(fpath),
                    file_size=pdf_path.stat().st_size if pdf_path.exists() else 0,
                    chunk_count=len(chunks),
                    row_count=len(rows),
                    enrich_tables_enabled=enrich_tables,
                    enrich_figures_enabled=enrich_figures,
                    table_count=table_count,
                    figure_count=figure_count,
                    table_success=table_success,
                    figure_success=figure_success,
                    status="done",
                    content_hash=content_hashes.get(fpath, ""),
                )
            except Exception as pe:
                logger.warning("paper_store write failed: %s", pe)
        except Exception as e:
            logger.error("Upsert failed for %s: %s\n%s", file_name, e, traceback.format_exc())
            errors.append({"file": file_name, "stage": "upsert", "error": str(e)})
            _emit_job_event(job_id, "file_error", {"file": file_name, "stage": "upsert", "error": str(e)})
            try:
                from src.indexing.paper_store import upsert_paper
                upsert_paper(
                    collection=collection_name,
                    paper_id=doc_id,
                    filename=file_name,
                    file_path=str(fpath),
                    enrich_tables_enabled=enrich_tables,
                    enrich_figures_enabled=enrich_figures,
                    status="error",
                    error_message=str(e),
                    content_hash=content_hashes.get(fpath, ""),
                )
            except Exception:
                pass
            processed_files += 1
            failed_files += 1
            update_job(job_id, processed_files=processed_files, failed_files=failed_files, current_stage="upsert_error")
            continue

        _emit_job_event(job_id, "file_done", {"file": file_name, "chunks": len(chunks), "upserted": len(rows)})
        processed_files += 1
        update_job(
            job_id,
            processed_files=processed_files,
            failed_files=failed_files,
            total_chunks=total_chunks,
            total_upserted=total_upserted,
            current_stage="done",
            message=f"已完成 {processed_files}/{total}",
        )

    _emit_job_event(job_id, "done", {
        "total_files": total,
        "total_chunks": total_chunks,
        "total_upserted": total_upserted,
        "errors": errors,
    })
    update_job(
        job_id,
        status="done",
        processed_files=processed_files,
        failed_files=failed_files,
        total_chunks=total_chunks,
        total_upserted=total_upserted,
        current_stage="done",
        finished_at=time.time(),
        message=f"完成: {processed_files}/{total}",
    )


def _run_ingest_job_safe(job_id: str, cfg: dict) -> None:
    from src.indexing.ingest_job_store import update_job
    try:
        _run_ingest_job(job_id, cfg)
    except Exception as e:
        logger.error("ingest job failed unexpectedly job_id=%s err=%s\n%s", job_id, e, traceback.format_exc())
        _emit_job_event(job_id, "error", {"message": str(e)})
        _emit_job_event(
            job_id,
            "done",
            {"total_files": len(cfg.get("file_paths") or []), "total_chunks": 0, "total_upserted": 0, "errors": [{"stage": "job", "error": str(e)}]},
        )
        update_job(job_id, status="error", error_message=str(e), finished_at=time.time(), message=f"异常终止: {e}")
    finally:
        _clear_cancel_event(job_id)


@router.post("/process")
def process_files(body: dict) -> JSONResponse:
    """创建入库任务并立即返回 job_id（Worker 将自动领取并执行）。"""
    from src.indexing.ingest_job_store import create_job

    cfg = _normalize_process_body(body)
    job = create_job(cfg["collection_name"], cfg, total_files=len(cfg["file_paths"]))
    job_id = job.get("job_id")
    return JSONResponse({"ok": True, "job_id": job_id})


@router.get("/jobs")
def list_ingest_jobs(limit: int = 20, status: Optional[str] = None) -> dict:
    from src.indexing.ingest_job_store import list_jobs
    jobs = list_jobs(limit=limit, status=status)
    return {"jobs": jobs}


@router.get("/jobs/{job_id}")
def get_ingest_job(job_id: str) -> dict:
    from src.indexing.ingest_job_store import get_job
    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    return {"job": job}


@router.post("/jobs/{job_id}/cancel")
def cancel_ingest_job(job_id: str) -> dict:
    from src.indexing.ingest_job_store import get_job, update_job

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")

    current_status = str(job.get("status") or "")
    if current_status in {"done", "error", "cancelled"}:
        return {"ok": True, "job_id": job_id, "status": current_status}

    if current_status == "pending":
        update_job(job_id, status="cancelled", message="任务已取消（未启动）", finished_at=time.time())
        _emit_job_event(job_id, "cancelled", {"job_id": job_id, "message": "任务已取消（未启动）"})
        _emit_job_event(job_id, "done", {"cancelled": True, "total_files": 0, "total_chunks": 0, "total_upserted": 0, "errors": []})
        return {"ok": True, "job_id": job_id, "status": "cancelled"}

    _request_cancel(job_id)
    update_job(job_id, message="收到取消请求，正在停止任务...")
    _emit_job_event(job_id, "cancel_requested", {"job_id": job_id, "message": "已请求取消"})
    return {"ok": True, "job_id": job_id, "status": "cancelling"}


@router.get("/jobs/{job_id}/events")
def stream_ingest_job_events(job_id: str, after_id: int = 0) -> StreamingResponse:
    from src.indexing.ingest_job_store import get_job, list_events

    if not get_job(job_id):
        raise HTTPException(status_code=404, detail="job 不存在")

    def event_stream():
        cursor = max(0, int(after_id))
        idle_ticks = 0
        while True:
            events = list_events(job_id, after_id=cursor, limit=500)
            if events:
                idle_ticks = 0
                for ev in events:
                    cursor = max(cursor, int(ev["event_id"]))
                    yield _sse(ev["event"], ev["data"])
                continue

            job = get_job(job_id)
            if not job:
                yield _sse("error", {"message": "job 不存在"})
                break

            if job.get("status") in {"done", "error", "cancelled"}:
                # 终态时再做一次拉取，确保最后事件不丢
                final_events = list_events(job_id, after_id=cursor, limit=500)
                for ev in final_events:
                    cursor = max(cursor, int(ev["event_id"]))
                    yield _sse(ev["event"], ev["data"])
                break

            idle_ticks += 1
            if idle_ticks % 5 == 0:
                yield _sse(
                    "heartbeat",
                    {
                        "job_id": job_id,
                        "file": job.get("current_file", ""),
                        "stage": job.get("current_stage", ""),
                        "message": job.get("message", "waiting"),
                    },
                )
            time.sleep(1)

    return StreamingResponse(event_stream(), media_type="text/event-stream")


def _truncate(content: str, max_len: int = 65000) -> str:
    return content[:max_len] if len(content) > max_len else content


def _update_paper_metadata(doc_id: str, doc_metadata: dict) -> None:
    """写入论文 DOI/Title 到 SQLite 持久化存储（供跨源去重使用）"""
    try:
        from src.indexing.paper_metadata_store import paper_meta_store
        paper_meta_store.upsert(
            paper_id=doc_id,
            doi=doc_metadata.get("doi"),
            title=doc_metadata.get("title"),
            source="ingestion",
        )
    except Exception as e:
        logger.warning("Failed to update paper_metadata store: %s", e)


def _get_field_max_lengths(collection_name: str) -> dict:
    """查询集合 schema 获取各 VARCHAR 字段的 max_length"""
    defaults = {
        "paper_id": 250, "chunk_id": 120, "domain": 30,
        "content_type": 30, "chunk_type": 30, "section_path": 500,
    }
    try:
        from src.indexing.milvus_ops import milvus
        info = milvus.client.describe_collection(collection_name)
        for field in info.get("fields", []):
            name = field.get("name", "")
            params = field.get("params", {})
            if name in defaults and "max_length" in params:
                defaults[name] = int(params["max_length"]) - 2  # 留 2 字符安全余量
    except Exception:
        pass
    return defaults


def _build_rows(chunks, doc_id: str, collection_name: str = "",
                 doc_metadata: dict | None = None) -> list:
    """将 chunks 转为 Milvus upsert 行格式，截断上限动态匹配集合 schema"""
    limits = _get_field_max_lengths(collection_name) if collection_name else {}
    l_paper = limits.get("paper_id", 250)
    l_chunk = limits.get("chunk_id", 120)
    l_ctype = limits.get("content_type", 30)
    l_cktype = limits.get("chunk_type", 30)
    l_sp = limits.get("section_path", 500)

    dm = doc_metadata or {}
    doi = dm.get("doi") or ""
    doc_title = dm.get("title") or ""

    rows = []
    for c in chunks:
        text = _truncate(c.text)
        meta = c.meta or {}
        page_range = meta.get("page_range", [0, 0])
        page = page_range[0] if isinstance(page_range, (list, tuple)) else meta.get("page", 0)
        sp_raw = meta.get("section_path", "")
        if isinstance(sp_raw, (list, tuple)):
            sp_raw = " > ".join(str(s) for s in sp_raw)
        row = {
            "paper_id": str(doc_id)[:l_paper],
            "chunk_id": str(c.chunk_id)[:l_chunk],
            "content": text,
            "raw_content": text,
            "domain": "global",
            "content_type": str(c.content_type or "text")[:l_ctype],
            "chunk_type": ",".join(meta.get("block_types", []))[:l_cktype] or "paragraph",
            "section_path": str(sp_raw)[:l_sp],
            "page": int(page) if isinstance(page, (int, float)) else 0,
            "_text_for_embed": text,
        }
        if doi:
            row["doi"] = doi
        if doc_title:
            row["doc_title"] = doc_title
        rows.append(row)
    return rows


def _sse(event: str, data: dict) -> str:
    return f"event: {event}\ndata: {json.dumps(data, ensure_ascii=False)}\n\n"
</file>

<file path="src/collaboration/citation/formatter.py">
"""
引用格式化：BibTeX 与参考文献段落。

支持三种 cite_key 格式的输出：
- numeric: [1], [2], [3]
- hash: [a3f7b2c91e04]
- author_date: [Smith2023], [Smith2023a]
"""

from typing import List, Literal, Optional

from src.collaboration.canvas.models import Citation


def format_bibtex(citations: List[Citation]) -> str:
    """
    输出 BibTeX 块。

    使用 Citation 的 cite_key 作为 BibTeX 条目键。
    """
    lines = []
    for c in citations:
        if c.bibtex:
            lines.append(c.bibtex.strip())
            continue
        key = c.cite_key or c.id
        author = " and ".join(c.authors) if c.authors else ""
        title = (c.title or "").replace("{", "{{").replace("}", "}}")
        year = str(c.year) if c.year else ""
        url = c.url or ""
        doi_line = f"\n  doi = {{{c.doi}}}," if c.doi else ""
        lines.append(
            f"@misc{{{key},\n  title = {{{title}}},\n  author = {{{author}}},\n  year = {{{year}}},{doi_line}\n  url = {{{url}}}\n}}"
        )
    return "\n\n".join(lines) if lines else ""


def format_reference_list(
    citations: List[Citation],
    use_cite_key: bool = True,
    style: Literal["apa", "ieee", "numeric", "custom"] = "custom",
) -> str:
    """
    输出参考文献段落。

    Args:
        citations: 引用列表
        use_cite_key: 是否使用 cite_key 作为标签（否则使用数字序号）
        style: 格式风格
            - "apa": APA-like
            - "ieee": IEEE-like
            - "numeric": 数字序号（兼容旧参数，等同 IEEE 标签风格）
            - "custom": 自定义格式（默认，使用 cite_key）

    Returns:
        格式化的参考文献文本
    """
    def _format_authors_apa(authors: List[str]) -> str:
        if not authors:
            return "Anonymous"

        def _one(author: str) -> str:
            parts = [p for p in str(author or "").replace(",", " ").split() if p]
            if not parts:
                return ""
            if len(parts) == 1:
                return parts[0]
            last = parts[-1]
            initials = " ".join(f"{p[0].upper()}." for p in parts[:-1] if p)
            return f"{last}, {initials}" if initials else last

        normalized = [_one(a) for a in authors if str(a or "").strip()]
        normalized = [a for a in normalized if a]
        if not normalized:
            return "Anonymous"
        if len(normalized) == 1:
            return normalized[0]
        if len(normalized) == 2:
            return f"{normalized[0]} & {normalized[1]}"
        return f"{', '.join(normalized[:-1])}, & {normalized[-1]}"

    lines = []
    for i, c in enumerate(citations, 1):
        author_part = ", ".join(c.authors) if c.authors else "Anonymous"
        year_part = f"{c.year}" if c.year else "n.d."
        title_part = c.title or "(无标题)"
        doi_norm = _normalize_doi(c.doi)
        link_part = c.url or (f"https://doi.org/{doi_norm}" if doi_norm else "")

        if style in ("numeric", "ieee") or not use_cite_key:
            label = str(i)
        else:
            label = c.cite_key or c.id or str(i)

        if style == "apa":
            apa_authors = _format_authors_apa(c.authors or [])
            suffix = f" {link_part}" if link_part else ""
            lines.append(f"{apa_authors} ({year_part}). {title_part}.{suffix}")
        elif style in ("ieee", "numeric"):
            year_suffix = f", {year_part}" if c.year else ""
            link_suffix = f", {link_part}" if link_part else ""
            lines.append(f"[{label}] {author_part}, \"{title_part}\"{year_suffix}{link_suffix}.")
        else:
            # 自定义风格
            link_suffix = f" {link_part}" if link_part else ""
            year_suffix = f" ({c.year})" if c.year else ""
            lines.append(f"[{label}] {author_part}{year_suffix}. {title_part}.{link_suffix}")

    return "\n\n".join(lines) if lines else ""


def _normalize_doi(doi: Optional[str]) -> str:
    value = (doi or "").strip()
    if not value:
        return ""
    lower = value.lower()
    if lower.startswith("https://doi.org/"):
        return value[16:].strip()
    if lower.startswith("http://doi.org/"):
        return value[15:].strip()
    if lower.startswith("doi:"):
        return value[4:].strip()
    return value


def format_ris(citations: List[Citation]) -> str:
    """
    输出 RIS 引文格式。

    字段覆盖：
    - TY (固定 GEN)
    - TI
    - AU (可多行)
    - PY
    - DO
    - UR
    - ER
    """
    records: list[str] = []
    for c in citations:
        lines: list[str] = ["TY  - GEN"]
        title = (c.title or "").strip()
        if title:
            lines.append(f"TI  - {title}")
        for author in (c.authors or []):
            a = str(author or "").strip()
            if a:
                lines.append(f"AU  - {a}")
        if c.year:
            lines.append(f"PY  - {c.year}")
        doi = _normalize_doi(c.doi)
        if doi:
            lines.append(f"DO  - {doi}")
        url = (c.url or "").strip()
        if url:
            lines.append(f"UR  - {url}")
        lines.append("ER  -")
        records.append("\n".join(lines))
    return "\n\n".join(records) + ("\n" if records else "")


def format_inline_citation(cite_key: str, style: Literal["bracket", "parenthetical"] = "bracket") -> str:
    """
    格式化行内引用标记。

    Args:
        cite_key: 引用键
        style: 格式风格
            - "bracket": [Smith2023]
            - "parenthetical": (Smith, 2023)

    Returns:
        格式化的引用标记
    """
    if style == "parenthetical":
        # 尝试解析 author_date 格式
        import re
        match = re.match(r"^([A-Za-z]+)(\d{4})([a-z]?)$", cite_key)
        if match:
            author, year, suffix = match.groups()
            return f"({author}, {year}{suffix})"
        return f"({cite_key})"
    return f"[{cite_key}]"


def citations_to_markdown_list(citations: List[Citation]) -> str:
    """
    将引用列表转换为 Markdown 列表格式。

    Returns:
        Markdown 格式的引用列表
    """
    lines = []
    for c in citations:
        key = c.cite_key or c.id
        author_part = ", ".join(c.authors) if c.authors else "佚名"
        year_part = f" ({c.year})" if c.year else ""
        title_part = c.title or "(无标题)"
        
        # 带链接的标题
        if c.url:
            title_md = f"[{title_part}]({c.url})"
        elif c.doi:
            title_md = f"[{title_part}](https://doi.org/{c.doi})"
        else:
            title_md = title_part
        
        lines.append(f"- **[{key}]** {author_part}{year_part}. {title_md}")
    
    return "\n".join(lines) if lines else ""
</file>

<file path="src/llm/llm_manager.py">
"""
统一 LLM 管理模块

功能：
- Provider 基类统一封装（OpenAI-compatible / Anthropic）
- 输出规范化：final_text / reasoning_text 隔离
- Raw JSON 落库（JSONL）+ 清理策略（10天/100MB）
- API key 环境变量覆盖 + 脱敏
- dry_run 模式支持

配置来源：config/rag_config.json（可选 config/rag_config.local.json 覆盖）
"""

from __future__ import annotations

import logging
import os
import json
import threading
import time
import hashlib

_log = logging.getLogger(__name__)

# ── Observability ──
try:
    from src.observability import metrics as _obs_metrics, tracer as _obs_tracer
except Exception:
    _obs_metrics = None  # type: ignore
    _obs_tracer = None  # type: ignore

from src.observability.tracing import traceable
from pathlib import Path
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from typing import Dict, Any, List, Optional
from abc import ABC, abstractmethod

import requests

try:
    from config.settings import settings
except Exception:
    settings = None  # type: ignore

# ============================================================
# Constants
# ============================================================

ANTHROPIC_VERSION = "2023-06-01"
DEFAULT_TIMEOUT = 120  # seconds
DEFAULT_MAX_TOKENS = 4096
LOG_DIR_NAME = "llm_raw"
LOG_MAX_AGE_DAYS = 10
LOG_MAX_TOTAL_MB = 100
MESSAGE_DIGEST_LENGTH = 200


# ============================================================
# Dataclasses
# ============================================================

@dataclass
class ProviderConfig:
    """单个 provider 的配置"""
    name: str
    api_key: str
    base_url: str
    default_model: str
    models: Dict[str, str] = field(default_factory=dict)
    params: Dict[str, Any] = field(default_factory=dict)

    def is_anthropic(self) -> bool:
        """判断是否为 Anthropic 协议"""
        return "anthropic.com" in self.base_url or self.name.startswith("claude")


@dataclass
class LLMConfig:
    """完整 LLM 配置"""
    default: str
    dry_run: bool
    providers: Dict[str, ProviderConfig] = field(default_factory=dict)


# ============================================================
# Helper Functions
# ============================================================

def load_json(path: str | Path) -> Dict[str, Any]:
    """加载 JSON 配置文件"""
    with open(path, "r", encoding="utf-8") as f:
        return json.load(f)


def load_json_with_local(path: str | Path) -> Dict[str, Any]:
    """
    加载 JSON 配置文件并合并本地覆盖文件：
    - base: config/rag_config.json
    - local: config/rag_config.local.json（可选）
    """
    base_path = Path(path)
    base = load_json(base_path)
    local_path = base_path.with_name(f"{base_path.stem}.local{base_path.suffix}")
    if local_path.exists():
        base = deep_merge(base, load_json(local_path))
    return base


def deep_merge(base: Dict[str, Any], override: Dict[str, Any]) -> Dict[str, Any]:
    """
    递归深合并两个字典。
    override 中的值会覆盖 base 中的同名键；嵌套 dict 会递归合并。
    """
    result = base.copy()
    for key, value in override.items():
        if key in result and isinstance(result[key], dict) and isinstance(value, dict):
            result[key] = deep_merge(result[key], value)
        else:
            result[key] = value
    return result


def mask_secret(secret: str, show_chars: int = 4) -> str:
    """
    脱敏显示密钥，保留前后若干字符。
    例如: "sk-abc...xyz"
    """
    if not secret:
        return "(empty)"
    if len(secret) <= show_chars * 2 + 3:
        return "*" * len(secret)
    return f"{secret[:show_chars]}...{secret[-show_chars:]}"


def provider_env_var(provider_name: str) -> str:
    """
    生成 provider 对应的环境变量名。
    规则: RAG_LLM__{PROVIDER}__API_KEY
    provider 名称转大写，- 替换为 _
    例如: claude-thinking => RAG_LLM__CLAUDE_THINKING__API_KEY
    """
    normalized = provider_name.upper().replace("-", "_")
    return f"RAG_LLM__{normalized}__API_KEY"


def now_iso() -> str:
    """返回 ISO 格式的当前时间戳"""
    return datetime.now().isoformat()


def messages_digest(messages: List[Dict[str, Any]], max_len: int = MESSAGE_DIGEST_LENGTH) -> str:
    """
    生成 messages 的摘要（用于日志，不含完整内容）。
    格式: role: content[:max_len]
    """
    parts = []
    for msg in messages:
        role = msg.get("role", "?")
        content = msg.get("content", "")
        if isinstance(content, list):
            # 多模态消息，提取文本部分
            text_parts = [p.get("text", "") for p in content if isinstance(p, dict) and p.get("type") == "text"]
            content = " ".join(text_parts)
        if isinstance(content, str):
            truncated = content[:max_len] + ("..." if len(content) > max_len else "")
            parts.append(f"[{role}] {truncated}")
    return "\n".join(parts)


# ============================================================
# RawLogStore
# ============================================================

class RawLogStore:
    """
    原始 JSON 响应日志存储。
    - 按天写入 JSONL 文件: YYYY-MM-DD.jsonl
    - 支持清理策略: 超过 N 天删除，总大小超过 M MB 删除最旧文件
    """

    def __init__(self, log_dir: Path | str | None = None):
        if log_dir is None:
            # 默认: 项目根目录/logs/llm_raw/
            log_dir = Path(__file__).parent.parent.parent / "logs" / LOG_DIR_NAME
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(parents=True, exist_ok=True)

    def _today_file(self) -> Path:
        """当天的日志文件路径"""
        return self.log_dir / f"{datetime.now().strftime('%Y-%m-%d')}.jsonl"

    def write(self, record: Dict[str, Any]) -> None:
        """
        写入一条日志记录。
        record 应包含: timestamp, provider, model, params, messages_digest,
                      final_text, reasoning_text, raw_response, meta, error
        """
        record.setdefault("timestamp", now_iso())
        log_file = self._today_file()
        with open(log_file, "a", encoding="utf-8") as f:
            f.write(json.dumps(record, ensure_ascii=False) + "\n")

    def cleanup(
        self,
        max_age_days: int = LOG_MAX_AGE_DAYS,
        max_total_mb: int = LOG_MAX_TOTAL_MB
    ) -> Dict[str, Any]:
        """
        清理日志文件。
        1. 删除超过 max_age_days 天的文件
        2. 若总大小 > max_total_mb MB，从最旧文件开始删除

        Returns:
            清理报告: {"deleted_by_age": [...], "deleted_by_size": [...], "remaining_mb": float}
        """
        report = {"deleted_by_age": [], "deleted_by_size": [], "remaining_mb": 0.0}
        
        if not self.log_dir.exists():
            return report

        # 获取所有 jsonl 文件，按修改时间排序（旧 -> 新）
        log_files = sorted(
            self.log_dir.glob("*.jsonl"),
            key=lambda p: p.stat().st_mtime
        )

        cutoff = datetime.now() - timedelta(days=max_age_days)

        # 1. 删除超龄文件
        remaining_files = []
        for f in log_files:
            mtime = datetime.fromtimestamp(f.stat().st_mtime)
            if mtime < cutoff:
                report["deleted_by_age"].append(f.name)
                f.unlink()
            else:
                remaining_files.append(f)

        # 2. 检查总大小，删除最旧文件
        max_bytes = max_total_mb * 1024 * 1024
        while remaining_files:
            total_size = sum(f.stat().st_size for f in remaining_files)
            if total_size <= max_bytes:
                break
            oldest = remaining_files.pop(0)
            report["deleted_by_size"].append(oldest.name)
            oldest.unlink()

        # 计算剩余大小
        if remaining_files:
            report["remaining_mb"] = sum(f.stat().st_size for f in remaining_files) / (1024 * 1024)

        return report


# ============================================================
# Provider Classes
# ============================================================

class Provider(ABC):
    """Provider 基类：负责 HTTP 请求"""

    def __init__(self, config: ProviderConfig):
        self.config = config

    @abstractmethod
    def request(self, payload: Dict[str, Any], timeout: Optional[int] = None) -> Dict[str, Any]:
        """
        发送请求到 LLM API。
        
        Args:
            payload: 请求体（已合并参数）
            
        Returns:
            原始 JSON 响应
        """
        raise NotImplementedError


def _llm_perf_timeout() -> int:
    if settings and hasattr(settings, "perf_llm"):
        return getattr(settings.perf_llm, "timeout_seconds", DEFAULT_TIMEOUT) or DEFAULT_TIMEOUT
    return DEFAULT_TIMEOUT


def _llm_perf_retry() -> tuple:
    if settings and hasattr(settings, "perf_llm"):
        max_r = getattr(settings.perf_llm, "max_retries", 2) or 0
        backoff = getattr(settings.perf_llm, "retry_backoff", 1.5) or 1.5
        return max_r, backoff
    return 0, 1.5


def _request_with_retry(
    session: requests.Session,
    method: str,
    url: str,
    timeout: int,
    **kwargs: Any,
) -> requests.Response:
    max_retries, backoff = _llm_perf_retry()
    last_err = None
    for attempt in range(max_retries + 1):
        try:
            resp = session.request(method, url, timeout=timeout, **kwargs)
            if resp.status_code in (429, 500, 503) and attempt < max_retries:
                time.sleep(backoff ** attempt)
                continue
            resp.raise_for_status()
            return resp
        except requests.exceptions.HTTPError as e:
            last_err = e
            if e.response.status_code not in (429, 500, 503) or attempt >= max_retries:
                raise
            time.sleep(backoff ** attempt)
        except Exception as e:
            last_err = e
            if attempt >= max_retries:
                raise
            time.sleep(backoff ** attempt)
    if last_err:
        raise last_err
    raise RuntimeError("request_with_retry failed")


class OpenAICompatProvider(Provider):
    """
    OpenAI 兼容协议 Provider。
    适用于: OpenAI, DeepSeek, Gemini, Kimi 等
    使用 Session 复用连接，支持可配置超时与重试。
    """

    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self._session = requests.Session()

    def request(self, payload: Dict[str, Any], timeout: Optional[int] = None) -> Dict[str, Any]:
        url = f"{self.config.base_url.rstrip('/')}/chat/completions"
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.config.api_key}",
        }
        timeout = int(timeout or _llm_perf_timeout())
        resp = _request_with_retry(
            self._session, "POST", url, timeout,
            headers=headers, json=payload,
        )
        return resp.json()


class AnthropicProvider(Provider):
    """
    Anthropic 协议 Provider。
    适用于: Claude 系列
    使用 Session 复用连接，支持可配置超时与重试。
    """

    def __init__(self, config: ProviderConfig):
        super().__init__(config)
        self._session = requests.Session()

    def request(self, payload: Dict[str, Any], timeout: Optional[int] = None) -> Dict[str, Any]:
        url = f"{self.config.base_url.rstrip('/')}/v1/messages"
        headers = {
            "Content-Type": "application/json",
            "x-api-key": self.config.api_key,
            "anthropic-version": ANTHROPIC_VERSION,
        }
        timeout = int(timeout or _llm_perf_timeout())
        resp = _request_with_retry(
            self._session, "POST", url, timeout,
            headers=headers, json=payload,
        )
        return resp.json()


# ============================================================
# Response Normalization
# ============================================================

def normalize_response(provider_name: str, raw: Dict[str, Any], is_anthropic: bool = False) -> Dict[str, Any]:
    """
    规范化 LLM 响应，提取 final_text 和 reasoning_text。
    
    Args:
        provider_name: provider 名称（用于日志/调试）
        raw: 原始 JSON 响应
        is_anthropic: 是否为 Anthropic 协议
        
    Returns:
        {
            "final_text": str | None,
            "reasoning_text": str | None,
            "usage": dict | None,
            "refusal": bool | None
        }
    """
    result = {
        "final_text": None,
        "reasoning_text": None,
        "usage": None,
        "refusal": None,
    }

    try:
        if is_anthropic:
            result.update(_normalize_anthropic(raw))
        else:
            result.update(_normalize_openai_compat(raw))
    except Exception:
        # 容错：抽取失败不抛异常，raw 已保存
        pass

    return result


def _normalize_openai_compat(raw: Dict[str, Any]) -> Dict[str, Any]:
    """
    OpenAI-compatible 响应抽取规则。
    
    final_text: 
        - choices[0].message.content (str)
        - 若为 list，拼接 type=="text" 的段落
        
    reasoning_text (best-effort):
        - choices[0].message.reasoning / thoughts
        - content 为 list 时，拼接 type in ("reasoning", "thinking") 的段落
    """
    result = {"final_text": None, "reasoning_text": None, "usage": None, "refusal": None}

    # usage
    result["usage"] = raw.get("usage")

    choices = raw.get("choices") or []
    if not choices:
        return result

    message = choices[0].get("message") or {}
    content = message.get("content")
    
    # refusal 检测
    if message.get("refusal"):
        result["refusal"] = True

    # 处理 content
    if isinstance(content, str):
        result["final_text"] = content
    elif isinstance(content, list):
        # 多段内容
        text_parts = []
        reasoning_parts = []
        for block in content:
            if not isinstance(block, dict):
                continue
            block_type = block.get("type", "")
            text = block.get("text", "")
            if block_type == "text":
                text_parts.append(text)
            elif block_type in ("reasoning", "thinking"):
                reasoning_parts.append(text)
        result["final_text"] = "".join(text_parts) if text_parts else None
        if reasoning_parts:
            result["reasoning_text"] = "".join(reasoning_parts)

    # 尝试从其他字段抽取 reasoning（某些模型的特殊字段）
    if not result["reasoning_text"]:
        for field_name in ("reasoning", "thoughts", "reasoning_content"):
            reasoning = message.get(field_name)
            if reasoning and isinstance(reasoning, str):
                result["reasoning_text"] = reasoning
                break

    return result


def _normalize_anthropic(raw: Dict[str, Any]) -> Dict[str, Any]:
    """
    Anthropic 响应抽取规则。
    
    final_text: 拼接 content[] 中 type=="text" 的 .text
    reasoning_text: 拼接 content[] 中 type=="thinking" 的 .thinking
    """
    result = {"final_text": None, "reasoning_text": None, "usage": None, "refusal": None}

    # usage
    result["usage"] = raw.get("usage")

    # stop_reason 检测
    stop_reason = raw.get("stop_reason")
    if stop_reason == "refusal":
        result["refusal"] = True

    content = raw.get("content") or []
    text_parts = []
    thinking_parts = []

    for block in content:
        if not isinstance(block, dict):
            continue
        block_type = block.get("type", "")
        if block_type == "text":
            text_parts.append(block.get("text", ""))
        elif block_type == "thinking":
            thinking_parts.append(block.get("thinking", ""))

    result["final_text"] = "".join(text_parts) if text_parts else None
    result["reasoning_text"] = "".join(thinking_parts) if thinking_parts else None

    return result


# ============================================================
# Chat Clients
# ============================================================

class BaseChatClient(ABC):
    """Chat 客户端基类"""

    @abstractmethod
    def chat(
        self,
        messages: List[Dict[str, Any]],
        model: Optional[str] = None,
        return_reasoning: bool = False,
        response_model: Optional[Any] = None,
        **overrides
    ) -> Dict[str, Any]:
        """
        发送消息并获取响应。
        
        Args:
            messages: OpenAI 格式消息列表 [{"role": "user", "content": "..."}]
            model: 可选的模型名或别名，覆盖默认
            return_reasoning: 是否在返回中包含 reasoning_text（默认 False）
            response_model: 可选 Pydantic BaseModel 类；设置后自动启用 JSON 模式，
                            并将解析结果存入返回字典的 ``parsed_object`` 字段。
                            解析失败时自动追加错误信息并重试一次。
            **overrides: 覆盖 provider 默认参数
            
        Returns:
            {
                "provider": str,
                "model": str,
                "final_text": str,
                "reasoning_text": str | None,  # 仅当 return_reasoning=True 或始终返回
                "parsed_object": BaseModel | None,  # 仅当 response_model 不为 None
                "raw": dict,
                "params": dict,
                "meta": {"usage": dict, "latency_ms": int, "refusal": bool}
            }
        """
        raise NotImplementedError


class DryRunChatClient(BaseChatClient):
    """
    Dry-run 模式客户端。
    不实际调用 API，返回模拟结构。
    """

    def __init__(self, config: ProviderConfig, log_store: Optional[RawLogStore] = None):
        self.config = config
        self.log_store = log_store

    def chat(
        self,
        messages: List[Dict[str, Any]],
        model: Optional[str] = None,
        return_reasoning: bool = False,
        response_model: Optional[Any] = None,
        **overrides
    ) -> Dict[str, Any]:
        resolved_model = self._resolve_model(model)
        merged_params = deep_merge(self.config.params, overrides)

        result = {
            "provider": self.config.name,
            "model": resolved_model,
            "final_text": f"[DRY_RUN] provider={self.config.name}, model={resolved_model}",
            "reasoning_text": None,
            "parsed_object": None,
            "raw": {
                "dry_run": True,
                "note": "This is a dry-run response. No actual API call was made.",
                "messages_count": len(messages),
            },
            "params": merged_params,
            "meta": {
                "usage": None,
                "latency_ms": 0,
                "refusal": None,
            },
        }

        # 记录日志
        if self.log_store:
            self.log_store.write({
                "timestamp": now_iso(),
                "provider": self.config.name,
                "model": resolved_model,
                "params": merged_params,
                "messages_digest": messages_digest(messages),
                "final_text": result["final_text"],
                "reasoning_text": None,
                "raw_response": result["raw"],
                "meta": result["meta"],
                "error": None,
                "dry_run": True,
            })

        return result

    def _resolve_model(self, model: Optional[str]) -> str:
        if model:
            return self.config.models.get(model, model)
        default = self.config.default_model
        return self.config.models.get(default, default)


class HTTPChatClient(BaseChatClient):
    """
    HTTP 实际调用客户端。
    根据 provider 类型选择 OpenAI-compatible 或 Anthropic 协议。
    """

    def __init__(
        self,
        config: ProviderConfig,
        provider: Provider,
        log_store: Optional[RawLogStore] = None,
        semaphore: Optional[threading.Semaphore] = None,
    ):
        self.config = config
        self.provider = provider
        self.log_store = log_store
        self._semaphore = semaphore

    @traceable(run_type="llm", name="llm.chat")
    def chat(
        self,
        messages: List[Dict[str, Any]],
        model: Optional[str] = None,
        return_reasoning: bool = False,
        tools: Optional[List] = None,
        response_model: Optional[Any] = None,
        **overrides
    ) -> Dict[str, Any]:
        resolved_model = self._resolve_model(model)
        timeout_override = overrides.pop("timeout_seconds", None)
        if timeout_override is not None:
            try:
                timeout_override = int(timeout_override)
                if timeout_override <= 0:
                    timeout_override = None
            except Exception:
                timeout_override = None
        merged_params = deep_merge(self.config.params, overrides)
        is_anthropic = self.config.is_anthropic()

        # 结构化输出：为 OpenAI-compat 协议注入 JSON 模式
        if response_model is not None and not is_anthropic:
            merged_params.setdefault("response_format", {"type": "json_object"})

        # 构建请求 payload
        if is_anthropic:
            payload = self._build_anthropic_payload(messages, resolved_model, merged_params, tools=tools)
        else:
            payload = self._build_openai_payload(messages, resolved_model, merged_params, tools=tools)

        # 发送请求（可选并发限流）
        start_time = time.time()
        error = None
        raw = {}

        def _do_request():
            return self.provider.request(payload, timeout=timeout_override)

        try:
            if self._semaphore:
                with self._semaphore:
                    raw = _do_request()
            else:
                raw = _do_request()
        except requests.exceptions.HTTPError as e:
            error = f"HTTP {e.response.status_code}: {e.response.text[:500]}"
            raise
        except Exception as e:
            error = str(e)
            raise
        finally:
            latency_ms = int((time.time() - start_time) * 1000)

            # ── Observability: LLM 指标 ──
            if _obs_metrics:
                _prov = self.config.name
                _mod = resolved_model
                _obs_metrics.llm_requests_total.labels(provider=_prov, model=_mod).inc()
                _obs_metrics.llm_duration_seconds.labels(provider=_prov, model=_mod).observe(latency_ms / 1000.0)
                if error:
                    _obs_metrics.llm_errors_total.labels(provider=_prov, model=_mod).inc()
                # token 计量
                _norm = normalize_response(self.config.name, raw, is_anthropic) if raw else {}
                _usage = _norm.get("usage") or {}
                if _usage.get("prompt_tokens"):
                    _obs_metrics.llm_tokens_used.labels(provider=_prov, model=_mod, direction="input").inc(_usage["prompt_tokens"])
                if _usage.get("completion_tokens"):
                    _obs_metrics.llm_tokens_used.labels(provider=_prov, model=_mod, direction="output").inc(_usage["completion_tokens"])

            # 记录日志
            if self.log_store:
                normalized = normalize_response(self.config.name, raw, is_anthropic) if raw else {}
                self.log_store.write({
                    "timestamp": now_iso(),
                    "provider": self.config.name,
                    "model": resolved_model,
                    "params": merged_params,
                    "messages_digest": messages_digest(messages),
                    "final_text": normalized.get("final_text"),
                    "reasoning_text": normalized.get("reasoning_text"),
                    "raw_response": raw,
                    "meta": {
                        "usage": normalized.get("usage"),
                        "latency_ms": latency_ms,
                        "refusal": normalized.get("refusal"),
                    },
                    "error": error,
                })

        # 规范化响应
        normalized = normalize_response(self.config.name, raw, is_anthropic)
        latency_ms = int((time.time() - start_time) * 1000)

        result = {
            "provider": self.config.name,
            "model": resolved_model,
            "final_text": normalized["final_text"] or "",
            "reasoning_text": normalized["reasoning_text"],
            "raw": raw,
            "params": merged_params,
            "meta": {
                "usage": normalized["usage"],
                "latency_ms": latency_ms,
                "refusal": normalized["refusal"],
            },
        }

        # ── Function Calling: 解析 tool_calls ──
        if tools:
            from src.llm.tools import parse_tool_calls, has_tool_calls
            if has_tool_calls(raw, is_anthropic):
                result["tool_calls"] = parse_tool_calls(raw, is_anthropic)
            else:
                result["tool_calls"] = []

        # ── Structured Output: Pydantic 解析 + 自动重试一次 ──
        if response_model is not None:
            final_text = result.get("final_text") or ""
            try:
                result["parsed_object"] = response_model.model_validate_json(final_text)
            except Exception as val_err:
                retry_msgs = list(messages) + [
                    {"role": "assistant", "content": final_text},
                    {"role": "user", "content": (
                        f"Your previous response could not be parsed as JSON. "
                        f"Validation error: {val_err}. "
                        "Please return ONLY valid JSON matching the required schema, with no markdown."
                    )},
                ]
                retry_payload = (
                    self._build_anthropic_payload(retry_msgs, resolved_model, merged_params, tools=tools)
                    if is_anthropic
                    else self._build_openai_payload(retry_msgs, resolved_model, merged_params, tools=tools)
                )
                try:
                    if self._semaphore:
                        with self._semaphore:
                            retry_raw = self.provider.request(retry_payload, timeout=timeout_override)
                    else:
                        retry_raw = self.provider.request(retry_payload, timeout=timeout_override)
                    retry_norm = normalize_response(self.config.name, retry_raw, is_anthropic)
                    retry_text = retry_norm.get("final_text") or ""
                    result["final_text"] = retry_text
                    result["reasoning_text"] = retry_norm.get("reasoning_text")
                    result["raw"] = retry_raw
                    result["meta"]["usage"] = retry_norm.get("usage")
                    result["meta"]["refusal"] = retry_norm.get("refusal")
                    result["meta"]["latency_ms"] = int((time.time() - start_time) * 1000)
                    result["parsed_object"] = response_model.model_validate_json(retry_text)
                    _log.debug("Structured output validation succeeded on retry")
                except Exception as retry_err:
                    _log.warning("Structured output retry failed: %s", retry_err)
                    result["parsed_object"] = None

        return result

    def _resolve_model(self, model: Optional[str]) -> str:
        if model:
            return self.config.models.get(model, model)
        default = self.config.default_model
        return self.config.models.get(default, default)

    def _build_openai_payload(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        params: Dict[str, Any],
        tools: Optional[List] = None,
    ) -> Dict[str, Any]:
        """构建 OpenAI-compatible 请求 payload"""
        payload = {
            "model": model,
            "messages": messages,
        }

        # ── Function Calling: 注入 tools ──
        if tools:
            from src.llm.tools import ToolDef
            openai_tools = [t.to_openai() if isinstance(t, ToolDef) else t for t in tools]
            payload["tools"] = openai_tools

        # 标准参数直接放入 payload
        standard_params = {
            "max_tokens", "max_completion_tokens", "temperature", "top_p",
            "frequency_penalty", "presence_penalty", "stop",
            "stream", "n", "response_format",
        }

        # Gemini OpenAI-compatible endpoint does not accept some params
        is_gemini = "generativelanguage.googleapis.com" in (self.config.base_url or "")
        for key, value in params.items():
            if is_gemini and key == "media_resolution":
                continue
            if key in standard_params:
                payload[key] = value
            else:
                # 非标准参数也放入（某些平台支持）
                payload[key] = value

        # 默认 max_tokens
        if "max_tokens" not in payload and "max_completion_tokens" not in payload:
            payload["max_tokens"] = DEFAULT_MAX_TOKENS

        # OpenAI 新 API: 使用 max_completion_tokens
        is_openai = "api.openai.com" in (self.config.base_url or "")
        if is_openai and "max_tokens" in payload and "max_completion_tokens" not in payload:
            payload["max_completion_tokens"] = payload.pop("max_tokens")

        return payload

    def _build_anthropic_payload(
        self,
        messages: List[Dict[str, Any]],
        model: str,
        params: Dict[str, Any],
        tools: Optional[List] = None,
    ) -> Dict[str, Any]:
        """构建 Anthropic 请求 payload"""
        # 分离 system 消息
        system_content = None
        user_messages = []

        for msg in messages:
            if msg.get("role") == "system":
                system_content = msg.get("content", "")
            else:
                user_messages.append(msg)

        payload = {
            "model": model,
            "messages": user_messages,
        }

        if system_content:
            payload["system"] = system_content

        # 合并参数
        for key, value in params.items():
            payload[key] = value

        # 默认 max_tokens
        if "max_tokens" not in payload:
            payload["max_tokens"] = DEFAULT_MAX_TOKENS

        # ── Function Calling: 注入 tools ──
        if tools:
            from src.llm.tools import ToolDef
            anthropic_tools = [t.to_anthropic() if isinstance(t, ToolDef) else t for t in tools]
            payload["tools"] = anthropic_tools

        return payload


# ============================================================
# LLMManager
# ============================================================

class LLMManager:
    """
    LLM 统一管理器。
    
    职责：
    - 加载配置
    - 获取/创建 ChatClient
    - 解析模型别名
    """

    def __init__(self, config: LLMConfig, log_store: Optional[RawLogStore] = None):
        self.config = config
        self.log_store = log_store or RawLogStore()
        self._clients: Dict[str, BaseChatClient] = {}
        self._semaphores: Dict[str, threading.Semaphore] = {}
        self._sem_lock = threading.Lock()

    @classmethod
    def from_json(cls, path: str | Path) -> "LLMManager":
        """
        从 JSON 配置文件加载 LLMManager。
        
        Args:
            path: 配置文件路径（如 config/rag_config.json）
            
        Returns:
            LLMManager 实例
        """
        raw = load_json_with_local(path)
        llm_section = raw.get("llm", {})

        default = llm_section.get("default", "claude")
        dry_run = llm_section.get("dry_run", False)
        providers_raw = llm_section.get("providers", {})

        providers = {}
        for name, pcfg in providers_raw.items():
            # API key 优先级: 环境变量 > JSON
            env_var = provider_env_var(name)
            api_key = os.getenv(env_var) or pcfg.get("api_key", "")

            providers[name] = ProviderConfig(
                name=name,
                api_key=api_key,
                base_url=pcfg.get("base_url", ""),
                default_model=pcfg.get("default_model", ""),
                models=pcfg.get("models", {}),
                params=pcfg.get("params", {}),
            )

        config = LLMConfig(default=default, dry_run=dry_run, providers=providers)
        return cls(config)

    def get_provider_names(self) -> List[str]:
        """获取所有可用的 provider 名称列表"""
        return list(self.config.providers.keys())

    def get_client(
        self,
        provider: Optional[str] = None,
        api_key: Optional[str] = None
    ) -> BaseChatClient:
        """
        获取指定 provider 的 ChatClient。
        
        Args:
            provider: provider 名称（默认使用 config.default）
            api_key: 可选，覆盖配置中的 api_key
            
        Returns:
            BaseChatClient 实例
        """
        provider_name = provider or self.config.default

        if provider_name not in self.config.providers:
            raise ValueError(f"Unknown provider: {provider_name}. Available: {self.get_provider_names()}")

        pcfg = self.config.providers[provider_name]

        # api_key 优先级: 参数 > 环境变量 > JSON（已在 from_json 处理环境变量）
        if api_key:
            pcfg = ProviderConfig(
                name=pcfg.name,
                api_key=api_key,
                base_url=pcfg.base_url,
                default_model=pcfg.default_model,
                models=pcfg.models,
                params=pcfg.params,
            )

        # dry_run 模式
        if self.config.dry_run:
            return DryRunChatClient(pcfg, self.log_store)

        # 检查 api_key
        if not pcfg.api_key or pcfg.api_key in ("sk-xxx", "sk-ant-xxx", "AIzxxx"):
            raise ValueError(
                f"Invalid or missing API key for provider '{provider_name}'. "
                f"Set via environment variable {provider_env_var(provider_name)} or in config file. "
                f"Current key: {mask_secret(pcfg.api_key)}"
            )

        # 创建实际 Provider
        if pcfg.is_anthropic():
            http_provider = AnthropicProvider(pcfg)
        else:
            http_provider = OpenAICompatProvider(pcfg)

        semaphore = None
        if settings and hasattr(settings, "perf_llm"):
            max_conc = getattr(settings.perf_llm, "max_concurrent_per_provider", 5) or 5
            with self._sem_lock:
                if provider_name not in self._semaphores:
                    self._semaphores[provider_name] = threading.Semaphore(max_conc)
                semaphore = self._semaphores[provider_name]

        return HTTPChatClient(pcfg, http_provider, self.log_store, semaphore=semaphore)

    def resolve_model(self, provider: str, model: Optional[str] = None) -> str:
        """
        解析模型名。
        
        优先级:
        1. model 参数（若存在于 models 映射则使用映射值）
        2. provider.default_model（若存在于 models 映射则使用映射值）
        
        Args:
            provider: provider 名称
            model: 可选的模型名或别名
            
        Returns:
            解析后的实际模型名
        """
        if provider not in self.config.providers:
            raise ValueError(f"Unknown provider: {provider}")

        pcfg = self.config.providers[provider]
        models = pcfg.models

        if model:
            return models.get(model, model)

        default = pcfg.default_model
        return models.get(default, default)

    def is_available(self, provider: str) -> bool:
        """检查 provider 是否有有效的 API key"""
        if provider not in self.config.providers:
            return False
        pcfg = self.config.providers[provider]
        key = pcfg.api_key
        return bool(key and key not in ("sk-xxx", "sk-ant-xxx", "AIzxxx", ""))

    def cleanup_logs(
        self,
        max_age_days: int = LOG_MAX_AGE_DAYS,
        max_total_mb: int = LOG_MAX_TOTAL_MB
    ) -> Dict[str, Any]:
        """清理日志文件"""
        return self.log_store.cleanup(max_age_days, max_total_mb)


# ============================================================
# Convenience Functions
# ============================================================

# 全局单例（延迟初始化）
_manager: Optional[LLMManager] = None


def get_manager(config_path: Optional[str | Path] = None) -> LLMManager:
    """
    获取全局 LLMManager 单例。
    
    Args:
        config_path: 配置文件路径（仅首次调用时生效）
        
    Returns:
        LLMManager 实例
    """
    global _manager
    if _manager is None:
        if config_path is None:
            config_path = Path(__file__).parent.parent.parent / "config" / "rag_config.json"
        _manager = LLMManager.from_json(config_path)
    return _manager


def chat(
    messages: List[Dict[str, Any]],
    provider: Optional[str] = None,
    model: Optional[str] = None,
    **kwargs
) -> Dict[str, Any]:
    """
    便捷函数：快速调用 LLM。
    
    Args:
        messages: 消息列表
        provider: provider 名称（默认使用配置中的 default）
        model: 模型名或别名
        **kwargs: 传递给 chat() 的其他参数
        
    Returns:
        响应字典
    """
    manager = get_manager()
    client = manager.get_client(provider)
    return client.chat(messages, model=model, **kwargs)
</file>

<file path="src/parser/pdf_parser.py">
"""
PDF Parser - EnrichedDoc 完整解析流程

设计原则：
- Docling First：主解析引擎
- Deterministic Before LLM：几何匹配、上下文关联用确定性算法
- Anti-Hallucination：表格摘要必须基于 computed_stats
- Fail-Safe：失败标记 needs_review=True，保留 provenance
"""

from __future__ import annotations

import base64
import hashlib
import json
import logging
import re
import statistics
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Any, Callable, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

from pydantic import BaseModel, ConfigDict, Field

logger = logging.getLogger(__name__)


class _AnyJSONObject(BaseModel):
    model_config = ConfigDict(extra="allow")


class _FigureEnrichmentResponse(BaseModel):
    figure_type: str = "unknown"
    description: str = ""
    qa_pairs: list[dict[str, Any]] = Field(default_factory=list)
    key_findings: list[str] = Field(default_factory=list)
    evidence: list[str] = Field(default_factory=list)

# ============================================================
# ENUMS
# ============================================================


class BlockType(str, Enum):
    TEXT = "text"
    HEADING = "heading"
    TABLE = "table"
    FIGURE = "figure"
    CAPTION = "caption"
    FOOTNOTE = "footnote"
    FORMULA = "formula"
    LIST = "list"


class LayoutType(str, Enum):
    SINGLE_COLUMN = "single_column"
    DOUBLE_COLUMN = "double_column"
    MIXED = "mixed"


class CaptionMatchMethod(str, Enum):
    GRAVITY_DOWN = "gravity_down"
    GRAVITY_UP = "gravity_up"
    GRAVITY_SIDE = "gravity_side"
    CROSS_PAGE = "cross_page"
    NONE = "none"


class EnrichmentStatus(str, Enum):
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"


# ============================================================
# SCHEMA (dataclass)
# ============================================================


@dataclass
class SourceMeta:
    filename: str
    file_hash: Optional[str] = None
    num_pages: int = 0


@dataclass
class ParseMeta:
    parser_version: str
    parse_errors: list[str] = field(default_factory=list)


@dataclass
class EnrichmentMeta:
    table_count: int = 0
    figure_count: int = 0
    table_success: int = 0
    figure_success: int = 0
    table_failed: int = 0
    figure_failed: int = 0
    table_skipped: int = 0
    figure_skipped: int = 0


@dataclass
class PageData:
    page_index: int
    width: float
    height: float
    layout_type: LayoutType = LayoutType.SINGLE_COLUMN
    column_boundaries: Optional[list[float]] = None


@dataclass
class CaptionMatch:
    method: CaptionMatchMethod
    score: float
    source_block_id: Optional[str] = None


@dataclass
class TableData:
    markdown: str
    structured: list[list[str]]
    row_count: int
    col_count: int
    title: Optional[str] = None
    footnotes: list[str] = field(default_factory=list)
    computed_stats: Optional[dict[str, dict[str, float]]] = None


@dataclass
class FigureData:
    image_path: str
    image_hash: str
    width_px: int
    height_px: int
    caption: Optional[str] = None
    caption_match: Optional[CaptionMatch] = None
    ocr_text: Optional[list[str]] = None


@dataclass
class FigureInterpretation:
    figure_type: str
    description: str
    qa_pairs: list[dict[str, str]]
    key_findings: list[str] = field(default_factory=list)
    evidence: list[str] = field(default_factory=list)


@dataclass
class BlockEnrichment:
    status: EnrichmentStatus
    semantic_summary: Optional[str] = None
    interpretation: Optional[FigureInterpretation] = None
    error_message: Optional[str] = None
    provider: Optional[str] = None
    model: Optional[str] = None
    prompt_hash: Optional[str] = None
    input_hash: Optional[str] = None


@dataclass
class BlockProvenance:
    source: str
    input_hash: Optional[str] = None


@dataclass
class HeadingNode:
    level: int
    text: str
    reading_order: int
    children: list["HeadingNode"] = field(default_factory=list)


@dataclass
class ContentBlock:
    block_id: str
    block_type: BlockType
    page_index: int
    bbox: tuple[float, float, float, float]
    reading_order: int = -1
    column_index: Optional[int] = None
    heading_path: list[str] = field(default_factory=list)
    text: Optional[str] = None
    table_data: Optional[TableData] = None
    figure_data: Optional[FigureData] = None
    enrichment: Optional[BlockEnrichment] = None
    needs_review: bool = False
    review_reasons: list[str] = field(default_factory=list)
    provenance: Optional[BlockProvenance] = None


@dataclass
class EnrichedDoc:
    doc_id: str
    source: SourceMeta
    global_summary: Optional[str] = None
    pages: list[PageData] = field(default_factory=list)
    content_flow: list[ContentBlock] = field(default_factory=list)
    hierarchy: list[HeadingNode] = field(default_factory=list)
    parse_meta: ParseMeta = field(default_factory=lambda: ParseMeta(parser_version="1.0.0"))
    enrichment_meta: Optional[EnrichmentMeta] = None
    claims: list[dict] = field(default_factory=list)  # ClaimExtractor 提取的核心声明
    doc_metadata: Optional[dict] = None  # DOI / title / authors / year


# ============================================================
# CONFIG
# ============================================================


@dataclass
class ParserConfig:
    parser_version: str = "1.0.0"
    docling_ocr_enabled: bool = False
    coordinate_origin: str = "top_left"
    column_gap_min_ratio: float = 0.02
    column_gap_max_ratio: float = 0.15
    gravity_max_v_down_ratio: float = 0.06
    gravity_min_x_overlap: float = 0.30
    gravity_cross_page_penalty: float = 0.20
    table_title_patterns: list[str] = field(
        default_factory=lambda: [r"^Table\s*\d+", r"^Tab\.\s*\d+", r"^表\s*\d+"]
    )
    table_footnote_patterns: list[str] = field(
        default_factory=lambda: [r"^Note:", r"^\*", r"^注[：:]", r"^Source:"]
    )
    caption_patterns: list[str] = field(
        default_factory=lambda: [r"^(Fig\.|Figure)\s*\d+", r"^图\s*\d+"]
    )
    ocr_enabled: bool = False
    llm_text_provider: str = "deepseek"
    llm_vision_provider: str = "gemini-vision"
    llm_text_model: Optional[str] = None
    llm_vision_model: Optional[str] = None
    llm_text_concurrency: int = 1
    llm_vision_concurrency: int = 1
    llm_text_max_tokens: int = 500
    llm_vision_max_tokens: int = 1024
    llm_json_repair_max_tokens: int = 800
    llm_temperature: float = 0.1
    llm_max_retries: int = 2
    llm_call_timeout_seconds: int = 90
    enrich_tables: bool = True
    enrich_figures: bool = True
    output_dir: str = "./output"
    bottom_page_ratio: float = 0.85
    full_width_ratio: float = 0.60
    table_title_distance_pt: float = 72.0
    table_footnote_distance_pt: float = 48.0

    @classmethod
    def from_json(cls, path: str | Path) -> "ParserConfig":
        with open(path, "r", encoding="utf-8") as f:
            raw = json.load(f)
        p = raw.get("parser", {})
        return cls(
            parser_version=p.get("version", "1.0.0"),
            docling_ocr_enabled=p.get("docling_ocr", False),
            column_gap_min_ratio=p.get("column_gap_min_ratio", 0.02),
            column_gap_max_ratio=p.get("column_gap_max_ratio", 0.15),
            gravity_max_v_down_ratio=p.get("gravity_max_v_down_ratio", 0.06),
            gravity_min_x_overlap=p.get("gravity_min_x_overlap", 0.30),
            gravity_cross_page_penalty=p.get("gravity_cross_page_penalty", 0.20),
            table_title_patterns=p.get("table_title_patterns", [r"^Table\s*\d+", r"^表\s*\d+"]),
            table_footnote_patterns=p.get("table_footnote_patterns", [r"^Note:", r"^\*"]),
            caption_patterns=p.get("caption_patterns", [r"^(Fig\.|Figure)\s*\d+", r"^图\s*\d+"]),
            llm_text_provider=p.get("llm_text_provider", "deepseek"),
            llm_vision_provider=p.get("llm_vision_provider", "gemini-vision"),
            llm_text_model=p.get("llm_text_model"),
            llm_vision_model=p.get("llm_vision_model"),
            llm_text_concurrency=max(1, int(p.get("llm_text_concurrency", 1) or 1)),
            llm_vision_concurrency=max(1, int(p.get("llm_vision_concurrency", 1) or 1)),
            llm_text_max_tokens=int(p.get("llm_text_max_tokens", 500)),
            llm_vision_max_tokens=int(p.get("llm_vision_max_tokens", 1024)),
            llm_json_repair_max_tokens=int(p.get("llm_json_repair_max_tokens", 800)),
            llm_temperature=p.get("llm_temperature", 0.1),
            llm_max_retries=p.get("llm_max_retries", 2),
            llm_call_timeout_seconds=int(p.get("llm_call_timeout_seconds", 90)),
            enrich_tables=bool(p.get("enrich_tables", True)),
            enrich_figures=bool(p.get("enrich_figures", True)),
        )


# ============================================================
# UTILS
# ============================================================


def compute_hash(data: bytes | str) -> str:
    if isinstance(data, str):
        data = data.encode("utf-8")
    return hashlib.sha256(data).hexdigest()[:16]


def normalize_bbox(
    bbox: tuple[float, float, float, float], page_height: float, source_origin: str = "top_left"
) -> tuple[float, float, float, float]:
    x0, y0, x1, y1 = bbox
    if x0 > x1:
        x0, x1 = x1, x0
    if y0 > y1:
        y0, y1 = y1, y0
    if source_origin == "bottom_left":
        y0, y1 = page_height - y1, page_height - y0
    return (x0, y0, x1, y1)


def parse_numeric(text: str) -> Optional[float]:
    if not text or not isinstance(text, str):
        return None
    text = text.strip().replace(",", "")
    if not text:
        return None
    if text.endswith("%"):
        try:
            return float(text[:-1]) / 100
        except ValueError:
            return None
    if "±" in text:
        text = text.split("±")[0].strip()
    try:
        return float(re.sub(r"[^\d.eE+-]", "", text) or "0")
    except ValueError:
        return None


def compute_table_stats(structured: list[list[str]]) -> dict[str, dict[str, float]]:
    """只处理 >50% 行是数值的列。"""
    result: dict[str, dict[str, float]] = {}
    if not structured or len(structured) < 2:
        return result
    headers = structured[0]
    rows = structured[1:]
    for col_idx, col_name in enumerate(headers):
        col_name = str(col_name or f"col_{col_idx}").strip()
        vals: list[float] = []
        for row in rows:
            if col_idx < len(row):
                v = parse_numeric(row[col_idx])
                if v is not None:
                    vals.append(v)
        if len(vals) >= len(rows) * 0.5 and vals:
            result[col_name] = {
                "min": min(vals),
                "max": max(vals),
                "mean": statistics.mean(vals),
                "n": len(vals),
            }
    return result


def _pattern_match(text: str, patterns: list[str]) -> bool:
    text = str(text or "")
    for pat in patterns:
        if re.search(pat, text.strip(), re.IGNORECASE):
            return True
    return False


# ============================================================
# DoclingParser
# ============================================================


class DoclingParser:
    def __init__(self, config: ParserConfig):
        self.config = config

    def parse(
        self, pdf_path: str | Path
    ) -> tuple[list[ContentBlock], list[PageData], dict[str, Any]]:
        try:
            from docling.document_converter import DocumentConverter
            from docling.datamodel.base_models import InputFormat
            from docling.datamodel.pipeline_options import PdfPipelineOptions
            from docling.document_converter import PdfFormatOption
        except ImportError as e:
            raise ImportError(f"docling 未安装: {e}. pip install docling")

        pipeline_options = PdfPipelineOptions()
        pipeline_options.do_ocr = self.config.docling_ocr_enabled
        pipeline_options.do_table_structure = True

        converter = DocumentConverter(
            format_options={
                InputFormat.PDF: PdfFormatOption(pipeline_options=pipeline_options)
            }
        )
        result = converter.convert(str(pdf_path))
        doc = result.document

        blocks: list[ContentBlock] = []
        pages: list[PageData] = []
        block_counter = 0

        # 收集页面尺寸
        pages_dict = getattr(doc, "pages", None) or {}
        if isinstance(pages_dict, dict):
            for page_no, page_item in sorted(pages_dict.items(), key=lambda x: x[0]):
                size = getattr(page_item, "size", None)
                if size:
                    w = getattr(size, "width", 612) or 612
                    h = getattr(size, "height", 792) or 792
                else:
                    w, h = 612, 792
                pages.append(PageData(page_index=int(page_no) - 1, width=w, height=h))
        if not pages:
            pages = [PageData(page_index=0, width=612, height=792)]

        def get_bbox_and_page(elem: Any) -> tuple[tuple[float, float, float, float], int]:
            prov = getattr(elem, "prov", None) or []
            if prov:
                p = prov[0] if isinstance(prov, (list, tuple)) else prov
                bbox = getattr(p, "bbox", None)
                page_no = int(getattr(p, "page_no", 1)) - 1
                if bbox:
                    l_ = getattr(bbox, "l", 0) or 0
                    t_ = getattr(bbox, "t", 0) or 0
                    r_ = getattr(bbox, "r", 0) or 0
                    b_ = getattr(bbox, "b", 0) or 0
                    origin = getattr(bbox, "coord_origin", "TOPLEFT") or "TOPLEFT"
                    ph = pages[page_no].height if page_no < len(pages) else 792
                    return normalize_bbox((l_, t_, r_, b_), ph, "bottom_left" if "BOTTOM" in str(origin).upper() else "top_left"), page_no
            return (0, 0, 100, 100), 0

        def make_block_id() -> str:
            nonlocal block_counter
            block_counter += 1
            return f"blk_{block_counter:04d}"

        # Texts
        texts = getattr(doc, "texts", []) or []
        for t in texts:
            label = str(getattr(t, "label", "text")).lower()
            txt = str(getattr(t, "text", "") or "")
            bbox_norm, page_ix = get_bbox_and_page(t)
            bt = BlockType.TEXT
            if "section" in label or "heading" in label or "title" in label:
                bt = BlockType.HEADING
            elif label == "caption":
                bt = BlockType.CAPTION
            elif label == "footnote":
                bt = BlockType.FOOTNOTE
            elif "list" in label:
                bt = BlockType.LIST
            blocks.append(
                ContentBlock(
                    block_id=make_block_id(),
                    block_type=bt,
                    page_index=page_ix,
                    bbox=bbox_norm,
                    reading_order=-1,
                    text=txt,
                    provenance=BlockProvenance(source="docling"),
                )
            )

        # Tables
        tables = getattr(doc, "tables", []) or []
        for tbl in tables:
            bbox_norm, page_ix = get_bbox_and_page(tbl)
            md = ""
            structured: list[list[str]] = []
            try:
                if hasattr(tbl, "export_to_dataframe"):
                    df = tbl.export_to_dataframe(doc=doc)
                    if df is not None and not df.empty:
                        md = df.to_markdown() if hasattr(df, "to_markdown") else str(df)
                        cols = df.columns.tolist()
                        if cols and isinstance(cols[0], tuple):
                            cols = [" ".join(str(x) for x in c) for c in cols]
                        cols = [str(c) for c in cols]
                        structured = [cols]
                        for _, row in df.iterrows():
                            structured.append([str(v) if v is not None and str(v) != "nan" else "" for v in row.tolist()])
                if not md and hasattr(tbl, "export_to_markdown"):
                    try:
                        md = tbl.export_to_markdown(doc=doc) or ""
                    except TypeError:
                        md = tbl.export_to_markdown() or ""
            except Exception:
                pass
            if not structured and md:
                lines = [ln for ln in md.split("\n") if "|" in ln]
                for ln in lines:
                    cells = [c.strip() for c in ln.split("|") if c.strip() and c.strip() != "---"]
                    if cells:
                        structured.append(cells)
            row_count = len(structured)
            col_count = max(len(r) for r in structured) if structured else 0
            td = TableData(
                markdown=md,
                structured=structured,
                row_count=row_count,
                col_count=col_count,
                computed_stats=compute_table_stats(structured),
            )
            blocks.append(
                ContentBlock(
                    block_id=make_block_id(),
                    block_type=BlockType.TABLE,
                    page_index=page_ix,
                    bbox=bbox_norm,
                    reading_order=-1,
                    table_data=td,
                    provenance=BlockProvenance(source="docling"),
                )
            )

        # Pictures
        pictures = getattr(doc, "pictures", []) or []
        for pic in pictures:
            bbox_norm, page_ix = get_bbox_and_page(pic)
            blocks.append(
                ContentBlock(
                    block_id=make_block_id(),
                    block_type=BlockType.FIGURE,
                    page_index=page_ix,
                    bbox=bbox_norm,
                    reading_order=-1,
                    figure_data=FigureData(
                        image_path="",
                        image_hash="",
                        width_px=0,
                        height_px=0,
                    ),
                    provenance=BlockProvenance(source="docling"),
                )
            )

        raw_output = {"text_count": len(texts), "table_count": len(tables), "picture_count": len(pictures)}
        return blocks, pages, raw_output


# ============================================================
# LayoutAnalyzer
# ============================================================


class LayoutAnalyzer:
    def __init__(self, config: ParserConfig):
        self.config = config

    def analyze(
        self, blocks: list[ContentBlock], pages: list[PageData]
    ) -> tuple[list[ContentBlock], list[PageData]]:
        # 按页分组
        by_page: dict[int, list[ContentBlock]] = {}
        for b in blocks:
            by_page.setdefault(b.page_index, []).append(b)

        for page_ix, page in enumerate(pages):
            if page_ix not in by_page:
                continue
            page_blocks = by_page[page_ix]
            page_width = page.width
            page_height = page.height

            # 双栏检测：只用 TEXT/HEADING
            text_blocks = [b for b in page_blocks if b.block_type in (BlockType.TEXT, BlockType.HEADING)]
            if len(text_blocks) >= 4:
                centers_x = sorted([(b.bbox[0] + b.bbox[2]) / 2 for b in text_blocks])
                max_gap, gap_pos = 0, 0
                for i in range(1, len(centers_x)):
                    gap = centers_x[i] - centers_x[i - 1]
                    if gap > max_gap:
                        max_gap, gap_pos = gap, (centers_x[i] + centers_x[i - 1]) / 2
                gap_ratio = max_gap / page_width if page_width else 0
                pos_ratio = gap_pos / page_width if page_width else 0.5
                if (
                    self.config.column_gap_min_ratio < gap_ratio < self.config.column_gap_max_ratio
                    and 0.35 < pos_ratio < 0.65
                ):
                    page.layout_type = LayoutType.DOUBLE_COLUMN
                    page.column_boundaries = [gap_pos]

            # 分配 column_index
            col_bound = page.column_boundaries
            full_width = page_width * self.config.full_width_ratio
            for b in page_blocks:
                w = b.bbox[2] - b.bbox[0]
                if page.layout_type == LayoutType.DOUBLE_COLUMN and col_bound and w < full_width:
                    cx = (b.bbox[0] + b.bbox[2]) / 2
                    b.column_index = 0 if cx < col_bound[0] else 1
                else:
                    b.column_index = 0

        # 阅读顺序
        def key_fn(b: ContentBlock):
            p = pages[b.page_index] if b.page_index < len(pages) else pages[0]
            col = b.column_index or 0
            return (b.page_index, col, b.bbox[1], b.bbox[0])

        sorted_blocks = sorted(blocks, key=key_fn)
        for i, b in enumerate(sorted_blocks):
            b.reading_order = i

        return sorted_blocks, pages


# ============================================================
# ContextLinker
# ============================================================


class ContextLinker:
    def __init__(self, config: ParserConfig):
        self.config = config

    def link_all(
        self, blocks: list[ContentBlock], pages: list[PageData]
    ) -> tuple[list[ContentBlock], list[HeadingNode]]:
        blocks = self._build_hierarchy(blocks)
        blocks = self._link_table_context(blocks, pages)
        blocks = self._link_figure_caption(blocks, pages)
        hierarchy = self._extract_hierarchy(blocks)
        return blocks, hierarchy

    def _build_hierarchy(self, blocks: list[ContentBlock]) -> list[ContentBlock]:
        headings = [(b, i) for i, b in enumerate(blocks) if b.block_type == BlockType.HEADING]
        headings_sorted = sorted(headings, key=lambda x: x[0].reading_order)

        def level_from_text(t: str) -> int:
            m = re.match(r"^(\d+)(\.\d+)*", (t or "").strip())
            if m:
                return len(m.group(0).split("."))
            return 1

        current_path: list[str] = []
        current_levels: list[int] = []
        heading_idx = 0

        for b in sorted(blocks, key=lambda x: x.reading_order):
            while heading_idx < len(headings_sorted) and headings_sorted[heading_idx][0].reading_order <= b.reading_order:
                h = headings_sorted[heading_idx][0]
                lv = level_from_text(h.text or "")
                while current_levels and current_levels[-1] >= lv:
                    current_path.pop()
                    current_levels.pop()
                current_path.append((h.text or "").strip())
                current_levels.append(lv)
                heading_idx += 1
            b.heading_path = current_path.copy()
        return blocks

    def _extract_hierarchy(self, blocks: list[ContentBlock]) -> list[HeadingNode]:
        headings = [b for b in blocks if b.block_type == BlockType.HEADING]
        headings = sorted(headings, key=lambda x: x.reading_order)

        def level_from_text(t: str) -> int:
            m = re.match(r"^(\d+)(\.\d+)*", (t or "").strip())
            if m:
                return len(m.group(0).split("."))
            return 1

        nodes: list[HeadingNode] = []
        stack: list[tuple[HeadingNode, int]] = []
        for h in headings:
            lv = level_from_text(h.text or "")
            node = HeadingNode(level=lv, text=(h.text or "").strip(), reading_order=h.reading_order)
            while stack and stack[-1][1] >= lv:
                stack.pop()
            if stack:
                stack[-1][0].children.append(node)
            else:
                nodes.append(node)
            stack.append((node, lv))
        return nodes

    def _link_table_context(
        self, blocks: list[ContentBlock], pages: list[PageData]
    ) -> list[ContentBlock]:
        for b in blocks:
            if b.block_type != BlockType.TABLE or not b.table_data:
                continue
            page = pages[b.page_index] if b.page_index < len(pages) else None
            if not page:
                continue
            page_h = page.height
            x0, y0, x1, y1 = b.bbox
            dist_pt = self.config.table_title_distance_pt
            fn_dist = self.config.table_footnote_distance_pt

            # Title 向上
            candidates_above: list[tuple[ContentBlock, float]] = []
            for other in blocks:
                if other.block_type not in (BlockType.TEXT, BlockType.CAPTION) or not other.text:
                    continue
                if other.page_index != b.page_index:
                    continue
                oy0 = other.bbox[1]
                oy1 = other.bbox[3]
                if oy1 <= y0 and y0 - oy1 <= dist_pt:
                    candidates_above.append((other, y0 - oy1))
            candidates_above.sort(key=lambda x: x[1])
            for other, _ in candidates_above:
                if _pattern_match(other.text, self.config.table_title_patterns):
                    b.table_data.title = other.text.strip()
                    break

            # Footnote 向下
            footnotes: list[str] = []
            for other in blocks:
                if other.block_type != BlockType.TEXT or not other.text:
                    continue
                if other.page_index != b.page_index:
                    continue
                oy0 = other.bbox[1]
                if oy0 >= y1 and oy0 - y1 <= fn_dist:
                    if _pattern_match(other.text, self.config.table_footnote_patterns):
                        footnotes.append(other.text.strip())
            if footnotes:
                b.table_data.footnotes = footnotes

            # 跨页脚注
            if not footnotes and y1 > page_h * self.config.bottom_page_ratio:
                next_page_ix = b.page_index + 1
                if next_page_ix < len(pages):
                    top_blocks = [
                        o for o in blocks
                        if o.page_index == next_page_ix and o.block_type == BlockType.TEXT and o.text
                    ][:10]
                    for o in top_blocks:
                        if _pattern_match(o.text, self.config.table_footnote_patterns):
                            b.table_data.footnotes.append(o.text.strip())
                            break
        return blocks

    def _link_figure_caption(
        self, blocks: list[ContentBlock], pages: list[PageData]
    ) -> list[ContentBlock]:
        picture_blocks = [b for b in blocks if b.block_type == BlockType.FIGURE and b.figure_data]
        caption_candidates: list[ContentBlock] = [
            b for b in blocks
            if (
                b.block_type == BlockType.CAPTION
                or _pattern_match(b.text or "", self.config.caption_patterns)
                or (10 <= len((b.text or "").strip()) <= 600 and b.block_type == BlockType.TEXT)
            )
            and b.text
        ]
        if not picture_blocks:
            return blocks

        # 评分并为每个 picture 选择最佳 caption
        assignments: dict[str, tuple[ContentBlock, float, CaptionMatchMethod]] = {}
        for cap in caption_candidates:
            page = pages[cap.page_index] if cap.page_index < len(pages) else None
            if not page:
                continue
            page_h = page.height
            page_w = page.width
            cx0, cy0, cx1, cy1 = cap.bbox
            cap_w = cx1 - cx0
            cap_col = cap.column_index

            max_v_down = min(72, page_h * self.config.gravity_max_v_down_ratio)
            max_v_up = min(48, page_h * 0.04)

            for pic in picture_blocks:
                if pic.page_index != cap.page_index:
                    continue
                fx0, fy0, fx1, fy1 = pic.bbox
                fig_w = fx1 - fx0
                is_full_width = fig_w > page_w * self.config.full_width_ratio

                # 同栏约束（跨栏图例外）
                if not is_full_width and cap_col is not None and pic.column_index is not None:
                    if cap_col != pic.column_index:
                        continue

                # caption 在图下方（更常见）
                if cy0 >= fy1:
                    v_dist = cy0 - fy1
                    if v_dist > max_v_down:
                        continue
                    method = CaptionMatchMethod.GRAVITY_DOWN
                    penalty = 0.0
                # caption 在图上方
                elif fy0 >= cy1:
                    v_dist = fy0 - cy1
                    if v_dist > max_v_up:
                        continue
                    method = CaptionMatchMethod.GRAVITY_UP
                    penalty = 0.15
                else:
                    continue

                x_overlap = max(0, min(fx1, cx1) - max(fx0, cx0))
                x_overlap_ratio = x_overlap / min(fig_w, cap_w) if min(fig_w, cap_w) > 0 else 0
                if x_overlap_ratio < self.config.gravity_min_x_overlap:
                    continue

                score = 1.2 * x_overlap_ratio - 0.008 * v_dist - penalty
                if pic.block_id not in assignments or score > assignments[pic.block_id][1]:
                    assignments[pic.block_id] = (cap, score, method)

        # 按 caption 聚合图片块
        cap_to_pics: dict[str, list[ContentBlock]] = {}
        cap_scores: dict[str, tuple[float, CaptionMatchMethod]] = {}
        for pic in picture_blocks:
            if pic.block_id not in assignments:
                pic.needs_review = True
                pic.review_reasons.append("caption_not_matched")
                continue
            cap, score, method = assignments[pic.block_id]
            cap_to_pics.setdefault(cap.block_id, []).append(pic)
            prev = cap_scores.get(cap.block_id)
            if not prev or score > prev[0]:
                cap_scores[cap.block_id] = (score, method)

        merged_blocks: list[ContentBlock] = []
        assigned_picture_ids = set(assignments.keys())

        for cap in caption_candidates:
            pics = cap_to_pics.get(cap.block_id, [])
            if not pics:
                continue
            x0 = min(p.bbox[0] for p in pics)
            y0 = min(p.bbox[1] for p in pics)
            x1 = max(p.bbox[2] for p in pics)
            y1 = max(p.bbox[3] for p in pics)
            reading_order = min(p.reading_order for p in pics)
            score, method = cap_scores.get(cap.block_id, (0.0, CaptionMatchMethod.NONE))

            merged_blocks.append(
                ContentBlock(
                    block_id=f"{cap.block_id}_fig",
                    block_type=BlockType.FIGURE,
                    page_index=cap.page_index,
                    bbox=(x0, y0, x1, y1),
                    reading_order=reading_order,
                    column_index=cap.column_index,
                    heading_path=cap.heading_path.copy(),
                    figure_data=FigureData(
                        image_path="",
                        image_hash="",
                        width_px=0,
                        height_px=0,
                        caption=str(cap.text or "").strip(),
                        caption_match=CaptionMatch(
                            method=method,
                            score=score,
                            source_block_id=cap.block_id,
                        ),
                    ),
                    provenance=BlockProvenance(source="docling"),
                )
            )

        # 移除已归并的碎片图块，保留未匹配的图块
        remaining_blocks = [b for b in blocks if b.block_id not in assigned_picture_ids]
        remaining_blocks.extend(merged_blocks)
        remaining_blocks.sort(key=lambda b: b.reading_order)
        return remaining_blocks


# ============================================================
# FigureExtractor
# ============================================================


class FigureExtractor:
    def __init__(self, config: ParserConfig):
        self.config = config

    def extract_figures(
        self,
        pdf_path: str | Path,
        figure_blocks: list[ContentBlock],
        output_dir: str | Path,
    ) -> list[ContentBlock]:
        try:
            import fitz  # PyMuPDF
        except ImportError:
            for b in figure_blocks:
                if b.figure_data:
                    b.needs_review = True
                    b.review_reasons.append("pymupdf_not_installed")
            return figure_blocks

        output_dir = Path(output_dir)
        assets_dir = output_dir / "assets"
        assets_dir.mkdir(parents=True, exist_ok=True)

        doc = fitz.open(str(pdf_path))
        for i, block in enumerate(figure_blocks):
            if block.block_type != BlockType.FIGURE or not block.figure_data:
                continue
            fd = block.figure_data
            page = doc[block.page_index]
            x0, y0, x1, y1 = block.bbox
            # PyMuPDF 坐标系为左上原点，直接使用归一化后的 bbox
            rect = fitz.Rect(x0, y0, x1, y1)
            rect = rect & page.rect
            try:
                pix = page.get_pixmap(clip=rect, dpi=150)
                img_path = assets_dir / f"fig_{i + 1:03d}.png"
                pix.save(str(img_path))
                with open(img_path, "rb") as f:
                    fd.image_hash = compute_hash(f.read())
                fd.image_path = f"assets/{img_path.name}"
                fd.width_px = pix.width
                fd.height_px = pix.height
            except Exception:
                block.needs_review = True
                block.review_reasons.append("figure_extract_failed")
        doc.close()
        return figure_blocks


# ============================================================
# LLMEnricher
# ============================================================

TABLE_SUMMARY_PROMPT = """Role: Senior Data Analyst
Task: Generate a semantic summary for the table.

Context:
- Section: {heading_path}
- Title: {table_title}
- Markdown: {table_markdown}
- Computed Stats (trusted): {computed_stats}

Requirements:
- ONE paragraph, NO bullets
- Reference ≥2 column headers by name
- ALL numbers must come from table or computed_stats
- If uncertain, say "not explicitly stated"
- Output ONLY the summary text, no JSON."""

FIGURE_JSON_SCHEMA = """{
  "figure_type": "line_plot|bar_chart|diagram|photo|map|multi-panel figure|...",
  "description": "1-2 sentences for overall summary",
  "key_findings": ["finding1", "finding2", "finding3"],
  "evidence": ["panel/legend/axis cues supporting findings", "..."],
  "qa_pairs": []
}"""

FIGURE_INTERPRET_PROMPT = """Role: Scientific Visualization Expert
Task: Provide a concise overall summary and key evidence.

Context:
- Section: {heading_path}
- Caption: {caption}

Requirements:
- Output must be concise: 1-2 sentences overall summary.
- Provide up to 3 key findings with direct evidence from panels/legend/axes.
- If numeric values are not clearly readable, say "not readable" and do not guess.
- Reference panel labels (a/b/c/d) or legend cues when present.

Output JSON only:
{{
  "figure_type": "line_plot|bar_chart|diagram|photo|map|multi-panel figure|...",
  "description": "1-2 sentences overall summary",
  "key_findings": ["finding1", "finding2", "finding3"],
  "evidence": ["where in the figure this comes from (panel/legend/axis)", "..."],
  "qa_pairs": []
}}
Return ONLY valid JSON."""


class LLMEnricher:
    def __init__(self, config: ParserConfig, llm_manager: Any):
        self.config = config
        self.llm_manager = llm_manager
        self._text_client = None
        self._vision_client = None

    def _get_text_client(self, fresh: bool = False):
        if fresh:
            return self.llm_manager.get_client(self.config.llm_text_provider)
        if self._text_client is None:
            self._text_client = self.llm_manager.get_client(self.config.llm_text_provider)
        return self._text_client

    def _get_vision_client(self, fresh: bool = False):
        if fresh:
            return self.llm_manager.get_client(self.config.llm_vision_provider)
        if self._vision_client is None:
            self._vision_client = self.llm_manager.get_client(self.config.llm_vision_provider)
        return self._vision_client

    def _resolve_model(self, provider: str, model_override: Optional[str] = None) -> Optional[str]:
        if not self.llm_manager or not hasattr(self.llm_manager, "resolve_model"):
            return None
        try:
            return self.llm_manager.resolve_model(provider, model_override)
        except Exception:
            return None

    def _repair_json(self, raw_output: str, schema: str) -> Optional[dict]:
        if not raw_output:
            return None
        prompt = (
            "You produced invalid JSON. Fix it ONLY.\n"
            f"Schema: {schema}\n"
            f"Your output: {raw_output}\n"
            "Return ONLY valid JSON."
        )
        try:
            client = self._get_text_client(fresh=True)
            resp = client.chat(
                messages=[
                    {"role": "system", "content": "Return ONLY valid JSON."},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=self.config.llm_json_repair_max_tokens,
                temperature=0.0,
                timeout_seconds=self.config.llm_call_timeout_seconds,
                response_model=_AnyJSONObject,
            )
            parsed: Optional[_AnyJSONObject] = resp.get("parsed_object")
            if parsed is None:
                fixed = (resp.get("final_text") or "").strip()
                if not fixed:
                    return None
                parsed = _AnyJSONObject.model_validate_json(fixed)
            return parsed.model_dump()
        except Exception:
            return None

    def enrich_all(
        self,
        blocks: list[ContentBlock],
        global_context: str = "",
        progress_callback: Optional[Callable[[str, dict], None]] = None,
    ) -> tuple[list[ContentBlock], EnrichmentMeta]:
        meta = EnrichmentMeta()
        doc_id = Path(global_context).name if global_context else "?"
        n_tables = sum(1 for b in blocks if b.block_type == BlockType.TABLE)
        n_figures = sum(
            1
            for b in blocks
            if b.block_type == BlockType.FIGURE and b.figure_data and b.figure_data.image_path
        )
        table_indices: list[int] = []
        figure_indices: list[int] = []
        for idx, b in enumerate(blocks):
            if b.block_type == BlockType.TABLE:
                meta.table_count += 1
                if self.config.enrich_tables:
                    table_indices.append(idx)
                else:
                    b.enrichment = BlockEnrichment(
                        status=EnrichmentStatus.SKIPPED, error_message="enrich_tables_disabled"
                    )
                    table_ix = len(table_indices) + meta.table_skipped + 1
                    logger.info("[%s] table %d/%d skip: enrich_tables_disabled", doc_id, table_ix, n_tables)
                    if progress_callback:
                        progress_callback(
                            "enrich_table",
                            {
                                "index": table_ix,
                                "total": n_tables,
                                "status": "skip",
                                "message": "enrich_tables_disabled",
                            },
                        )
                    blocks[idx] = b
                    meta.table_skipped += 1
            elif b.block_type == BlockType.FIGURE and b.figure_data and b.figure_data.image_path:
                meta.figure_count += 1
                if self.config.enrich_figures:
                    figure_indices.append(idx)
                else:
                    b.enrichment = BlockEnrichment(
                        status=EnrichmentStatus.SKIPPED, error_message="enrich_figures_disabled"
                    )
                    blocks[idx] = b
                    meta.figure_skipped += 1
                    logger.info("[%s] figure skip: enrich_figures_disabled", doc_id)

        if table_indices:
            max_workers = max(1, int(self.config.llm_text_concurrency or 1))
            actual_workers = min(max_workers, len(table_indices))
            if actual_workers <= 1 or len(table_indices) <= 1:
                for table_ix, idx in enumerate(table_indices, 1):
                    b = self.enrich_table(
                        blocks[idx],
                        global_context,
                        use_fresh_client=False,
                        table_index=table_ix,
                        table_total=len(table_indices),
                        progress_callback=progress_callback,
                    )
                    blocks[idx] = b
                    if b.enrichment and b.enrichment.status == EnrichmentStatus.SUCCESS:
                        meta.table_success += 1
                    elif b.enrichment and b.enrichment.status == EnrichmentStatus.SKIPPED:
                        meta.table_skipped += 1
                    else:
                        meta.table_failed += 1
            else:
                with ThreadPoolExecutor(max_workers=actual_workers) as executor:
                    futures = {
                        executor.submit(
                            self.enrich_table,
                            blocks[idx],
                            global_context,
                            True,
                            table_ix,
                            len(table_indices),
                            progress_callback,
                        ): idx
                        for table_ix, idx in enumerate(table_indices, 1)
                    }
                    for future in as_completed(futures):
                        idx = futures[future]
                        try:
                            b = future.result()
                        except Exception as e:
                            b = blocks[idx]
                            b.enrichment = BlockEnrichment(
                                status=EnrichmentStatus.FAILED,
                                error_message=str(e),
                                provider=self.config.llm_text_provider,
                                model=self._resolve_model(
                                    self.config.llm_text_provider, self.config.llm_text_model
                                ),
                            )
                            b.needs_review = True
                            b.review_reasons.append("llm_table_enrich_failed")
                        blocks[idx] = b
                        if b.enrichment and b.enrichment.status == EnrichmentStatus.SUCCESS:
                            meta.table_success += 1
                        elif b.enrichment and b.enrichment.status == EnrichmentStatus.SKIPPED:
                            meta.table_skipped += 1
                        else:
                            meta.table_failed += 1

        if figure_indices:
            max_workers = max(1, int(self.config.llm_vision_concurrency or 1))
            actual_workers = min(max_workers, len(figure_indices))
            if actual_workers <= 1 or len(figure_indices) <= 1:
                for fig_ix, idx in enumerate(figure_indices, 1):
                    b = self.enrich_figure(
                        blocks[idx],
                        global_context,
                        use_fresh_client=False,
                        figure_index=fig_ix,
                        figure_total=len(figure_indices),
                        progress_callback=progress_callback,
                    )
                    blocks[idx] = b
                    if b.enrichment and b.enrichment.status == EnrichmentStatus.SUCCESS:
                        meta.figure_success += 1
                    elif b.enrichment and b.enrichment.status == EnrichmentStatus.SKIPPED:
                        meta.figure_skipped += 1
                    else:
                        meta.figure_failed += 1
            else:
                with ThreadPoolExecutor(max_workers=actual_workers) as executor:
                    futures = {
                        executor.submit(
                            self.enrich_figure,
                            blocks[idx],
                            global_context,
                            True,
                            fig_ix,
                            len(figure_indices),
                            progress_callback,
                        ): idx
                        for fig_ix, idx in enumerate(figure_indices, 1)
                    }
                    for future in as_completed(futures):
                        idx = futures[future]
                        try:
                            b = future.result()
                        except Exception as e:
                            b = blocks[idx]
                            b.enrichment = BlockEnrichment(
                                status=EnrichmentStatus.FAILED,
                                error_message=str(e),
                                provider=self.config.llm_vision_provider,
                                model=self._resolve_model(
                                    self.config.llm_vision_provider, self.config.llm_vision_model
                                ),
                            )
                            b.needs_review = True
                            b.review_reasons.append("llm_figure_enrich_failed")
                        blocks[idx] = b
                        if b.enrichment and b.enrichment.status == EnrichmentStatus.SUCCESS:
                            meta.figure_success += 1
                        elif b.enrichment and b.enrichment.status == EnrichmentStatus.SKIPPED:
                            meta.figure_skipped += 1
                        else:
                            meta.figure_failed += 1
        logger.info(
            "[%s] enrich_all done: tables total=%d success=%d failed=%d skipped=%d; figures total=%d success=%d failed=%d skipped=%d",
            doc_id,
            meta.table_count,
            meta.table_success,
            meta.table_failed,
            meta.table_skipped,
            meta.figure_count,
            meta.figure_success,
            meta.figure_failed,
            meta.figure_skipped,
        )
        return blocks, meta

    def enrich_table(
        self,
        block: ContentBlock,
        global_context: str = "",
        use_fresh_client: bool = False,
        table_index: Optional[int] = None,
        table_total: Optional[int] = None,
        progress_callback: Optional[Callable[[str, dict], None]] = None,
    ) -> ContentBlock:
        doc_id = Path(global_context).name if global_context else "?"
        idx_label = f"{table_index or '?'}/{table_total or '?'}"
        if not block.table_data:
            block.enrichment = BlockEnrichment(status=EnrichmentStatus.SKIPPED, error_message="no_table_data")
            logger.info("[%s] table %s skip: no_table_data", doc_id, idx_label)
            if progress_callback:
                progress_callback("enrich_table", {"index": table_index, "total": table_total, "status": "skip", "message": "no_table_data"})
            return block
        td = block.table_data
        if not td.computed_stats:
            td.computed_stats = compute_table_stats(td.structured)
        prompt = TABLE_SUMMARY_PROMPT.format(
            heading_path=" > ".join(block.heading_path) if block.heading_path else "(root)",
            table_title=td.title or "(no title)",
            table_markdown=td.markdown[:3000] if td.markdown else "",
            computed_stats=json.dumps(td.computed_stats, ensure_ascii=False, indent=2),
        )
        provider = self.config.llm_text_provider
        model_override = self.config.llm_text_model
        model_resolved = self._resolve_model(provider, model_override)
        logger.info(
            "[%s] table %s start provider=%s model=%s",
            doc_id,
            idx_label,
            provider,
            model_resolved,
        )
        if progress_callback:
            progress_callback(
                "enrich_table",
                {
                    "index": table_index,
                    "total": table_total,
                    "status": "start",
                    "message": f"{provider}/{model_resolved or ''}".strip("/"),
                },
            )
        prompt_hash = compute_hash(prompt)
        input_hash = compute_hash((td.markdown or "") + json.dumps(td.computed_stats or {}, ensure_ascii=False))
        for attempt in range(self.config.llm_max_retries + 1):
            try:
                client = self._get_text_client(fresh=use_fresh_client)
                resp = client.chat(
                    messages=[
                        {"role": "system", "content": "You are a senior data analyst. Output only the requested content."},
                        {"role": "user", "content": prompt},
                    ],
                    model=model_override,
                    max_tokens=self.config.llm_text_max_tokens,
                    temperature=self.config.llm_temperature,
                    timeout_seconds=self.config.llm_call_timeout_seconds,
                )
                text = (resp.get("final_text") or "").strip()
                if text:
                    provider_used = resp.get("provider") or provider
                    model_used = resp.get("model") or model_resolved
                    block.enrichment = BlockEnrichment(
                        status=EnrichmentStatus.SUCCESS,
                        semantic_summary=text,
                        provider=provider_used,
                        model=model_used,
                        prompt_hash=prompt_hash,
                        input_hash=input_hash,
                    )
                    logger.info("[%s] table %s success", doc_id, idx_label)
                    if progress_callback:
                        progress_callback("enrich_table", {"index": table_index, "total": table_total, "status": "success"})
                    return block
            except Exception as e:
                if attempt == self.config.llm_max_retries:
                    block.enrichment = BlockEnrichment(
                        status=EnrichmentStatus.FAILED,
                        error_message=str(e),
                        provider=provider,
                        model=model_resolved,
                        prompt_hash=prompt_hash,
                        input_hash=input_hash,
                    )
                    block.needs_review = True
                    block.review_reasons.append("llm_table_enrich_failed")
                    logger.info("[%s] table %s failed: %s", doc_id, idx_label, str(e)[:200])
                    if progress_callback:
                        progress_callback("enrich_table", {"index": table_index, "total": table_total, "status": "fail", "message": str(e)[:200]})
                    return block
        block.enrichment = block.enrichment or BlockEnrichment(
            status=EnrichmentStatus.FAILED,
            error_message="max_retries",
            provider=provider,
            model=model_resolved,
            prompt_hash=prompt_hash,
            input_hash=input_hash,
        )
        logger.info("[%s] table %s failed: max_retries", doc_id, idx_label)
        if progress_callback:
            progress_callback("enrich_table", {"index": table_index, "total": table_total, "status": "fail", "message": "max_retries"})
        return block

    def enrich_figure(
        self,
        block: ContentBlock,
        global_context: str = "",
        use_fresh_client: bool = False,
        figure_index: Optional[int] = None,
        figure_total: Optional[int] = None,
        progress_callback: Optional[Callable[[str, dict], None]] = None,
    ) -> ContentBlock:
        doc_id = Path(global_context).name if global_context else "?"
        idx_label = f"{figure_index or '?'}/{figure_total or '?'}"
        if not block.figure_data or not block.figure_data.image_path:
            block.enrichment = BlockEnrichment(status=EnrichmentStatus.SKIPPED, error_message="no_image")
            logger.info("[%s] figure %s skip: no_image", doc_id, idx_label)
            if progress_callback:
                progress_callback("enrich_figure", {"index": figure_index, "total": figure_total, "status": "skip", "message": "no_image"})
            return block
        fd = block.figure_data
        # 图片路径可能是相对于 output_dir 的
        img_full = Path(global_context) / fd.image_path if global_context else Path(fd.image_path)
        if not img_full.exists():
            # 尝试 assets 子路径
            img_full = Path(global_context) / "assets" / Path(fd.image_path).name if global_context else Path(fd.image_path)
        if not img_full.exists():
            block.enrichment = BlockEnrichment(status=EnrichmentStatus.FAILED, error_message="image_not_found")
            block.needs_review = True
            block.review_reasons.append("image_not_found")
            logger.info("[%s] figure %s failed: image_not_found", doc_id, idx_label)
            if progress_callback:
                progress_callback("enrich_figure", {"index": figure_index, "total": figure_total, "status": "fail", "message": "image_not_found"})
            return block

        with open(img_full, "rb") as f:
            b64 = base64.standard_b64encode(f.read()).decode("utf-8")

        prompt = FIGURE_INTERPRET_PROMPT.format(
            heading_path=" > ".join(block.heading_path) if block.heading_path else "(root)",
            caption=fd.caption or "(no caption)",
        )
        provider = self.config.llm_vision_provider
        model_override = self.config.llm_vision_model
        model_resolved = self._resolve_model(provider, model_override)
        logger.info(
            "[%s] figure %s start provider=%s model=%s",
            doc_id,
            idx_label,
            provider,
            model_resolved,
        )
        if progress_callback:
            progress_callback(
                "enrich_figure",
                {
                    "index": figure_index,
                    "total": figure_total,
                    "status": "start",
                    "message": f"{provider}/{model_resolved or ''}".strip("/"),
                },
            )
        prompt_hash = compute_hash(prompt)
        input_hash = compute_hash((fd.image_hash or "") + (fd.caption or ""))
        # Vision API 通常接受 content 为 list，包含 image_url 或 type=image_url
        user_content: list[dict] = [
            {"type": "text", "text": prompt},
            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{b64}"}},
        ]

        for attempt in range(self.config.llm_max_retries + 1):
            try:
                client = self._get_vision_client(fresh=use_fresh_client)
                resp = client.chat(
                    messages=[
                        {"role": "system", "content": "You are a scientific visualization expert. Return ONLY valid JSON."},
                        {"role": "user", "content": user_content},
                    ],
                    model=model_override,
                    max_tokens=self.config.llm_vision_max_tokens,
                    temperature=self.config.llm_temperature,
                    timeout_seconds=self.config.llm_call_timeout_seconds,
                    response_model=_FigureEnrichmentResponse,
                )
                parsed: Optional[_FigureEnrichmentResponse] = resp.get("parsed_object")
                text = (resp.get("final_text") or "").strip()
                if parsed is None and text:
                    try:
                        parsed = _FigureEnrichmentResponse.model_validate_json(text)
                    except Exception:
                        parsed = None
                if parsed is None:
                    repaired = self._repair_json(text, FIGURE_JSON_SCHEMA)
                    if repaired:
                        parsed = _FigureEnrichmentResponse.model_validate(repaired)
                if parsed:
                    interp = FigureInterpretation(
                        figure_type=parsed.figure_type,
                        description=parsed.description,
                        qa_pairs=parsed.qa_pairs[:3],
                        key_findings=parsed.key_findings[:5],
                        evidence=parsed.evidence[:5],
                    )
                    provider_used = resp.get("provider") or provider
                    model_used = resp.get("model") or model_resolved
                    block.enrichment = BlockEnrichment(
                        status=EnrichmentStatus.SUCCESS,
                        interpretation=interp,
                        provider=provider_used,
                        model=model_used,
                        prompt_hash=prompt_hash,
                        input_hash=input_hash,
                    )
                    logger.info("[%s] figure %s success", doc_id, idx_label)
                    if progress_callback:
                        progress_callback("enrich_figure", {"index": figure_index, "total": figure_total, "status": "success"})
                    return block
            except Exception as e:
                if attempt == self.config.llm_max_retries:
                    block.enrichment = BlockEnrichment(
                        status=EnrichmentStatus.FAILED,
                        error_message=str(e),
                        provider=provider,
                        model=model_resolved,
                        prompt_hash=prompt_hash,
                        input_hash=input_hash,
                    )
                    block.needs_review = True
                    block.review_reasons.append("llm_figure_enrich_failed")
                    logger.info("[%s] figure %s failed: %s", doc_id, idx_label, str(e)[:200])
                    if progress_callback:
                        progress_callback("enrich_figure", {"index": figure_index, "total": figure_total, "status": "fail", "message": str(e)[:200]})
                    return block
        block.enrichment = block.enrichment or BlockEnrichment(
            status=EnrichmentStatus.FAILED,
            error_message="max_retries",
            provider=provider,
            model=model_resolved,
            prompt_hash=prompt_hash,
            input_hash=input_hash,
        )
        logger.info("[%s] figure %s failed: max_retries", doc_id, idx_label)
        if progress_callback:
            progress_callback("enrich_figure", {"index": figure_index, "total": figure_total, "status": "fail", "message": "max_retries"})
        return block


# ============================================================
# PDFProcessor
# ============================================================


def _serialize_obj(obj: Any) -> Any:
    if isinstance(obj, Enum):
        return obj.value
    if isinstance(obj, tuple):
        return [_serialize_obj(x) for x in obj]
    if hasattr(obj, "__dict__") and not isinstance(obj, (str, dict, list, int, float, bool, type(None))):
        return {k: _serialize_obj(v) for k, v in obj.__dict__.items() if not k.startswith("_")}
    if isinstance(obj, dict):
        return {k: _serialize_obj(v) for k, v in obj.items()}
    if isinstance(obj, list):
        return [_serialize_obj(x) for x in obj]
    return obj


def _deserialize_enriched(raw: dict) -> EnrichedDoc:
    def to_block(d: dict) -> ContentBlock:
        bt = BlockType(d.get("block_type", "text"))
        td = None
        if d.get("table_data"):
            tdd = d["table_data"]
            td = TableData(
                markdown=tdd.get("markdown", ""),
                structured=tdd.get("structured", []),
                row_count=tdd.get("row_count", 0),
                col_count=tdd.get("col_count", 0),
                title=tdd.get("title"),
                footnotes=tdd.get("footnotes", []),
                computed_stats=tdd.get("computed_stats"),
            )
        fd = None
        if d.get("figure_data"):
            fdd = d["figure_data"]
            fd = FigureData(
                image_path=fdd.get("image_path", ""),
                image_hash=fdd.get("image_hash", ""),
                width_px=fdd.get("width_px", 0),
                height_px=fdd.get("height_px", 0),
                caption=fdd.get("caption"),
                ocr_text=fdd.get("ocr_text"),
            )
        interp = None
        if d.get("enrichment") and d["enrichment"].get("interpretation"):
            it = d["enrichment"]["interpretation"]
            interp = FigureInterpretation(
                figure_type=it.get("figure_type", "unknown"),
                description=it.get("description", ""),
                qa_pairs=it.get("qa_pairs", [])[:3],
                key_findings=it.get("key_findings", [])[:5],
                evidence=it.get("evidence", [])[:5],
            )
        enrichment = None
        if d.get("enrichment"):
            status_val = d["enrichment"].get("status")
            try:
                status_enum = EnrichmentStatus(status_val) if status_val else EnrichmentStatus.SKIPPED
            except Exception:
                status_enum = EnrichmentStatus.SKIPPED
            enrichment = BlockEnrichment(
                status=status_enum,
                semantic_summary=d["enrichment"].get("semantic_summary"),
                interpretation=interp,
                error_message=d["enrichment"].get("error_message"),
                provider=d["enrichment"].get("provider"),
                model=d["enrichment"].get("model"),
                prompt_hash=d["enrichment"].get("prompt_hash"),
                input_hash=d["enrichment"].get("input_hash"),
            )
        return ContentBlock(
            block_id=d.get("block_id", ""),
            block_type=bt,
            page_index=d.get("page_index", 0),
            bbox=tuple(d.get("bbox", [0, 0, 0, 0])),
            reading_order=d.get("reading_order", -1),
            column_index=d.get("column_index"),
            heading_path=d.get("heading_path", []),
            text=d.get("text"),
            table_data=td,
            figure_data=fd,
            enrichment=enrichment,
            needs_review=d.get("needs_review", False),
            review_reasons=d.get("review_reasons", []),
        )

    src = raw.get("source", {})
    source = SourceMeta(
        filename=src.get("filename", ""),
        file_hash=src.get("file_hash"),
        num_pages=src.get("num_pages", 0),
    )
    pm = raw.get("parse_meta", {})
    parse_meta = ParseMeta(parser_version=pm.get("parser_version", "1.0.0"), parse_errors=pm.get("parse_errors", []))
    pages = [
        PageData(
            page_index=p.get("page_index", 0),
            width=p.get("width", 612),
            height=p.get("height", 792),
            layout_type=LayoutType(p.get("layout_type", "single_column")),
            column_boundaries=p.get("column_boundaries"),
        )
        for p in raw.get("pages", [])
    ]
    content_flow = [to_block(b) for b in raw.get("content_flow", [])]
    return EnrichedDoc(
        doc_id=raw.get("doc_id", ""),
        source=source,
        global_summary=raw.get("global_summary"),
        pages=pages,
        content_flow=content_flow,
        hierarchy=[],  # 简化反序列化
        parse_meta=parse_meta,
        enrichment_meta=None,
        claims=raw.get("claims", []),
        doc_metadata=raw.get("doc_metadata"),
    )


_DOI_RE = re.compile(
    r"(?:doi[:\s]*|(?:https?://)?(?:dx\.)?doi\.org/)"
    r"?(10\.\d{4,9}/[^\s,;)\]\"'<>]+[^\s,;)\]\"'<>.:])",
    re.IGNORECASE,
)

_SKIP_HEADING_LABELS = {
    "original article", "research article", "review", "letter",
    "communication", "open", "open access", "article", "brief report",
    "research paper", "full paper", "short communication",
    "abstract", "abstract:", "data note", "introduction",
    "keywords", "highlights", "graphical abstract", "contents",
}


def extract_doc_metadata(blocks: list[ContentBlock], scan_blocks: int = 30) -> dict:
    """
    从 content_flow 前 N 个 block 提取论文级元数据（DOI + 标题）。
    在 PDFProcessor.process() 完成解析后调用。
    """
    doi = None
    title = None

    for block in blocks[:scan_blocks]:
        text = block.text
        if not text or not isinstance(text, str):
            continue
        m = _DOI_RE.search(text)
        if m:
            doi = m.group(1).rstrip(".:")
            break

    # Pass 1: heading blocks
    for block in blocks[:15]:
        bt = block.block_type.value if isinstance(block.block_type, Enum) else str(block.block_type or "")
        bt = bt.lower()
        text = (block.text or "").strip()
        if bt != "heading" or not text:
            continue
        if text.lower() in _SKIP_HEADING_LABELS:
            continue
        if 10 <= len(text) <= 300:
            title = text
            break

    # Pass 2: fallback to text blocks on page 0
    if not title:
        for block in blocks[:15]:
            bt = block.block_type.value if isinstance(block.block_type, Enum) else str(block.block_type or "")
            bt = bt.lower()
            if bt not in ("text", ""):
                continue
            text = (block.text or "").strip()
            page = getattr(block, "page_index", 0) or 0
            if page != 0 or not text or not (15 <= len(text) <= 300):
                continue
            tl = text.lower()
            if tl in _SKIP_HEADING_LABELS:
                continue
            if tl.startswith(("http", "www.", "\u00a9", "copyright")):
                continue
            if re.search(r"\d{4}\s*(international|society|elsevier|springer|wiley|nature)", tl):
                continue
            digit_ratio = sum(c.isdigit() for c in text) / max(len(text), 1)
            if digit_ratio > 0.08:
                continue
            title = text
            break

    result: dict = {}
    if doi:
        result["doi"] = doi
    if title:
        result["title"] = title
    return result or None


class PDFProcessor:
    def __init__(
        self,
        config: Optional[ParserConfig] = None,
        llm_manager: Optional[Any] = None,
    ):
        self.config = config or ParserConfig()
        self.llm_manager = llm_manager

    def process(
        self,
        pdf_path: str | Path,
        output_dir: Optional[str | Path] = None,
        skip_enrichment: bool = False,
        progress_callback: Optional[Callable[[str, dict], None]] = None,
    ) -> EnrichedDoc:
        pdf_path = Path(pdf_path)
        doc_id = pdf_path.stem
        out = output_dir or Path(self.config.output_dir) / doc_id
        out = Path(out)
        out.mkdir(parents=True, exist_ok=True)

        # 1. Parse
        parser = DoclingParser(self.config)
        blocks, pages, raw_out = parser.parse(pdf_path)
        source = SourceMeta(filename=pdf_path.name, num_pages=len(pages))
        parse_meta = ParseMeta(parser_version=self.config.parser_version)

        # 2. Layout
        layout = LayoutAnalyzer(self.config)
        blocks, pages = layout.analyze(blocks, pages)

        # 3. Context
        linker = ContextLinker(self.config)
        blocks, hierarchy = linker.link_all(blocks, pages)

        # 4. Extract figures
        figure_blocks = [
            b for b in blocks
            if b.block_type == BlockType.FIGURE and b.figure_data and b.figure_data.caption
        ]
        extractor = FigureExtractor(self.config)
        extractor.extract_figures(pdf_path, figure_blocks, out)

        # 5. LLM enrich
        enrichment_meta = None
        if not skip_enrichment and self.llm_manager:
            enricher = LLMEnricher(self.config, self.llm_manager)
            blocks, enrichment_meta = enricher.enrich_all(blocks, str(out), progress_callback=progress_callback)

        # 6. Extract doc-level metadata (DOI, title)
        doc_meta = extract_doc_metadata(blocks)
        if doc_meta:
            logger.info("doc_metadata for %s: doi=%s title=%s",
                        doc_id, doc_meta.get("doi", "-"), (doc_meta.get("title") or "-")[:60])

        doc = EnrichedDoc(
            doc_id=doc_id,
            source=source,
            pages=pages,
            content_flow=blocks,
            hierarchy=hierarchy,
            parse_meta=parse_meta,
            enrichment_meta=enrichment_meta,
            doc_metadata=doc_meta,
        )

        # 6. Claim extraction（需要 LLM）
        if not skip_enrichment and self.llm_manager:
            try:
                from src.parser.claim_extractor import ClaimExtractor
                claim_extractor = ClaimExtractor()
                client = self.llm_manager.get_client()
                claims = claim_extractor.extract(doc, client)
                doc.claims = [c.to_dict() for c in claims]
            except Exception as e:
                logger.warning("Claim extraction failed for %s: %s", doc_id, e)

        self.save(doc, str(out))
        return doc

    def save(self, doc: EnrichedDoc, output_dir: str | Path) -> str:
        out = Path(output_dir)
        out.mkdir(parents=True, exist_ok=True)
        data = _serialize_obj(doc)
        json_path = out / "enriched.json"
        with open(json_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        return str(json_path)

    @staticmethod
    def load(json_path: str | Path) -> EnrichedDoc:
        with open(json_path, "r", encoding="utf-8") as f:
            raw = json.load(f)
        return _deserialize_enriched(raw)
</file>

<file path="src/retrieval/smart_query_optimizer.py">
"""
LLM 驱动的智能查询优化器。

针对不同搜索引擎生成合适的搜索词：
- Google Scholar / Semantic Scholar: 学术关键词
- Google: 通用关键词
- Tavily: 自然语言问句

支持中英文双语（中文输入时生成英文查询）。
"""

import json
import re
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, ConfigDict, Field

from src.log import get_logger
from src.retrieval.query_optimizer import optimize_query

logger = get_logger(__name__)


# ── 时效性关键词（启发式）────────────────────────────────────────────────────

_FRESH_RE_EN = re.compile(
    r"\b(latest|recent(?:ly)?|newest|current(?:ly)?|today|this\s+week|this\s+month|"
    r"this\s+year|20(24|25|26)|breaking|trending|just\s+released?|"
    r"new\s+(?:research|study|paper|report|development)|state[\s-]of[\s-]the[\s-]art\s+2025|"
    r"cutting[\s-]edge|hot\s+topic)\b",
    re.IGNORECASE,
)
_FRESH_RE_ZH = re.compile(
    r"(最新|近期|最近|今年|今天|本年|当前|新进展|最新进展|新研究|最新研究|"
    r"最新动态|最新消息|最新进展|最近发展|刚发布|刚出|新出)"
)


def _is_fresh_query_heuristic(query: str) -> bool:
    """启发式检测查询是否具有时效性（LLM 路由的前置保险）。"""
    text = query or ""
    return bool(_FRESH_RE_EN.search(text) or _FRESH_RE_ZH.search(text))


# ── 路由计划数据类 ────────────────────────────────────────────────────────────

@dataclass
class RoutingPlan:
    """
    代价感知路由计划。

    primary:   首选引擎列表（先并发执行）
    fallback:  当 primary 结果 < min_results 时自动启动的备选引擎列表
    queries:   {engine: [query, ...]}，覆盖 primary + fallback 所有引擎
    is_fresh:  是否为时效性查询（true → tavily 出现在 primary）
    min_results: 判定"结果充足"的最低条数，不足则触发 fallback
    """

    primary: List[str] = field(default_factory=list)
    fallback: List[str] = field(default_factory=list)
    queries: Dict[str, List[str]] = field(default_factory=dict)
    is_fresh: bool = False
    min_results: int = 3


def _get_smart_optimizer_config() -> Dict[str, Any]:
    """从 config 读取 smart_optimizer 配置（settings 中 WebSearchConfig 无此字段，从 JSON 读）"""
    default = _default_smart_config()
    try:
        config_path = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"
        if config_path.exists():
            with open(config_path, "r", encoding="utf-8") as f:
                raw = json.load(f)
            ws = raw.get("web_search") or {}
            so = ws.get("smart_optimizer")
            if isinstance(so, dict):
                return {
                    "enabled": so.get("enabled", default["enabled"]),
                    "llm_provider": (so.get("llm_provider") or default["llm_provider"]).strip(),
                    "max_queries_per_provider": min(int(so.get("max_queries_per_provider", 3)), 5),
                    "enable_bilingual": so.get("enable_bilingual", default["enable_bilingual"]),
                    "fallback_to_simple": so.get("fallback_to_simple", default["fallback_to_simple"]),
                }
    except Exception:
        pass
    return default


def _default_smart_config() -> Dict[str, Any]:
    return {
        "enabled": True,
        "llm_provider": "deepseek",
        "max_queries_per_provider": 3,
        "enable_bilingual": True,
        "fallback_to_simple": True,
    }


# ── Pydantic Response Models (结构化输出) ────────────────────────────────────

class _DynamicQueryResponse(BaseModel):
    """查询优化响应：引擎名作为动态 key，接受任意额外字段。"""
    model_config = ConfigDict(extra="allow")


class _RoutingPlanLLMResponse(BaseModel):
    """路由计划 LLM 响应结构。"""
    is_fresh: bool = False
    primary: Dict[str, Any] = Field(default_factory=dict)
    fallback: Dict[str, Any] = Field(default_factory=dict)


def _is_chinese(text: str) -> bool:
    return bool(re.search(r"[\u4e00-\u9fff]", text or ""))


def _normalize_list(raw: Any) -> List[str]:
    if isinstance(raw, list):
        return [str(q).strip() for q in raw if q and str(q).strip()]
    if isinstance(raw, str) and raw.strip():
        return [raw.strip()]
    return []


def _fallback_en_query(provider: str, query: str) -> str:
    provider = (provider or "").lower()
    if provider in ("scholar", "semantic", "ncbi"):
        return query
    if provider == "tavily":
        return f"Overview of {query}"
    if provider == "google":
        return query
    return query


def _sanitize_query(provider: str, q: str) -> str:
    """Remove generic suffixes that can hurt snippet specificity for some engines."""
    text = (q or "").strip()
    p = (provider or "").lower()
    if not text:
        return text
    if p in ("scholar", "google", "semantic", "ncbi"):
        text = re.sub(r"\s+(review|survey|overview)\s*$", "", text, flags=re.I).strip()
    return text


class SmartQueryOptimizer:
    """
    LLM 驱动的智能查询优化器。

    为每个搜索引擎生成 2-3 个不同角度的搜索词，支持中英双语。
    """

    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self._config = config or _get_smart_optimizer_config()
        self._llm_client = None

    @property
    def enabled(self) -> bool:
        return bool(self._config.get("enabled", True))

    def _get_llm_client(
        self,
        llm_provider: Optional[str] = None,
        model_override: Optional[str] = None,
    ):
        """
        获取 LLM 客户端。
        优先使用外部传入的 llm_provider（跟随 UI 选择），否则回退到 config 中的固定值。
        """
        # 如果外部指定了 provider，每次创建新 client（不缓存，因为 provider 可能变化）
        if llm_provider:
            try:
                from src.llm import LLMManager
                cfg_path = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"
                manager = LLMManager.from_json(str(cfg_path))
                if not manager.is_available(llm_provider):
                    logger.warning(f"Smart optimizer: requested provider {llm_provider} not available, fallback")
                    return self._get_llm_client()  # 回退到默认
                return manager.get_client(llm_provider)
            except Exception as e:
                logger.warning(f"Smart optimizer: failed to use provider {llm_provider}: {e}")
                return self._get_llm_client()  # 回退到默认

        # 默认：使用 config 中的 provider（缓存）
        if self._llm_client is not None:
            return self._llm_client
        try:
            from src.llm import LLMManager
            cfg_path = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"
            manager = LLMManager.from_json(str(cfg_path))
            provider = (self._config.get("llm_provider") or "deepseek").strip()
            if not manager.is_available(provider):
                logger.warning(f"Smart optimizer: LLM provider {provider} not available")
                return None
            self._llm_client = manager.get_client(provider)
            return self._llm_client
        except Exception as e:
            logger.warning(f"Smart optimizer: failed to load LLM client: {e}")
            return None

    def optimize(
        self,
        query: str,
        providers: List[str],
        max_queries_per_provider: Optional[int] = None,
        llm_provider: Optional[str] = None,
        model_override: Optional[str] = None,
        auto_route: bool = False,
    ) -> Dict[str, List[str]]:
        """
        为搜索引擎生成优化查询词，支持两种工作模式：

        普通模式 (auto_route=False)：
            为 providers 中的**每个**引擎生成查询，前端手动选择，不改变引擎列表。

        代价感知路由模式 (auto_route=True)：
            LLM 从 providers（候选池）中**选出最合适的引擎子集**，
            并只为选中的引擎生成查询。返回 dict 的 key 即为实际选用的引擎。
            调用方应根据返回 keys 收窄实际搜索范围，避免滥用高代价 API。

        Returns:
            {engine: [query, ...], ...}
            auto_route=True 时只含被选中的引擎；False 时含全部 providers。
        """
        query = (query or "").strip()
        if not query:
            return {p: [query] for p in providers}

        if not self.enabled:
            return self._fallback_optimize(query, providers)

        client = self._get_llm_client(llm_provider=llm_provider, model_override=model_override)
        if not client:
            return self._fallback_optimize(query, providers)

        max_per = self._config.get("max_queries_per_provider", 3)
        if isinstance(max_queries_per_provider, int):
            max_per = min(max(max_queries_per_provider, 1), 5)
        bilingual = self._config.get("enable_bilingual", True)
        is_zh_input = _is_chinese(query)

        provider_list = ", ".join(providers)
        bilingual_instruction = ""
        if bilingual and is_zh_input:
            bilingual_instruction = (
                f" The input is Chinese. You MUST generate exactly {max_per} Chinese queries and "
                f"{max_per} English queries for each **selected** engine. "
                "Return them separately under keys \"zh\" and \"en\" for each engine."
            )

        # ── 代价感知路由模式 ─────────────────────────────────────────────────
        if auto_route:
            routing_rules = """**Cost-aware routing rules (STRICT):**
Select ONLY 1-2 most appropriate engines from the candidate list. Do NOT use all engines.
- **ncbi**: MUST select for biomedical / medicine / genetics / genomics / marine biology / marine ecology queries. Skip other academic engines when ncbi is selected.
- **scholar**: Select for general academic / scientific / interdisciplinary topics that are NOT purely biomedical.
- **semantic**: Select ONLY for highly specialized academic topics requiring deep literature depth AND when API budget allows (e.g. novel algorithms, niche technical terms). Do NOT select for common topics.
- **tavily**: Select for general/conceptual knowledge, current events, news, non-academic questions.
- **google**: Select for broad overviews, everyday concepts, general knowledge questions."""

            prompt = f"""You are a cost-aware search router and query optimizer.
Given the user query and a candidate list of search engines, first SELECT 1-2 best engines, then generate optimized queries for ONLY those selected engines.
Output ONLY a valid JSON object, no markdown.

**User query:** {query}

**Candidate engines:** {provider_list}

{routing_rules}

**Per-engine query style:**
- **ncbi**: Medical Subject Headings (MeSH) style keywords, 3-6 words, English preferred. Example: "cold seep methane bacteria", "coral reef bleaching gene expression".
- **scholar**: Short academic keyword phrases, 3-6 words. Avoid "review/survey/overview". Example: "deep sea cold seep biodiversity".
- **semantic**: Precise academic terms, no natural language. Example: "cold seep chemosynthesis ecosystem".
- **tavily**: Natural language questions. Example: "What causes coral reef bleaching?".
- **google**: Concise keywords. Example: "coral reef bleaching causes".
{bilingual_instruction}

**Output format:**
- Include ONLY the selected engines as JSON keys (omit all others).
- Each value: array of 1-{max_per} query strings (or zh/en object for Chinese input).

Example (biomedical query, English): {{"ncbi": ["cold seep methane oxidizing bacteria", "anaerobic methane oxidation archaea"]}}
Example (general academic, English): {{"scholar": ["deep learning image segmentation", "convolutional neural network segmentation"]}}
Example (everyday query, English): {{"tavily": ["What is machine learning and how does it work?"]}}
"""
        # ── 普通优化模式（尊重前端选择）────────────────────────────────────────
        else:
            prompt = f"""You are a search query optimizer. Given a user query and a list of search engines, generate 1-{max_per} optimized search queries **per engine**. Output ONLY a valid JSON object, no markdown.

**User query:** {query}

**Search engines to optimize for:** {provider_list}

**Rules per engine:**
- **ncbi**: Medical/biological keyword phrases (3-6 words), English preferred. MeSH-style. Example: "cold seep microbial diversity", "marine invertebrate chemosynthesis".
- **scholar**: Academic keywords, short keyword phrases (3-6 words). Avoid generic suffixes like "review/survey/overview". Example: "deep sea cold seep biodiversity", "cold seep chemosynthesis mechanism".
- **semantic**: Precise academic terms, no natural language. Example: "cold seep chemosynthesis", "cold seep ecosystem diversity".
- **google**: General keywords, concise. Avoid generic suffixes like "overview/review". Example: "deep sea cold seep characteristics", "cold seep definition".
- **tavily**: Natural language questions or full sentences. Example: "What are deep sea cold seeps and where do they occur?"
{bilingual_instruction}

**Output format:**
- If input is Chinese: each engine value is an object with keys **zh** and **en**, each an array of exactly {max_per} queries.
- Otherwise: each engine value is an array of 1-{max_per} query strings.
Only include keys for engines that are in the list above; omit others.

Example (Chinese input, providers include scholar and tavily):
{{"scholar": {{"zh": ["深海冷泉 生物多样性", "冷泉 生态", "冷泉 形成 机制"], "en": ["deep sea cold seep biodiversity", "cold seep ecology", "cold seep formation mechanism"]}}, "tavily": {{"zh": ["深海冷泉是什么？", "深海冷泉的生态作用是什么？", "深海冷泉如何形成？"], "en": ["What is a deep sea cold seep?", "What is the ecological role of cold seeps?", "How do cold seeps form?"]}}}}

Example (English input, providers include scholar and tavily):
{{"scholar": ["cold seep biodiversity", "deep sea cold seep ecosystem"], "tavily": ["What is a deep sea cold seep?"]}}
"""

        try:
            resp = client.chat(
                messages=[
                    {"role": "system", "content": "You output only valid JSON. No markdown, no explanation."},
                    {"role": "user", "content": prompt},
                ],
                model=model_override or None,
                max_tokens=800,
                response_model=_DynamicQueryResponse,
            )
            parsed_resp: Optional[_DynamicQueryResponse] = resp.get("parsed_object")
            if parsed_resp is None:
                raw_text = (resp.get("final_text") or "").strip()
                if raw_text:
                    parsed_resp = _DynamicQueryResponse.model_validate_json(raw_text)
            data: Optional[Dict[str, Any]] = parsed_resp.model_dump() if parsed_resp is not None else None
            if not data or not isinstance(data, dict):
                return self._fallback_optimize(query, providers)

            # auto_route：直接以 LLM 返回的 keys 为准（只处理候选集内的引擎）
            target_providers = (
                [p for p in providers if p in data] if auto_route else providers
            )
            if not target_providers:
                logger.warning("Auto-route: LLM 未选中任何候选引擎，回退到全量")
                return self._fallback_optimize(query, providers)

            out: Dict[str, List[str]] = {}
            for p in target_providers:
                raw = data.get(p)
                if bilingual and is_zh_input:
                    if isinstance(raw, dict):
                        zh = _normalize_list(raw.get("zh"))
                        en = _normalize_list(raw.get("en"))
                    else:
                        items = _normalize_list(raw)
                        zh = [q for q in items if _is_chinese(q)]
                        en = [q for q in items if not _is_chinese(q)]

                    if not zh:
                        zh = [query]
                    if not en:
                        en = [_fallback_en_query(p, query)]

                    while len(zh) < max_per:
                        zh.append(zh[-1])
                    while len(en) < max_per:
                        en.append(en[-1])

                    zh = zh[:max_per]
                    en = en[:max_per]
                    out[p] = [_sanitize_query(p, q) for q in (zh + en)]
                else:
                    queries = _normalize_list(raw)[:max_per]
                    if not queries:
                        queries = [query]
                    out[p] = [_sanitize_query(p, q) for q in queries]

            if auto_route:
                logger.info(f"代价感知路由选定引擎: {list(out.keys())}")
            return out
        except Exception as e:
            logger.warning(f"Smart query optimizer LLM call failed: {e}")
            return self._fallback_optimize(query, providers)

    def get_routing_plan(
        self,
        query: str,
        candidate_providers: List[str],
        max_queries_per_provider: Optional[int] = None,
        llm_provider: Optional[str] = None,
        model_override: Optional[str] = None,
    ) -> RoutingPlan:
        """
        生成代价感知路由计划。

        LLM 从候选引擎中选出：
        - primary：1-2 个最匹配的引擎（并发首发）
        - fallback：1 个备选引擎（primary 结果 < min_results 时启用）

        时效性保险：若启发式检测到查询含"最新/recent/latest"等，
        即使 LLM 未选 tavily，也会将其插入 primary（如果在候选池中）。
        """
        query = (query or "").strip()
        if not query or not candidate_providers:
            return RoutingPlan(primary=candidate_providers, queries={p: [query] for p in candidate_providers})

        is_fresh = _is_fresh_query_heuristic(query)
        max_per = self._config.get("max_queries_per_provider", 2)
        if isinstance(max_queries_per_provider, int):
            max_per = min(max(max_queries_per_provider, 1), 4)
        is_zh = _is_chinese(query)

        client = self._get_llm_client(llm_provider=llm_provider, model_override=model_override)
        if not client:
            return self._fallback_routing_plan(query, candidate_providers, is_fresh)

        bilingual_note = ""
        if is_zh:
            bilingual_note = (
                f" Input is Chinese. For each selected engine, generate {max_per} Chinese queries "
                f"AND {max_per} English queries. Return them as {{\"zh\": [...], \"en\": [...]}} objects."
            )

        provider_list = ", ".join(candidate_providers)
        prompt = f"""You are a cost-aware search router and query optimizer.
Given the user query and available search engines, output a JSON routing plan.
Output ONLY valid JSON. No markdown, no explanation.

**User query:** {query}
**Available engines:** {provider_list}

---
**STEP 1 — Freshness detection:**
If the query asks about "latest", "recent", "new", "current", "2025", "2026", trending topics,
or is clearly about a recent/breaking event → set "is_fresh": true AND put "tavily" in primary
(tavily has the freshest web index). Otherwise "is_fresh": false.

**STEP 2 — Domain routing (select PRIMARY 1-2 engines):**
- **ncbi**: MUST select for biomedical / medicine / genetics / genomics /
            marine biology / marine ecology / pharmacology queries.
- **tavily**: MUST select when is_fresh=true (fastest for breaking news & recent events).
              Also select for general factual questions, how-to, news.
- **scholar**: Select for general academic / scientific / interdisciplinary topics
               that are NOT purely biomedical and NOT time-sensitive.
- **semantic**: Select ONLY for highly specialized academic topics needing deep literature
                (niche algorithms, specific technical terms). Expensive — avoid for broad queries.
- **google**: Select for broad overviews, everyday concepts, general knowledge.

**STEP 3 — Select FALLBACK (1 engine, different domain from primary):**
Choose 1 backup engine. Rules:
- After ncbi → fallback to scholar (broader academic coverage)
- After tavily → fallback to scholar or google (more structured results)
- After scholar → fallback to tavily (fresh coverage) or ncbi (if bio-adjacent)
- After semantic → fallback to scholar
- Do NOT repeat a primary engine in fallback.

**STEP 4 — Generate queries:**
Generate 1-{max_per} optimized queries for each selected engine (primary + fallback).
Query styles:
- ncbi: MeSH-style medical keywords, 3-6 words, English preferred.
- scholar: Short academic keyword phrases, 3-6 words. No "review/survey/overview".
- semantic: Precise technical terms only, no natural language.
- tavily: Natural language question or full sentence. Prefer recency framing if is_fresh.
- google: Concise general keywords.{bilingual_note}

---
**Output JSON format:**
{{
  "is_fresh": <bool>,
  "primary": {{
    "<engine>": ["query1", "query2"]
  }},
  "fallback": {{
    "<engine>": ["query1"]
  }}
}}

Rules:
- primary can have 1-2 engines, fallback must have exactly 1 engine.
- Only include engines from the available list.
- For Chinese input: each engine value is {{"zh": [...], "en": [...]}} instead of an array.
- Do NOT include engines not in the available list.

Examples:
Query="latest breakthroughs in LLM 2026", engines=[tavily, scholar, ncbi]:
{{"is_fresh": true, "primary": {{"tavily": ["latest LLM breakthroughs 2026"]}}, "fallback": {{"scholar": ["large language model recent advances"]}}}}

Query="deep sea cold seep microbiology", engines=[ncbi, scholar, tavily, google]:
{{"is_fresh": false, "primary": {{"ncbi": ["cold seep microbial community methane oxidation"]}}, "fallback": {{"scholar": ["cold seep microbiology chemosynthesis"]}}}}

Query="如何治疗新冠肺炎", engines=[ncbi, tavily, scholar]:
{{"is_fresh": false, "primary": {{"ncbi": {{"zh": ["新冠肺炎 治疗方案"], "en": ["COVID-19 treatment clinical outcomes"]}}}}, "fallback": {{"scholar": {{"zh": ["新冠 临床治疗"], "en": ["SARS-CoV-2 treatment efficacy"]}}}}}}
"""

        try:
            resp = client.chat(
                messages=[
                    {"role": "system", "content": "You output only valid JSON. No markdown."},
                    {"role": "user", "content": prompt},
                ],
                model=model_override or None,
                max_tokens=700,
                response_model=_RoutingPlanLLMResponse,
            )
            parsed_plan: Optional[_RoutingPlanLLMResponse] = resp.get("parsed_object")
            if parsed_plan is None:
                raw_text = (resp.get("final_text") or "").strip()
                if raw_text:
                    parsed_plan = _RoutingPlanLLMResponse.model_validate_json(raw_text)
            data: Optional[Dict[str, Any]] = parsed_plan.model_dump() if parsed_plan is not None else None
            if not data or not isinstance(data, dict):
                return self._fallback_routing_plan(query, candidate_providers, is_fresh)

            llm_is_fresh = bool(data.get("is_fresh", is_fresh))
            # 时效性保险：启发式检测到 fresh 但 LLM 遗漏时强制插入 tavily
            if is_fresh and not llm_is_fresh:
                logger.info("时效性保险触发: 启发式检测为 fresh，覆盖 LLM 路由")
                llm_is_fresh = True

            raw_primary = data.get("primary") or {}
            raw_fallback = data.get("fallback") or {}
            if not isinstance(raw_primary, dict):
                raw_primary = {}
            if not isinstance(raw_fallback, dict):
                raw_fallback = {}
            candidate_set = {
                str(p).strip().lower()
                for p in candidate_providers
                if str(p).strip()
            }

            # 时效性保险：fresh 但 primary 没有 tavily → 强插
            primary_keys_norm = {str(k).strip().lower() for k in raw_primary.keys()}
            if llm_is_fresh and "tavily" in candidate_set and "tavily" not in primary_keys_norm:
                # 生成一个简单的时效性查询
                fresh_q = f"latest news {query}" if not _is_chinese(query) else f"最新进展 {query}"
                raw_primary["tavily"] = [fresh_q]
                for k in list(raw_fallback.keys()):
                    if str(k).strip().lower() == "tavily":
                        raw_fallback.pop(k, None)

            def _parse_engine_queries(raw: Any, engine: str) -> List[str]:
                if is_zh:
                    if isinstance(raw, dict):
                        zh = _normalize_list(raw.get("zh"))
                        en = _normalize_list(raw.get("en"))
                    else:
                        items = _normalize_list(raw)
                        zh = [q for q in items if _is_chinese(q)]
                        en = [q for q in items if not _is_chinese(q)]
                    if not zh:
                        zh = [query]
                    if not en:
                        en = [_fallback_en_query(engine, query)]
                    zh = (zh * max_per)[:max_per]
                    en = (en * max_per)[:max_per]
                    return [_sanitize_query(engine, q) for q in (zh + en)]
                else:
                    qs = _normalize_list(raw)[:max_per]
                    return [_sanitize_query(engine, q) for q in (qs or [query])]

            queries: Dict[str, List[str]] = {}
            primary_engines: List[str] = []
            for eng, raw_q in raw_primary.items():
                eng_norm = str(eng).strip().lower()
                if eng_norm not in candidate_set or eng_norm in primary_engines:
                    continue
                queries[eng_norm] = _parse_engine_queries(raw_q, eng_norm)
                primary_engines.append(eng_norm)

            fallback_engines: List[str] = []
            for eng, raw_q in raw_fallback.items():
                eng_norm = str(eng).strip().lower()
                if eng_norm not in candidate_set or eng_norm in primary_engines or eng_norm in fallback_engines:
                    continue
                queries[eng_norm] = _parse_engine_queries(raw_q, eng_norm)
                fallback_engines.append(eng_norm)

            if not primary_engines:
                return self._fallback_routing_plan(query, candidate_providers, llm_is_fresh)

            plan = RoutingPlan(
                primary=primary_engines,
                fallback=fallback_engines,
                queries=queries,
                is_fresh=llm_is_fresh,
                min_results=3,
            )
            logger.info(
                f"路由计划: primary={plan.primary}, fallback={plan.fallback}, "
                f"is_fresh={plan.is_fresh}"
            )
            return plan

        except Exception as e:
            logger.warning(f"get_routing_plan LLM call failed: {e}")
            return self._fallback_routing_plan(query, candidate_providers, is_fresh)

    def _fallback_routing_plan(
        self,
        query: str,
        providers: List[str],
        is_fresh: bool = False,
    ) -> RoutingPlan:
        """规则回退：基于简单规则生成路由计划（无需 LLM）。"""
        # 时效性：tavily 优先
        if is_fresh and "tavily" in providers:
            primary = ["tavily"]
            fallback = [p for p in ("scholar", "google", "ncbi") if p in providers][:1]
        elif "ncbi" in providers:
            primary = ["ncbi"]
            fallback = [p for p in ("scholar", "tavily") if p in providers][:1]
        elif "scholar" in providers:
            primary = ["scholar"]
            fallback = [p for p in ("tavily", "google") if p in providers][:1]
        elif "tavily" in providers:
            primary = ["tavily"]
            fallback = [p for p in ("google", "scholar") if p in providers][:1]
        else:
            primary = providers[:1]
            fallback = providers[1:2]

        queries = self._fallback_optimize(query, primary + fallback)
        return RoutingPlan(
            primary=primary,
            fallback=fallback,
            queries=queries,
            is_fresh=is_fresh,
            min_results=3,
        )

    def _fallback_optimize(self, query: str, providers: List[str]) -> Dict[str, List[str]]:
        """使用简单规则优化器作为 fallback"""
        if not self._config.get("fallback_to_simple", True):
            return {p: [query] for p in providers}
        return {
            p: [optimize_query(p, query) or query]
            for p in providers
        }


# 全局单例（懒加载配置）
_smart_optimizer_instance: Optional[SmartQueryOptimizer] = None


def get_smart_query_optimizer(config: Optional[Dict[str, Any]] = None) -> SmartQueryOptimizer:
    global _smart_optimizer_instance
    if _smart_optimizer_instance is None:
        _smart_optimizer_instance = SmartQueryOptimizer(config=config)
    return _smart_optimizer_instance
</file>

<file path="src/retrieval/unified_web_search.py">
"""
统一网络搜索聚合器

整合 Tavily、Google Scholar、Google、Semantic Scholar、NCBI 五种搜索来源，按来源权重去重。
输出格式与 hybrid_retriever 兼容，可直接送入 reranker。

使用方法:
---------
from src.retrieval.unified_web_search import unified_web_searcher

# 异步搜索（默认启用所有已配置的来源）
results = await unified_web_searcher.search("deep learning")

# 指定来源（前端手动选择，绝对尊重）
results = await unified_web_searcher.search("machine learning", providers=["scholar", "tavily", "semantic"])

# 智能自动路由（优化器选最优引擎）
results = await unified_web_searcher.search("deep sea cold seep", providers=["auto"])

# 同步搜索
results = unified_web_searcher.search_sync("deep learning")

来源权重（去重时优先保留权重高的）:
- ncbi:    0.98 (生物医学专库，最高)
- scholar: 1.0  (Google Scholar，最高)
- semantic: 0.95
- web/tavily: 0.8
- google: 0.6
"""

import asyncio
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional, Tuple

from config.settings import settings
from src.log import get_logger
from src.retrieval.smart_query_optimizer import RoutingPlan, get_smart_query_optimizer

logger = get_logger(__name__)


# 来源权重（去重时保留权重高的）
SOURCE_WEIGHTS = {
    "ncbi": 0.98,    # NCBI PubMed - 生物医学专库
    "scholar": 1.0,  # Google Scholar - 最高
    "semantic": 0.95,  # Semantic Scholar
    "web": 0.8,      # Tavily
    "tavily": 0.8,   # Tavily（别名）
    "google": 0.6,   # Google 普通搜索
}


def _get_source_weight(source: str) -> float:
    """获取来源权重"""
    return SOURCE_WEIGHTS.get(source, 0.5)


def _merge_and_dedup(all_hits: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    合并并去重搜索结果
    
    规则：
    - 按 URL 去重
    - 同一 URL 被多个来源返回时，保留权重高的来源版本
    - 不做排序（由后续 reranker 处理）
    
    Args:
        all_hits: 所有来源的搜索结果
    
    Returns:
        去重后的结果列表
    """
    # url -> (hit, source_weight)
    seen_urls: Dict[str, Tuple[Dict, float]] = {}
    # 无 URL 的结果单独收集
    no_url_hits: List[Dict] = []
    
    for hit in all_hits:
        metadata = hit.get("metadata", {}) or {}
        url = (metadata.get("url") or "").strip()
        source = metadata.get("source", "")
        weight = _get_source_weight(source)
        
        if not url:
            no_url_hits.append(hit)
            continue
        
        # URL 去重：保留权重高的
        if url not in seen_urls or weight > seen_urls[url][1]:
            seen_urls[url] = (hit, weight)
    
    # 合并结果（先有 URL 的，再无 URL 的）
    results = [item[0] for item in seen_urls.values()]
    results.extend(no_url_hits)
    
    return results


@dataclass
class UnifiedWebSearcher:
    """
    统一网络搜索聚合器

    整合 Tavily、Google Scholar、Google、Semantic Scholar、NCBI 五种搜索来源。
    """

    _tavily_searcher: Any = field(default=None, repr=False)
    _google_searcher: Any = field(default=None, repr=False)
    _semantic_searcher: Any = field(default=None, repr=False)
    _ncbi_searcher: Any = field(default=None, repr=False)
    
    def __post_init__(self):
        # 延迟加载搜索器
        self._content_fetcher = None
    
    def _get_content_fetcher(self):
        """获取 WebContentFetcher（延迟加载）"""
        if self._content_fetcher is None:
            try:
                from src.retrieval.web_content_fetcher import WebContentFetcher
                self._content_fetcher = WebContentFetcher.from_settings()
            except Exception as e:
                logger.warning(f"WebContentFetcher not available: {e}")
                self._content_fetcher = False
        return self._content_fetcher if self._content_fetcher else None

    def _get_tavily_searcher(self):
        """获取 Tavily 搜索器（延迟加载）"""
        if self._tavily_searcher is None:
            try:
                from src.retrieval.web_search import TavilySearcher
                self._tavily_searcher = TavilySearcher()
            except Exception as e:
                logger.warning(f"Tavily searcher not available: {e}")
                self._tavily_searcher = False  # 标记为不可用
        return self._tavily_searcher if self._tavily_searcher else None
    
    def _get_google_searcher(self):
        """获取 Google 搜索器（延迟加载）"""
        if self._google_searcher is None:
            try:
                from src.retrieval.google_search import GoogleSearcher
                self._google_searcher = GoogleSearcher()
            except Exception as e:
                logger.warning(f"Google searcher not available: {e}")
                self._google_searcher = False  # 标记为不可用
        return self._google_searcher if self._google_searcher else None

    def _get_semantic_searcher(self):
        """获取 Semantic Scholar 搜索器（延迟加载）"""
        if self._semantic_searcher is None:
            try:
                from src.retrieval.semantic_scholar import semantic_scholar_searcher
                self._semantic_searcher = semantic_scholar_searcher
            except Exception as e:
                logger.warning(f"Semantic Scholar searcher not available: {e}")
                self._semantic_searcher = False
        return self._semantic_searcher if self._semantic_searcher else None

    def _get_ncbi_searcher(self):
        """获取 NCBI PubMed 搜索器（延迟加载，无需 API Key）"""
        if self._ncbi_searcher is None:
            try:
                from src.retrieval.ncbi_search import get_ncbi_searcher
                self._ncbi_searcher = get_ncbi_searcher()
            except Exception as e:
                logger.warning(f"NCBI searcher not available: {e}")
                self._ncbi_searcher = False
        return self._ncbi_searcher if self._ncbi_searcher else None

    def _resolve_providers(self, providers: Optional[List[str]] = None) -> List[str]:
        """
        解析要使用的搜索来源。

        - providers=None 或含 "auto"：自动检测所有已启用的来源（交由优化器路由）
        - 其他显式列表：前端手动选择，绝对尊重，不允许优化器增减
        """
        if providers is not None:
            cleaned = [str(p).strip().lower() for p in providers if str(p).strip()]
            # 不含 "auto"：严格尊重前端选择
            if "auto" not in cleaned:
                return cleaned

        # auto 或 None：检测所有可用来源，作为优化器的候选池
        result = []

        # NCBI（免费 API，默认启用；可通过配置 ncbi.enabled=false 关闭）
        try:
            from config.settings import settings as _s
            ncbi_enabled = bool(getattr(getattr(_s, "ncbi", None), "enabled", True))
        except Exception:
            ncbi_enabled = True
        ncbi = self._get_ncbi_searcher()
        if ncbi and ncbi_enabled:
            result.append("ncbi")

        # Tavily
        tavily = self._get_tavily_searcher()
        if tavily and getattr(tavily, "enabled", False):
            result.append("tavily")

        # Google Scholar / Google
        google = self._get_google_searcher()
        if google:
            if getattr(google, "scholar_enabled", False):
                result.append("scholar")
            if getattr(google, "google_enabled", False):
                result.append("google")

        # Semantic Scholar
        semantic = self._get_semantic_searcher()
        if semantic and getattr(semantic, "enabled", False):
            result.append("semantic")

        return result
    
    async def search(
        self,
        query: str,
        providers: Optional[List[str]] = None,
        source_configs: Optional[Dict[str, Dict[str, Any]]] = None,
        max_results_per_provider: int = 5,
        use_query_expansion: Optional[bool] = None,
        use_query_optimizer: Optional[bool] = None,
        query_optimizer_max_queries: Optional[int] = None,
        llm_provider: Optional[str] = None,
        model_override: Optional[str] = None,
        use_content_fetcher: Optional[bool] = None,
        llm_client: Optional[Any] = None,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        异步搜索多个来源并合并去重
        
        Args:
            query: 搜索查询
            providers: 要使用的来源列表 ["scholar", "tavily", "google", "semantic"]，
                      None 表示使用所有已启用的来源
            source_configs: 每个来源的独立配置 {provider_id: {topK: int, threshold: float}}
            max_results_per_provider: 每个来源的最大结果数（当 source_configs 未指定时使用）
            use_query_expansion: 是否使用查询扩展（仅 Tavily 支持）
            use_query_optimizer: 是否启用智能查询优化器
            query_optimizer_max_queries: 优化器每个来源生成的最大查询数
            llm_client: LLMManager 客户端，用于 Lazy Fetching 的 LLM 预判
        
        Returns:
            合并去重后的结果列表
        """
        # 检测是否允许智能自动路由（providers=None 或含 "auto"）
        raw_providers = providers
        is_auto_route = raw_providers is None or (
            raw_providers is not None
            and "auto" in [p.strip().lower() for p in raw_providers if p]
        )
        providers = self._resolve_providers(raw_providers)

        if not providers:
            return []

        logger.info(
            f"统一搜索: query={query!r}, providers={providers}, auto_route={is_auto_route}"
        )

        source_configs = source_configs or {}

        optimizer_enabled = use_query_optimizer
        if optimizer_enabled is None:
            optimizer_enabled = bool(
                getattr(getattr(settings, "web_search", None), "enable_query_optimizer", True)
            )

        smart_optimizer = get_smart_query_optimizer()
        use_smart = optimizer_enabled and smart_optimizer.enabled

        all_hits: List[Dict[str, Any]] = []

        # ── 代价感知路由模式（auto） ────────────────────────────────────────────
        if is_auto_route and use_smart:
            plan: RoutingPlan = smart_optimizer.get_routing_plan(
                query,
                providers,
                max_queries_per_provider=query_optimizer_max_queries,
                llm_provider=llm_provider,
                model_override=model_override,
            )
            logger.info(
                f"路由计划执行: primary={plan.primary}, fallback={plan.fallback}, "
                f"is_fresh={plan.is_fresh}"
            )

            # 执行 primary 引擎
            primary_hits = await self._run_providers(
                plan.primary,
                plan.queries,
                query,
                source_configs,
                max_results_per_provider,
                year_start=year_start,
                year_end=year_end,
            )
            primary_unique_hits = _merge_and_dedup(primary_hits)
            all_hits.extend(primary_unique_hits)

            # 结果不足时自动启动 fallback 引擎
            if len(primary_unique_hits) < plan.min_results and plan.fallback:
                logger.info(
                    f"Primary 结果不足 ({len(primary_unique_hits)}/{plan.min_results})，"
                    f"启动 fallback: {plan.fallback}"
                )
                fallback_hits = await self._run_providers(
                    plan.fallback,
                    plan.queries,
                    query,
                    source_configs,
                    max_results_per_provider,
                    year_start=year_start,
                    year_end=year_end,
                )
                all_hits.extend(fallback_hits)
                logger.info(f"Fallback 补充 {len(fallback_hits)} 条，合计 {len(all_hits)} 条")

        # ── 普通模式（前端手动指定引擎）──────────────────────────────────────────
        else:
            queries_per_provider: Dict[str, List[str]] = {}
            if use_smart:
                queries_per_provider = smart_optimizer.optimize(
                    query,
                    providers,
                    max_queries_per_provider=query_optimizer_max_queries,
                    llm_provider=llm_provider,
                    model_override=model_override,
                    auto_route=False,
                )
                logger.info(f"Smart optimizer 生成多组查询: {list(queries_per_provider.keys())}")

            all_hits = await self._run_providers(
                providers,
                queries_per_provider,
                query,
                source_configs,
                max_results_per_provider,
                year_start=year_start,
                year_end=year_end,
            )

        # 合并去重
        merged = _merge_and_dedup(all_hits)
        logger.info(f"统一搜索完成: 合并前 {len(all_hits)} 条, 去重后 {len(merged)} 条")

        # 全文抓取增强：
        #   True  → 硬强制，全量抓取，不做 LLM 预判
        #   False → 跳过
        #   None  → 智能模式：有 llm_client 则先预判，否则按后端 enabled 配置全量抓
        fetcher = self._get_content_fetcher()
        do_enrich = use_content_fetcher is True or (
            use_content_fetcher is None and fetcher and getattr(fetcher, "enabled", False)
        )
        if do_enrich and fetcher:
            try:
                merged = await fetcher.enrich_results(
                    merged,
                    query=query,
                    llm_client=llm_client,
                    use_content_fetcher=use_content_fetcher,
                )
                full_count = sum(
                    1 for h in merged
                    if (h.get("metadata") or {}).get("content_type") == "full_text"
                )
                sufficient_count = sum(
                    1 for h in merged
                    if (h.get("metadata") or {}).get("content_type") == "snippet_sufficient"
                )
                logger.info(
                    f"全文抓取完成: {full_count} 条全文 / {sufficient_count} 条摘要已足够 / "
                    f"{len(merged)} 条总计"
                )
            except Exception as e:
                logger.warning(f"全文抓取失败，使用原始片段: {e}")

        return merged
    
    async def _search_tavily(
        self,
        searcher,
        query: str,
        limit: int,
        use_query_expansion: bool,
    ) -> List[Dict[str, Any]]:
        """Tavily 搜索"""
        try:
            results = await searcher.async_search(query, use_query_expansion=use_query_expansion)
            return results[:limit]
        except Exception as e:
            logger.error(f"Tavily 搜索失败: {e}")
            return []
    
    async def _search_scholar(
        self,
        searcher,
        query: str,
        limit: int,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """Google Scholar 搜索"""
        try:
            return await searcher.search_scholar(
                query,
                limit=limit,
                year_start=year_start,
                year_end=year_end,
            )
        except Exception as e:
            logger.error(f"Google Scholar 搜索失败: {e}")
            return []
    
    async def _search_google(
        self,
        searcher,
        query: str,
        limit: int,
    ) -> List[Dict[str, Any]]:
        """Google 搜索"""
        try:
            return await searcher.search_google(query, limit=limit)
        except Exception as e:
            logger.error(f"Google 搜索失败: {e}")
            return []

    async def _search_scholar_batch(
        self,
        searcher,
        queries: List[str],
        limit_per_query: int,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """Google Scholar 批量搜索（串行执行）"""
        try:
            logger.info(f"Scholar 批量搜索：{len(queries)} 个查询，每个最多 {limit_per_query} 条")
            return await searcher.search_scholar_batch(
                queries,
                limit_per_query=limit_per_query,
                year_start=year_start,
                year_end=year_end,
            )
        except Exception as e:
            logger.error(f"Google Scholar 批量搜索失败: {e}")
            return []

    async def _search_google_batch(
        self,
        searcher,
        queries: List[str],
        limit_per_query: int,
    ) -> List[Dict[str, Any]]:
        """Google 批量搜索（串行执行）"""
        try:
            logger.info(f"Google 批量搜索：{len(queries)} 个查询，每个最多 {limit_per_query} 条")
            return await searcher.search_google_batch(queries, limit_per_query=limit_per_query)
        except Exception as e:
            logger.error(f"Google 批量搜索失败: {e}")
            return []

    async def _search_semantic(
        self,
        searcher,
        query: str,
        limit: int,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """Semantic Scholar 搜索"""
        try:
            return await searcher.search(query, limit=limit, year_start=year_start, year_end=year_end)
        except Exception as e:
            logger.error(f"Semantic Scholar 搜索失败: {e}")
            return []

    async def _search_ncbi(
        self,
        searcher,
        query: str,
        limit: int,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """NCBI PubMed 搜索"""
        try:
            return await searcher.search(query, limit=limit, year_start=year_start, year_end=year_end)
        except Exception as e:
            logger.error(f"NCBI 搜索失败: {e}")
            return []

    async def _run_providers(
        self,
        providers: List[str],
        queries_per_provider: Dict[str, List[str]],
        default_query: str,
        source_configs: Dict[str, Any],
        max_results_per_provider: int,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        并发执行指定引擎列表，返回所有命中结果（未去重）。

        - providers: 要执行的引擎列表
        - queries_per_provider: {engine: [query, ...]}；缺失时回退到 default_query
        - Scholar/Google 走批量浏览器方法（串行，防风控）
        - 其他引擎全并发
        """
        if not providers:
            return []

        def _get_max(p: str) -> int:
            return (source_configs.get(p) or {}).get("topK", max_results_per_provider)

        def _queries_for(p: str) -> List[str]:
            qs = queries_per_provider.get(p)
            return qs if qs else [default_query]

        tasks_with_flags: List[tuple] = []
        scholar_queries: List[str] = []
        google_queries: List[str] = []
        scholar_max = max_results_per_provider
        google_max = max_results_per_provider

        for provider in providers:
            qs = _queries_for(provider)
            pmax = _get_max(provider)

            if provider == "tavily":
                tavily = self._get_tavily_searcher()
                if tavily and getattr(tavily, "enabled", False):
                    for q in qs:
                        tasks_with_flags.append(
                            (self._search_tavily(tavily, q, pmax, False), False)
                        )
            elif provider == "scholar":
                google = self._get_google_searcher()
                if google and getattr(google, "scholar_enabled", False):
                    scholar_queries.extend(qs)
                    scholar_max = pmax
            elif provider == "google":
                google = self._get_google_searcher()
                if google and getattr(google, "google_enabled", False):
                    google_queries.extend(qs)
                    google_max = pmax
            elif provider == "semantic":
                semantic = self._get_semantic_searcher()
                if semantic and getattr(semantic, "enabled", False):
                    for q in qs:
                        tasks_with_flags.append(
                            (
                                self._search_semantic(
                                    semantic,
                                    q,
                                    pmax,
                                    year_start=year_start,
                                    year_end=year_end,
                                ),
                                False,
                            )
                        )
            elif provider == "ncbi":
                ncbi = self._get_ncbi_searcher()
                if ncbi:
                    for q in qs:
                        tasks_with_flags.append(
                            (
                                self._search_ncbi(
                                    ncbi,
                                    q,
                                    pmax,
                                    year_start=year_start,
                                    year_end=year_end,
                                ),
                                False,
                            )
                        )

        if scholar_queries:
            gsearcher = self._get_google_searcher()
            if gsearcher:
                tasks_with_flags.append(
                    (
                        self._search_scholar_batch(
                            gsearcher,
                            scholar_queries,
                            scholar_max,
                            year_start=year_start,
                            year_end=year_end,
                        ),
                        True,
                    )
                )

        if google_queries:
            gsearcher = self._get_google_searcher()
            if gsearcher:
                tasks_with_flags.append(
                    (self._search_google_batch(gsearcher, google_queries, google_max), True)
                )

        if not tasks_with_flags:
            return []

        perf = getattr(settings, "perf_unified_web", None)
        max_parallel = getattr(perf, "max_parallel_providers", 3) or 3
        timeout_s = getattr(perf, "per_provider_timeout_seconds", 30) or 30
        browser_max = getattr(perf, "browser_providers_max_parallel", 1) or 1
        max_browser_q = max(len(scholar_queries), len(google_queries), 1)
        browser_timeout = max(timeout_s * max_browser_q, 120)

        sem = asyncio.Semaphore(max_parallel)
        sem_browser = asyncio.Semaphore(browser_max)

        async def _run_one(coro, is_browser: bool):
            if is_browser:
                async with sem_browser:
                    async with sem:
                        return await asyncio.wait_for(coro, timeout=float(browser_timeout))
            else:
                async with sem:
                    return await asyncio.wait_for(coro, timeout=float(timeout_s))

        hits: List[Dict[str, Any]] = []
        wrapped = [_run_one(coro, is_b) for coro, is_b in tasks_with_flags]
        results = await asyncio.gather(*wrapped, return_exceptions=True)
        for r in results:
            if isinstance(r, (asyncio.TimeoutError, asyncio.CancelledError)):
                logger.warning("搜索任务超时或取消")
            elif isinstance(r, Exception):
                logger.error(f"搜索任务出错: {r}")
            elif isinstance(r, list):
                hits.extend(r)
        return hits

    def search_sync(
        self,
        query: str,
        providers: Optional[List[str]] = None,
        source_configs: Optional[Dict[str, Dict[str, Any]]] = None,
        max_results_per_provider: int = 5,
        use_query_expansion: Optional[bool] = None,
        use_query_optimizer: Optional[bool] = None,
        query_optimizer_max_queries: Optional[int] = None,
        llm_provider: Optional[str] = None,
        model_override: Optional[str] = None,
        use_content_fetcher: Optional[bool] = None,
        llm_client: Optional[Any] = None,
        year_start: Optional[int] = None,
        year_end: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        同步搜索（在 executor 中运行异步版本）
        """
        _search_kwargs = dict(
            query=query,
            providers=providers,
            source_configs=source_configs,
            max_results_per_provider=max_results_per_provider,
            use_query_expansion=use_query_expansion,
            use_query_optimizer=use_query_optimizer,
            query_optimizer_max_queries=query_optimizer_max_queries,
            llm_provider=llm_provider,
            model_override=model_override,
            use_content_fetcher=use_content_fetcher,
            llm_client=llm_client,
            year_start=year_start,
            year_end=year_end,
        )
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                # 在已有事件循环中，使用 run_in_executor
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(
                        asyncio.run,
                        self.search(**_search_kwargs)
                    )
                    return future.result()
            else:
                return loop.run_until_complete(self.search(**_search_kwargs))
        except RuntimeError:
            # 没有事件循环，创建新的
            return asyncio.run(self.search(**_search_kwargs))


# 全局单例
unified_web_searcher = UnifiedWebSearcher()
</file>

<file path="src/retrieval/web_content_fetcher.py">
"""
WebContentFetcher — 网络搜索结果全文抓取模块

三级提取策略自动降级：
1. trafilatura — 纯 HTTP 抓取 + 正文提取，轻量快速，无需浏览器
2. BrightData Web Unlocker — 对付反爬站点，需配置 API Key
3. Playwright 浏览器 — JS 渲染页面，含 Cloudflare 检测、stealth 反检测

URL 过滤机制：
- is_qualifying_url() 自动跳过搜索引擎、社交媒体、二进制文件、登录页
- 支持 only_academic=True 模式仅提取学术域名

与 RAG 项目完全兼容：
- 输入/输出格式与 UnifiedWebSearcher 一致（List[Dict]，含 content + metadata）
- 提取成功后回写 hit["content"] 为全文，原始片段保留在 metadata.original_snippet
- 通过 metadata.content_type = "full_text" 标记已提取的结果
- 支持 TTLCache、get_logger、from_settings() 配置加载
- 提供 enrich_results_sync() 同步版本兼容 RetrievalService

使用方法:
---------
from src.retrieval.web_content_fetcher import WebContentFetcher

# 从配置加载
fetcher = WebContentFetcher.from_settings()

# 异步增强搜索结果
enriched = await fetcher.enrich_results(results, query="deep sea microbiome")

# 同步版本（兼容 RetrievalService）
enriched = fetcher.enrich_results_sync(results, query="deep sea microbiome")
"""

import asyncio
import concurrent.futures
import json
import re
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional
from urllib.parse import urlparse

from pydantic import BaseModel, Field

from src.log import get_logger
from src.utils.cache import TTLCache, _make_key, get_cache

logger = get_logger(__name__)


class _FetchDecisionResponse(BaseModel):
    urls_to_fetch: List[str] = Field(default_factory=list)

# ============================================================
# URL 过滤常量
# ============================================================

# 跳过的域名（搜索引擎、社交媒体、视频、论坛等）
SKIP_DOMAINS = {
    # 搜索引擎
    "google.com", "google.co", "bing.com", "yahoo.com", "baidu.com",
    "duckduckgo.com", "yandex.com", "sogou.com",
    # 社交媒体
    "twitter.com", "x.com", "facebook.com", "linkedin.com",
    "weibo.com", "zhihu.com",
    # 视频 / 图片
    "youtube.com", "tiktok.com", "instagram.com", "vimeo.com",
    # 论坛 / 问答
    "reddit.com", "quora.com", "stackexchange.com", "stackoverflow.com",
    # 代码托管
    "github.com", "gitlab.com", "bitbucket.org",
}

# 域名中包含这些子串时跳过（登录页等）
SKIP_DOMAIN_SUBSTRINGS = {"login.", "signin.", "auth.", "account.", "sso."}

# 跳过的文件扩展名（二进制 / 多媒体）
SKIP_EXTENSIONS = {
    ".pdf", ".zip", ".tar", ".gz", ".bz2", ".xz", ".rar", ".7z",
    ".exe", ".dmg", ".msi", ".deb", ".rpm",
    ".mp4", ".mp3", ".avi", ".mov", ".mkv", ".wav", ".flac",
    ".jpg", ".jpeg", ".png", ".gif", ".bmp", ".svg", ".webp",
    ".doc", ".docx", ".xls", ".xlsx", ".ppt", ".pptx",
}

# 学术域名白名单（only_academic 模式）
ACADEMIC_DOMAINS = {
    "scholar.google.com", "arxiv.org", "pubmed.ncbi.nlm.nih.gov",
    "ncbi.nlm.nih.gov", "doi.org", "dx.doi.org",
    "sciencedirect.com", "springer.com", "link.springer.com",
    "wiley.com", "onlinelibrary.wiley.com",
    "nature.com", "science.org", "pnas.org",
    "cell.com", "thelancet.com", "bmj.com",
    "frontiersin.org", "mdpi.com", "plos.org", "peerj.com",
    "academic.oup.com", "tandfonline.com", "sagepub.com",
    "biomedcentral.com", "biorxiv.org", "medrxiv.org",
    "researchgate.net", "semanticscholar.org",
    "jstor.org", "ieee.org", "ieeexplore.ieee.org",
    "acm.org", "dl.acm.org",
}

# 学术域名子串匹配
ACADEMIC_DOMAIN_SUBSTRINGS = {
    "scholar", "arxiv", "pubmed", "doi.org", "sciencedirect",
    "springer", "wiley", "nature.com", "science.org",
    "ncbi.nlm.nih.gov", "frontiersin", "mdpi", "plos",
    "biomedcentral", "biorxiv", "medrxiv", "researchgate",
    "semanticscholar", "ieee", "acm.org",
}


def _extract_domain(url: str) -> str:
    """从 URL 提取域名（不含端口）"""
    try:
        parsed = urlparse(url)
        return (parsed.hostname or "").lower()
    except Exception:
        return ""


def _get_path_ext(url: str) -> str:
    """从 URL 路径提取扩展名"""
    try:
        parsed = urlparse(url)
        path = parsed.path or ""
        dot_idx = path.rfind(".")
        if dot_idx >= 0:
            return path[dot_idx:].lower().split("?")[0].split("#")[0]
    except Exception:
        pass
    return ""


def is_qualifying_url(url: str, only_academic: bool = False) -> bool:
    """
    判断 URL 是否值得抓取全文。

    过滤逻辑：
    - 跳过搜索引擎、社交媒体、视频、二进制文件、登录页
    - only_academic=True 时仅保留学术域名

    Args:
        url: 待检查的 URL
        only_academic: 是否仅允许学术域名

    Returns:
        True 表示可以抓取
    """
    if not url or not url.startswith(("http://", "https://")):
        return False

    domain = _extract_domain(url)
    if not domain:
        return False

    # 检查域名子串黑名单（登录页等）
    for sub in SKIP_DOMAIN_SUBSTRINGS:
        if sub in domain:
            return False

    # 检查域名黑名单
    for skip in SKIP_DOMAINS:
        if domain == skip or domain.endswith("." + skip):
            return False

    # 检查文件扩展名
    ext = _get_path_ext(url)
    if ext in SKIP_EXTENSIONS:
        return False

    # 学术域名过滤
    if only_academic:
        # 精确匹配
        for acad in ACADEMIC_DOMAINS:
            if domain == acad or domain.endswith("." + acad):
                return True
        # 子串匹配
        for sub in ACADEMIC_DOMAIN_SUBSTRINGS:
            if sub in domain:
                return True
        return False

    return True


# ============================================================
# WebContentFetcher
# ============================================================

@dataclass
class WebContentFetcher:
    """
    网络搜索结果全文抓取器。

    三级提取策略自动降级：trafilatura → BrightData → Playwright
    """

    enabled: bool = True
    only_academic: bool = False
    max_content_length: int = 8000
    timeout_seconds: int = 15
    brightdata_api_key: str = ""
    brightdata_zone: str = ""
    cache_enabled: bool = True
    cache_ttl_seconds: int = 3600
    max_concurrent: int = 5
    _cache: Optional[TTLCache] = field(default=None, repr=False, init=False)

    def __post_init__(self):
        if self.cache_enabled:
            self._cache = get_cache(
                enabled=True,
                ttl_seconds=self.cache_ttl_seconds,
                maxsize=512,
                prefix="content_fetcher",
            )

    @classmethod
    def from_settings(cls) -> "WebContentFetcher":
        """从 config/settings.py 全局 settings 加载配置"""
        try:
            from config.settings import settings
            cfg = getattr(settings, "content_fetcher", None)
            if cfg is not None:
                return cls(
                    enabled=getattr(cfg, "enabled", False),
                    only_academic=getattr(cfg, "only_academic", False),
                    max_content_length=getattr(cfg, "max_content_length", 8000),
                    timeout_seconds=getattr(cfg, "timeout_seconds", 15),
                    brightdata_api_key=getattr(cfg, "brightdata_api_key", ""),
                    brightdata_zone=getattr(cfg, "brightdata_zone", ""),
                    cache_enabled=getattr(cfg, "cache_enabled", True),
                    cache_ttl_seconds=getattr(cfg, "cache_ttl_seconds", 3600),
                    max_concurrent=getattr(cfg, "max_concurrent", 5),
                )
        except Exception as e:
            logger.warning(f"从 settings 加载 content_fetcher 配置失败: {e}")
        return cls(enabled=False)

    # --------------------------------------------------------
    # 第一级：trafilatura（纯 HTTP + 正文提取）
    # --------------------------------------------------------

    async def _fetch_trafilatura(self, url: str) -> Optional[str]:
        """
        使用 trafilatura 抓取并提取正文。

        轻量快速，不启动浏览器，适合绝大多数正常网页。
        """
        try:
            import trafilatura

            loop = asyncio.get_event_loop()
            # trafilatura.fetch_url 是同步的，放到 executor 避免阻塞
            downloaded = await asyncio.wait_for(
                loop.run_in_executor(
                    None,
                    lambda: trafilatura.fetch_url(url),
                ),
                timeout=self.timeout_seconds,
            )
            if not downloaded:
                return None

            text = await loop.run_in_executor(
                None,
                lambda: trafilatura.extract(
                    downloaded,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False,
                    favor_precision=True,
                ),
            )
            if text and len(text.strip()) > 100:
                logger.debug(f"trafilatura 成功: {url} ({len(text)} chars)")
                return text.strip()
            return None
        except asyncio.TimeoutError:
            logger.debug(f"trafilatura 超时: {url}")
            return None
        except ImportError:
            logger.warning("trafilatura 未安装，跳过第一级提取")
            return None
        except Exception as e:
            logger.debug(f"trafilatura 失败: {url} - {e}")
            return None

    # --------------------------------------------------------
    # 第二级：BrightData Web Unlocker
    # --------------------------------------------------------

    async def _fetch_brightdata(self, url: str) -> Optional[str]:
        """
        使用 BrightData Web Unlocker API 抓取。

        适合反爬站点，需要配置 brightdata_api_key。
        """
        if not self.brightdata_api_key:
            return None

        try:
            import aiohttp
            import trafilatura

            zone = self.brightdata_zone or "web_unlocker1"
            proxy_url = f"https://brd-customer-{self.brightdata_api_key}-zone-{zone}:@brd.superproxy.io:33335"

            async with aiohttp.ClientSession() as session:
                async with session.get(
                    url,
                    proxy=proxy_url,
                    timeout=aiohttp.ClientTimeout(total=self.timeout_seconds),
                    headers={"User-Agent": "Mozilla/5.0 (compatible; DeepSeaRAG/1.0)"},
                    ssl=False,
                ) as resp:
                    if resp.status != 200:
                        logger.debug(f"BrightData 返回 {resp.status}: {url}")
                        return None
                    html = await resp.text()

            if not html or len(html) < 200:
                return None

            # 用 trafilatura 从 HTML 提取正文
            loop = asyncio.get_event_loop()
            text = await loop.run_in_executor(
                None,
                lambda: trafilatura.extract(
                    html,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False,
                ),
            )
            if text and len(text.strip()) > 100:
                logger.debug(f"BrightData 成功: {url} ({len(text)} chars)")
                return text.strip()
            return None
        except asyncio.TimeoutError:
            logger.debug(f"BrightData 超时: {url}")
            return None
        except ImportError as e:
            logger.debug(f"BrightData 依赖缺失: {e}")
            return None
        except Exception as e:
            logger.debug(f"BrightData 失败: {url} - {e}")
            return None

    # --------------------------------------------------------
    # 第三级：Playwright 浏览器
    # --------------------------------------------------------

    async def _fetch_playwright(self, url: str) -> Optional[str]:
        """
        使用 Playwright headless 浏览器抓取。

        支持 JS 渲染页面，含 stealth 反检测。
        浏览器实例临时创建，不复用 google_search.py 的全局浏览器。
        """
        try:
            from playwright.async_api import async_playwright

            # 可选 stealth 插件
            stealth_fn = None
            try:
                from playwright_stealth import stealth_async
                stealth_fn = stealth_async
            except ImportError:
                pass

            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                try:
                    context = await browser.new_context(
                        user_agent=(
                            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
                            "AppleWebKit/537.36 (KHTML, like Gecko) "
                            "Chrome/120.0.0.0 Safari/537.36"
                        ),
                        viewport={"width": 1280, "height": 800},
                    )
                    page = await context.new_page()

                    if stealth_fn:
                        await stealth_fn(page)

                    await page.goto(
                        url,
                        wait_until="domcontentloaded",
                        timeout=self.timeout_seconds * 1000,
                    )

                    # 等待正文加载
                    await page.wait_for_timeout(2000)

                    html = await page.content()
                    await context.close()
                finally:
                    await browser.close()

            if not html or len(html) < 200:
                return None

            # 用 trafilatura 从渲染后的 HTML 提取正文
            try:
                import trafilatura

                loop = asyncio.get_event_loop()
                text = await loop.run_in_executor(
                    None,
                    lambda: trafilatura.extract(
                        html,
                        include_comments=False,
                        include_tables=True,
                        no_fallback=False,
                    ),
                )
                if text and len(text.strip()) > 100:
                    logger.debug(f"Playwright 成功: {url} ({len(text)} chars)")
                    return text.strip()
            except ImportError:
                # trafilatura 不可用时用 BeautifulSoup 降级
                try:
                    from bs4 import BeautifulSoup

                    soup = BeautifulSoup(html, "html.parser")
                    # 移除 script/style
                    for tag in soup(["script", "style", "nav", "header", "footer"]):
                        tag.decompose()
                    text = soup.get_text(separator="\n", strip=True)
                    if text and len(text) > 100:
                        logger.debug(f"Playwright+BS4 成功: {url} ({len(text)} chars)")
                        return text
                except Exception:
                    pass

            return None
        except asyncio.TimeoutError:
            logger.debug(f"Playwright 超时: {url}")
            return None
        except ImportError:
            logger.debug("Playwright 未安装，跳过第三级提取")
            return None
        except Exception as e:
            logger.debug(f"Playwright 失败: {url} - {e}")
            return None

    # --------------------------------------------------------
    # 三级自动降级
    # --------------------------------------------------------

    async def fetch_content(self, url: str) -> Optional[str]:
        """
        抓取 URL 全文内容，三级策略自动降级。

        trafilatura → BrightData → Playwright

        Args:
            url: 目标 URL

        Returns:
            提取的正文文本，失败返回 None
        """
        # 检查缓存
        if self._cache:
            cache_key = _make_key("content", url)
            cached = self._cache.get(cache_key)
            if cached is not None:
                logger.debug(f"缓存命中: {url}")
                return cached

        # 第一级：trafilatura
        text = await self._fetch_trafilatura(url)

        # 第二级：BrightData
        if text is None:
            text = await self._fetch_brightdata(url)

        # 第三级：Playwright
        if text is None:
            text = await self._fetch_playwright(url)

        # 截断到最大长度
        if text and len(text) > self.max_content_length:
            text = text[: self.max_content_length]

        # 写入缓存
        if text and self._cache:
            cache_key = _make_key("content", url)
            self._cache.set(cache_key, text)

        return text

    # --------------------------------------------------------
    # LLM 预判：哪些 URL 需要抓取全文
    # --------------------------------------------------------

    async def evaluate_snippets_need_fetch(
        self,
        query: str,
        results: List[Dict[str, Any]],
        llm_client: Any,
    ) -> List[str]:
        """
        使用 LLM 预判搜索摘要是否足够，返回需要抓取全文的 URL 列表。

        只评估来源为 scholar/google 的条目（这两种来源摘要最容易被截断）。
        LLM 返回 JSON: {"urls_to_fetch": ["url1", "url2"]}

        若 LLM 调用失败，降级为返回所有候选 URL（等同全量抓取）。

        Args:
            query:      原始用户查询
            results:    UnifiedWebSearcher 返回的结果列表
            llm_client: LLMManager 客户端实例

        Returns:
            需要抓取全文的 URL 集合（list）
        """
        # 只评估 scholar / google 来源且有合法 URL 的条目
        candidates = []
        for hit in results:
            metadata = hit.get("metadata") or {}
            source = metadata.get("source", "")
            url = (metadata.get("url") or "").strip()
            snippet = (hit.get("content") or "").strip()
            if url and source in ("scholar", "google") and snippet:
                candidates.append({"url": url, "snippet": snippet})

        if not candidates:
            return []

        items_text = "\n".join(
            f"{i + 1}. URL: {c['url']}\n   摘要: {c['snippet'][:400]}"
            for i, c in enumerate(candidates)
        )
        prompt = (
            f'用户问题："{query}"\n\n'
            f"以下是搜索结果的摘要片段：\n{items_text}\n\n"
            "请判断这些摘要是否已包含回答问题所需的具体数据。\n"
            "如果某条摘要信息残缺（例如有省略号、核心数值/结论被截断），"
            "则需要抓取原文。如果现有摘要已足够，无需抓取。\n"
            '只返回 JSON，格式：{"urls_to_fetch": ["url1", "url2"]}。'
            "如果全部摘要已足够，返回 {\"urls_to_fetch\": []}。"
        )

        try:
            resp = llm_client.chat(
                messages=[
                    {"role": "system", "content": "你是文献质量评估助手，只输出纯 JSON，不加任何解释。"},
                    {"role": "user", "content": prompt},
                ],
                max_tokens=512,
                response_model=_FetchDecisionResponse,
            )
            parsed: Optional[_FetchDecisionResponse] = resp.get("parsed_object")
            if parsed is None:
                raw_text = (resp.get("final_text") or "").strip()
                if raw_text:
                    parsed = _FetchDecisionResponse.model_validate_json(raw_text)

            urls = parsed.urls_to_fetch if parsed is not None else []
            allowed = {c["url"] for c in candidates}
            urls = [u for u in urls if isinstance(u, str) and u in allowed]
            logger.info(
                f"LLM 预判：{len(urls)}/{len(candidates)} 条需抓取全文"
                + (f" (query={query!r})" if query else "")
            )
            return urls
        except Exception as e:
            logger.warning(f"LLM 预判失败，降级为全量抓取: {e}")
            return [c["url"] for c in candidates]

    # --------------------------------------------------------
    # 批量增强搜索结果
    # --------------------------------------------------------

    async def enrich_results(
        self,
        results: List[Dict[str, Any]],
        query: Optional[str] = None,
        llm_client: Optional[Any] = None,
        use_content_fetcher: Optional[bool] = None,
    ) -> List[Dict[str, Any]]:
        """
        异步批量抓取搜索结果的全文内容。

        抓取策略由 use_content_fetcher 和 llm_client 共同决定：

        - use_content_fetcher=True  （硬强制）：跳过 LLM 预判，对所有合格 URL 全量抓取。
        - use_content_fetcher=None  （智能模式）且提供 llm_client：
            先调用 evaluate_snippets_need_fetch 获取需要抓取的 URL 子集，
            仅对这些 URL 执行 fetch_content；其余合格 URL 标记
            metadata.content_type = "snippet_sufficient"。
        - use_content_fetcher=None  且无 llm_client：全量抓取（兼容旧行为）。

        成功抓取后：
        - 原始 content 保存到 metadata.original_snippet
        - 全文替换 hit["content"]
        - 标记 metadata.content_type = "full_text"

        Args:
            results:              UnifiedWebSearcher 返回的搜索结果列表
            query:                原始查询（用于日志与 LLM prompt）
            llm_client:           LLMManager 客户端，不为 None 时启用智能预判
            use_content_fetcher:  True=强制全量, None=智能/配置, False 由调用层拦截

        Returns:
            增强后的结果列表（原地修改）
        """
        if not results:
            return results

        # 智能模式 + LLM 可用：先预判，再选择性抓取
        urls_to_fetch: Optional[set] = None  # None 表示"全量抓取"
        if use_content_fetcher is None and llm_client is not None:
            fetching_urls = await self.evaluate_snippets_need_fetch(
                query or "", results, llm_client
            )
            urls_to_fetch = set(fetching_urls)

            # 对不需要抓取的合格 URL 打上 snippet_sufficient 标记
            for hit in results:
                metadata = hit.get("metadata") or {}
                url = (metadata.get("url") or "").strip()
                if (
                    url
                    and url not in urls_to_fetch
                    and is_qualifying_url(url, only_academic=self.only_academic)
                    and metadata.get("content_type") not in ("full_text",)
                ):
                    metadata["content_type"] = "snippet_sufficient"
                    hit["metadata"] = metadata

        sem = asyncio.Semaphore(self.max_concurrent)

        async def _process_one(hit: Dict[str, Any]) -> None:
            metadata = hit.get("metadata") or {}
            url = (metadata.get("url") or "").strip()

            if not url or not is_qualifying_url(url, only_academic=self.only_academic):
                return

            # 已经是全文的跳过
            if metadata.get("content_type") == "full_text":
                return

            # 智能模式：跳过未被 LLM 选中的 URL
            if urls_to_fetch is not None and url not in urls_to_fetch:
                return

            async with sem:
                try:
                    full_text = await self.fetch_content(url)
                except Exception as e:
                    logger.debug(f"抓取失败 {url}: {e}")
                    return

                if full_text:
                    original = (hit.get("content") or "").strip()
                    if original:
                        metadata["original_snippet"] = original
                    hit["content"] = full_text
                    metadata["content_type"] = "full_text"
                    hit["metadata"] = metadata

        tasks = [_process_one(hit) for hit in results]
        await asyncio.gather(*tasks, return_exceptions=True)

        enriched_count = sum(
            1 for h in results
            if (h.get("metadata") or {}).get("content_type") == "full_text"
        )
        sufficient_count = sum(
            1 for h in results
            if (h.get("metadata") or {}).get("content_type") == "snippet_sufficient"
        )
        logger.info(
            f"全文抓取: {enriched_count} 条全文 / {sufficient_count} 条摘要已足够 / "
            f"{len(results)} 条总计"
            + (f" (query={query!r})" if query else "")
        )

        return results

    def enrich_results_sync(
        self,
        results: List[Dict[str, Any]],
        query: Optional[str] = None,
        llm_client: Optional[Any] = None,
        use_content_fetcher: Optional[bool] = None,
    ) -> List[Dict[str, Any]]:
        """
        同步版本的 enrich_results，兼容 RetrievalService。
        """
        if not results or not self.enabled:
            return results

        coro = self.enrich_results(
            results,
            query=query,
            llm_client=llm_client,
            use_content_fetcher=use_content_fetcher,
        )
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    future = executor.submit(asyncio.run, coro)
                    return future.result()
            else:
                return loop.run_until_complete(coro)
        except RuntimeError:
            return asyncio.run(coro)
</file>

<file path="requirements.txt">
# ============================================================
#  深海科研知识库 RAG 系统 - Python 依赖清单
# ============================================================
#
#  ⚠️ 重要提示：
#  1. PyTorch, Torchvision, Timm 必须通过 Conda 安装，
#     严禁在此文件中列出，否则会导致底层 C++ 算子冲突。
#  2. Transformers 被严格限制在 4.42 - 5.0 之间。
#  3. 请在 conda 环境 deepsea-rag 中安装本文件依赖。
#  4. Python 建议版本：3.10（3.10+）。
#
# ============================================================

# ----------------------------------------------------------
# 核心组件
# ----------------------------------------------------------
pymilvus[model]>=2.5.0
python-dotenv>=1.0.0

# ----------------------------------------------------------
# PDF 解析 & 文档处理
# ----------------------------------------------------------
# Docling 依赖 transformers < 5.0.0
docling>=2.0.0
pymupdf>=1.24.0
pillow>=10.0.0
python-docx>=1.1.0
markdown2>=2.4.0

# ----------------------------------------------------------
# AI 模型库 (关键依赖)
# ----------------------------------------------------------
# 🔴 [CRITICAL] 必须 >=4.42.0 以支持 RTDetrImageProcessor
# 🔴 [CRITICAL] 必须 <5.0.0 以兼容 Docling
transformers>=4.42.0,<5.0.0

# 🔴 [CRITICAL] 指定稳定版本以防元数据损坏
huggingface-hub==0.36.0

sentence-transformers>=2.2.0
FlagEmbedding>=1.2.0
datasets>=2.19.0

# ----------------------------------------------------------
# Reranking (重排序) - jina-colbert-v2 多语言模型依赖
# ----------------------------------------------------------
ragatouille>=0.0.8
einops>=0.7.0
# flash-attn 需 CUDA + GPU，macOS/CPU-only 环境无需安装：
#   pip install flash-attn --no-build-isolation

# ----------------------------------------------------------
# LLM 客户端
# ----------------------------------------------------------
anthropic>=0.18.0
openai>=1.0.0,<3.0.0

# ----------------------------------------------------------
# API & 服务端
# ----------------------------------------------------------
fastapi>=0.100.0,<1.0.0
uvicorn[standard]>=0.22.0
pandas>=2.0.0
tqdm>=4.65.0

# ----------------------------------------------------------
# Web 搜索 (Tavily)
# ----------------------------------------------------------
tavily-python>=0.5.0

# ----------------------------------------------------------
# Web 内容抓取 (WebContentFetcher)
# ----------------------------------------------------------
trafilatura>=2.0.0

# ----------------------------------------------------------
# Semantic Scholar (API)
# ----------------------------------------------------------
aiohttp>=3.9.0

# ----------------------------------------------------------
# Google Scholar / Google 搜索 (Playwright)
# ----------------------------------------------------------
playwright>=1.40.0
playwright-stealth>=1.0.0
beautifulsoup4>=4.12.0
# pyvirtualdisplay>=3.0  # Linux 服务器可选，用于虚拟显示器

# ----------------------------------------------------------
# Agent & 工作流
# ----------------------------------------------------------
langgraph>=0.2.0
langgraph-checkpoint-sqlite>=0.0.20
networkx>=3.0

# ----------------------------------------------------------
# 协作层 & API
# ----------------------------------------------------------
pydantic>=2.0.0,<3.0.0
requests>=2.28.0

# ----------------------------------------------------------
# 认证
# ----------------------------------------------------------
bcrypt>=4.0.0

# ----------------------------------------------------------
# Observability (OpenTelemetry + Prometheus + LangSmith)
# ----------------------------------------------------------
opentelemetry-api>=1.20.0
opentelemetry-sdk>=1.20.0
opentelemetry-instrumentation-fastapi>=0.41b0
opentelemetry-exporter-prometheus>=0.41b0
prometheus-client>=0.20.0
langsmith>=0.6.0

# ----------------------------------------------------------
# MCP (Model Context Protocol)
# ----------------------------------------------------------
mcp>=1.26.0

# ----------------------------------------------------------
# Testing
# ----------------------------------------------------------
pytest>=8.0.0
</file>

<file path="src/collaboration/research/verifier.py">
"""
Chain of Verification (CoV) — 对已生成内容执行事实验证。

流程:
1. 提取所有事实性声明
2. 检查每个声明是否有引文支撑
3. 对无支撑声明触发补充搜索
4. 标记置信度并生成修订建议
"""

from __future__ import annotations

from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, model_validator

from src.log import get_logger
from src.utils.prompt_manager import PromptManager

logger = get_logger(__name__)
_pm = PromptManager()


# ============================================================
# Pydantic Response Models (结构化输出)
# ============================================================

class _ExtractedClaimItem(BaseModel):
    claim: str = ""
    has_citation: bool = False
    citation_keys: List[str] = Field(default_factory=list)


class _ExtractedClaimsResponse(BaseModel):
    claims: List[_ExtractedClaimItem] = Field(default_factory=list)

    @model_validator(mode="before")
    @classmethod
    def _accept_legacy_array(cls, data: Any) -> Any:
        if isinstance(data, list):
            return {"claims": data}
        return data


class _VerificationItem(BaseModel):
    claim_index: int = 0
    confidence: str = "low"
    evidence_found: str = ""
    needs_revision: bool = False
    revision_note: str = ""
    attribution_analysis: str = ""
    conflict_notes: List[str] = Field(default_factory=list)
    supplementary_query: str = ""


class _VerificationsResponse(BaseModel):
    verifications: List[_VerificationItem] = Field(default_factory=list)

    @model_validator(mode="before")
    @classmethod
    def _accept_legacy_array(cls, data: Any) -> Any:
        if isinstance(data, list):
            return {"verifications": data}
        return data


@dataclass
class ClaimVerification:
    """单个声明的验证结果"""
    claim_text: str
    has_citation: bool = False
    citation_keys: List[str] = field(default_factory=list)
    confidence: str = "low"  # low | medium | high
    evidence_found: str = ""
    needs_revision: bool = False
    revision_note: str = ""
    attribution_analysis: str = ""


@dataclass
class VerificationResult:
    """整体验证结果"""
    total_claims: int = 0
    verified_claims: int = 0
    unsupported_claims: int = 0
    claims: List[ClaimVerification] = field(default_factory=list)
    overall_confidence: str = "low"
    revision_suggestions: List[str] = field(default_factory=list)
    supplementary_queries: List[str] = field(default_factory=list)
    conflict_notes: List[str] = field(default_factory=list)



def extract_claims(
    text: str,
    llm_client: Any,
    model: Optional[str] = None,
) -> List[Dict[str, Any]]:
    """从文本中提取事实性声明"""
    prompt = _pm.render("extract_claims.txt", text=text[:4000])
    try:
        resp = llm_client.chat(
            messages=[
                {"role": "system", "content": "你是事实核查专家，只返回 JSON。"},
                {"role": "user", "content": prompt},
            ],
            model=model,
            max_tokens=2000,
            response_model=_ExtractedClaimsResponse,
        )
        parsed: Optional[_ExtractedClaimsResponse] = resp.get("parsed_object")
        if parsed is None:
            raw = (resp.get("final_text") or "").strip()
            if raw:
                parsed = _ExtractedClaimsResponse.model_validate_json(raw)
        if parsed is not None:
            return [item.model_dump() for item in parsed.claims]
    except Exception as e:
        logger.warning(f"Claim extraction failed: {e}")
    return []


def verify_claims(
    section_text: str,
    citations: List[Any],
    llm_client: Any,
    model: Optional[str] = None,
    search_callback: Optional[Any] = None,
) -> VerificationResult:
    """
    对已生成内容执行 Chain of Verification。

    Args:
        section_text: 待验证的章节文本
        citations: 引用列表（用于匹配引文标记）
        llm_client: LLM 客户端
        model: 可选模型覆盖
        search_callback: 可选的补充搜索函数 (query) -> str

    Returns:
        VerificationResult
    """
    result = VerificationResult()

    # Step 1: 提取事实性声明
    raw_claims = extract_claims(section_text, llm_client, model)
    if not raw_claims:
        return result

    result.total_claims = len(raw_claims)

    # Step 2: 构建引文文本（供 LLM 验证）
    citation_text = ""
    if citations:
        lines = []
        for c in citations[:20]:
            if hasattr(c, "title"):
                lines.append(f"[{getattr(c, 'cite_key', '')}] {c.title} ({getattr(c, 'year', '')})")
            elif isinstance(c, dict):
                lines.append(f"[{c.get('cite_key', '')}] {c.get('title', '')} ({c.get('year', '')})")
        citation_text = "\n".join(lines)

    # 准备声明列表
    claims_text = "\n".join(
        f"{i}. {c.get('claim', '')}" + (f" [{', '.join(c.get('citation_keys', []))}]" if c.get('citation_keys') else "")
        for i, c in enumerate(raw_claims)
    )

    # Step 3: 用 LLM 验证
    prompt = _pm.render("verify_claims.txt", claims=claims_text, evidence=citation_text[:3000])
    try:
        resp = llm_client.chat(
            messages=[
                {"role": "system", "content": "你是学术事实核查专家，只返回 JSON。"},
                {"role": "user", "content": prompt},
            ],
            model=model,
            max_tokens=2000,
            response_model=_VerificationsResponse,
        )
        parsed_verif: Optional[_VerificationsResponse] = resp.get("parsed_object")
        if parsed_verif is None:
            raw = (resp.get("final_text") or "").strip()
            if raw:
                parsed_verif = _VerificationsResponse.model_validate_json(raw)
        verifications = (
            [item.model_dump() for item in parsed_verif.verifications]
            if parsed_verif is not None
            else []
        )
    except Exception as e:
        logger.warning(f"Claim verification failed: {e}")
        verifications = []

    # Step 4: 整合结果
    for i, raw_claim in enumerate(raw_claims):
        cv = ClaimVerification(
            claim_text=raw_claim.get("claim", ""),
            has_citation=bool(raw_claim.get("citation_keys")),
            citation_keys=raw_claim.get("citation_keys", []),
        )

        # 匹配验证结果
        v = next((v for v in verifications if v.get("claim_index") == i), None)
        if v:
            cv.confidence = v.get("confidence", "low")
            cv.evidence_found = v.get("evidence_found", "")
            cv.needs_revision = v.get("needs_revision", False)
            cv.revision_note = v.get("revision_note", "")
            cv.attribution_analysis = (v.get("attribution_analysis", "") or "").strip()

            raw_conflicts = v.get("conflict_notes", [])
            if isinstance(raw_conflicts, str):
                raw_conflicts = [raw_conflicts]
            conflict_items = [
                str(item).strip()
                for item in (raw_conflicts or [])
                if str(item).strip()
            ]
            if cv.attribution_analysis and not conflict_items:
                conflict_items = [cv.attribution_analysis]

            if conflict_items:
                if cv.attribution_analysis and cv.attribution_analysis not in cv.revision_note:
                    cv.revision_note = (
                        f"{cv.revision_note}\nAttribution Analysis: {cv.attribution_analysis}".strip()
                    )
                for note in conflict_items:
                    result.conflict_notes.append(
                        f"声明「{cv.claim_text[:50]}...」的冲突归因: {note}"
                    )

            sup_query = v.get("supplementary_query", "")
            if sup_query:
                result.supplementary_queries.append(sup_query)
        else:
            cv.confidence = "medium" if cv.has_citation else "low"
            cv.needs_revision = not cv.has_citation

        if cv.confidence == "high":
            result.verified_claims += 1
        elif cv.needs_revision:
            result.unsupported_claims += 1
            result.revision_suggestions.append(
                f"声明「{cv.claim_text[:50]}...」需要补充证据。{cv.revision_note}"
            )

        result.claims.append(cv)

    # 计算总体置信度
    if result.total_claims > 0:
        ratio = result.verified_claims / result.total_claims
        if ratio >= 0.8:
            result.overall_confidence = "high"
        elif ratio >= 0.5:
            result.overall_confidence = "medium"
        else:
            result.overall_confidence = "low"

    # Step 5: 补充搜索（如果有回调）
    if search_callback and result.supplementary_queries:
        for sq in result.supplementary_queries[:3]:
            try:
                search_callback(sq)
            except Exception:
                pass

    return result
</file>

<file path="frontend/src/types/index.ts">
// ============================================================
// User & Auth
// ============================================================

export interface User {
  user_id: string;
  username?: string;
  role: 'user' | 'admin';
  avatar?: string;
}

export interface LoginRequest {
  user_id: string;
  password: string;
}

export interface LoginResponse {
  token: string;
  user_id: string;
  role: string;
}

export interface UserItem {
  user_id: string;
  role: string;
  is_active: boolean;
  created_at: string;
  updated_at: string;
}

// ============================================================
// Chat & Messages
// ============================================================

export interface Source {
  id: string | number;
  cite_key: string;
  title: string;
  authors: string[];
  year?: number | null;
  doc_id?: string | null;
  url?: string | null;
  doi?: string | null;
  score?: number;
  snippet?: string;
  path?: string;
  type?: 'local' | 'web';
  bbox?: number[];
  page_num?: number | null;
}

export interface Message {
  id?: string;
  role: 'user' | 'assistant';
  content: string;
  sources?: Source[];
  timestamp?: string;
}

export interface RetrievalStageDiag {
  count: number;
  time_ms: number;
}

export interface RetrievalDiagnostics {
  optimized_queries?: string[];
  stages?: Record<string, RetrievalStageDiag>;
  web_providers?: Record<string, RetrievalStageDiag>;
  content_fetcher?: { enriched: number; total: number };
  cross_source_dedup?: { removed: number; remaining: number };
  cache_hit?: boolean;
}

export interface EvidenceSummary {
  query: string;
  total_chunks: number;
  sources_used: string[];
  retrieval_time_ms: number;
  // P0 证据综合元数据
  year_range?: [number | null, number | null];
  source_breakdown?: Record<string, number>;
  evidence_type_breakdown?: Record<string, number>;
  cross_validated_count?: number;
  total_documents?: number;
  // P1 检索诊断
  diagnostics?: RetrievalDiagnostics;
}

export interface ChatRequest {
  session_id?: string;
  user_id?: string;
  canvas_id?: string;
  message: string;
  collection?: string;
  search_mode: 'local' | 'web' | 'hybrid' | 'none';
  web_providers?: string[];
  web_source_configs?: Record<string, { topK: number; threshold: number }>;  // 每个搜索源的独立配置
  use_query_optimizer?: boolean;  // 是否启用查询优化器
  query_optimizer_max_queries?: number; // 每个搜索引擎最多生成的查询数
  use_query_expansion?: boolean;  // 兼容字段（已弃用）
  local_top_k?: number;  // 本地检索返回的最大文档数
  local_threshold?: number;  // 本地检索的相似度阈值 (0-1)
  year_start?: number;  // 年份窗口起始（硬过滤）
  year_end?: number;  // 年份窗口结束（硬过滤）
  final_top_k?: number;  // 最终保留的文档数（local + web 合并重排后）
  llm_provider?: string;  // LLM 提供商: deepseek | openai | gemini | claude | kimi 等
  model_override?: string;  // 覆盖默认模型，如 claude-opus-4-6
  mode?: ChatMode;  // 执行模式: chat（默认）| deep_research
  use_content_fetcher?: boolean;  // 是否对网络搜索结果做全文抓取（None 用后端默认）
  use_agent?: boolean;  // 是否启用 Agent 模式（ReAct 循环 / LangGraph 引擎）
  clarification_answers?: Record<string, string>;
  output_language?: 'auto' | 'en' | 'zh';
  step_models?: Record<string, string | null | undefined>;
}

export interface ChatCitation {
  cite_key: string;
  title: string;
  authors: string[];
  year?: number | null;
  doc_id?: string | null;
  url?: string | null;
  doi?: string | null;
  bbox?: number[];
  page_num?: number | null;
}

export interface ChatResponse {
  session_id: string;
  response: string;
  citations: ChatCitation[];
  evidence_summary?: EvidenceSummary;
}

export interface SessionInfo {
  session_id: string;
  canvas_id: string;
  stage: string;
  turn_count: number;
  turns: { role: string; content: string; sources?: ChatCitation[] }[];
  research_dashboard?: ResearchDashboardData | null;
}

export interface SessionListItem {
  session_id: string;
  title: string;
  canvas_id: string;
  stage: string;
  turn_count: number;
  created_at: string;
  updated_at: string;
}

// ============================================================
// Intent / Mode（简化版：Chat vs Deep Research）
// ============================================================

/**
 * 执行模式（与后端 IntentType 对齐）
 */
export type ChatMode = 'chat' | 'deep_research';

/**
 * 兼容旧的 IntentType（旧值在运行时会被后端映射为 chat/deep_research）
 */
export type IntentType = ChatMode | string;

// 兼容旧代码的类型别名
export type IntentMode = 'auto' | 'search' | 'write' | 'chat';  // deprecated, kept for type compatibility

/**
 * 显式命令定义（用于命令面板）
 */
export interface CommandDefinition {
  command: string;
  label: string;
  description: string;
  mode: ChatMode;  // 命令触发的模式
  example?: string;
}

export interface IntentDetectRequest {
  message: string;
  session_id?: string;
  current_stage?: string;
}

export interface IntentDetectResponse {
  mode: ChatMode;
  confidence: number;
  suggested_topic: string;
  params: Record<string, unknown>;
  // 兼容旧字段
  intent_type?: string;
  needs_retrieval?: boolean;
  suggested_search_mode?: string;
}

export interface IntentInfo {
  mode: ChatMode;
  confidence: number;
  from_command: boolean;
}

// ============================================================
// Deep Research
// ============================================================

export interface ClarifyQuestion {
  id: string;
  text: string;
  question_type: 'text' | 'choice' | 'multi_choice';
  options: string[];
  default: string;
}

export interface ClarifyResponse {
  questions: ClarifyQuestion[];
  suggested_topic: string;
  suggested_outline: string[];
  used_fallback?: boolean;
  fallback_reason?: string;
  llm_provider_used?: string;
  llm_model_used?: string;
}

export interface DeepResearchRequest {
  topic: string;
  session_id?: string;
  canvas_id?: string;
  user_id?: string;
  search_mode: 'local' | 'web' | 'hybrid';
  max_sections?: number;
  clarification_answers?: Record<string, string>;
  // 完整检索参数
  web_providers?: string[];
  web_source_configs?: Record<string, { topK: number; threshold: number }>;
  use_query_optimizer?: boolean;
  query_optimizer_max_queries?: number;
  local_top_k?: number;
  local_threshold?: number;
  year_start?: number;
  year_end?: number;
  final_top_k?: number;
  llm_provider?: string;
  model_override?: string;
  output_language?: 'auto' | 'en' | 'zh';
  step_models?: Record<string, string | null | undefined>;
}

export interface DeepResearchStartRequest {
  topic: string;
  session_id?: string;
  canvas_id?: string;
  user_id?: string;
  collection?: string;
  search_mode: 'local' | 'web' | 'hybrid';
  clarification_answers?: Record<string, string>;
  output_language?: 'auto' | 'en' | 'zh';
  step_models?: Record<string, string | null | undefined>;
  step_model_strict?: boolean;
  web_providers?: string[];
  web_source_configs?: Record<string, { topK: number; threshold: number }>;
  use_query_optimizer?: boolean;
  query_optimizer_max_queries?: number;
  local_top_k?: number;
  local_threshold?: number;
  year_start?: number;
  year_end?: number;
  final_top_k?: number;
  llm_provider?: string;
  model_override?: string;
}

export interface DeepResearchStartResponse {
  session_id: string;
  canvas_id: string;
  brief: Record<string, unknown>;
  outline: string[];
  initial_stats: Record<string, unknown>;
  used_fallback?: boolean;
  fallback_reason?: string;
  llm_provider_used?: string;
  llm_model_used?: string;
}

export interface DeepResearchConfirmRequest {
  topic: string;
  session_id?: string;
  canvas_id?: string;
  user_id?: string;
  collection?: string;
  search_mode: 'local' | 'web' | 'hybrid';
  confirmed_outline: string[];
  confirmed_brief?: Record<string, unknown>;
  output_language?: 'auto' | 'en' | 'zh';
  step_models?: Record<string, string | null | undefined>;
  step_model_strict?: boolean;
  web_providers?: string[];
  web_source_configs?: Record<string, { topK: number; threshold: number }>;
  use_query_optimizer?: boolean;
  query_optimizer_max_queries?: number;
  local_top_k?: number;
  local_threshold?: number;
  year_start?: number;
  year_end?: number;
  final_top_k?: number;
  llm_provider?: string;
  model_override?: string;
  user_context?: string;
  user_context_mode?: 'supporting' | 'direct_injection';
  user_documents?: Array<{ name: string; content: string }>;
  // 研究深度
  depth?: 'lite' | 'comprehensive';
  // 阶段跳过控制
  skip_draft_review?: boolean;
  skip_refine_review?: boolean;
  skip_claim_generation?: boolean;
}

export interface DeepResearchSubmitResponse {
  ok: boolean;
  job_id: string;
  session_id: string;
  canvas_id: string;
}

export interface DeepResearchJobInfo {
  job_id: string;
  topic: string;
  session_id: string;
  canvas_id: string;
  status: 'pending' | 'running' | 'cancelling' | 'done' | 'error' | 'cancelled' | string;
  current_stage: string;
  message: string;
  error_message: string;
  result_markdown: string;
  result_citations: ChatCitation[];
  result_dashboard: Record<string, unknown>;
  total_time_ms: number;
  created_at: number;
  updated_at: number;
  finished_at?: number | null;
}

export interface DeepResearchJobEvent {
  event_id: number;
  event: string;
  created_at: number;
  data: Record<string, unknown>;
}

// ============================================================
// Model Sync
// ============================================================

export interface ModelStatusItem {
  name: string;
  model_id: string;
  cache_dir: string;
  exists: boolean;
  local_files_only: boolean;
  error?: string | null;
}

export interface ModelSyncItem {
  name: string;
  model_id: string;
  cache_dir: string;
  local_files_only: boolean;
  updated: boolean;
  status: string;
  message?: string | null;
  error?: string | null;
  resolved_path?: string | null;
}

export interface ModelStatusResponse {
  items: ModelStatusItem[];
}

export interface ModelSyncRequest {
  force_update?: boolean;
  local_files_only?: boolean;
}

export interface ModelSyncResponse {
  items: ModelSyncItem[];
}

// ============================================================
// Canvas
// ============================================================

export type CanvasStage = 'explore' | 'outline' | 'drafting' | 'refine';

export interface OutlineSection {
  id: string;
  title: string;
  level: number;
  order: number;
  parent_id?: string;
  status: string;
  guidance?: string;
}

export interface DraftBlock {
  section_id: string;
  content_md: string;
  version: number;
  used_fragment_ids: string[];
  used_citation_ids: string[];
  updated_at?: string;
}

export interface Annotation {
  id: string;
  section_id: string;
  target_text: string;
  directive: string;
  status: 'pending' | 'applied' | 'rejected';
  created_at?: string;
}

export interface CanvasResearchBrief {
  scope: string;
  success_criteria: string[];
  key_questions: string[];
  exclusions: string[];
  time_range: string;
  source_priority: string[];
  action_plan: string;
}

export interface Canvas {
  id: string;
  session_id: string;
  topic: string;
  working_title: string;
  abstract: string;
  keywords: string[];
  stage: CanvasStage;
  refined_markdown: string;
  outline: OutlineSection[];
  drafts: Record<string, DraftBlock>;
  citation_pool: Citation[];
  identified_gaps: string[];
  user_directives: string[];
  annotations: Annotation[];
  research_brief: CanvasResearchBrief | null;
  research_insights: string[];
  skip_draft_review: boolean;
  skip_refine_review: boolean;
  version: number;
}

// ── Gap Supplement Types ──

export type GapSupplementStatus = 'pending' | 'consumed';

export interface GapSupplement {
  id: number;
  job_id: string;
  section_id: string;
  gap_text: string;
  supplement_type: 'material' | 'direct_info';
  content: Record<string, unknown>;
  status: GapSupplementStatus;
  created_at: number;
  consumed_at?: number | null;
}

// ── Research Insight Types ──

export type InsightType = 'gap' | 'conflict' | 'limitation' | 'future_direction';
export type InsightStatus = 'open' | 'addressed' | 'deferred';

export interface ResearchInsight {
  id: number;
  job_id: string;
  section_id: string;
  insight_type: InsightType;
  text: string;
  source_context: string;
  status: InsightStatus;
  created_at: number;
}

export interface Citation {
  id?: string;
  cite_key: string;
  title: string;
  authors: string[];
  year?: number;
  doi?: string;
  url?: string;
  bibtex?: string;
}

// ============================================================
// Projects / History
// ============================================================

export interface Project {
  id: string;
  title: string;           // 后端返回 working_title || topic
  topic?: string;
  working_title?: string;
  stage?: string;
  created_at: string;
  updated_at: string;
  archived: boolean;
  session_id?: string;
}

// ============================================================
// Config
// ============================================================

export interface WebSource {
  id: string;
  name: string;
  enabled: boolean;
  topK: number;
  threshold: number;
}

export interface RagConfig {
  enabled: boolean;  // 是否启用本地 RAG 检索
  localTopK: number;
  localThreshold: number;  // 相似度阈值 (0-1)
  finalTopK: number;  // 最终保留的文档数（local + web 合并重排后）
  enableHippoRAG: boolean;
  enableReranker: boolean;
  enableAgent: boolean;  // 是否启用 Agent 模式（ReAct / LangGraph）
}

export interface WebSearchConfig {
  enabled: boolean;
  sources: WebSource[];
  queryOptimizer: boolean;   // 查询优化器（针对不同搜索引擎优化查询格式）
  maxQueriesPerProvider: number; // 每个搜索引擎每种语言的查询数
  enableContentFetcher: boolean;  // 是否对网络搜索结果做全文抓取
}

// ============================================================
// Deep Research Defaults (persistent settings via ⚙ popover)
// ============================================================

export interface DeepResearchDefaults {
  depth: 'lite' | 'comprehensive';
  outputLanguage: 'auto' | 'en' | 'zh';
  yearStart: number | null;
  yearEnd: number | null;
  stepModelStrict: boolean;
  stepModels: Record<string, string>;
  skipClaimGeneration: boolean;
}

// ============================================================
// Workflow
// ============================================================

export type WorkflowStep = 'idle' | 'explore' | 'outline' | 'drafting' | 'refine';

/**
 * 工作流阶段详细信息（用于 UI 显示）
 */
export interface WorkflowStageInfo {
  id: WorkflowStep;
  label: string;
  description: string;
  icon: string;
  color: string;
}

/**
 * 预定义的工作流阶段配置
 * label / description 使用 i18n key，在组件中通过 t() 渲染。
 */
export const WORKFLOW_STAGES: WorkflowStageInfo[] = [
  { id: 'explore', label: 'workflow.explore', description: 'workflow.exploreDesc', icon: '🔍', color: 'blue' },
  { id: 'outline', label: 'workflow.outline', description: 'workflow.outlineDesc', icon: '📋', color: 'purple' },
  { id: 'drafting', label: 'workflow.drafting', description: 'workflow.draftingDesc', icon: '✍️', color: 'orange' },
  { id: 'refine', label: 'workflow.refine', description: 'workflow.refineDesc', icon: '✨', color: 'green' },
];

/**
 * 简化命令列表（/auto 触发 Deep Research，其余为 Chat 内 prompt hints）
 * label / description 使用 i18n key，在组件中通过 t() 渲染。
 */
export const COMMAND_LIST: CommandDefinition[] = [
  { command: '/auto', label: 'commands.deepResearch', description: 'commands.deepResearchDesc', mode: 'deep_research', example: '/auto 深海冷泉生态系统' },
  { command: '/search', label: 'commands.search', description: 'commands.searchDesc', mode: 'chat', example: '/search deep sea cold seep' },
  { command: '/outline', label: 'commands.generateOutline', description: 'commands.generateOutlineDesc', mode: 'chat', example: '/outline' },
  { command: '/draft', label: 'commands.draftChapter', description: 'commands.draftChapterDesc', mode: 'chat', example: '/draft introduction' },
  { command: '/export', label: 'commands.exportDoc', description: 'commands.exportDocDesc', mode: 'chat', example: '/export' },
  { command: '/status', label: 'commands.viewStatus', description: 'commands.viewStatusDesc', mode: 'chat', example: '/status' },
];

// ============================================================
// Tool Trace (Agent ReAct Loop)
// ============================================================

export interface ToolTraceItem {
  iteration: number;
  tool: string;
  arguments: Record<string, unknown>;
  result: string;
  is_error: boolean;
}

// ============================================================
// Research Dashboard (Deep Research Agent Progress)
// ============================================================

export interface ResearchSectionStatus {
  title: string;
  status: 'pending' | 'researching' | 'writing' | 'reviewing' | 'done';
  coverage_score: number;
  source_count: number;
  gaps: string[];
}

export interface ResearchDashboardData {
  topic: string;
  scope: string;
  progress: number;       // 0-1
  coverage: number;       // 0-1
  confidence: 'low' | 'medium' | 'high';
  total_sources: number;
  total_iterations: number;
  sections: ResearchSectionStatus[];
  coverage_gaps: string[];
  conflict_notes: string[];
}

// ============================================================
// Toast
// ============================================================

export interface Toast {
  id: number;
  msg: string;
  type: 'info' | 'success' | 'error' | 'warning';
}
</file>

<file path="src/api/schemas.py">
"""
API 请求/响应 Pydantic 模型
"""

from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field


class ChatRequest(BaseModel):
    """对话请求"""

    session_id: Optional[str] = Field(None, description="会话 ID，不传则创建新会话")
    user_id: Optional[str] = Field(None, description="用户 ID，用于 Persistent Store 偏好")
    canvas_id: Optional[str] = Field(None, description="画布 ID，新建会话时可绑定")
    message: str = Field(..., min_length=1, description="用户消息")
    collection: Optional[str] = Field(
        None,
        description="本地检索目标知识库（Milvus collection），None 表示使用默认库",
    )
    search_mode: str = Field("local", description="检索模式: local | web | hybrid | none")
    web_providers: Optional[List[str]] = Field(
        None,
        description="Web 搜索来源选择: tavily | scholar | google | semantic，可传一个或多个",
    )
    web_source_configs: Optional[Dict[str, Dict[str, Any]]] = Field(
        None,
        description="每个搜索源的独立配置 {provider_id: {topK: int, threshold: float}}",
    )
    use_query_expansion: Optional[bool] = Field(
        None,
        description="仅 Tavily：是否启用多查询扩展，None 表示使用配置默认值",
    )
    use_query_optimizer: Optional[bool] = Field(
        None,
        description="是否启用查询优化器（针对不同搜索引擎优化查询格式），None 表示使用配置默认值",
    )
    query_optimizer_max_queries: Optional[int] = Field(
        None,
        description="查询优化器：每个搜索引擎每种语言查询数（1-5，中文输入时=中文N+英文N），None 表示使用配置默认值",
    )
    local_top_k: Optional[int] = Field(
        None,
        description="本地检索返回的最大文档数，None 表示使用配置默认值",
    )
    local_threshold: Optional[float] = Field(
        None,
        description="本地检索的相似度阈值 (0-1)，低于此阈值的结果会被过滤，None 表示不过滤",
    )
    year_start: Optional[int] = Field(
        None,
        ge=1900,
        le=2100,
        description="年份窗口起始（硬过滤，含边界）",
    )
    year_end: Optional[int] = Field(
        None,
        ge=1900,
        le=2100,
        description="年份窗口结束（硬过滤，含边界）",
    )
    final_top_k: Optional[int] = Field(
        None,
        description="最终保留的文档数（local + web 合并重排后），None 表示使用配置默认值",
    )
    llm_provider: Optional[str] = Field(
        None,
        description="LLM 提供商: deepseek | openai | gemini | claude | kimi 等，None 表示使用配置默认值",
    )
    model_override: Optional[str] = Field(
        None,
        description="覆盖 provider 的默认模型，值为 models map 中的 key（如 claude-opus-4-6），None 表示使用 provider 默认模型",
    )
    use_content_fetcher: Optional[bool] = Field(
        None,
        description="是否对网络搜索结果做全文抓取，None 表示使用后端配置默认值",
    )
    use_agent: Optional[bool] = Field(
        None,
        description="是否启用 Agent 模式（ReAct 循环，LLM 自主调用工具），None/False 使用传统模式",
    )
    clarification_answers: Optional[Dict[str, str]] = Field(
        None,
        description="Deep Research 澄清问题回答 {question_id: answer_text}",
    )
    output_language: Optional[str] = Field(
        None,
        description="Deep Research 输出语言: auto | en | zh",
    )
    step_models: Optional[Dict[str, Optional[str]]] = Field(
        None,
        description="Deep Research 各步骤模型覆盖，格式 provider::model，键支持 scope/plan/research/evaluate/write/verify/synthesize",
    )
    mode: Optional[str] = Field(
        "chat",
        description="执行模式: chat（普通对话）| deep_research（多步综述流水线），默认 chat",
    )


class EvidenceSummary(BaseModel):
    """检索证据摘要（用于响应）"""

    query: str = ""
    total_chunks: int = 0
    sources_used: List[str] = Field(default_factory=list)
    retrieval_time_ms: float = 0.0
    # 证据综合元数据
    year_range: Optional[List[Optional[int]]] = Field(None, description="证据时间跨度 [earliest, latest]")
    source_breakdown: Optional[Dict[str, int]] = Field(None, description="来源分布 {local: N, web: M}")
    evidence_type_breakdown: Optional[Dict[str, int]] = Field(None, description="证据类型分布 {finding: N, method: M, ...}")
    cross_validated_count: int = Field(0, description="本地+网络交叉验证的文献数")
    total_documents: int = Field(0, description="涉及的独立文献总数")
    # 检索诊断信息
    diagnostics: Optional[Dict[str, Any]] = Field(None, description="检索诊断: stages/web_providers/content_fetcher")


class ChatCitation(BaseModel):
    """对话引用项（轻量版）"""

    cite_key: str = ""
    title: str = ""
    authors: List[str] = Field(default_factory=list)
    year: Optional[int] = None
    doc_id: Optional[str] = None
    url: Optional[str] = None
    doi: Optional[str] = None
    bbox: Optional[List[float]] = Field(None, description="Docling bbox 坐标 [x0,y0,x1,y1]")
    page_num: Optional[int] = Field(None, description="证据所在页码")


class ChatResponse(BaseModel):
    """对话响应"""

    session_id: str = Field(..., description="会话 ID")
    response: str = Field(..., description="助手回复")
    citations: List[ChatCitation] = Field(default_factory=list, description="引用来源列表")
    evidence_summary: Optional[EvidenceSummary] = Field(None, description="本轮检索摘要")


class TurnItem(BaseModel):
    """单轮对话项"""

    role: str
    content: str
    sources: List[ChatCitation] = Field(default_factory=list, description="该轮对话的引用来源")


class SessionInfo(BaseModel):
    """会话信息"""

    session_id: str
    canvas_id: str = ""
    stage: str = Field("explore", description="工作流阶段: explore | outline | drafting | refine")
    turn_count: int = 0
    turns: List[TurnItem] = Field(default_factory=list)
    research_dashboard: Optional[Dict[str, Any]] = Field(
        None,
        description="最近一次 Deep Research 的仪表盘数据（用于刷新后恢复进度列表）",
    )


class SessionListItem(BaseModel):
    """会话列表项（用于历史记录）"""

    session_id: str
    title: str = Field(..., description="会话标题（取第一轮用户消息）")
    canvas_id: str = ""
    stage: str = "explore"
    turn_count: int = 0
    created_at: str
    updated_at: str


# ---- Canvas ----

class CanvasCreateRequest(BaseModel):
    session_id: str = ""
    topic: str = ""


class CanvasUpdateRequest(BaseModel):
    session_id: Optional[str] = None
    topic: Optional[str] = None
    working_title: Optional[str] = None
    abstract: Optional[str] = None
    keywords: Optional[List[str]] = None
    stage: Optional[str] = None
    refined_markdown: Optional[str] = None
    user_directives: Optional[List[str]] = None
    skip_draft_review: Optional[bool] = None
    skip_refine_review: Optional[bool] = None


class OutlineSectionSchema(BaseModel):
    id: str = ""
    title: str = ""
    level: int = 1
    order: int = 0
    parent_id: Optional[str] = None
    status: str = "todo"
    guidance: Optional[str] = None


class OutlineUpsertRequest(BaseModel):
    sections: List[OutlineSectionSchema] = Field(default_factory=list)


class DraftBlockSchema(BaseModel):
    section_id: str = ""
    content_md: str = ""
    version: int = 1
    used_fragment_ids: List[str] = Field(default_factory=list)
    used_citation_ids: List[str] = Field(default_factory=list)


class DraftUpsertRequest(BaseModel):
    block: DraftBlockSchema


class CitationResponse(BaseModel):
    cite_key: str = ""
    title: str = ""
    authors: List[str] = Field(default_factory=list)
    year: Optional[int] = None
    doi: Optional[str] = None
    url: Optional[str] = None
    bibtex: Optional[str] = None


class CitationFilterRequest(BaseModel):
    """引用筛选请求：保留或删除指定 cite_key"""

    keep_keys: Optional[List[str]] = Field(None, description="仅保留的 cite_key 列表（与 remove_keys 二选一）")
    remove_keys: Optional[List[str]] = Field(None, description="要删除的 cite_key 列表（与 keep_keys 二选一）")


class CitationFilterResponse(BaseModel):
    """引用筛选响应"""

    removed_count: int = 0
    remaining_keys: List[str] = Field(default_factory=list)


class CanvasAIEditRequest(BaseModel):
    """Canvas AI 段落编辑请求"""

    section_text: str = Field(..., min_length=1, description="选中的段落文本")
    action: str = Field(..., description="操作: rewrite | expand | condense | add_citations | targeted_refine")
    context: str = Field("", description="可选：周围上下文")
    search_mode: str = Field("local", description="是否检索补充资料: local | web | hybrid | none")
    directive: str = Field("", description="可选：定向精炼指令（targeted_refine 推荐）")
    preserve_citations: bool = Field(True, description="是否启用引用保护（默认开启）")


class CanvasAIEditResponse(BaseModel):
    """Canvas AI 段落编辑响应"""

    edited_text: str = Field(..., description="AI 修改后的文本")
    citations_added: List[str] = Field(default_factory=list, description="新增的引用 keys")
    citation_guard_triggered: bool = Field(False, description="是否触发了引用保护兜底")
    citation_guard_message: str = Field("", description="引用保护提示信息")


class ExportRequest(BaseModel):
    session_id: Optional[str] = Field(None, description="会话 ID")
    canvas_id: Optional[str] = Field(None, description="画布 ID")
    format: str = Field("markdown", description="导出格式: markdown | docx")
    cite_key_format: Optional[str] = Field(
        None,
        description="引用键格式: numeric | hash | author_date（默认从配置读取）"
    )


class ExportResponse(BaseModel):
    format: str = "markdown"
    content: str = ""
    session_id: str = ""
    canvas_id: str = ""


class AnnotationSchema(BaseModel):
    """行内批注"""

    id: str = ""
    section_id: str = ""
    target_text: str = ""
    directive: str = ""
    status: str = "pending"
    created_at: Optional[str] = None


class ResearchBriefSchema(BaseModel):
    """研究简报（Explore 阶段的 Survey Canvas 板块）"""

    scope: str = ""
    success_criteria: List[str] = Field(default_factory=list)
    key_questions: List[str] = Field(default_factory=list)
    exclusions: List[str] = Field(default_factory=list)
    time_range: str = ""
    source_priority: List[str] = Field(default_factory=list)
    action_plan: str = ""


class CanvasResponse(BaseModel):
    id: str
    session_id: str = ""
    topic: str = ""
    working_title: str = ""
    abstract: str = ""
    keywords: List[str] = Field(default_factory=list)
    stage: str = "explore"
    refined_markdown: str = ""
    outline: List[Dict[str, Any]] = Field(default_factory=list)
    drafts: Dict[str, Dict[str, Any]] = Field(default_factory=dict)
    citation_pool: List[Dict[str, Any]] = Field(default_factory=list)
    identified_gaps: List[str] = Field(default_factory=list)
    user_directives: List[str] = Field(default_factory=list)
    annotations: List[AnnotationSchema] = Field(default_factory=list)
    research_brief: Optional[ResearchBriefSchema] = None
    research_insights: List[str] = Field(default_factory=list)
    skip_draft_review: bool = False
    skip_refine_review: bool = False
    version: int = 1


class CanvasVersionItem(BaseModel):
    version_number: int
    created_at: str


class CanvasRefineRequest(BaseModel):
    """全文精炼请求（支持多轮迭代）。"""

    content_md: str = Field("", description="当前全文 Markdown；为空时使用画布已保存版本")
    directives: List[str] = Field(default_factory=list, description="本轮附加指令")
    save_snapshot_before: bool = Field(True, description="精炼前是否创建快照")
    locked_ranges: List[Dict[str, Any]] = Field(
        default_factory=list,
        description="锁定片段范围列表，元素格式: {start, end, text}",
    )


class CanvasRefineResponse(BaseModel):
    edited_markdown: str = Field("", description="精炼后的全文")
    snapshot_version: Optional[int] = Field(None, description="精炼前快照版本号（若启用）")
    locked_applied: int = Field(0, description="本轮成功应用的锁定片段数")
    locked_skipped: int = Field(0, description="本轮跳过的锁定片段数（越界/失配/重叠）")
    lock_guard_triggered: bool = Field(False, description="是否触发锁定保护兜底")
    lock_guard_message: str = Field("", description="锁定保护提示")


# ---- Intent Detection ----

class IntentDetectRequest(BaseModel):
    """意图检测请求"""

    message: str = Field(..., min_length=1, description="用户消息")
    session_id: Optional[str] = Field(None, description="会话 ID，用于获取历史上下文")
    current_stage: str = Field("explore", description="当前工作流阶段")


class IntentDetectResponse(BaseModel):
    """意图检测响应（简化版：chat vs deep_research）"""

    mode: str = Field("chat", description="执行模式: chat | deep_research")
    confidence: float = Field(..., description="置信度 0-1")
    suggested_topic: str = Field("", description="如果为 deep_research，建议的综述主题")
    params: Dict[str, Any] = Field(default_factory=dict, description="解析出的参数")
    # 兼容旧字段
    intent_type: str = Field("chat", description="[兼容] 意图类型")
    needs_retrieval: bool = Field(True, description="[兼容] 是否需要检索（现由 UI search_mode 决定）")
    suggested_search_mode: str = Field("hybrid", description="[兼容] 建议的检索模式")


# ---- Model Sync ----

class ModelSyncRequest(BaseModel):
    """模型同步请求"""

    force_update: bool = Field(False, description="是否强制更新模型缓存")
    local_files_only: Optional[bool] = Field(None, description="仅使用本地缓存（不联网）")


class ModelStatusItem(BaseModel):
    name: str
    model_id: str
    cache_dir: str
    exists: bool
    local_files_only: bool
    error: Optional[str] = None


class ModelSyncItem(BaseModel):
    name: str
    model_id: str
    cache_dir: str
    local_files_only: bool
    updated: bool
    status: str
    message: Optional[str] = None
    error: Optional[str] = None
    resolved_path: Optional[str] = None


class ModelStatusResponse(BaseModel):
    items: List[ModelStatusItem]


class ModelSyncResponse(BaseModel):
    items: List[ModelSyncItem]


class AutoCompleteRequest(BaseModel):
    """自动完成综述请求"""

    topic: str = Field(..., min_length=1, description="综述主题")
    session_id: Optional[str] = Field(None, description="会话 ID")
    canvas_id: Optional[str] = Field(None, description="画布 ID")
    search_mode: str = Field("hybrid", description="检索模式: local | web | hybrid")
    max_sections: int = Field(6, ge=1, le=12, description="最大章节数")


class AutoCompleteResponse(BaseModel):
    """自动完成综述响应"""

    session_id: str = ""
    canvas_id: str = ""
    markdown: str = ""
    outline: List[str] = Field(default_factory=list, description="大纲章节列表")
    citations: List[str] = Field(default_factory=list, description="引用键列表")
    total_time_ms: float = 0.0


# ---- Deep Research ----

class ClarifyRequest(BaseModel):
    """Deep Research 澄清问题生成请求"""

    message: str = Field(..., min_length=1, description="用户的主题描述")
    session_id: Optional[str] = Field(None, description="会话 ID（用于获取 chat 历史上下文）")
    search_mode: str = Field("hybrid", description="检索模式，用于预检索领域资料辅助生成问题")
    llm_provider: Optional[str] = Field(None, description="LLM 提供商")
    model_override: Optional[str] = Field(None, description="覆盖 provider 默认模型")


class ClarifyQuestion(BaseModel):
    """单个澄清问题"""

    id: str = Field(..., description="问题唯一标识")
    text: str = Field(..., description="问题文本")
    question_type: str = Field("text", description="问题类型: text | choice | multi_choice")
    options: List[str] = Field(default_factory=list, description="选项（choice/multi_choice 类型时）")
    default: str = Field("", description="默认回答/建议值")


class ClarifyResponse(BaseModel):
    """Deep Research 澄清问题响应"""

    questions: List[ClarifyQuestion] = Field(default_factory=list, description="澄清问题列表（1-6个）")
    suggested_topic: str = Field("", description="系统建议的综述主题")
    suggested_outline: List[str] = Field(default_factory=list, description="初步建议的大纲章节")
    research_brief: Optional[Dict[str, Any]] = Field(None, description="结构化研究简报（Scoping Agent 输出）")
    used_fallback: bool = Field(False, description="是否触发了澄清问题降级回退")
    fallback_reason: str = Field("", description="回退原因（若有）")
    llm_provider_used: str = Field("", description="本次实际使用的 provider")
    llm_model_used: str = Field("", description="本次实际使用的模型（尽力标注）")


class DeepResearchRequest(BaseModel):
    """Deep Research 执行请求（携带澄清回答和完整检索参数）"""

    topic: str = Field(..., min_length=1, description="综述主题")
    session_id: Optional[str] = Field(None, description="会话 ID")
    canvas_id: Optional[str] = Field(None, description="画布 ID")
    user_id: Optional[str] = Field(None, description="用户 ID")
    search_mode: str = Field("hybrid", description="检索模式: local | web | hybrid")
    max_sections: int = Field(6, ge=1, le=12, description="最大章节数")
    clarification_answers: Dict[str, str] = Field(
        default_factory=dict,
        description="澄清问题回答 {question_id: answer_text}",
    )
    # ---- 完整检索参数（与 ChatRequest 保持一致）----
    web_providers: Optional[List[str]] = Field(None, description="Web 搜索来源")
    web_source_configs: Optional[Dict[str, Dict[str, Any]]] = Field(None, description="每个搜索源配置")
    use_query_optimizer: Optional[bool] = Field(None, description="启用查询优化器")
    query_optimizer_max_queries: Optional[int] = Field(None, description="每个搜索引擎查询数")
    local_top_k: Optional[int] = Field(None, description="本地检索 top_k")
    local_threshold: Optional[float] = Field(None, description="本地检索阈值")
    year_start: Optional[int] = Field(None, ge=1900, le=2100, description="年份窗口起始（硬过滤）")
    year_end: Optional[int] = Field(None, ge=1900, le=2100, description="年份窗口结束（硬过滤）")
    final_top_k: Optional[int] = Field(None, description="最终保留文档数（合并重排后）")
    llm_provider: Optional[str] = Field(None, description="LLM 提供商")
    model_override: Optional[str] = Field(None, description="覆盖 provider 默认模型")
    use_agent: bool = Field(False, description="是否使用递归研究 Agent（LangGraph 引擎）")
    output_language: str = Field("auto", description="输出语言: auto | en | zh")
    step_models: Optional[Dict[str, Optional[str]]] = Field(
        None,
        description="各步骤模型覆盖：{step: 'provider::model' | null}",
    )


class DeepResearchStartRequest(BaseModel):
    """Deep Research 第一阶段请求：生成 Brief + Outline（待用户确认）"""

    topic: str = Field(..., min_length=1, description="综述主题")
    session_id: Optional[str] = Field(None, description="会话 ID")
    canvas_id: Optional[str] = Field(None, description="画布 ID")
    user_id: Optional[str] = Field(None, description="用户 ID")
    search_mode: str = Field("hybrid", description="检索模式: local | web | hybrid")
    clarification_answers: Optional[Dict[str, str]] = Field(
        None,
        description="澄清问题回答 {question_id: answer_text}",
    )
    output_language: str = Field("auto", description="输出语言: auto | en | zh")
    step_models: Optional[Dict[str, Optional[str]]] = Field(
        None,
        description="各步骤模型覆盖：{step: 'provider::model' | null}",
    )
    step_model_strict: bool = Field(
        False,
        description="步骤模型解析是否严格模式：true=任一步骤 provider 解析失败直接报错；false=自动回退默认模型并记录告警",
    )
    web_providers: Optional[List[str]] = Field(None, description="Web 搜索来源")
    web_source_configs: Optional[Dict[str, Dict[str, Any]]] = Field(None, description="每个搜索源配置")
    use_query_optimizer: Optional[bool] = Field(None, description="启用查询优化器")
    query_optimizer_max_queries: Optional[int] = Field(None, description="每个搜索引擎查询数")
    local_top_k: Optional[int] = Field(None, description="本地检索 top_k")
    local_threshold: Optional[float] = Field(None, description="本地检索阈值")
    year_start: Optional[int] = Field(None, ge=1900, le=2100, description="年份窗口起始（硬过滤）")
    year_end: Optional[int] = Field(None, ge=1900, le=2100, description="年份窗口结束（硬过滤）")
    final_top_k: Optional[int] = Field(None, description="最终保留文档数")
    llm_provider: Optional[str] = Field(None, description="LLM 提供商")
    model_override: Optional[str] = Field(None, description="覆盖 provider 默认模型")
    collection: Optional[str] = Field(None, description="本地检索目标 collection")


class DeepResearchStartResponse(BaseModel):
    """Deep Research 第一阶段响应"""

    session_id: str = Field("", description="会话 ID")
    canvas_id: str = Field("", description="画布 ID")
    brief: Dict[str, Any] = Field(default_factory=dict, description="研究简报")
    outline: List[str] = Field(default_factory=list, description="建议大纲")
    initial_stats: Dict[str, Any] = Field(default_factory=dict, description="初始检索统计")


class DeepResearchConfirmRequest(BaseModel):
    """Deep Research 第二阶段请求：确认后执行研究"""

    topic: str = Field(..., min_length=1, description="综述主题")
    session_id: Optional[str] = Field(None, description="会话 ID")
    canvas_id: Optional[str] = Field(None, description="画布 ID")
    user_id: Optional[str] = Field(None, description="用户 ID")
    search_mode: str = Field("hybrid", description="检索模式: local | web | hybrid")
    confirmed_outline: List[str] = Field(default_factory=list, description="用户确认后的大纲")
    confirmed_brief: Optional[Dict[str, Any]] = Field(None, description="用户确认后的简报（可选）")
    depth: str = Field(
        "comprehensive",
        description=(
            "研究深度: lite（快速探索，~3-10 min）| comprehensive（全面学术综述，~15-40 min）。"
            "控制每章研究轮次、覆盖度阈值、搜索量和审核超时等全部循环上限。"
        ),
    )
    output_language: str = Field("auto", description="输出语言: auto | en | zh")
    step_models: Optional[Dict[str, Optional[str]]] = Field(
        None,
        description="各步骤模型覆盖：{step: 'provider::model' | null}",
    )
    step_model_strict: bool = Field(
        False,
        description="步骤模型解析是否严格模式：true=任一步骤 provider 解析失败直接报错；false=自动回退默认模型并记录告警",
    )
    web_providers: Optional[List[str]] = Field(None, description="Web 搜索来源")
    web_source_configs: Optional[Dict[str, Dict[str, Any]]] = Field(None, description="每个搜索源配置")
    use_query_optimizer: Optional[bool] = Field(None, description="启用查询优化器")
    query_optimizer_max_queries: Optional[int] = Field(None, description="每个搜索引擎查询数")
    local_top_k: Optional[int] = Field(None, description="本地检索 top_k")
    local_threshold: Optional[float] = Field(None, description="本地检索阈值")
    year_start: Optional[int] = Field(None, ge=1900, le=2100, description="年份窗口起始（硬过滤）")
    year_end: Optional[int] = Field(None, ge=1900, le=2100, description="年份窗口结束（硬过滤）")
    final_top_k: Optional[int] = Field(None, description="最终保留文档数")
    llm_provider: Optional[str] = Field(None, description="LLM 提供商")
    model_override: Optional[str] = Field(None, description="覆盖 provider 默认模型")
    collection: Optional[str] = Field(None, description="本地检索目标 collection")
    user_context: Optional[str] = Field(
        None,
        description="用户补充的观点/约束（仅本次 Deep Research 使用，不写入持久知识库）",
    )
    user_context_mode: Optional[str] = Field(
        "supporting",
        description="用户文本上下文模式: supporting | direct_injection",
    )
    user_documents: Optional[List[Dict[str, Any]]] = Field(
        None,
        description="用户上传的临时材料摘要列表 [{name, content}]，仅本次任务使用",
    )
    # ---- 阶段跳过控制 ----
    skip_draft_review: bool = Field(
        False,
        description="跳过逐章审阅（Drafting 阶段 Agent 连续写完所有章节）",
    )
    skip_refine_review: bool = Field(
        False,
        description="跳过精炼审阅（Refine 阶段直接展示最终结果）",
    )
    skip_claim_generation: bool = Field(
        False,
        description="跳过前置论点提炼（Claim Generation）阶段，直接进入写作",
    )


class DeepResearchSubmitResponse(BaseModel):
    """Deep Research 后台任务提交响应"""

    ok: bool = True
    job_id: str = Field(..., description="后台任务 ID")
    session_id: str = Field("", description="会话 ID")
    canvas_id: str = Field("", description="画布 ID")


class DeepResearchJobInfo(BaseModel):
    """Deep Research 后台任务状态"""

    job_id: str
    topic: str
    session_id: str = ""
    canvas_id: str = ""
    status: str
    current_stage: str = ""
    message: str = ""
    error_message: str = ""
    result_markdown: str = ""
    result_citations: List[Dict[str, Any]] = Field(default_factory=list)
    result_dashboard: Dict[str, Any] = Field(default_factory=dict)
    total_time_ms: float = 0.0
    created_at: float
    updated_at: float
    finished_at: Optional[float] = None


class DeepResearchContextExtractResponse(BaseModel):
    """Deep Research 临时上下文文件提取结果"""

    documents: List[Dict[str, str]] = Field(default_factory=list, description="提取后的临时文档 [{name, content}]")


# ---- Auth ----

class LoginRequest(BaseModel):
    user_id: str = Field(..., min_length=1, description="用户名/用户 ID")
    password: str = Field(..., min_length=1, description="密码")


class LoginResponse(BaseModel):
    token: str = Field(..., description="登录后使用的 token")
    user_id: str = ""
    role: str = "user"


class CreateUserRequest(BaseModel):
    user_id: str = Field(..., min_length=1, description="用户名/用户 ID")
    password: str = Field(..., min_length=1, description="密码")
    role: str = Field("user", description="角色: user | admin")


class UserItem(BaseModel):
    user_id: str = ""
    role: str = "user"
    is_active: bool = True
    created_at: str = ""
    updated_at: str = ""
</file>

<file path="src/llm/tools.py">
"""
统一 Tool 抽象层。

一次定义 Tool Schema，自动适配 OpenAI / Anthropic function calling 格式，
并提供 prompt-based 降级方案。

Usage:
    from src.llm.tools import CORE_TOOLS, ToolDef, to_openai_tools, to_anthropic_tools, execute_tool_call
    from src.llm.tools import get_routed_skills, get_tools_by_names
"""

from __future__ import annotations

import json
import os
import re
import subprocess
import sys
import tempfile
import traceback
from dataclasses import dataclass, field
from typing import Any, Callable, Dict, List, Optional

from src.log import get_logger

logger = get_logger(__name__)


# ────────────────────────────────────────────────
# Tool 定义
# ────────────────────────────────────────────────

@dataclass
class ToolDef:
    """Provider 无关的 Tool 定义"""
    name: str
    description: str
    parameters: Dict[str, Any]  # JSON Schema (type: object)
    handler: Optional[Callable[..., Any]] = None  # 实际执行函数

    # ── 转换为 OpenAI function calling 格式 ──
    def to_openai(self) -> Dict[str, Any]:
        return {
            "type": "function",
            "function": {
                "name": self.name,
                "description": self.description,
                "parameters": self.parameters,
            },
        }

    # ── 转换为 Anthropic tool 格式 ──
    def to_anthropic(self) -> Dict[str, Any]:
        return {
            "name": self.name,
            "description": self.description,
            "input_schema": self.parameters,
        }


@dataclass
class ToolCall:
    """统一的 tool call 解析结果（provider 无关）"""
    id: str
    name: str
    arguments: Dict[str, Any]


@dataclass
class ToolResult:
    """Tool 执行结果"""
    tool_call_id: str
    name: str
    content: str
    is_error: bool = False


# ────────────────────────────────────────────────
# 格式转换工具
# ────────────────────────────────────────────────

def to_openai_tools(tools: List[ToolDef]) -> List[Dict[str, Any]]:
    return [t.to_openai() for t in tools]


def to_anthropic_tools(tools: List[ToolDef]) -> List[Dict[str, Any]]:
    return [t.to_anthropic() for t in tools]


def tools_to_prompt(tools: List[ToolDef]) -> str:
    """降级：将 tools 描述注入 system prompt（用于不支持 FC 的模型）"""
    lines = ["你可以调用以下工具来完成任务。调用格式：",
             '<tool_call>{"name": "tool_name", "arguments": {...}}</tool_call>',
             "", "可用工具："]
    for t in tools:
        params_desc = []
        props = t.parameters.get("properties", {})
        for pname, pinfo in props.items():
            req = "必填" if pname in t.parameters.get("required", []) else "可选"
            params_desc.append(f"    - {pname} ({pinfo.get('type', 'any')}, {req}): {pinfo.get('description', '')}")
        lines.append(f"\n**{t.name}**: {t.description}")
        if params_desc:
            lines.append("  参数:")
            lines.extend(params_desc)
    return "\n".join(lines)


# ────────────────────────────────────────────────
# 响应解析：从 raw LLM response 中提取 tool calls
# ────────────────────────────────────────────────

def parse_tool_calls_openai(raw: Dict[str, Any]) -> List[ToolCall]:
    """从 OpenAI-compatible 响应中解析 tool_calls"""
    calls = []
    choices = raw.get("choices") or []
    if not choices:
        return calls
    message = choices[0].get("message") or {}
    tool_calls = message.get("tool_calls") or []
    for tc in tool_calls:
        fn = tc.get("function") or {}
        args_str = fn.get("arguments", "{}")
        try:
            args = json.loads(args_str) if isinstance(args_str, str) else args_str
        except json.JSONDecodeError:
            args = {"_raw": args_str}
        calls.append(ToolCall(
            id=tc.get("id", ""),
            name=fn.get("name", ""),
            arguments=args,
        ))
    return calls


def parse_tool_calls_anthropic(raw: Dict[str, Any]) -> List[ToolCall]:
    """从 Anthropic 响应中解析 tool_use content blocks"""
    calls = []
    content = raw.get("content") or []
    for block in content:
        if not isinstance(block, dict):
            continue
        if block.get("type") == "tool_use":
            calls.append(ToolCall(
                id=block.get("id", ""),
                name=block.get("name", ""),
                arguments=block.get("input") or {},
            ))
    return calls


def parse_tool_calls_prompt(text: str) -> List[ToolCall]:
    """从纯文本响应中解析 <tool_call> 标签（降级模式）"""
    calls = []
    pattern = r"<tool_call>\s*(\{.*?\})\s*</tool_call>"
    for i, m in enumerate(re.finditer(pattern, text, re.DOTALL)):
        try:
            data = json.loads(m.group(1))
            calls.append(ToolCall(
                id=f"prompt_tc_{i}",
                name=data.get("name", ""),
                arguments=data.get("arguments") or data.get("args") or {},
            ))
        except json.JSONDecodeError:
            continue
    return calls


def parse_tool_calls(raw: Dict[str, Any], is_anthropic: bool = False) -> List[ToolCall]:
    """统一解析入口"""
    if is_anthropic:
        calls = parse_tool_calls_anthropic(raw)
    else:
        calls = parse_tool_calls_openai(raw)
    # 如果原生 FC 没有返回 tool_calls，尝试从文本中降级解析
    if not calls:
        text = ""
        if is_anthropic:
            for block in (raw.get("content") or []):
                if isinstance(block, dict) and block.get("type") == "text":
                    text += block.get("text", "")
        else:
            choices = raw.get("choices") or []
            if choices:
                msg = choices[0].get("message") or {}
                c = msg.get("content")
                if isinstance(c, str):
                    text = c
        if "<tool_call>" in text:
            calls = parse_tool_calls_prompt(text)
    return calls


def has_tool_calls(raw: Dict[str, Any], is_anthropic: bool = False) -> bool:
    """快速判断响应中是否包含 tool calls"""
    if is_anthropic:
        stop = raw.get("stop_reason", "")
        if stop == "tool_use":
            return True
        for block in (raw.get("content") or []):
            if isinstance(block, dict) and block.get("type") == "tool_use":
                return True
    else:
        choices = raw.get("choices") or []
        if choices:
            msg = choices[0].get("message") or {}
            if msg.get("tool_calls"):
                return True
            finish = choices[0].get("finish_reason", "")
            if finish in ("tool_calls", "function_call"):
                return True
    return False


# ────────────────────────────────────────────────
# Tool 执行
# ────────────────────────────────────────────────

def execute_tool_call(
    tool_call: ToolCall,
    tools: List[ToolDef],
) -> ToolResult:
    """查找并执行 tool call，返回结果"""
    tool_map = {t.name: t for t in tools}
    tool = tool_map.get(tool_call.name)
    if tool is None:
        return ToolResult(
            tool_call_id=tool_call.id,
            name=tool_call.name,
            content=f"Error: unknown tool '{tool_call.name}'",
            is_error=True,
        )
    if tool.handler is None:
        return ToolResult(
            tool_call_id=tool_call.id,
            name=tool_call.name,
            content=f"Error: tool '{tool_call.name}' has no handler",
            is_error=True,
        )
    try:
        result = tool.handler(**tool_call.arguments)
        # 将结果序列化为字符串
        if isinstance(result, str):
            content = result
        elif isinstance(result, dict) or isinstance(result, list):
            content = json.dumps(result, ensure_ascii=False, default=str)
        else:
            content = str(result)
        # 截断过长结果
        if len(content) > 8000:
            content = content[:7500] + "\n... (truncated)"
        return ToolResult(
            tool_call_id=tool_call.id,
            name=tool_call.name,
            content=content,
        )
    except Exception as e:
        logger.warning(f"Tool execution error [{tool_call.name}]: {e}")
        return ToolResult(
            tool_call_id=tool_call.id,
            name=tool_call.name,
            content=f"Error executing {tool_call.name}: {e}",
            is_error=True,
        )


# ────────────────────────────────────────────────
# 构建 tool result 消息（喂回 LLM）
# ────────────────────────────────────────────────

def tool_result_to_openai_message(result: ToolResult) -> Dict[str, Any]:
    """将 ToolResult 转为 OpenAI tool message"""
    return {
        "role": "tool",
        "tool_call_id": result.tool_call_id,
        "content": result.content,
    }


def tool_result_to_anthropic_content(result: ToolResult) -> Dict[str, Any]:
    """将 ToolResult 转为 Anthropic tool_result content block"""
    return {
        "type": "tool_result",
        "tool_use_id": result.tool_call_id,
        "content": result.content,
        "is_error": result.is_error,
    }


# ────────────────────────────────────────────────
# 8 个核心 Tool 定义（handler 在 register 时绑定）
# ────────────────────────────────────────────────

_SEARCH_LOCAL_SCHEMA = {
    "type": "object",
    "properties": {
        "query": {"type": "string", "description": "搜索查询文本"},
        "top_k": {"type": "integer", "description": "返回结果数量", "default": 10},
    },
    "required": ["query"],
}

_SEARCH_WEB_SCHEMA = {
    "type": "object",
    "properties": {
        "query": {"type": "string", "description": "网络搜索查询"},
        "top_k": {"type": "integer", "description": "返回结果数量", "default": 10},
    },
    "required": ["query"],
}

_SEARCH_SCHOLAR_SCHEMA = {
    "type": "object",
    "properties": {
        "query": {"type": "string", "description": "学术论文搜索查询"},
        "year_from": {"type": "integer", "description": "起始年份（可选）"},
        "limit": {"type": "integer", "description": "结果数量", "default": 5},
    },
    "required": ["query"],
}

_EXPLORE_GRAPH_SCHEMA = {
    "type": "object",
    "properties": {
        "entity_name": {"type": "string", "description": "实体名称"},
        "depth": {"type": "integer", "description": "扩展深度 (1-3)", "default": 1},
    },
    "required": ["entity_name"],
}

_CANVAS_SCHEMA = {
    "type": "object",
    "properties": {
        "action": {"type": "string", "enum": ["create", "update", "get"], "description": "操作类型"},
        "canvas_id": {"type": "string", "description": "画布 ID（update/get 时必填）"},
        "topic": {"type": "string", "description": "主题（create 时必填）"},
        "content": {"type": "string", "description": "要更新的 Markdown 内容"},
    },
    "required": ["action"],
}

_CITATIONS_SCHEMA = {
    "type": "object",
    "properties": {
        "canvas_id": {"type": "string", "description": "画布 ID"},
        "format": {"type": "string", "enum": ["text", "bibtex"], "description": "引文格式", "default": "text"},
    },
    "required": ["canvas_id"],
}

_COMPARE_SCHEMA = {
    "type": "object",
    "properties": {
        "paper_ids": {
            "type": "array",
            "items": {"type": "string"},
            "description": "要比较的论文 ID 列表 (2-5)",
            "minItems": 2,
            "maxItems": 5,
        },
        "aspects": {
            "type": "array",
            "items": {"type": "string"},
            "description": "比较维度",
        },
    },
    "required": ["paper_ids"],
}

_RUN_CODE_SCHEMA = {
    "type": "object",
    "properties": {
        "code": {"type": "string", "description": "要执行的 Python 代码（简单计算/数据处理）"},
    },
    "required": ["code"],
}

_SEARCH_NCBI_SCHEMA = {
    "type": "object",
    "properties": {
        "query": {
            "type": "string",
            "description": "PubMed 生物医学搜索查询（英文关键词效果最佳）",
        },
        "limit": {
            "type": "integer",
            "description": "返回结果数量",
            "default": 5,
        },
    },
    "required": ["query"],
}


# ── Handler 实现 ──

def _handle_search_local(query: str, top_k: int = 10, **_) -> str:
    from src.retrieval.service import get_retrieval_service
    svc = get_retrieval_service()
    pack = svc.search(query=query, mode="local", top_k=top_k)
    return pack.to_context_string(max_chunks=min(top_k, 15))


def _handle_search_web(query: str, top_k: int = 10, **_) -> str:
    from src.retrieval.service import get_retrieval_service
    svc = get_retrieval_service()
    pack = svc.search(query=query, mode="web", top_k=top_k)
    return pack.to_context_string(max_chunks=min(top_k, 15))


def _handle_search_scholar(query: str, year_from: Optional[int] = None, limit: int = 5, **_) -> str:
    try:
        from src.retrieval.semantic_scholar import SemanticScholarSearch
        ss = SemanticScholarSearch()
        results = ss.search(query, year_from=year_from, limit=limit)
        if not results:
            return "未找到相关学术论文。"
        lines = []
        for r in results[:limit]:
            title = r.get("title", "")
            year = r.get("year", "")
            abstract = (r.get("abstract") or "")[:300]
            doi = r.get("externalIds", {}).get("DOI", "")
            lines.append(f"- **{title}** ({year}) DOI:{doi}\n  {abstract}")
        return "\n".join(lines)
    except Exception as e:
        return f"学术搜索失败: {e}"


def _handle_explore_graph(entity_name: str, depth: int = 1, **_) -> str:
    try:
        from src.api.routes_graph import graph_neighbors
        result = graph_neighbors(entity_name, depth=depth)
        nodes = result.get("nodes", [])
        edges = result.get("edges", [])
        lines = [f"实体 '{entity_name}' 的知识图谱（深度={depth}）:"]
        lines.append(f"节点数: {len(nodes)}, 边数: {len(edges)}")
        entity_nodes = [n for n in nodes if n.get("type") != "CHUNK"]
        for n in entity_nodes[:20]:
            marker = " [中心]" if n.get("is_center") else ""
            lines.append(f"  - {n['id']} ({n['type']}){marker}")
        for e in edges[:20]:
            lines.append(f"  {e['source']} --[{e['relation']}]--> {e['target']}")
        return "\n".join(lines)
    except Exception as e:
        return f"图谱查询失败: {e}"


def _handle_canvas(action: str, canvas_id: str = "", topic: str = "", content: str = "", **_) -> str:
    from src.collaboration.canvas.canvas_manager import create_canvas, get_canvas, update_canvas
    if action == "create":
        canvas = create_canvas(topic=topic or "Untitled")
        return json.dumps({"canvas_id": canvas.id, "topic": canvas.topic}, ensure_ascii=False)
    elif action == "get":
        canvas = get_canvas(canvas_id)
        if canvas is None:
            return f"画布 '{canvas_id}' 不存在"
        return json.dumps({"canvas_id": canvas.id, "topic": canvas.topic, "markdown": canvas.markdown[:3000]}, ensure_ascii=False)
    elif action == "update":
        update_canvas(canvas_id, markdown=content)
        return f"画布 '{canvas_id}' 已更新"
    return f"未知操作: {action}"


def _handle_citations(canvas_id: str, format: str = "text", **_) -> str:
    from src.collaboration.citation.formatter import format_reference_list
    from src.collaboration.canvas.canvas_manager import get_canvas_citations
    citations = get_canvas_citations(canvas_id)
    if not citations:
        return "该画布暂无引文。"
    # 映射前端参数到有效 style
    style_map = {
        "text": "custom",
        "bibtex": "apa",
        "apa": "apa",
        "ieee": "ieee",
        "numeric": "numeric",
        "custom": "custom",
    }
    style = style_map.get(format, "custom")
    return format_reference_list(citations, style=style)


def _handle_compare(paper_ids: List[str], aspects: Optional[List[str]] = None, **_) -> str:
    try:
        from src.api.routes_compare import compare_papers, CompareRequest
        req = CompareRequest(paper_ids=paper_ids, aspects=aspects or ["objective", "methodology", "key_findings", "limitations"])
        resp = compare_papers(req)
        parts = []
        if resp.narrative:
            parts.append(f"综合分析: {resp.narrative}")
        for aspect, cells in resp.comparison_matrix.items():
            parts.append(f"\n[{aspect}]")
            for pid, desc in cells.items():
                parts.append(f"  {pid}: {desc}")
        return "\n".join(parts) if parts else "对比结果为空"
    except Exception as e:
        return f"论文对比失败: {e}"


def _handle_search_ncbi(query: str, limit: int = 5, **_) -> str:
    """调用 NCBI PubMed E-Utilities，返回生物医学文献摘要信息。"""
    try:
        import asyncio
        from src.retrieval.ncbi_search import get_ncbi_searcher

        searcher = get_ncbi_searcher()
        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor() as pool:
                    future = pool.submit(asyncio.run, searcher.search(query, limit=limit))
                    results = future.result(timeout=30)
            else:
                results = loop.run_until_complete(searcher.search(query, limit=limit))
        except RuntimeError:
            results = asyncio.run(searcher.search(query, limit=limit))

        if not results:
            return "PubMed 未找到相关文献。"

        lines = []
        for r in results[:limit]:
            meta = r.get("metadata", {})
            title = meta.get("title", r.get("content", ""))
            year = meta.get("year", "")
            doi = meta.get("doi", "")
            authors = meta.get("authors", [])
            authors_str = ", ".join(authors[:3]) + ("..." if len(authors) > 3 else "")
            url = meta.get("url", "")
            lines.append(
                f"- **{title}** ({year})\n"
                f"  Authors: {authors_str}\n"
                f"  DOI: {doi or '—'}  URL: {url}"
            )
        return "\n".join(lines)
    except Exception as e:
        return f"NCBI 搜索失败: {e}"


def _handle_run_code(code: str, **_) -> str:
    """以子进程方式执行 Python 代码。"""
    # TODO(Security): 生产环境强烈建议将 subprocess 替换为安全的隔离沙盒 API (如 E2B)。
    tmp_file = None
    try:
        tmp_file = tempfile.NamedTemporaryFile(
            suffix=".py",
            delete=False,
            mode="w",
            encoding="utf-8",
        )
        with tmp_file:
            tmp_file.write(code)

        result = subprocess.run(
            [sys.executable, tmp_file.name],
            capture_output=True,
            text=True,
            timeout=15,
        )
        if result.returncode == 0:
            return result.stdout
        return f"{result.stdout}{result.stderr}"
    except subprocess.TimeoutExpired:
        return "代码执行超时（15秒），已中止。"
    except Exception as e:
        return f"执行错误: {e}"
    finally:
        if tmp_file is not None:
            try:
                os.remove(tmp_file.name)
            except OSError:
                pass


# ── 注册核心 Tools ──

CORE_TOOLS: List[ToolDef] = [
    ToolDef(
        name="search_local",
        description="检索本地知识库（向量数据库 + 图谱融合检索），适用于查找已入库的论文和文档内容。",
        parameters=_SEARCH_LOCAL_SCHEMA,
        handler=_handle_search_local,
    ),
    ToolDef(
        name="search_web",
        description="网络搜索（Tavily/Google），获取最新的在线信息和网页内容。",
        parameters=_SEARCH_WEB_SCHEMA,
        handler=_handle_search_web,
    ),
    ToolDef(
        name="search_scholar",
        description="学术论文搜索（Semantic Scholar），查找特定领域的学术文献，获取标题、摘要、DOI。",
        parameters=_SEARCH_SCHOLAR_SCHEMA,
        handler=_handle_search_scholar,
    ),
    ToolDef(
        name="explore_graph",
        description="知识图谱探索，查看指定实体的关联实体和关系，发现跨文档的知识连接。",
        parameters=_EXPLORE_GRAPH_SCHEMA,
        handler=_handle_explore_graph,
    ),
    ToolDef(
        name="canvas",
        description="操作研究画布：创建(create)、获取(get)、更新(update)画布内容。",
        parameters=_CANVAS_SCHEMA,
        handler=_handle_canvas,
    ),
    ToolDef(
        name="get_citations",
        description="获取画布的引文列表，支持 text 和 bibtex 格式。",
        parameters=_CITATIONS_SCHEMA,
        handler=_handle_citations,
    ),
    ToolDef(
        name="compare_papers",
        description="多文档对比：选择 2-5 篇论文，自动生成结构化对比矩阵和分析。",
        parameters=_COMPARE_SCHEMA,
        handler=_handle_compare,
    ),
    ToolDef(
        name="run_code",
        description="执行简单的 Python 代码进行数据计算、统计验证或格式转换。",
        parameters=_RUN_CODE_SCHEMA,
        handler=_handle_run_code,
    ),
    ToolDef(
        name="search_ncbi",
        description=(
            "搜索 NCBI PubMed 生物医学文献库，专攻生物学、医学、基因组学、海洋生态等领域。"
            "返回标题、作者、年份、DOI，适合精确的生物医学文献检索。"
        ),
        parameters=_SEARCH_NCBI_SCHEMA,
        handler=_handle_search_ncbi,
    ),
]


# ────────────────────────────────────────────────
# Tool Registry & Dynamic Skill Routing
# ────────────────────────────────────────────────

_TOOL_REGISTRY: Dict[str, ToolDef] = {t.name: t for t in CORE_TOOLS}

_GROUP_SEARCH_LOCAL = frozenset({"search_local"})
_GROUP_WEB = frozenset({"search_web", "search_scholar", "search_ncbi"})
_GROUP_ANALYSIS = frozenset({"compare_papers", "run_code"})
_GROUP_GRAPH = frozenset({"explore_graph"})
_GROUP_COLLAB = frozenset({"canvas", "get_citations"})

_WEB_PROVIDER_TO_TOOL: Dict[str, str] = {
    "tavily": "search_web",
    "google": "search_web",
    "scholar": "search_scholar",
    "semantic": "search_scholar",
    "ncbi": "search_ncbi",
    "pubmed": "search_ncbi",
}

_RE_ANALYSIS = re.compile(
    r"对比|比较|差异|统计|计算|代码|数据分析|分析数据"
    r"|compare|contrast|diff|statistic|calculat|code|data\s*analy",
    re.IGNORECASE,
)
_RE_GRAPH = re.compile(
    r"关系|网络|图谱|关联|知识图"
    r"|relation|network|graph|connection|linked|topology",
    re.IGNORECASE,
)
_RE_COLLAB = re.compile(
    r"画布|草稿|大纲|引用|参考文献|引文"
    r"|canvas|draft|outline|citation|reference|bibliography",
    re.IGNORECASE,
)

_TOOL_ORDER: Dict[str, int] = {t.name: i for i, t in enumerate(CORE_TOOLS)}


def get_tools_by_names(names: List[str]) -> List[ToolDef]:
    """Return ToolDef instances matching the given tool names, preserving CORE_TOOLS order."""
    tools = [_TOOL_REGISTRY[n] for n in names if n in _TOOL_REGISTRY]
    tools.sort(key=lambda t: _TOOL_ORDER.get(t.name, 999))
    return tools


def get_routed_skills(
    message: str,
    current_stage: str,
    search_mode: str,
    allowed_web_providers: Optional[List[str]] = None,
) -> List[ToolDef]:
    """
    Dynamic skill routing — select only the tools relevant to the current
    request instead of mounting all CORE_TOOLS.

    This reduces prompt token cost and lowers the probability of the LLM
    hallucinating tool calls to irrelevant tools.

    Routing rules
    ─────────────
    1. search_local: always on when search_mode != "none"
    2. Web group (search_web / search_scholar / search_ncbi):
       active when search_mode allows web; narrowed by allowed_web_providers
    3. Analysis group (compare_papers / run_code):
       keyword-triggered by comparison / statistics / code mentions
    4. Graph group (explore_graph):
       keyword-triggered by relationship / graph / network mentions
    5. Collab group (canvas / get_citations):
       stage-triggered (drafting / refine) or keyword-triggered
    """
    selected: set[str] = set()

    # 1. Local search — the backbone of RAG
    if search_mode != "none":
        selected |= _GROUP_SEARCH_LOCAL

    # 2. Web tools — gated by search_mode and optionally by explicit provider list
    if search_mode in ("web", "hybrid"):
        if allowed_web_providers is not None:
            for provider in allowed_web_providers:
                tool_name = _WEB_PROVIDER_TO_TOOL.get(provider.lower().strip())
                if tool_name and tool_name in _GROUP_WEB:
                    selected.add(tool_name)
        else:
            selected |= _GROUP_WEB

    # 3. Analysis group — keyword activated
    if _RE_ANALYSIS.search(message):
        selected |= _GROUP_ANALYSIS

    # 4. Graph group — keyword activated
    if _RE_GRAPH.search(message):
        selected |= _GROUP_GRAPH

    # 5. Collaboration group — stage or keyword activated
    stage_lower = (current_stage or "").lower()
    if stage_lower in ("drafting", "draft", "refine", "writing"):
        selected |= _GROUP_COLLAB
    elif _RE_COLLAB.search(message):
        selected |= _GROUP_COLLAB

    # Fallback: guarantee at least search_local so the agent is never toolless
    if not selected and search_mode != "none":
        selected.add("search_local")

    tools = [_TOOL_REGISTRY[name] for name in selected if name in _TOOL_REGISTRY]
    tools.sort(key=lambda t: _TOOL_ORDER.get(t.name, 999))

    logger.info(
        "skill_router | stage=%s mode=%s providers=%s → tools=[%s] (%d/%d)",
        current_stage, search_mode, allowed_web_providers,
        ", ".join(t.name for t in tools), len(tools), len(CORE_TOOLS),
    )
    return tools
</file>

<file path="src/retrieval/service.py">
"""
统一检索服务 - RetrievalService

封装 HybridRetriever + UnifiedWebSearch，对外提供 search(query, mode, filters) -> EvidencePack。
支持 local / web / hybrid 三种模式。hybrid 时 local 与 web 并行执行。
"""

import logging
import math
import re
import time
from concurrent.futures import ThreadPoolExecutor, TimeoutError as FuturesTimeoutError
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

from config.settings import settings
from src.retrieval.evidence import EvidenceChunk, EvidencePack
from src.retrieval.dedup import cross_source_dedup
from src.retrieval.hybrid_retriever import HybridRetriever, RetrievalConfig, _rerank_candidates
from src.retrieval.unified_web_search import unified_web_searcher

logger = logging.getLogger(__name__)

# ── Observability ──
try:
    from src.observability import metrics as obs_metrics, tracer as obs_tracer
except Exception:
    obs_metrics = None  # type: ignore
    obs_tracer = None  # type: ignore


def _parse_paper_id(paper_id: str) -> Tuple[Optional[int], Optional[List[str]], Optional[str]]:
    """
    从 paper_id（文件名格式）解析 year、authors、title。
    
    支持格式：
    - "2026_Botté_et_al_Artificial_Light_at_Night" → (2026, ["Botté et al."], "Artificial Light at Night")
    - "Smith_Jones_2023_Deep_Sea_Cold_Seeps" → (2023, ["Smith", "Jones"], "Deep Sea Cold Seeps")
    - "2024_Wang_Microbiome_Analysis" → (2024, ["Wang"], "Microbiome Analysis")
    
    Returns:
        (year, authors, title) - 任一字段可能为 None
    """
    if not paper_id:
        return None, None, None
    
    year = None
    authors = None
    title = None
    
    # 按下划线分割
    parts = paper_id.replace("-", "_").split("_")
    if not parts:
        return None, None, None
    
    # 查找年份位置（4位数字，1900-2100）
    year_idx = -1
    for i, part in enumerate(parts):
        if re.match(r"^(19|20)\d{2}$", part):
            year = int(part)
            year_idx = i
            break
    
    # 常见的非作者名单词（可能是标题开头）
    TITLE_INDICATORS = {
        "a", "an", "the", "on", "in", "of", "for", "with", "to", "and", "or",
        "deep", "sea", "cold", "seep", "marine", "ocean", "coral", "reef",
        "analysis", "study", "review", "research", "investigation", "evidence",
        "effect", "effects", "impact", "role", "new", "novel", "first",
        "artificial", "natural", "environmental", "ecological", "biological",
        "microbiome", "microbiota", "bacteria", "microbial", "community",
    }
    
    def _is_likely_author(word: str) -> bool:
        """判断一个词是否可能是作者名"""
        if not word:
            return False
        # 全大写（如 DNA, RNA）不是作者名
        if word.isupper() and len(word) > 1:
            return False
        # 常见标题词不是作者名
        if word.lower() in TITLE_INDICATORS:
            return False
        # 太长的词（>15字符）不太可能是作者名
        if len(word) > 15:
            return False
        # 首字母大写，且比较短，可能是作者名
        return word[0].isupper()
    
    # 根据年份位置确定作者和标题
    if year_idx == 0:
        # 格式: 2026_Botté_et_al_Title...
        author_parts = []
        title_start_idx = 1
        for i in range(1, len(parts)):
            p = parts[i]
            # "et" 是 "et al." 的一部分
            if p.lower() == "et" and i + 1 < len(parts) and parts[i + 1].lower() == "al":
                author_parts.append("et al.")
                title_start_idx = i + 2
                break
            # 判断是否为作者名
            elif _is_likely_author(p):
                author_parts.append(p)
                title_start_idx = i + 1
            else:
                # 标题开始
                break
        
        if author_parts:
            authors = author_parts
        if title_start_idx < len(parts):
            title = " ".join(parts[title_start_idx:]).replace("_", " ")
    
    elif year_idx > 0:
        # 格式: Smith_Jones_2023_Title...
        authors = parts[:year_idx]
        if year_idx + 1 < len(parts):
            title = " ".join(parts[year_idx + 1:]).replace("_", " ")
    
    else:
        # 没找到年份，尝试从第一个部分判断
        # 可能整个就是标题
        title = " ".join(parts).replace("_", " ")
    
    return year, authors, title


def _hit_to_chunk(hit: Dict[str, Any], source_type: str, query: str) -> EvidenceChunk:
    """将 hybrid 或 web 的 hit 转为 EvidenceChunk"""
    meta = hit.get("metadata") or {}
    content = (hit.get("content") or "").strip()
    chunk_id = meta.get("chunk_id") or hit.get("chunk_id") or meta.get("doc_id") or meta.get("url") or str(id(hit))
    doc_id = meta.get("paper_id") or meta.get("doc_id") or chunk_id
    score = float(hit.get("score", 0.0))
    
    # 提取 authors（支持多种格式）
    authors_raw = meta.get("authors")
    if isinstance(authors_raw, list):
        authors = authors_raw
    elif isinstance(authors_raw, str) and authors_raw:
        # 可能是逗号分隔的字符串
        authors = [a.strip() for a in authors_raw.split(",") if a.strip()]
    else:
        authors = None
    
    # 提取 year（支持 int 或 str）
    year_raw = meta.get("year")
    if isinstance(year_raw, int):
        year = year_raw
    elif isinstance(year_raw, str) and year_raw.isdigit():
        year = int(year_raw)
    else:
        year = None
    
    # 提取 title
    title = meta.get("title")
    
    # 如果缺少 authors/year/title，尝试从 paper_id（文件名）解析
    if (authors is None or year is None or title is None) and doc_id:
        parsed_year, parsed_authors, parsed_title = _parse_paper_id(doc_id)
        if authors is None and parsed_authors:
            authors = parsed_authors
        if year is None and parsed_year:
            year = parsed_year
        if title is None and parsed_title:
            title = parsed_title
    
    # 提取 bbox（Docling 物理坐标列表）
    raw_bbox = meta.get("bbox")
    bbox = None
    if isinstance(raw_bbox, list) and raw_bbox:
        # 兼容两种格式：
        # - [x0, y0, x1, y1]
        # - [[x0, y0, x1, y1], ...]（取第一块用于前端单框高亮）
        first = raw_bbox[0]
        if isinstance(first, (int, float)):
            bbox = raw_bbox
        elif isinstance(first, list) and len(first) >= 4:
            bbox = first

    return EvidenceChunk(
        chunk_id=str(chunk_id),
        doc_id=str(doc_id),
        text=content,
        score=score,
        source_type=source_type,
        doc_title=title,
        authors=authors,
        year=year,
        url=meta.get("url"),
        doi=meta.get("doi"),
        page_num=meta.get("page") if isinstance(meta.get("page"), int) else None,
        section_title=meta.get("section_path"),
        bbox=bbox,
    )


def _coerce_year(value: Any) -> Optional[int]:
    if value is None or value == "":
        return None
    try:
        year = int(value)
    except (TypeError, ValueError):
        return None
    if year < 1900 or year > 2100:
        return None
    return year


def _normalize_year_window(filters: Optional[Dict[str, Any]]) -> tuple[Optional[int], Optional[int]]:
    year_start = _coerce_year((filters or {}).get("year_start"))
    year_end = _coerce_year((filters or {}).get("year_end"))
    if year_start is not None and year_end is not None and year_start > year_end:
        year_start, year_end = year_end, year_start
    return year_start, year_end


class RetrievalService:
    """
    统一检索服务。
    search(query, mode, filters) -> EvidencePack
    """

    def __init__(
        self,
        retriever: Optional[HybridRetriever] = None,
        collection: Optional[str] = None,
        top_k: int = 10,
    ):
        self.retriever = retriever or HybridRetriever()
        self.collection = collection or settings.collection.global_
        self.top_k = top_k

    def search(
        self,
        query: str,
        mode: str = "local",
        filters: Optional[Dict[str, Any]] = None,
        top_k: Optional[int] = None,
    ) -> EvidencePack:
        """
        统一检索入口。

        Args:
            query: 查询文本
            mode: "local" 仅本地向量/图, "web" 仅网络搜索, "hybrid" 本地+网络合并
            filters: 预留过滤条件
            top_k: 返回条数，默认使用实例 top_k

        Returns:
            EvidencePack
        """
        k = top_k if top_k is not None else self.top_k
        sources_used: List[str] = []
        all_chunks: List[EvidenceChunk] = []
        total_candidates = 0
        t0 = time.perf_counter()
        timeout_s = getattr(getattr(settings, "perf_retrieval", None), "timeout_seconds", 60) or 60

        # Amplify retrieval pool so that the reranker always operates on a
        # sufficiently large candidate set, regardless of the caller's top_k.
        actual_recall = max(80, k * 4)
        year_start, year_end = _normalize_year_window(filters)

        # 诊断信息收集
        diag: Dict[str, Any] = {}
        if actual_recall != k:
            diag["recall_amplification"] = {"requested_k": k, "actual_recall": actual_recall}
        if year_start is not None or year_end is not None:
            diag["year_window"] = {"year_start": year_start, "year_end": year_end}

        def _do_local() -> List[Dict[str, Any]]:
            config = RetrievalConfig(
                mode="hybrid",
                top_k=actual_recall,
                rerank=True,
                year_start=year_start,
                year_end=year_end,
            )
            return self.retriever.retrieve(query, self.collection, config, diagnostics=diag)

        def _do_web() -> List[Dict[str, Any]]:
            web_providers = (filters or {}).get("web_providers")
            if web_providers is not None and len(web_providers) == 0:
                return []
            web_source_configs = (filters or {}).get("web_source_configs") or {}
            use_query_expansion = (filters or {}).get("use_query_expansion")
            use_query_optimizer = (filters or {}).get("use_query_optimizer")
            query_optimizer_max_queries = (filters or {}).get("query_optimizer_max_queries")
            _llm_provider = (filters or {}).get("llm_provider")
            _model_override = (filters or {}).get("model_override")
            # 参数级联：根据 final_top_k 自动放大 per-provider 采集量
            _final = (filters or {}).get("final_top_k") or k
            _n_providers = max(len(web_providers or []), 1)
            _n_queries = query_optimizer_max_queries or 3
            _auto_per_provider = max(5, math.ceil(_final * 3.5 / (_n_providers * _n_queries)))
            logger.info(
                "web search params: providers=%s auto_per_provider=%s final_top_k=%s llm=%s/%s",
                web_providers,
                _auto_per_provider,
                _final,
                _llm_provider,
                _model_override,
            )
            use_content_fetcher = (filters or {}).get("use_content_fetcher")

            # 为 Lazy Fetching 预判构建 LLM 客户端（仅智能模式 use_content_fetcher=None）
            _llm_client = None
            if use_content_fetcher is None:
                try:
                    from src.llm.llm_manager import get_manager
                    _llm_client = get_manager().get_client(_llm_provider or "deepseek")
                except Exception as _e:
                    logger.debug("Lazy Fetching LLM 客户端初始化失败，降级全量抓取: %s", _e)

            t_web = time.perf_counter()
            results = unified_web_searcher.search_sync(
                query,
                providers=web_providers,
                source_configs=web_source_configs,
                max_results_per_provider=_auto_per_provider,
                use_query_expansion=use_query_expansion,
                use_query_optimizer=use_query_optimizer,
                query_optimizer_max_queries=query_optimizer_max_queries,
                llm_provider=_llm_provider,
                model_override=_model_override,
                use_content_fetcher=use_content_fetcher,
                llm_client=_llm_client,
                year_start=year_start,
                year_end=year_end,
            )
            web_ms = (time.perf_counter() - t_web) * 1000
            # 收集 web provider 诊断
            wp_diag: Dict[str, Any] = {}
            for h in results:
                prov = (h.get("metadata") or {}).get("provider") or "unknown"
                if prov not in wp_diag:
                    wp_diag[prov] = {"count": 0, "time_ms": round(web_ms, 1)}
                wp_diag[prov]["count"] += 1
            diag["web_providers"] = wp_diag
            # content_fetcher 统计（由 enrich_results 添加到 metadata）
            enriched = sum(1 for h in results if (h.get("metadata") or {}).get("content_type") == "full_text")
            if enriched > 0:
                diag["content_fetcher"] = {"enriched": enriched, "total": len(results)}
            return results

        # 获取阈值过滤参数和最终保留数量
        local_threshold = (filters or {}).get("local_threshold")
        final_top_k = (filters or {}).get("final_top_k")

        if mode == "hybrid":
            local_hits: List[Dict[str, Any]] = []
            web_hits: List[Dict[str, Any]] = []
            with ThreadPoolExecutor(max_workers=2) as ex:
                fl = ex.submit(_do_local)
                fw = ex.submit(_do_web)
                try:
                    local_hits = fl.result(timeout=timeout_s)
                except (FuturesTimeoutError, Exception):
                    pass
                try:
                    web_hits = fw.result(timeout=timeout_s)
                except (FuturesTimeoutError, Exception):
                    pass
            for h in local_hits:
                # 应用阈值过滤
                score = float(h.get("score", 0.0))
                if local_threshold is not None and score < local_threshold:
                    continue
                st = h.get("source") or (h.get("metadata") or {}).get("source")
                source_type = "graph" if st == "graph" else "dense"
                if "dense" not in sources_used and source_type == "dense":
                    sources_used.append("dense")
                if source_type == "graph" and "graph" not in sources_used:
                    sources_used.append("graph")
                all_chunks.append(_hit_to_chunk(h, source_type, query))
            total_candidates += len(local_hits) * 2 + len(web_hits)
            if not sources_used and all_chunks:
                sources_used.append("dense")
            # ── 跨源去重：拦截 web 中与本地重叠的文献 ──
            if web_hits and all_chunks:
                web_before = len(web_hits)
                web_hits = cross_source_dedup(all_chunks, web_hits)
                dedup_removed = web_before - len(web_hits)
                if dedup_removed > 0:
                    diag["cross_source_dedup"] = {"removed": dedup_removed, "remaining": len(web_hits)}
            # Web 结果 rerank（多语言 ColBERT 支持中英文）
            if web_hits:
                try:
                    web_hits = _rerank_candidates(
                        query, web_hits, top_k=min(k, len(web_hits))
                    )
                except Exception as e:
                    logger.warning("web rerank failed: %s", e)
            for h in web_hits:
                if "web" not in sources_used:
                    sources_used.append("web")
                all_chunks.append(_hit_to_chunk(h, "web", query))
        else:
            if mode == "local":
                config = RetrievalConfig(
                    mode="hybrid",
                    top_k=actual_recall,
                    rerank=True,
                    year_start=year_start,
                    year_end=year_end,
                )
                hits = self.retriever.retrieve(query, self.collection, config, diagnostics=diag)
                total_candidates += len(hits) * 2
                for h in hits:
                    # 应用阈值过滤
                    score = float(h.get("score", 0.0))
                    if local_threshold is not None and score < local_threshold:
                        continue
                    st = h.get("source") or (h.get("metadata") or {}).get("source")
                    source_type = "graph" if st == "graph" else "dense"
                    if "dense" not in sources_used and source_type == "dense":
                        sources_used.append("dense")
                    if source_type == "graph" and "graph" not in sources_used:
                        sources_used.append("graph")
                    all_chunks.append(_hit_to_chunk(h, source_type, query))
                if not sources_used and all_chunks:
                    sources_used.append("dense")
            elif mode == "web":
                try:
                    web_providers = (filters or {}).get("web_providers")
                    web_source_configs = (filters or {}).get("web_source_configs") or {}
                    use_query_expansion = (filters or {}).get("use_query_expansion")
                    use_query_optimizer = (filters or {}).get("use_query_optimizer")
                    query_optimizer_max_queries = (filters or {}).get("query_optimizer_max_queries")
                    _llm_provider = (filters or {}).get("llm_provider")
                    _model_override = (filters or {}).get("model_override")
                    # 参数级联
                    _final = (filters or {}).get("final_top_k") or k
                    _n_providers = max(len(web_providers or []), 1)
                    _n_queries = query_optimizer_max_queries or 3
                    _auto_per_provider = max(5, math.ceil(_final * 3.5 / (_n_providers * _n_queries)))
                    _use_content_fetcher = (filters or {}).get("use_content_fetcher")

                    # 为 Lazy Fetching 预判构建 LLM 客户端（仅智能模式 use_content_fetcher=None）
                    _llm_client = None
                    if _use_content_fetcher is None:
                        try:
                            from src.llm.llm_manager import get_manager
                            _llm_client = get_manager().get_client(_llm_provider or "deepseek")
                        except Exception as _e:
                            logger.debug("Lazy Fetching LLM 客户端初始化失败，降级全量抓取: %s", _e)

                    web_hits = unified_web_searcher.search_sync(
                        query,
                        providers=web_providers,
                        source_configs=web_source_configs,
                        max_results_per_provider=_auto_per_provider,
                        use_query_expansion=use_query_expansion,
                        use_query_optimizer=use_query_optimizer,
                        query_optimizer_max_queries=query_optimizer_max_queries,
                        llm_provider=_llm_provider,
                        model_override=_model_override,
                        use_content_fetcher=_use_content_fetcher,
                        llm_client=_llm_client,
                        year_start=year_start,
                        year_end=year_end,
                    )
                    total_candidates += len(web_hits)
                    sources_used.append("web")
                    # Web 结果 rerank
                    if web_hits:
                        try:
                            web_hits = _rerank_candidates(
                                query, web_hits, top_k=min(k, len(web_hits))
                            )
                        except Exception as e:
                            logger.warning("web rerank failed: %s", e)
                    for h in web_hits:
                        all_chunks.append(_hit_to_chunk(h, "web", query))
                except Exception:
                    pass

        retrieval_time_ms = (time.perf_counter() - t0) * 1000

        # ── Observability: 记录检索指标 ──
        if obs_metrics:
            elapsed_s = retrieval_time_ms / 1000.0
            obs_metrics.retrieval_total.labels(mode=mode).inc()
            obs_metrics.retrieval_duration_seconds.labels(mode=mode).observe(elapsed_s)
            obs_metrics.retrieval_chunks_returned.labels(mode=mode).observe(len(all_chunks))

        # 使用 final_top_k 作为最终保留数量，否则使用 k
        result_limit = final_top_k if final_top_k is not None else k
        
        return EvidencePack(
            query=query,
            chunks=all_chunks[:result_limit],
            total_candidates=total_candidates,
            retrieval_time_ms=retrieval_time_ms,
            sources_used=list(dict.fromkeys(sources_used)),
            diagnostics=diag if diag else None,
        )


# 全局实例（按 collection 复用）
_retrieval_services: Dict[str, RetrievalService] = {}


def get_retrieval_service(
    collection: Optional[str] = None,
    top_k: int = 10,
) -> RetrievalService:
    """获取 RetrievalService（按 collection 维度复用实例）"""
    key = (collection or settings.collection.global_ or "").strip() or settings.collection.global_
    svc = _retrieval_services.get(key)
    if svc is None:
        svc = RetrievalService(collection=key, top_k=top_k)
        _retrieval_services[key] = svc
    return svc
</file>

<file path="src/api/routes_chat.py">
"""
对话 API：POST /chat, POST /chat/stream, GET /sessions/{id}, DELETE /sessions/{id}
"""

import json
import re
import threading
from io import BytesIO
from pathlib import Path
from typing import Any, Dict, List
from fastapi import APIRouter, Depends, HTTPException, UploadFile, File
from fastapi.responses import StreamingResponse

from config.settings import settings
from src.api.routes_auth import get_optional_user_id

# ── Observability ──
try:
    from src.observability import metrics as _obs_metrics
except Exception:
    _obs_metrics = None  # type: ignore

from src.api.schemas import (
    ChatCitation,
    ChatRequest,
    ChatResponse,
    ClarifyRequest,
    ClarifyResponse,
    ClarifyQuestion,
    DeepResearchRequest,
    DeepResearchStartRequest,
    DeepResearchStartResponse,
    DeepResearchConfirmRequest,
    DeepResearchSubmitResponse,
    DeepResearchJobInfo,
    DeepResearchContextExtractResponse,
    EvidenceSummary,
    IntentDetectRequest,
    IntentDetectResponse,
    SessionInfo,
    SessionListItem,
    TurnItem,
)
from src.collaboration.intent import (
    IntentParser,
    IntentType,
    ParsedIntent,
    build_search_query_from_context,
    is_deep_research,
)
from src.collaboration.memory.session_memory import (
    get_session_store,
    load_session_memory,
)
from src.collaboration.memory.working_memory import get_or_generate_working_memory
from src.collaboration.memory.persistent_store import get_user_profile
from src.collaboration.workflow import run_workflow
from src.llm.llm_manager import get_manager
from src.llm.react_loop import react_loop
from src.llm.tools import CORE_TOOLS, get_routed_skills
from src.collaboration.citation.manager import (
    _dedupe_citations,
    chunk_to_citation,
    merge_citations_by_document,
    resolve_response_citations,
    sync_evidence_to_canvas,
)
from src.collaboration.canvas.models import Citation
from dataclasses import asdict
from src.retrieval.service import get_retrieval_service
from src.generation.evidence_synthesizer import EvidenceSynthesizer, build_synthesis_system_prompt
from src.collaboration.auto_complete import AutoCompleteService

router = APIRouter(tags=["chat"])

_CONFIG_PATH = Path(__file__).resolve().parents[2] / "config" / "rag_config.json"
_DEEP_RESEARCH_CANCEL_EVENTS: dict[str, threading.Event] = {}
_DEEP_RESEARCH_CANCEL_LOCK = threading.Lock()
_DEEP_RESEARCH_SUSPENDED: dict[str, dict[str, Any]] = {}
_DEEP_RESEARCH_SUSPENDED_LOCK = threading.Lock()


def _dr_request_cancel(job_id: str) -> None:
    with _DEEP_RESEARCH_CANCEL_LOCK:
        ev = _DEEP_RESEARCH_CANCEL_EVENTS.get(job_id)
        if ev is None:
            ev = threading.Event()
            _DEEP_RESEARCH_CANCEL_EVENTS[job_id] = ev
        ev.set()


def _dr_is_cancel_requested(job_id: str) -> bool:
    with _DEEP_RESEARCH_CANCEL_LOCK:
        ev = _DEEP_RESEARCH_CANCEL_EVENTS.get(job_id)
    return bool(ev and ev.is_set())


def _dr_clear_cancel_event(job_id: str) -> None:
    with _DEEP_RESEARCH_CANCEL_LOCK:
        _DEEP_RESEARCH_CANCEL_EVENTS.pop(job_id, None)


def _dr_store_suspended_runtime(job_id: str, runtime: dict[str, Any]) -> None:
    with _DEEP_RESEARCH_SUSPENDED_LOCK:
        current = _DEEP_RESEARCH_SUSPENDED.get(job_id) or {}
        merged = {**current, **runtime}
        merged.setdefault("resume_inflight", False)
        _DEEP_RESEARCH_SUSPENDED[job_id] = merged


def _dr_get_suspended_runtime(job_id: str) -> dict[str, Any] | None:
    with _DEEP_RESEARCH_SUSPENDED_LOCK:
        runtime = _DEEP_RESEARCH_SUSPENDED.get(job_id)
        return dict(runtime) if runtime else None


def _dr_pop_suspended_runtime(job_id: str) -> dict[str, Any] | None:
    with _DEEP_RESEARCH_SUSPENDED_LOCK:
        return _DEEP_RESEARCH_SUSPENDED.pop(job_id, None)


def _dr_mark_resume_inflight(job_id: str) -> bool:
    with _DEEP_RESEARCH_SUSPENDED_LOCK:
        runtime = _DEEP_RESEARCH_SUSPENDED.get(job_id)
        if not runtime:
            return False
        if bool(runtime.get("resume_inflight", False)):
            return False
        runtime["resume_inflight"] = True
        return True


def _dr_set_resume_idle(job_id: str) -> None:
    with _DEEP_RESEARCH_SUSPENDED_LOCK:
        runtime = _DEEP_RESEARCH_SUSPENDED.get(job_id)
        if runtime is not None:
            runtime["resume_inflight"] = False

# ── 智能查询路由：三层判断是否需要 RAG ──
#
# 第一层：正则强制 rag（零成本，关键词命中直接走检索，不调 LLM）
# 第二层：LLM 轻量分类（~300 tokens，带上下文判断 chat/rag）
# 第三层：严格解析 + 保守兜底（只有明确 chat 才跳过，其他一律 rag）

from src.log import get_logger as _get_chat_logger

_chat_logger = _get_chat_logger(__name__)

# ── 第一层：正则强制走 rag 的关键词（只做"强制 rag"，不做"强制 chat"）──
_FORCE_RAG_RE = re.compile(
    r'(材料|文献|论文|资料|数据|根据|对比|对照|比较|验证|引用|出处|来源|原文'
    r'|最大|最小|最深|最高|最低|多少|几种|哪些|哪个|有多大|有多深|有多高'
    r'|图表|表格|实验|结果|结论|证据|研究|分析|综述|检索|查一下|帮我查|帮我找'
    r'|according\s+to|based\s+on\s+(the\s+)?(data|paper|material|literature|evidence))',
    re.IGNORECASE,
)

# ── 第二层：LLM 分类 prompt ──
_ROUTE_SYSTEM = "你是一个查询路由分类器。只回答一个词：rag 或 chat。"

_ROUTE_PROMPT = """判断用户消息是否需要检索外部知识库。

## chat（不检索，LLM 直接回答）：
- 社交寒暄：问候、感谢、告别、确认（你好、谢谢、明白了）
- 情绪闲聊：哈哈、嗯嗯
- 用户明确要求用 LLM 自身知识："基于你的训练/知识""你觉得""不用查资料""用你自己的话"
- 纯编程/数学/逻辑推理（不需要领域知识库）
- 对上一轮回答的追问，且上下文已包含足够信息（如"详细说说第三点"）

## rag（需要检索知识库）：
- 涉及特定领域的事实、数据、数值
- 要求基于文献/材料做分析、对比、验证
- 涉及学术概念或专业术语
- 不确定时一律 rag

## 最近对话
{history}

## 当前消息
{message}

只回答 rag 或 chat"""

# ── 第三层解析：严格正则校验 LLM 输出 ──
_ROUTE_ANSWER_RE = re.compile(r'^(chat|rag)\b', re.IGNORECASE)


def _classify_query(message: str, history_turns: list, llm_client) -> bool:
    """
    三层查询路由：判断消息是否需要 RAG 检索。
    返回 True = 需要检索, False = 直接对话。

    第一层：正则关键词 → 强制 rag（零成本）
    第二层：LLM 分类（~300 tokens）
    第三层：严格解析 + 保守兜底
    """
    msg = message.strip()
    if not msg:
        return False

    # ── 第一层：关键词强制 rag ──
    if _FORCE_RAG_RE.search(msg):
        _chat_logger.info("query_route | msg=%r → rag (keyword hit)", msg[:80])
        return True

    # ── 第二层：LLM 分类 ──
    ctx_lines = []
    recent = history_turns[-6:] if len(history_turns) > 6 else history_turns
    for t in recent:
        role_label = "用户" if t.role == "user" else "助手"
        text = (t.content or "").strip().replace("\n", " ")
        if len(text) > 150:
            text = text[:150] + "…"
        ctx_lines.append(f"{role_label}: {text}")
    history_block = "\n".join(ctx_lines) if ctx_lines else "（首轮对话）"

    prompt = _ROUTE_PROMPT.format(history=history_block, message=msg)

    try:
        resp = llm_client.chat(
            [
                {"role": "system", "content": _ROUTE_SYSTEM},
                {"role": "user", "content": prompt},
            ],
            max_tokens=128,  # thinking 模型的 reasoning 也消耗 tokens，需留足空间
        )
        # 取 final_text；某些 thinking 模型答案可能在 reasoning_text 里
        raw_answer = (resp.get("final_text") or "").strip().lower()
        if not raw_answer:
            raw_answer = (resp.get("reasoning_text") or "").strip().lower()

        # ── 第三层：严格解析 ──
        # 在整个回复中搜索 chat/rag 关键词（兼容模型输出 "chat" / "答案：chat" 等格式）
        if re.search(r'\bchat\b', raw_answer):
            _chat_logger.info("query_route | msg=%r → chat (llm, raw=%r)", msg[:80], raw_answer[:50])
            return False
        elif re.search(r'\brag\b', raw_answer):
            _chat_logger.info("query_route | msg=%r → rag (llm, raw=%r)", msg[:80], raw_answer[:50])
            return True
        else:
            # 无法识别 → 保守走 rag
            _chat_logger.info(
                "query_route | msg=%r → rag (unrecognized, raw=%r)", msg[:80], raw_answer[:50],
            )
            return True
    except Exception as e:
        _chat_logger.warning("query_route failed (%s), fallback to rag", e)
        return True


def _build_system_with_context(context: str) -> str:
    return (
        "你是一个基于检索增强的学术助手。请基于以下检索到的参考资料回答用户问题，"
        "保持多轮对话连贯。每条证据前有方括号引用标记（如 [a1b2c3d4]），"
        "请在行文中使用该标记引用对应证据。若无直接相关，可基于常识简要回答并说明。\n\n"
        "参考资料：\n"
        f"{context or '（本轮暂无检索结果）'}"
    )


def _chunk_text(text: str, chunk_size: int = 20):
    if not text:
        return
    for i in range(0, len(text), chunk_size):
        yield text[i:i + chunk_size]


def _serialize_citation(c: Citation | str) -> dict:
    """将 Citation 对象或字符串序列化为字典。"""
    if isinstance(c, str):
        return {"cite_key": c, "title": "", "authors": [], "year": None, "doc_id": None, "url": None}
    return {
        "cite_key": c.cite_key or c.id,
        "title": c.title or "",
        "authors": c.authors or [],
        "year": c.year,
        "doc_id": c.doc_id,
        "url": c.url,
        "doi": c.doi,
        "bbox": getattr(c, "bbox", None),
        "page_num": getattr(c, "page_num", None),
    }


def _build_filters(body: ChatRequest) -> dict:
    """从 ChatRequest 中提取检索参数构建 filters 字典。"""
    filters = {}
    if body.web_providers:
        filters["web_providers"] = body.web_providers
    if body.web_source_configs:
        filters["web_source_configs"] = body.web_source_configs
    if body.use_query_expansion is not None:
        filters["use_query_expansion"] = body.use_query_expansion
    if body.use_query_optimizer is not None:
        filters["use_query_optimizer"] = body.use_query_optimizer
    if body.query_optimizer_max_queries is not None:
        filters["query_optimizer_max_queries"] = body.query_optimizer_max_queries
    if body.local_threshold is not None:
        filters["local_threshold"] = body.local_threshold
    if body.year_start is not None:
        filters["year_start"] = body.year_start
    if body.year_end is not None:
        filters["year_end"] = body.year_end
    if body.final_top_k is not None:
        filters["final_top_k"] = body.final_top_k
    if body.llm_provider:
        filters["llm_provider"] = body.llm_provider
    if body.model_override:
        filters["model_override"] = body.model_override
    if body.use_content_fetcher is not None:
        filters["use_content_fetcher"] = body.use_content_fetcher
    if body.collection:
        filters["collection"] = body.collection
    return filters


def _build_deep_research_filters(body: Any) -> dict:
    """Build filters from deep-research start/confirm request objects."""
    filters: Dict[str, Any] = {}
    for key in (
        "web_providers",
        "web_source_configs",
        "use_query_optimizer",
        "query_optimizer_max_queries",
        "local_top_k",
        "local_threshold",
        "year_start",
        "year_end",
        "final_top_k",
        "llm_provider",
        "model_override",
        "collection",
    ):
        val = getattr(body, key, None)
        if val is not None and val != "":
            filters[key] = val
    return filters


def _infer_sources_from_citations(citations: List[Citation]) -> List[str]:
    has_web = any(bool(getattr(c, "url", None)) for c in citations)
    has_local = any(not bool(getattr(c, "url", None)) for c in citations)
    out: List[str] = []
    if has_local:
        out.append("local")
    if has_web:
        out.append("web")
    return out or ["deep_research"]


def _run_chat(
    body: ChatRequest,
    optional_user_id: str | None = None,
) -> tuple[str, str, list[Citation], EvidenceSummary, ParsedIntent, dict | None, list | None, dict[str, str]]:
    import time as _time
    t_start = _time.perf_counter()

    session_id = body.session_id
    store = get_session_store()
    if not session_id:
        session_id = store.create_session(canvas_id=body.canvas_id or "")
    memory = load_session_memory(session_id)
    if memory is None:
        raise HTTPException(status_code=404, detail="session not found")

    message = body.message.strip()
    search_mode = body.search_mode or "local"
    if search_mode not in ("local", "web", "hybrid", "none"):
        search_mode = "local"

    current_stage = store.get_session_stage(session_id)
    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client(body.llm_provider or None)
    history_for_intent = memory.get_context_window(n=6)

    # ── 1. 请求概览 ──
    _chat_logger.info(
        "[chat] ▶ 新请求 | session=%s | msg=%r | provider=%s | model=%s | search_mode=%s | collection=%s | agent=%s",
        session_id[:12], message[:60],
        body.llm_provider or "default", body.model_override or "default",
        search_mode, (body.collection or settings.collection.global_), body.use_agent,
    )

    # ── 2. 意图判断（Chat vs Deep Research）──
    request_mode = (body.mode or "chat").strip().lower()
    if request_mode == "deep_research":
        parsed = ParsedIntent(
            intent_type=IntentType.DEEP_RESEARCH,
            confidence=1.0,
            params={"args": message},
            raw_input=message,
        )
        _chat_logger.info("[chat] ② 意图判断 → deep_research (前端 mode 指定)")
    elif message.startswith("/"):
        parser = IntentParser(client)
        parsed = parser.parse(message, current_stage=current_stage, history=history_for_intent)
        _chat_logger.info(
            "[chat] ② 意图判断 → %s (命令解析, confidence=%.2f)",
            parsed.intent_type.value, parsed.confidence,
        )
    else:
        parsed = ParsedIntent(
            intent_type=IntentType.CHAT,
            confidence=1.0,
            raw_input=message,
        )

    meta = store.get_session_meta(session_id)
    canvas_id = (meta or {}).get("canvas_id") or ""

    # ── Deep Research 分支 ──
    if is_deep_research(parsed):
        _chat_logger.info("[chat] ── 进入 Deep Research 分支 ──")
        query = build_search_query_from_context(
            parsed, message, history_for_intent,
            llm_client=client, enforce_english_if_input_english=True,
            rolling_summary=memory.rolling_summary,
        )
        topic = (parsed.params.get("args") or message).strip() or "综述"
        filters = _build_filters(body)
        svc = AutoCompleteService(llm_client=client, max_sections=6, include_abstract=True)
        user_id = optional_user_id or body.user_id or ""
        use_agent_flag = getattr(body, "use_agent", None) or False
        result = svc.complete(
            topic=topic,
            canvas_id=canvas_id or None,
            session_id=session_id,
            search_mode=search_mode,
            filters=filters or None,
            user_id=user_id,
            clarification_answers=body.clarification_answers,
            output_language=body.output_language or "auto",
            step_models=body.step_models,
            use_agent=use_agent_flag,
        )
        response_text = result.markdown
        if result.canvas_id and not canvas_id:
            store.update_session_meta(session_id, {"canvas_id": result.canvas_id})
        store.update_session_stage(session_id, "refine")
        memory.add_turn("user", message)
        citations_data = [_serialize_citation(c) for c in result.citations] if result.citations else []
        memory.add_turn("assistant", response_text, citations=citations_data)
        evidence_summary = EvidenceSummary(
            query=topic,
            total_chunks=len(result.citations),
            sources_used=_infer_sources_from_citations(result.citations),
            retrieval_time_ms=result.total_time_ms,
        )
        dashboard_data = getattr(result, "dashboard", None)
        if dashboard_data is None and hasattr(result, "__dict__"):
            dashboard_data = getattr(result, "dashboard", None)
        elapsed = (_time.perf_counter() - t_start) * 1000
        _chat_logger.info(
            "[chat] ✔ Deep Research 完成 | citations=%d | time=%.0fms",
            len(result.citations) if result.citations else 0, elapsed,
        )
        return session_id, response_text, result.citations, evidence_summary, parsed, dashboard_data, None, {}

    # ── 3. Chat 分支：智能路由（正则 → LLM → 严格解析）──
    t_route = _time.perf_counter()
    query_needs_rag = _classify_query(message, history_for_intent, client)
    do_retrieval = search_mode != "none" and query_needs_rag
    route_ms = (_time.perf_counter() - t_route) * 1000

    _chat_logger.info(
        "[chat] ③ 查询路由 → %s | do_retrieval=%s (search_mode=%s) | 耗时=%.0fms",
        "rag" if query_needs_rag else "chat",
        do_retrieval, search_mode, route_ms,
    )

    # ── 4. 检索 query 构建（仅 rag 模式）──
    if do_retrieval:
        t_query = _time.perf_counter()
        query = build_search_query_from_context(
            parsed, message, history_for_intent,
            llm_client=client, enforce_english_if_input_english=True,
            rolling_summary=memory.rolling_summary,
        )
        query_ms = (_time.perf_counter() - t_query) * 1000
        _chat_logger.info(
            "[chat] ④ Query 构建 | original=%r → query=%r | 耗时=%.0fms",
            message[:40], query[:60], query_ms,
        )
    else:
        query = message
        _chat_logger.info("[chat] ④ Query 构建 → 跳过 (不需要检索)")

    # ── 5. 检索执行 ──
    if do_retrieval:
        t_retrieval = _time.perf_counter()
        target_collection = (body.collection or "").strip() or None
        retrieval = get_retrieval_service(collection=target_collection)
        filters = _build_filters(body)
        pack = retrieval.search(
            query=query or message,
            mode=search_mode,
            filters=filters or None,
            top_k=body.local_top_k,
        )
        synthesizer = EvidenceSynthesizer()
        context_str, synthesis_meta = synthesizer.synthesize(pack)
        synth_dict = synthesis_meta.to_dict()
        retrieval_ms = (_time.perf_counter() - t_retrieval) * 1000
        evidence_summary = EvidenceSummary(
            query=pack.query,
            total_chunks=len(pack.chunks),
            sources_used=pack.sources_used,
            retrieval_time_ms=pack.retrieval_time_ms,
            year_range=synth_dict.get("year_range"),
            source_breakdown=synth_dict.get("source_breakdown"),
            evidence_type_breakdown=synth_dict.get("evidence_type_breakdown"),
            cross_validated_count=synth_dict.get("cross_validated_count", 0),
            total_documents=synth_dict.get("total_documents", 0),
            diagnostics=pack.diagnostics,
        )
        if canvas_id:
            sync_evidence_to_canvas(canvas_id, pack)
        # 注意：citations 会在 LLM 生成后通过 resolve_response_citations() 获得
        citations: list[Citation] = []
        _chat_logger.info(
            "[chat] ⑤ 检索完成 | mode=%s | chunks=%d | sources=%s | 耗时=%.0fms",
            search_mode, len(pack.chunks),
            ",".join(pack.sources_used), retrieval_ms,
        )
    else:
        context_str = ""
        pack = None
        evidence_summary = EvidenceSummary(
            query=query or message,
            total_chunks=0,
            sources_used=[],
            retrieval_time_ms=0,
        )
        citations: list[Citation] = []
        _chat_logger.info("[chat] ⑤ 检索 → 跳过 (路由判定为 chat)")

    # ── 6. System Prompt 组装 ──
    wf = run_workflow(
        current_stage,
        parsed.intent_type,
        topic="",
        context=context_str or "（本轮暂无检索结果）",
    )
    next_stage = wf["next_stage"]
    stage_prompt = wf.get("system_prompt") or ""
    if not query_needs_rag:
        system_content = (
            "你是一个友好的学术助手。当前用户的问题不需要检索外部知识库。\n"
            "请基于你自身的训练知识回答，保持专业、准确、简洁。\n"
            "如果问题超出你的知识范围，坦诚说明并建议用户让你检索知识库获取更准确的信息。"
        )
        prompt_mode = "chat_direct"
    elif stage_prompt:
        system_content = stage_prompt
        prompt_mode = "workflow_stage"
    elif do_retrieval and context_str:
        system_content = build_synthesis_system_prompt(context_str)
        prompt_mode = "rag_synthesis"
    else:
        system_content = _build_system_with_context(context_str)
        prompt_mode = "rag_basic"

    if canvas_id:
        wm = get_or_generate_working_memory(canvas_id, _CONFIG_PATH)
        if wm and wm.get("summary"):
            system_content = system_content.rstrip() + "\n\n【当前画布摘要】\n" + wm["summary"] + "\n"

    effective_user_id = optional_user_id or body.user_id
    if effective_user_id:
        profile = get_user_profile(effective_user_id)
        if profile and profile.get("preferences"):
            prefs_str = ", ".join(f"{k}={v}" for k, v in list(profile["preferences"].items())[:5])
            system_content = system_content.rstrip() + "\n\n【用户偏好】\n" + prefs_str + "\n"

    _chat_logger.info(
        "[chat] ⑥ Prompt 组装 | mode=%s | stage=%s→%s | system_len=%d",
        prompt_mode, current_stage or "none", next_stage, len(system_content),
    )

    store.update_session_stage(session_id, next_stage)

    # ── 7. 构建消息列表 ──
    history = memory.get_context_window(n=10)
    messages = [{"role": "system", "content": system_content}]
    for t in history:
        messages.append({"role": t.role, "content": t.content})
    messages.append({"role": "user", "content": message})

    # ── 8. LLM 生成 ──
    use_agent = body.use_agent if hasattr(body, "use_agent") and body.use_agent is not None else False
    use_agent = use_agent and query_needs_rag  # 闲聊时关闭 Agent
    tool_trace = None

    # Agent + 预检索结果：指示优先使用已有资料
    if use_agent and do_retrieval and context_str:
        agent_hint = (
            "\n\n【重要：检索结果已就绪】\n"
            "我已为你检索了相关参考资料（见上文）。请优先基于这些资料回答用户问题。\n"
            "只有当现有资料明显不足以回答时，才使用 search_local 或 search_web 工具补充检索。\n"
            "避免重复搜索已有资料中已覆盖的内容。"
        )
        messages[0]["content"] = messages[0]["content"] + agent_hint

    gen_mode = "agent" if use_agent else "direct"
    _chat_logger.info(
        "[chat] ⑦ LLM 生成 | mode=%s | provider=%s | model=%s | history_turns=%d | msg_count=%d",
        gen_mode, body.llm_provider or "default", body.model_override or "default",
        len(history), len(messages),
    )

    t_llm = _time.perf_counter()
    try:
        if use_agent:
            routed_tools = get_routed_skills(
                message=message,
                current_stage=current_stage or "",
                search_mode=search_mode,
                allowed_web_providers=body.web_providers,
            )
            react_result = react_loop(
                messages=messages,
                tools=routed_tools,
                llm_client=client,
                max_iterations=8,
                model=body.model_override or None,
                max_tokens=2000,
            )
            response_text = react_result.final_text.strip()
            tool_trace = react_result.tool_trace if react_result.tool_trace else None
            llm_ms = (_time.perf_counter() - t_llm) * 1000
            _chat_logger.info(
                "[chat] ⑧ Agent 完成 | iterations=%d | tools_called=%d | routed=%d/%d | 耗时=%.0fms",
                react_result.iterations, len(react_result.tool_trace),
                len(routed_tools), len(CORE_TOOLS), llm_ms,
            )
        else:
            resp = client.chat(messages, model=body.model_override or None, max_tokens=2000)
            response_text = (resp.get("final_text") or "").strip()
            llm_ms = (_time.perf_counter() - t_llm) * 1000
            usage = resp.get("meta", {}).get("usage") or resp.get("usage") or {}
            _chat_logger.info(
                "[chat] ⑧ LLM 完成 | response_len=%d | tokens=%s | 耗时=%.0fms",
                len(response_text),
                f"in={usage.get('prompt_tokens', '?')}/out={usage.get('completion_tokens', '?')}" if usage else "N/A",
                llm_ms,
            )
    except Exception as llm_err:
        llm_ms = (_time.perf_counter() - t_llm) * 1000
        _chat_logger.error("[chat] ⑧ LLM 失败 | error=%s | 耗时=%.0fms", llm_err, llm_ms)
        response_text = f"[LLM 调用失败] {type(llm_err).__name__}: {llm_err}\n\n请尝试切换其他模型。"

    # ── 9. 引文后处理：将 [ref_hash] 替换为正式 cite_key ──
    ref_map: dict[str, str] = {}
    if do_retrieval and pack and pack.chunks:
        response_text, citations, ref_map = resolve_response_citations(
            response_text, pack.chunks,
        )
        _chat_logger.info(
            "[chat] ⑨ 引文后处理 | cited_docs=%d | ref_map_size=%d",
            len(citations), len(ref_map),
        )

    # ── 10. 写入 Memory ──
    memory.add_turn("user", message)
    citations_data = [_serialize_citation(c) for c in citations] if citations else []
    memory.add_turn("assistant", response_text, citations=citations_data)
    memory.update_rolling_summary(client, interval=4)

    # ── 11. 总结 ──
    total_ms = (_time.perf_counter() - t_start) * 1000
    _chat_logger.info(
        "[chat] ✔ 请求完成 | session=%s | route=%s | retrieval=%s | gen=%s "
        "| citations=%d | response_len=%d | 总耗时=%.0fms",
        session_id[:12],
        "rag" if query_needs_rag else "chat",
        f"{search_mode}({evidence_summary.total_chunks}chunks)" if do_retrieval else "skip",
        gen_mode,
        len(citations), len(response_text), total_ms,
    )

    return session_id, response_text, citations, evidence_summary, parsed, None, tool_trace, ref_map


def _citation_to_chat_citation(c: Citation) -> ChatCitation:
    """将 Citation 对象转换为 ChatCitation schema。"""
    return ChatCitation(
        cite_key=c.cite_key or c.id,
        title=c.title or "",
        authors=c.authors or [],
        year=c.year,
        doc_id=c.doc_id,
        url=c.url,
        doi=c.doi,
        bbox=getattr(c, "bbox", None),
        page_num=getattr(c, "page_num", None),
    )


@router.post("/chat", response_model=ChatResponse)
def chat_post(
    body: ChatRequest,
    optional_user_id: str | None = Depends(get_optional_user_id),
) -> ChatResponse:
    session_id, response_text, citations, evidence_summary, _parsed, _dashboard, _trace, _ref_map = _run_chat(body, optional_user_id)
    return ChatResponse(
        session_id=session_id,
        response=response_text,
        citations=[_citation_to_chat_citation(c) for c in citations],
        evidence_summary=evidence_summary,
    )


@router.post("/chat/stream")
def chat_stream(
    body: ChatRequest,
    optional_user_id: str | None = Depends(get_optional_user_id),
) -> StreamingResponse:
    session_id, response_text, citations, evidence_summary, parsed, dashboard_data, tool_trace_data, ref_map = _run_chat(body, optional_user_id)
    
    # 获取当前会话阶段
    store = get_session_store()
    current_stage = store.get_session_stage(session_id) or "explore"

    def event_stream():
        if _obs_metrics:
            _obs_metrics.active_connections.inc()
        try:
            # 序列化 citations 为完整的引用信息
            citations_data = [_serialize_citation(c) for c in citations]
            # 返回模式/意图信息
            intent_info = {
                "mode": parsed.intent_type.value,  # "chat" or "deep_research"
                "intent_type": parsed.intent_type.value,  # 兼容
                "confidence": parsed.confidence,
                "from_command": parsed.from_command,
            }
            # 获取 canvas_id
            session_meta = store.get_session_meta(session_id)
            canvas_id = (session_meta or {}).get("canvas_id") or ""
            meta = {
                "session_id": session_id,
                "canvas_id": canvas_id,  # 返回 canvas_id 供前端加载画布
                "citations": citations_data,
                "ref_map": ref_map or {},  # ref_hash → cite_key 映射
                "evidence_summary": evidence_summary.model_dump() if evidence_summary else None,
                "intent": intent_info,
                "current_stage": current_stage,  # 返回当前工作流阶段
            }
            yield f"event: meta\ndata: {json.dumps(meta, ensure_ascii=False)}\n\n"
            # Deep Research 进度仪表盘
            if dashboard_data:
                dash = dashboard_data if isinstance(dashboard_data, dict) else {}
                yield f"event: dashboard\ndata: {json.dumps(dash, ensure_ascii=False, default=str)}\n\n"
            # Agent 工具调用轨迹
            if tool_trace_data:
                yield f"event: tool_trace\ndata: {json.dumps(tool_trace_data, ensure_ascii=False, default=str)}\n\n"
            for chunk in _chunk_text(response_text):
                yield f"event: delta\ndata: {json.dumps({'delta': chunk}, ensure_ascii=False)}\n\n"
            yield "event: done\ndata: {}\n\n"
        finally:
            if _obs_metrics:
                _obs_metrics.active_connections.dec()

    return StreamingResponse(event_stream(), media_type="text/event-stream")


@router.post("/intent/detect", response_model=IntentDetectResponse)
def detect_intent(body: IntentDetectRequest) -> IntentDetectResponse:
    """
    意图检测 API（简化版）：Chat vs Deep Research 二分类。
    检索由前端 UI 开关决定，此处只判断执行模式。
    """
    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client()
    parser = IntentParser(client)

    history = None
    if body.session_id:
        mem = load_session_memory(body.session_id)
        if mem:
            history = mem.get_context_window(n=6)

    parsed = parser.parse(
        body.message,
        current_stage=body.current_stage,
        history=history,
    )

    mode = parsed.intent_type.value  # "chat" or "deep_research"
    suggested_topic = ""
    if is_deep_research(parsed):
        suggested_topic = (parsed.params.get("args") or body.message).strip()

    return IntentDetectResponse(
        mode=mode,
        confidence=parsed.confidence,
        suggested_topic=suggested_topic,
        params=parsed.params or {},
        # 兼容旧字段
        intent_type=mode,
        needs_retrieval=True,  # 检索由 UI 决定，此字段不再有实际意义
        suggested_search_mode="hybrid",
    )


@router.post("/deep-research/clarify", response_model=ClarifyResponse)
def clarify_for_deep_research(
    body: ClarifyRequest,
    optional_user_id: str | None = Depends(get_optional_user_id),
) -> ClarifyResponse:
    """
    Deep Research 澄清问题生成：基于 chat 历史和主题，
    至少生成 1 个关键澄清问题；若主题不明确则生成更多（最多 6 个）。
    前端在用户触发 Deep Research 时调用，显示澄清对话框。
    """
    manager = get_manager(str(_CONFIG_PATH))
    try:
        client = manager.get_client(body.llm_provider or None)
    except Exception as e:
        raise HTTPException(status_code=400, detail=f"Invalid clarify LLM provider/model configuration: {e}")
    used_fallback = False
    fallback_reason = ""
    provider_used = body.llm_provider or manager.config.default
    model_used = body.model_override or ""
    try:
        cfg = getattr(client, "config", None)
        if not model_used and cfg is not None:
            model_used = str(getattr(cfg, "default_model", "") or "")
    except Exception:
        pass

    # 获取 chat 历史上下文
    history_block = ""
    if body.session_id:
        mem = load_session_memory(body.session_id)
        if mem:
            turns = mem.get_context_window(n=8)
            lines = []
            for t in turns:
                role_label = "用户" if t.role == "user" else "助手"
                text = (t.content or "").strip().replace("\n", " ")
                if len(text) > 300:
                    text = text[:300] + "..."
                lines.append(f"{role_label}: {text}")
            history_block = "\n".join(lines)

    prompt = f"""用户想要进行深度研究。请先生成澄清问题：
- 至少 1 个问题（即使主题较明确，也要有 1 个关键确认问题）；
- 如果主题不明确或歧义较大，生成更多问题（最多 6 个）；
- 问题按优先级排序。
然后输出一个结构化研究简报。

用户的主题描述: "{body.message}"

对话历史上下文:
{history_block or "（无）"}

请生成问题帮助明确（优先级从高到低）：
1. 研究主题的精确范围
2. 重点关注的方向（如理论/应用/方法论）
3. 目标受众和写作风格（学术论文/科普/报告）
4. 篇幅和深度要求
5. 特别需要关注的子主题或排除的内容
6. 文献语言偏好

约束：
- questions 数量必须在 1 到 6 之间
- 问题应尽量具体、可回答，避免重复

返回 JSON 格式:
{{
  "suggested_topic": "建议的综述主题（一句话）",
  "suggested_outline": ["章节1", "章节2", ...],
  "research_brief": {{
    "scope": "研究范围描述",
    "success_criteria": ["完成标准1", "完成标准2"],
    "key_questions": ["核心问题1", "核心问题2"],
    "exclusions": ["排除内容"],
    "time_range": "文献时间范围",
    "source_priority": ["peer-reviewed papers", "reviews"]
  }},
  "questions": [
    {{"id": "q1", "text": "问题文本", "type": "text", "default": "建议答案"}},
    {{"id": "q2", "text": "问题文本", "type": "choice", "options": ["选项A", "选项B"], "default": "选项A"}}
  ]
}}

只返回 JSON，不要其他文字。"""

    try:
        resp = client.chat(
            [
                {"role": "system", "content": "你是一个学术研究规划助手，只返回 JSON。"},
                {"role": "user", "content": prompt},
            ],
            model=body.model_override or None,
            max_tokens=1024,
        )
        text = (resp.get("final_text") or "").strip()
        text = re.sub(r"^```(?:json)?\s*", "", text)
        text = re.sub(r"\s*```\s*$", "", text)
        data = json.loads(text)
    except Exception as e:
        # LLM 失败时至少返回 1 个关键问题
        used_fallback = True
        fallback_reason = f"clarify_llm_failed: {str(e)[:240]}"
        data = {
            "suggested_topic": body.message,
            "suggested_outline": [],
            "questions": [
                {"id": "q1", "text": "请确认本次研究最关键的目标与范围边界", "type": "text", "default": body.message},
            ],
        }

    questions = []
    for q in (data.get("questions") or [])[:6]:
        text = (q.get("text", "") or "").strip()
        if not text:
            continue
        qtype = q.get("type", "text")
        options = q.get("options", [])
        if qtype in ("choice", "multi_choice") and not isinstance(options, list):
            options = []
        questions.append(ClarifyQuestion(
            id=q.get("id", f"q{len(questions)+1}"),
            text=text,
            question_type=qtype,
            options=options or [],
            default=q.get("default", ""),
        ))

    if len(questions) == 0:
        used_fallback = True
        if not fallback_reason:
            fallback_reason = "empty_or_invalid_questions"
        questions.append(
            ClarifyQuestion(
                id="q1",
                text="请确认本次研究最关键的目标与范围边界",
                question_type="text",
                options=[],
                default=body.message,
            )
        )

    return ClarifyResponse(
        questions=questions,
        suggested_topic=data.get("suggested_topic", body.message),
        suggested_outline=data.get("suggested_outline", []),
        research_brief=data.get("research_brief"),
        used_fallback=used_fallback,
        fallback_reason=fallback_reason,
        llm_provider_used=provider_used,
        llm_model_used=model_used,
    )


def _extract_temp_context_from_file(file_name: str, raw_bytes: bytes) -> str:
    name = (file_name or "").lower()
    # 文本/Markdown 直接读取
    if name.endswith(".md") or name.endswith(".txt"):
        text = raw_bytes.decode("utf-8", errors="ignore")
        return text.strip()[:12000]
    # PDF：优先使用 pypdf 做轻量抽取
    if name.endswith(".pdf"):
        try:
            from pypdf import PdfReader  # type: ignore

            reader = PdfReader(BytesIO(raw_bytes))
            parts: List[str] = []
            for page in reader.pages[:40]:
                try:
                    parts.append((page.extract_text() or "").strip())
                except Exception:
                    continue
            text = "\n".join(x for x in parts if x)
            return text.strip()[:12000]
        except Exception:
            return ""
    return ""


@router.post("/deep-research/context-files", response_model=DeepResearchContextExtractResponse)
async def extract_deep_research_context_files(
    files: List[UploadFile] = File(...),
) -> DeepResearchContextExtractResponse:
    """提取用户上传文件为临时上下文（仅本次 Deep Research 使用，不入库）。"""
    documents: List[Dict[str, str]] = []
    for f in files:
        try:
            raw = await f.read()
        except Exception:
            continue
        if not raw:
            continue
        text = _extract_temp_context_from_file(f.filename or "file", raw)
        if not text:
            continue
        documents.append(
            {
                "name": (f.filename or "file").strip()[:200],
                "content": text,
            }
        )
    return DeepResearchContextExtractResponse(documents=documents)


@router.post("/deep-research/start", response_model=DeepResearchStartResponse)
def start_deep_research_endpoint(
    body: DeepResearchStartRequest,
    optional_user_id: str | None = Depends(get_optional_user_id),
) -> DeepResearchStartResponse:
    """Phase 1: run scope + plan, then return editable brief/outline."""
    from src.collaboration.research.agent import start_deep_research

    store = get_session_store()
    session_id = body.session_id or store.create_session(canvas_id=body.canvas_id or "")
    user_id = optional_user_id or body.user_id or ""

    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client(body.llm_provider or None)
    filters = _build_deep_research_filters(body)
    start_result = start_deep_research(
        topic=body.topic.strip(),
        llm_client=client,
        canvas_id=body.canvas_id,
        session_id=session_id,
        user_id=user_id,
        search_mode=body.search_mode,
        filters=filters or None,
        clarification_answers=body.clarification_answers,
        output_language=body.output_language or "auto",
        model_override=body.model_override or None,
        step_models=body.step_models,
        step_model_strict=bool(body.step_model_strict),
    )
    if start_result.get("canvas_id"):
        store.update_session_meta(session_id, {"canvas_id": start_result.get("canvas_id", "")})
    return DeepResearchStartResponse(
        session_id=session_id,
        canvas_id=start_result.get("canvas_id", "") or "",
        brief=start_result.get("brief") or {},
        outline=start_result.get("outline") or [],
        initial_stats=start_result.get("initial_stats") or {},
    )


def _run_deep_research_job_safe(
    *,
    job_id: str,
    body: DeepResearchConfirmRequest,
    optional_user_id: str | None,
) -> None:
    """后台执行 Deep Research 并持久化状态。"""
    from src.collaboration.research.agent import (
        prepare_deep_research_runtime,
        build_deep_research_result_from_state,
    )
    from src.collaboration.research.job_store import append_event, update_job, get_pending_review
    import time as _time

    t0 = _time.perf_counter()
    store = get_session_store()
    session_id = body.session_id or store.create_session(canvas_id=body.canvas_id or "")
    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client(body.llm_provider or None)
    user_id = optional_user_id or body.user_id or ""
    filters = _build_deep_research_filters(body)

    def _progress_cb(event_type: str, payload: Dict[str, Any]) -> None:
        append_event(job_id, "progress" if event_type != "warning" else "warning", {"type": event_type, **(payload or {})})
        stage = str(payload.get("section") or payload.get("type") or "")
        update_job(job_id, current_stage=stage, message=str(payload.get("message") or payload.get("section") or event_type))

    try:
        update_job(job_id, status="running", message="Deep Research 任务已启动")
        append_event(job_id, "start", {"job_id": job_id, "topic": body.topic, "session_id": session_id})

        runtime = prepare_deep_research_runtime(
            topic=body.topic.strip(),
            llm_client=client,
            confirmed_outline=body.confirmed_outline,
            confirmed_brief=body.confirmed_brief,
            canvas_id=body.canvas_id,
            session_id=session_id,
            user_id=user_id,
            search_mode=body.search_mode,
            filters=filters or None,
            model_override=body.model_override or None,
            output_language=body.output_language or "auto",
            step_models=body.step_models,
            step_model_strict=bool(body.step_model_strict),
            user_context=body.user_context,
            user_context_mode=body.user_context_mode or "supporting",
            user_documents=body.user_documents,
            progress_callback=_progress_cb,
            cancel_check=lambda: _dr_is_cancel_requested(job_id),
            review_waiter=lambda section_id: get_pending_review(job_id, section_id),
            skip_draft_review=bool(body.skip_draft_review),
            skip_refine_review=bool(body.skip_refine_review),
            skip_claim_generation=bool(body.skip_claim_generation),
            job_id=job_id,
            depth=body.depth or "comprehensive",
        )
        compiled = runtime["compiled"]
        config = runtime["config"]
        initial_state = runtime["initial_state"]
        compiled.invoke(initial_state, config=config)
        state_snapshot = compiled.get_state(config)
        if getattr(state_snapshot, "next", ()):
            _dr_store_suspended_runtime(
                job_id,
                {
                    "compiled": compiled,
                    "config": config,
                    "session_id": session_id,
                    "body": body,
                    "started_at_perf": t0,
                    "outline": runtime.get("outline") or [],
                    "topic": runtime.get("topic") or body.topic.strip(),
                },
            )
            update_job(
                job_id,
                status="waiting_review",
                session_id=session_id,
                current_stage="review_gate",
                message="等待人工审核",
            )
            append_event(
                job_id,
                "waiting_review",
                {
                    "job_id": job_id,
                    "session_id": session_id,
                    "next": list(getattr(state_snapshot, "next", ()) or ()),
                },
            )
            return

        final_state = getattr(state_snapshot, "values", {}) or {}
        result = build_deep_research_result_from_state(
            final_state,
            topic=str(runtime.get("topic") or body.topic.strip()),
            elapsed_ms=(_time.perf_counter() - t0) * 1000,
            fallback_outline=runtime.get("outline") or [],
        )
        _complete_deep_research_job(job_id=job_id, session_id=session_id, topic=body.topic, result=result)
        _dr_pop_suspended_runtime(job_id)
    except Exception as e:
        cancelled = _dr_is_cancel_requested(job_id) or "cancelled" in str(e).lower() or "canceled" in str(e).lower()
        status = "cancelled" if cancelled else "error"
        msg = "任务已取消" if cancelled else f"任务失败: {e}"
        update_job(
            job_id,
            status=status,
            message=msg,
            error_message="" if cancelled else str(e),
            finished_at=_time.time(),
            total_time_ms=(_time.perf_counter() - t0) * 1000,
        )
        append_event(job_id, status, {"message": msg, "error": "" if cancelled else str(e)})
        _dr_pop_suspended_runtime(job_id)
    finally:
        _dr_clear_cancel_event(job_id)


def _complete_deep_research_job(*, job_id: str, session_id: str, topic: str, result: Dict[str, Any]) -> None:
    from src.collaboration.research.job_store import append_event, update_job
    import time as _time

    store = get_session_store()
    response_text = result.get("markdown", "")
    citations = result.get("citations") or []
    dashboard_data = result.get("dashboard") or {}
    canvas_id = result.get("canvas_id", "") or ""
    total_time_ms = float(result.get("total_time_ms", 0.0))

    if canvas_id:
        store.update_session_meta(session_id, {"canvas_id": canvas_id})
        try:
            from src.collaboration.canvas.canvas_manager import update_canvas
            update_canvas(canvas_id, stage="refine")
        except Exception:
            _chat_logger.debug("Failed to update canvas stage to refine", exc_info=True)
    store.update_session_stage(session_id, "refine")
    memory = load_session_memory(session_id)
    if memory:
        memory.add_turn("user", f"[Deep Research Confirmed] {topic}")
        memory.add_turn("assistant", response_text, citations=[_serialize_citation(c) for c in citations])

    update_job(
        job_id,
        status="done",
        session_id=session_id,
        canvas_id=canvas_id,
        current_stage="refine",
        message="Deep Research 完成",
        result_markdown=response_text,
        result_citations=json.dumps([_serialize_citation(c) for c in citations], ensure_ascii=False, default=str),
        result_dashboard=json.dumps(dashboard_data, ensure_ascii=False, default=str),
        total_time_ms=total_time_ms,
        finished_at=_time.time(),
    )
    append_event(
        job_id,
        "done",
        {
            "session_id": session_id,
            "canvas_id": canvas_id,
            "total_time_ms": total_time_ms,
            "citations": [_serialize_citation(c) for c in citations],
            "dashboard": dashboard_data,
        },
    )


def _resume_suspended_job(job_id: str) -> None:
    from langgraph.types import Command
    from src.collaboration.research.agent import build_deep_research_result_from_state
    from src.collaboration.research.job_store import append_event, get_job, update_job
    import time as _time

    runtime = _dr_get_suspended_runtime(job_id)
    if not runtime:
        return

    compiled = runtime.get("compiled")
    config = runtime.get("config")
    if compiled is None or not isinstance(config, dict):
        _dr_pop_suspended_runtime(job_id)
        update_job(job_id, status="error", message="任务恢复失败：缺少挂起上下文")
        return

    job = get_job(job_id) or {}
    topic = str(job.get("topic") or runtime.get("topic") or "")
    session_id = str(runtime.get("session_id") or job.get("session_id") or "")
    started_at_perf = float(runtime.get("started_at_perf") or _time.perf_counter())

    try:
        update_job(job_id, status="running", message="收到审核输入，恢复执行")
        append_event(job_id, "resume_start", {"job_id": job_id})
        compiled.invoke(Command(resume=True), config=config)
        state_snapshot = compiled.get_state(config)
        if getattr(state_snapshot, "next", ()):
            update_job(
                job_id,
                status="waiting_review",
                session_id=session_id,
                current_stage="review_gate",
                message="等待人工审核",
            )
            append_event(
                job_id,
                "waiting_review",
                {
                    "job_id": job_id,
                    "session_id": session_id,
                    "next": list(getattr(state_snapshot, "next", ()) or ()),
                },
            )
            return

        final_state = getattr(state_snapshot, "values", {}) or {}
        result = build_deep_research_result_from_state(
            final_state,
            topic=topic or "",
            elapsed_ms=(_time.perf_counter() - started_at_perf) * 1000,
            fallback_outline=runtime.get("outline") or [],
        )
        _complete_deep_research_job(job_id=job_id, session_id=session_id, topic=topic or "", result=result)
        _dr_pop_suspended_runtime(job_id)
    except Exception as e:
        cancelled = _dr_is_cancel_requested(job_id) or "cancelled" in str(e).lower() or "canceled" in str(e).lower()
        status = "cancelled" if cancelled else "error"
        msg = "任务已取消" if cancelled else f"任务恢复失败: {e}"
        update_job(
            job_id,
            status=status,
            message=msg,
            error_message="" if cancelled else str(e),
            finished_at=_time.time(),
            total_time_ms=(_time.perf_counter() - started_at_perf) * 1000,
        )
        append_event(job_id, status, {"message": msg, "error": "" if cancelled else str(e)})
        _dr_pop_suspended_runtime(job_id)
    finally:
        _dr_set_resume_idle(job_id)
        _dr_clear_cancel_event(job_id)


@router.post("/deep-research/submit", response_model=DeepResearchSubmitResponse)
def submit_deep_research_endpoint(
    body: DeepResearchConfirmRequest,
    optional_user_id: str | None = Depends(get_optional_user_id),
) -> DeepResearchSubmitResponse:
    """提交 Deep Research 后台任务（默认推荐前端使用此接口）。"""
    from src.collaboration.research.job_store import create_job

    store = get_session_store()
    session_id = body.session_id or store.create_session(canvas_id=body.canvas_id or "")
    payload = body.model_dump()
    payload["session_id"] = session_id
    payload["_worker_user_id"] = optional_user_id
    job = create_job(
        topic=body.topic.strip(),
        session_id=session_id,
        canvas_id=body.canvas_id or "",
        request_payload=payload,
    )
    job_id = str(job.get("job_id") or "")
    return DeepResearchSubmitResponse(
        ok=True,
        job_id=job_id,
        session_id=session_id,
        canvas_id=body.canvas_id or "",
    )


@router.post("/deep-research/confirm")
def confirm_deep_research_endpoint(
    body: DeepResearchConfirmRequest,
    optional_user_id: str | None = Depends(get_optional_user_id),
) -> StreamingResponse:
    """Phase 2: execute deep research using confirmed brief/outline and stream progress."""
    from src.collaboration.research.agent import execute_deep_research

    session_id = body.session_id or get_session_store().create_session(canvas_id=body.canvas_id or "")
    store = get_session_store()
    manager = get_manager(str(_CONFIG_PATH))
    client = manager.get_client(body.llm_provider or None)
    user_id = optional_user_id or body.user_id or ""
    filters = _build_deep_research_filters(body)

    progress_events: List[Dict[str, Any]] = []

    def _progress_cb(event_type: str, payload: Dict[str, Any]) -> None:
        progress_events.append({"event": event_type, "data": payload})

    result = execute_deep_research(
        topic=body.topic.strip(),
        llm_client=client,
        confirmed_outline=body.confirmed_outline,
        confirmed_brief=body.confirmed_brief,
        canvas_id=body.canvas_id,
        session_id=session_id,
        user_id=user_id,
        search_mode=body.search_mode,
        filters=filters or None,
        model_override=body.model_override or None,
        output_language=body.output_language or "auto",
        step_models=body.step_models,
        step_model_strict=bool(body.step_model_strict),
        user_context=body.user_context,
        user_context_mode=body.user_context_mode or "supporting",
        user_documents=body.user_documents,
        progress_callback=_progress_cb,
        skip_draft_review=bool(body.skip_draft_review),
        skip_refine_review=bool(body.skip_refine_review),
        skip_claim_generation=bool(body.skip_claim_generation),
        depth=body.depth or "comprehensive",
    )
    response_text = result.get("markdown", "")
    citations = result.get("citations") or []
    if result.get("canvas_id"):
        store.update_session_meta(session_id, {"canvas_id": result.get("canvas_id", "")})
        try:
            from src.collaboration.canvas.canvas_manager import update_canvas
            update_canvas(result.get("canvas_id", ""), stage="refine")
        except Exception:
            _chat_logger.debug("Failed to update canvas stage to refine (stream confirm)", exc_info=True)
    store.update_session_stage(session_id, "refine")
    memory = load_session_memory(session_id)
    if memory:
        memory.add_turn("user", f"[Deep Research Confirmed] {body.topic}")
        memory.add_turn("assistant", response_text, citations=[_serialize_citation(c) for c in citations])
    evidence_summary = EvidenceSummary(
        query=body.topic,
        total_chunks=len(citations),
        sources_used=_infer_sources_from_citations(citations),
        retrieval_time_ms=float(result.get("total_time_ms", 0.0)),
    )
    dashboard_data = result.get("dashboard") or {}

    def event_stream():
        meta = {
            "session_id": session_id,
            "canvas_id": result.get("canvas_id", "") or "",
            "citations": [_serialize_citation(c) for c in citations],
            "evidence_summary": evidence_summary.model_dump(),
            "intent": {"mode": "deep_research", "intent_type": "deep_research", "confidence": 1.0, "from_command": False},
            "current_stage": "refine",
        }
        yield f"event: meta\ndata: {json.dumps(meta, ensure_ascii=False)}\n\n"
        for item in progress_events:
            ev = "warning" if item["event"] == "warning" else "progress"
            payload = {"type": item["event"], **(item.get("data") or {})}
            yield f"event: {ev}\ndata: {json.dumps(payload, ensure_ascii=False, default=str)}\n\n"
        if dashboard_data:
            yield f"event: dashboard\ndata: {json.dumps(dashboard_data, ensure_ascii=False, default=str)}\n\n"
        for chunk in _chunk_text(response_text):
            yield f"event: delta\ndata: {json.dumps({'delta': chunk}, ensure_ascii=False)}\n\n"
        yield "event: done\ndata: {}\n\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")


@router.get("/deep-research/jobs", response_model=List[DeepResearchJobInfo])
def list_deep_research_jobs(limit: int = 20, status: str | None = None) -> List[DeepResearchJobInfo]:
    from src.collaboration.research.job_store import list_jobs

    jobs = list_jobs(limit=limit, status=status)
    out: List[DeepResearchJobInfo] = []
    for j in jobs:
        out.append(DeepResearchJobInfo(**j))
    return out


@router.get("/deep-research/jobs/{job_id}", response_model=DeepResearchJobInfo)
def get_deep_research_job(job_id: str) -> DeepResearchJobInfo:
    from src.collaboration.research.job_store import get_job

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    return DeepResearchJobInfo(**job)


@router.get("/deep-research/jobs/{job_id}/events")
def get_deep_research_job_events(job_id: str, after_id: int = 0, limit: int = 200) -> dict:
    from src.collaboration.research.job_store import get_job, list_events

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    return {
        "job_id": job_id,
        "events": list_events(job_id, after_id=after_id, limit=limit),
    }


@router.post("/deep-research/jobs/{job_id}/cancel")
def cancel_deep_research_job(job_id: str) -> dict:
    from src.collaboration.research.job_store import get_job, update_job, append_event

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    current_status = str(job.get("status") or "")
    if current_status in {"done", "error", "cancelled"}:
        return {"ok": True, "job_id": job_id, "status": current_status}
    if current_status == "pending":
        import time as _t
        update_job(job_id, status="cancelled", message="任务已取消（未启动）", finished_at=_t.time())
        append_event(job_id, "cancelled", {"job_id": job_id, "message": "任务已取消（未启动）"})
        return {"ok": True, "job_id": job_id, "status": "cancelled"}
    _dr_request_cancel(job_id)
    update_job(job_id, status="cancelling", message="收到停止请求，正在终止任务...")
    append_event(job_id, "cancel_requested", {"job_id": job_id, "message": "已请求停止"})
    return {"ok": True, "job_id": job_id, "status": "cancelling"}


@router.post("/deep-research/jobs/{job_id}/review")
def review_deep_research_section(job_id: str, body: dict) -> dict:
    from src.collaboration.research.job_store import get_job, submit_review, append_event

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    section_id = str(body.get("section_id") or "").strip()
    if not section_id:
        raise HTTPException(status_code=400, detail="section_id 不能为空")
    # Normalize section id against confirmed outline to avoid tiny title drift
    # (e.g. accidental extra spaces / case changes) causing review-gate mismatch.
    req_payload = job.get("request") or {}
    outline = req_payload.get("confirmed_outline") or []
    if isinstance(outline, list) and outline:
        normalized = section_id.strip().lower()
        for item in outline:
            title = str(item or "").strip()
            if title and title.lower() == normalized:
                section_id = title
                break
    action = str(body.get("action") or "approve").strip().lower()
    feedback = str(body.get("feedback") or "")
    if action not in {"approve", "revise"}:
        raise HTTPException(status_code=400, detail="action must be approve|revise")
    result = submit_review(job_id, section_id, action=action, feedback=feedback)
    append_event(job_id, "section_review", {"section_id": section_id, "action": action, "feedback": feedback})
    resume_submitted = False
    runtime = _dr_get_suspended_runtime(job_id)
    if runtime:
        compiled = runtime.get("compiled")
        config = runtime.get("config")
        try:
            state_snapshot = compiled.get_state(config) if compiled is not None and isinstance(config, dict) else None
        except Exception:
            state_snapshot = None
        if state_snapshot is not None and getattr(state_snapshot, "next", ()):
            if _dr_mark_resume_inflight(job_id):
                try:
                    from src.collaboration.research.job_store import enqueue_resume_request
                    from src.utils.task_runner import get_worker_instance_id
                    enqueue_resume_request(
                        job_id=job_id,
                        owner_instance=get_worker_instance_id(),
                        source="review",
                        message="收到人工审核结果，等待恢复执行",
                    )
                    resume_submitted = True
                except Exception:
                    _dr_set_resume_idle(job_id)
    return {"ok": True, "resume_submitted": resume_submitted, **result}


@router.get("/deep-research/jobs/{job_id}/reviews")
def list_deep_research_reviews(job_id: str) -> dict:
    from src.collaboration.research.job_store import get_job, list_reviews

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    return {"job_id": job_id, "reviews": list_reviews(job_id)}


@router.get("/deep-research/resume-queue")
def list_deep_research_resume_queue(
    limit: int = 50,
    status: str | None = None,
    owner_instance: str | None = None,
    job_id: str | None = None,
) -> dict:
    from src.collaboration.research.job_store import list_resume_requests

    rows = list_resume_requests(
        limit=limit,
        status=status,
        owner_instance=owner_instance,
        job_id=job_id,
    )
    return {"items": rows, "count": len(rows)}


@router.post("/deep-research/resume-queue/cleanup")
def cleanup_deep_research_resume_queue(body: dict) -> dict:
    from src.collaboration.research.job_store import cleanup_resume_requests
    import time as _time

    statuses = body.get("statuses")
    if statuses is not None and not isinstance(statuses, list):
        raise HTTPException(status_code=400, detail="statuses 必须是字符串数组")
    before_hours = body.get("before_hours", 72)
    before_ts = None
    if before_hours is not None:
        try:
            hours = float(before_hours)
            if hours < 0:
                raise ValueError("hours < 0")
            before_ts = _time.time() - hours * 3600
        except Exception:
            raise HTTPException(status_code=400, detail="before_hours 必须是非负数字或 null")
    deleted = cleanup_resume_requests(
        statuses=[str(s) for s in (statuses or [])] if statuses else None,
        before_ts=before_ts,
        owner_instance=str(body.get("owner_instance") or "") or None,
        job_id=str(body.get("job_id") or "") or None,
    )
    return {"ok": True, "deleted": deleted}


@router.post("/deep-research/resume-queue/{resume_id}/retry")
def retry_deep_research_resume_request(resume_id: int, body: dict | None = None) -> dict:
    from src.collaboration.research.job_store import retry_resume_request, append_event
    from src.utils.task_runner import get_worker_instance_id

    payload = body or {}
    owner_instance = str(payload.get("owner_instance") or get_worker_instance_id())
    message = str(payload.get("message") or "手动重试恢复请求")
    try:
        row = retry_resume_request(
            resume_id=resume_id,
            owner_instance=owner_instance,
            message=message,
        )
    except ValueError as e:
        raise HTTPException(status_code=409, detail=str(e))
    if not row:
        raise HTTPException(status_code=404, detail="resume request 不存在")
    append_event(
        str(row.get("job_id") or ""),
        "resume_retry_requested",
        {"resume_id": resume_id, "owner_instance": owner_instance, "message": message},
    )
    return {"ok": True, "item": row}


# ── Gap Supplement Endpoints ──

@router.post("/deep-research/jobs/{job_id}/gap-supplement")
def submit_gap_supplement_endpoint(job_id: str, body: dict) -> dict:
    from src.collaboration.research.job_store import get_job, submit_gap_supplement, append_event

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    section_id = str(body.get("section_id") or "").strip()
    if not section_id:
        raise HTTPException(status_code=400, detail="section_id 不能为空")
    gap_text = str(body.get("gap_text") or "")
    supplement_type = str(body.get("supplement_type") or "material").strip()
    content = body.get("content") or {}
    result = submit_gap_supplement(
        job_id=job_id,
        section_id=section_id,
        gap_text=gap_text,
        supplement_type=supplement_type,
        content=content,
    )
    append_event(job_id, "gap_supplement", {
        "section_id": section_id,
        "gap_text": gap_text,
        "supplement_type": supplement_type,
    })
    return {"ok": True, **result}


@router.get("/deep-research/jobs/{job_id}/gap-supplements")
def list_gap_supplements_endpoint(job_id: str, section_id: str | None = None) -> dict:
    from src.collaboration.research.job_store import get_job, list_gap_supplements

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    supplements = list_gap_supplements(job_id, section_id=section_id)
    return {"job_id": job_id, "supplements": supplements}


# ── Research Insights Ledger Endpoints ──

@router.get("/deep-research/jobs/{job_id}/insights")
def list_insights_endpoint(
    job_id: str,
    insight_type: str | None = None,
    status: str | None = None,
) -> dict:
    from src.collaboration.research.job_store import get_job, list_insights

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    insights = list_insights(job_id, insight_type=insight_type, status=status)
    return {"job_id": job_id, "insights": insights}


@router.post("/deep-research/jobs/{job_id}/insights/{insight_id}/status")
def update_insight_status_endpoint(job_id: str, insight_id: int, body: dict) -> dict:
    from src.collaboration.research.job_store import get_job, update_insight_status

    job = get_job(job_id)
    if not job:
        raise HTTPException(status_code=404, detail="job 不存在")
    new_status = str(body.get("status") or "").strip()
    if new_status not in {"open", "addressed", "deferred"}:
        raise HTTPException(status_code=400, detail="status must be open|addressed|deferred")
    update_insight_status(insight_id, new_status)
    return {"ok": True, "insight_id": insight_id, "status": new_status}


@router.get("/sessions/{session_id}", response_model=SessionInfo)
def get_session(session_id: str) -> SessionInfo:
    store = get_session_store()
    meta = store.get_session_meta(session_id)
    if meta is None:
        raise HTTPException(status_code=404, detail="session not found")
    turns = store.get_turns(session_id)
    turn_items = []
    for t in turns:
        # 将存储的 citations 字典列表转换为 ChatCitation 对象
        sources = [
            ChatCitation(
                cite_key=c.get("cite_key", ""),
                title=c.get("title", ""),
                authors=c.get("authors", []),
                year=c.get("year"),
                doc_id=c.get("doc_id"),
                url=c.get("url"),
                doi=c.get("doi"),
                bbox=c.get("bbox"),
                page_num=c.get("page_num"),
            )
            for c in (t.citations or [])
        ]
        turn_items.append(TurnItem(role=t.role, content=t.content, sources=sources))

    # 恢复最近一次 Deep Research dashboard（刷新页面后仍可看到章节列表）
    latest_dashboard = None
    try:
        from src.collaboration.research.job_store import get_latest_job_by_session
        latest_job = get_latest_job_by_session(session_id)
        if latest_job:
            dashboard = latest_job.get("result_dashboard") or {}
            if isinstance(dashboard, dict) and dashboard:
                latest_dashboard = dashboard
    except Exception:
        latest_dashboard = None

    return SessionInfo(
        session_id=meta["session_id"],
        canvas_id=meta["canvas_id"] or "",
        stage=meta.get("stage") or "explore",
        turn_count=len(turns),
        turns=turn_items,
        research_dashboard=latest_dashboard,
    )


@router.delete("/sessions/{session_id}")
def delete_session(session_id: str) -> dict:
    store = get_session_store()
    if not store.delete_session(session_id):
        raise HTTPException(status_code=404, detail="session not found")
    return {"ok": True, "session_id": session_id}


@router.get("/sessions", response_model=List[SessionListItem])
def list_sessions(limit: int = 100) -> List[SessionListItem]:
    """获取所有会话列表"""
    store = get_session_store()
    sessions = store.list_all_sessions(limit=limit)
    return [
        SessionListItem(
            session_id=s["session_id"],
            title=s["title"],
            canvas_id=s["canvas_id"] or "",
            stage=s.get("stage", "explore"),
            turn_count=s["turn_count"],
            created_at=s["created_at"],
            updated_at=s["updated_at"],
        )
        for s in sessions
    ]
</file>

<file path="src/collaboration/research/agent.py">
"""
递归研究 Agent — 基于 LangGraph 的 Deep Research 引擎。

替代原有线性 auto_complete 流水线，实现：
- Scoping → Plan → 递归研究循环 → 写作 → 验证 → 综合
- RE-TRAC 轨迹压缩 + ReCAP 仪表盘
- 动态分支探索 + 信息充分度评估
"""

from __future__ import annotations

import json
import re
import time
import copy
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple, TypedDict

from langgraph.checkpoint.memory import MemorySaver
from langgraph.graph import StateGraph, END
from langgraph.types import interrupt
from pydantic import BaseModel, Field

from src.collaboration.research.dashboard import (
    ResearchBrief,
    ResearchDashboard,
    SectionStatus,
)
from src.collaboration.research.trajectory import (
    ResearchBranch,
    ResearchTrajectory,
    SearchAction,
    compress_trajectory,
)
from src.log import get_logger
from src.utils.prompt_manager import PromptManager

logger = get_logger(__name__)
_pm = PromptManager()
_CONFIG_PATH = Path(__file__).resolve().parents[3] / "config" / "rag_config.json"


class _ScopingResponse(BaseModel):
    scope: str = ""
    success_criteria: List[str] = Field(default_factory=list)
    key_questions: List[str] = Field(default_factory=list)
    exclusions: List[str] = Field(default_factory=list)
    time_range: str = ""
    source_priority: List[str] = Field(default_factory=list)


class _CoverageEvalResponse(BaseModel):
    coverage_score: float = 0.0
    gaps: List[str] = Field(default_factory=list)
    sufficient: bool = False


# ────────────────────────────────────────────────
# Depth Presets — "lite" vs "comprehensive"
# ────────────────────────────────────────────────
# Each preset defines the bounds for all loops in the graph, preventing the
# recursion-limit explosion that occurs when any loop runs unbounded.
#
#   lite          — fast but academically usable, ~5-15 min
#   comprehensive — thorough academic review, ~20-60 min
#
# Key thresholds (see architecture.md for full table):
#
# Iteration & rounds:
#   max_iterations_per_section  per-section iteration budget (× num_sections = global cap)
#   max_section_research_rounds per-section research→evaluate loop cap
#
# Coverage:
#   coverage_threshold          minimum coverage_score before moving to write
#
# Queries — split into recall (broad, synonym) + precision (specific, constrained):
#   recall_queries_per_section  broad synonym/variant queries per round
#   precision_queries_per_section  narrow method/time/object-constrained queries per round
#   (total queries = recall + precision + gap queries)
#
# Tiered search_top_k (chunks per query, varies by stage):
#   search_top_k_first          first-round broad sweep
#   search_top_k_gap            gap-fill / re-research rounds
#   search_top_k_write          write-node final evidence retrieval
#   search_top_k_write_max      hard cap for adaptive write retrieval window
#   verification_k              write-stage secondary evidence check for data-point citations
#
# Self-correction:
#   self_correction_trigger_coverage  if coverage is already high after early rounds, shrink gap retrieval
#   self_correction_min_round         round index to enable self-correction
#   search_top_k_gap_decay_factor     decay factor for search_top_k_gap
#   search_top_k_gap_min              lower bound after decay
#
# Early-stop by gain curve:
#   coverage_plateau_floor       coverage floor to enable plateau check
#   coverage_plateau_min_gain    minimal acceptable gain between rounds
#
# Verification — 3-tier (light / medium / severe):
#   verify_light_threshold      below this → just flag, no action
#   verify_medium_threshold     between light and severe → gap-fill only (no full re-research)
#   verify_severe_threshold     above this → full re-research with expanded queries
#
# Review gate:
#   review_gate_max_rounds      max polling rounds (with exponential backoff)
#   review_gate_base_sleep      initial sleep seconds (doubles each round, capped)
#   review_gate_max_sleep        max sleep seconds per round
#   review_gate_early_stop_unchanged  auto-stop after N consecutive unchanged polls
#
# LangGraph:
#   recursion_limit             compile-time recursion limit
#
# Cost monitor:
#   cost_warn_steps             warn for manual intervention when graph steps reach threshold
#   cost_force_summary_steps    force summarize mode when graph steps are extremely high
#   cost_tick_interval          emit progress heartbeat every N graph steps
# ────────────────────────────────────────────────

DEPTH_PRESETS: Dict[str, Dict[str, Any]] = {
    "lite": {
        # ── Iteration budget ──
        "max_iterations_per_section": 3,         # global = 3 × num_sections
        "max_section_research_rounds": 3,
        # ── Coverage ──
        "coverage_threshold": 0.60,
        # ── Queries (recall + precision) ──
        "recall_queries_per_section": 2,
        "precision_queries_per_section": 2,      # total = 4 + gaps
        # ── Tiered search_top_k ──
        "search_top_k_first": 18,                # broad first-round sweep
        "search_top_k_gap": 10,                  # targeted gap-fill
        "search_top_k_write": 10,                # precise evidence for writing
        "search_top_k_write_max": 40,            # hard cap for adaptive write retrieval
        "verification_k": 12,                    # secondary check context for data-point claims
        # ── Self-correction / adaptive cost control ──
        "self_correction_trigger_coverage": 0.75,
        "self_correction_min_round": 3,
        "search_top_k_gap_decay_factor": 0.60,
        "search_top_k_gap_min": 6,
        "coverage_plateau_floor": 0.70,
        "coverage_plateau_min_gain": 0.03,
        # ── Verification (3-tier) ──
        "verify_light_threshold": 0.20,          # < 20% → flag only
        "verify_medium_threshold": 0.40,         # 20-40% → gap-fill, no full re-research
        "verify_severe_threshold": 0.45,         # > 45% → full re-research
        # ── Review gate ──
        "review_gate_max_rounds": 80,            # ~5 min with backoff
        "review_gate_base_sleep": 2,
        "review_gate_max_sleep": 15,
        "review_gate_early_stop_unchanged": 8,   # stop after 8 unchanged polls
        # ── LangGraph ──
        "recursion_limit": 200,
        # ── Cost monitor (graph steps) ──
        "cost_warn_steps": 120,
        "cost_force_summary_steps": 180,
        "cost_tick_interval": 25,
    },
    "comprehensive": {
        # ── Iteration budget ──
        "max_iterations_per_section": 6,         # global = 6 × num_sections
        "max_section_research_rounds": 5,        # allows a 5th round for near-complete gaps
        # ── Coverage ──
        "coverage_threshold": 0.80,
        # ── Queries (recall + precision) ──
        "recall_queries_per_section": 4,
        "precision_queries_per_section": 4,      # total = 8 + gaps
        # ── Tiered search_top_k ──
        "search_top_k_first": 30,                # wide net
        "search_top_k_gap": 15,                  # focused supplement
        "search_top_k_write": 12,                # best-citation retrieval
        "search_top_k_write_max": 60,            # hard cap for adaptive write retrieval
        "verification_k": 16,                    # stronger secondary check context
        # ── Self-correction / adaptive cost control ──
        "self_correction_trigger_coverage": 0.78,
        "self_correction_min_round": 3,
        "search_top_k_gap_decay_factor": 0.70,
        "search_top_k_gap_min": 8,
        "coverage_plateau_floor": 0.78,
        "coverage_plateau_min_gain": 0.02,
        # ── Verification (3-tier) ──
        "verify_light_threshold": 0.15,          # < 15% → flag only
        "verify_medium_threshold": 0.30,         # 15-30% → gap-fill only
        "verify_severe_threshold": 0.35,         # > 35% → full re-research
        # ── Review gate ──
        "review_gate_max_rounds": 200,           # ~10 min with backoff
        "review_gate_base_sleep": 2,
        "review_gate_max_sleep": 20,
        "review_gate_early_stop_unchanged": 12,
        # ── LangGraph ──
        "recursion_limit": 500,
        # ── Cost monitor (graph steps) ──
        "cost_warn_steps": 300,
        "cost_force_summary_steps": 420,
        "cost_tick_interval": 30,
    },
}

DEFAULT_DEPTH = "comprehensive"


def get_depth_preset(depth: str) -> Dict[str, Any]:
    """Return depth preset dict; falls back to comprehensive for unknown values."""
    return dict(DEPTH_PRESETS.get(depth, DEPTH_PRESETS[DEFAULT_DEPTH]))


# ────────────────────────────────────────────────
# State 定义
# ────────────────────────────────────────────────

class DeepResearchState(TypedDict, total=False):
    """LangGraph Agent 的状态"""
    topic: str
    dashboard: ResearchDashboard
    trajectory: ResearchTrajectory
    canvas_id: str
    session_id: str
    user_id: str
    search_mode: str
    filters: Dict[str, Any]
    current_section: str
    sections_completed: List[str]
    markdown_parts: List[str]
    citations: List[Any]
    evidence_chunks: List[Any]  # 运行期累计的 EvidenceChunk（用于 hash->cite_key 后处理）
    evidence_chunk_empty_value: Any  # state 紧凑化时用于覆盖 text/raw_content 的值（默认 ""）
    citation_doc_key_map: Dict[str, str]  # doc_group_key -> cite_key（跨阶段保持稳定）
    citation_existing_keys: List[str]  # 已分配 cite_key（用于 numeric/hash/author_date 去重）
    iteration_count: int
    max_iterations: int
    llm_client: Any
    model_override: Optional[str]
    output_language: str
    clarification_answers: Dict[str, str]
    user_context: str
    user_context_mode: str
    user_documents: List[Dict[str, str]]
    step_models: Dict[str, Optional[str]]
    step_model_strict: bool
    progress_callback: Optional[Callable[[str, Dict[str, Any]], None]]
    cancel_check: Optional[Callable[[], bool]]
    review_waiter: Optional[Callable[[str], Optional[Dict[str, Any]]]]
    skip_draft_review: bool
    skip_refine_review: bool
    skip_claim_generation: bool
    verified_claims: str  # generated claims text, empty if skipped
    review_gate_next: str
    review_handled_at: Dict[str, float]
    # ── Depth preset (controls all loop bounds) ──
    depth: str                        # "lite" | "comprehensive"
    depth_preset: Dict[str, Any]      # resolved preset values
    review_gate_rounds: int           # counter for review_gate self-loop
    review_gate_unchanged: int        # consecutive unchanged poll counter (for early-stop)
    review_gate_last_snapshot: str    # last poll snapshot hash (for change detection)
    # ── Local Priority Revise ──
    revision_queue: List[str]         # sections queued for priority rework
    review_seen_at: Dict[str, float]  # last-seen review timestamp per section
    # ── Job reference (for insights / supplements persistence) ──
    job_id: str
    # ── Cost / stopping monitor ──
    graph_step_count: int
    cost_warned: bool
    force_synthesize: bool
    coverage_history: Dict[str, List[float]]
    last_cost_tick_step: int
    error: Optional[str]


def _emit_progress(state: DeepResearchState, event_type: str, payload: Dict[str, Any]) -> None:
    """Emit optional progress callbacks for SSE integration."""
    cb = state.get("progress_callback")
    if not cb:
        return
    try:
        cb(event_type, payload)
    except Exception:
        logger.debug("Progress callback failed", exc_info=True)


def _ensure_not_cancelled(state: DeepResearchState) -> None:
    """Cooperative cancellation checkpoint."""
    checker = state.get("cancel_check")
    if checker and checker():
        raise RuntimeError("Deep Research cancelled by user")


def _tick_cost_monitor(state: DeepResearchState, node_name: str) -> None:
    """Track graph step count and emit cost/recursion safety events."""
    preset = state.get("depth_preset") or get_depth_preset(state.get("depth", DEFAULT_DEPTH))
    steps = int(state.get("graph_step_count", 0)) + 1
    state["graph_step_count"] = steps

    warn_steps = int(preset.get("cost_warn_steps", 300))
    force_steps = int(preset.get("cost_force_summary_steps", max(360, warn_steps + 60)))
    tick_interval = max(1, int(preset.get("cost_tick_interval", 25)))
    last_tick = int(state.get("last_cost_tick_step", 0))

    if steps - last_tick >= tick_interval:
        state["last_cost_tick_step"] = steps
        _emit_progress(
            state,
            "cost_monitor_tick",
            {
                "node": node_name,
                "steps": steps,
                "warn_steps": warn_steps,
                "force_steps": force_steps,
            },
        )

    if steps >= warn_steps and not bool(state.get("cost_warned", False)):
        state["cost_warned"] = True
        _emit_progress(
            state,
            "cost_monitor_warn",
            {
                "node": node_name,
                "steps": steps,
                "warn_steps": warn_steps,
                "message": "Graph step cost is high. Consider human intervention or narrowing scope.",
            },
        )

    if steps >= force_steps and not bool(state.get("force_synthesize", False)):
        state["force_synthesize"] = True
        # Clamp further exploration budget to avoid runaway loops.
        state["max_iterations"] = min(
            int(state.get("max_iterations", 30)),
            int(state.get("iteration_count", 0)),
        )
        _emit_progress(
            state,
            "cost_monitor_force_summary",
            {
                "node": node_name,
                "steps": steps,
                "force_steps": force_steps,
                "message": "Forced summary mode activated due to high graph step cost.",
            },
        )


def _get_retrieval_svc(state: DeepResearchState):
    """Extract collection from filters and get the correct RetrievalService."""
    from src.retrieval.service import get_retrieval_service

    collection = ((state.get("filters") or {}).get("collection") or "").strip() or None
    return get_retrieval_service(collection=collection)


def _parse_step_model_value(value: Optional[str]) -> Tuple[Optional[str], Optional[str]]:
    if not value:
        return None, None
    raw = value.strip()
    if not raw:
        return None, None
    if "::" in raw:
        provider, model = raw.split("::", 1)
        return provider.strip() or None, model.strip() or None
    return None, raw


def _resolve_step_client_and_model(state: DeepResearchState, step: str) -> Tuple[Any, Optional[str]]:
    """Resolve provider/model override for a specific step."""
    default_client = state["llm_client"]
    default_model = state.get("model_override")
    step_models = state.get("step_models") or {}
    strict = bool(state.get("step_model_strict", False))
    requested = (step_models.get(step) or "").strip()
    provider, model = _parse_step_model_value(step_models.get(step))
    if not provider and not model:
        return default_client, default_model
    if not provider:
        _emit_progress(
            state,
            "step_model_resolved",
            {
                "step": step,
                "requested": requested,
                "actual_provider": "default",
                "actual_model": model or default_model or "",
                "strict": strict,
            },
        )
        return default_client, model
    try:
        from src.llm.llm_manager import get_manager

        manager = get_manager(str(_CONFIG_PATH))
        client = manager.get_client(provider)
        _emit_progress(
            state,
            "step_model_resolved",
            {
                "step": step,
                "requested": requested,
                "actual_provider": provider,
                "actual_model": model or "",
                "strict": strict,
            },
        )
        return client, model
    except Exception as e:
        logger.warning("Failed to switch step model provider '%s': %s", provider, e)
        _emit_progress(
            state,
            "step_model_fallback",
            {
                "step": step,
                "requested": requested,
                "fallback_provider": "default",
                "fallback_model": model or default_model or "",
                "strict": strict,
                "message": f"Step '{step}' model fallback to default due to provider resolve failure.",
                "error": str(e)[:240],
            },
        )
        if strict:
            raise RuntimeError(f"Step '{step}' model provider '{provider}' resolve failed: {e}") from e
        return default_client, (model or default_model)


def _language_instruction(state: DeepResearchState) -> str:
    lang = (state.get("output_language") or "auto").strip().lower()
    if lang == "zh":
        return "\n\nIMPORTANT: Write the output in Chinese (中文)."
    if lang == "en":
        return "\n\nIMPORTANT: Write the output in English."
    return ""


def _build_user_context_block(state: DeepResearchState, max_chars: int = 2500) -> str:
    """Build optional user-supplied temporary context block."""
    chunks: List[str] = []
    user_context = (state.get("user_context") or "").strip()
    mode = (state.get("user_context_mode") or "supporting").strip().lower()
    if user_context:
        if mode == "direct_injection":
            chunks.append(
                "HIGH PRIORITY USER ASSERTIONS (treat as strong constraints/hypotheses and explicitly verify):\n"
                + user_context
            )
        else:
            chunks.append("User Notes:\n" + user_context)
    docs = state.get("user_documents") or []
    if docs:
        rendered: List[str] = []
        for d in docs[:6]:
            name = str((d or {}).get("name") or "document")
            content = str((d or {}).get("content") or "").strip()
            if not content:
                continue
            rendered.append(f"[{name}]\n{content[:1200]}")
        if rendered:
            chunks.append("User Uploaded Temporary Documents:\n" + "\n\n".join(rendered))
    if not chunks:
        return ""
    text = "\n\n".join(chunks).strip()
    if len(text) > max_chars:
        text = text[:max_chars]
    return "\n\nAdditional temporary context:\n" + text


def _compute_effective_write_k(preset: Dict[str, Any], filters: Optional[Dict[str, Any]]) -> int:
    """Compute adaptive write retrieval window with a safety cap."""
    preset_write_k = int(preset.get("search_top_k_write", 12))
    write_k_cap = int(preset.get("search_top_k_write_max", 60))
    if write_k_cap <= 0:
        write_k_cap = 60
    # Keep cap sane: never below the preset floor.
    write_k_cap = max(write_k_cap, preset_write_k)

    ui_top_k = 0
    ui_top_k_raw = (filters or {}).get("final_top_k")
    try:
        ui_top_k = int(ui_top_k_raw or 0)
    except (TypeError, ValueError):
        ui_top_k = 0

    effective_write_k = max(preset_write_k, int(ui_top_k * 1.5)) if ui_top_k > 0 else preset_write_k
    return min(effective_write_k, write_k_cap)


def _tokenize_for_overlap(text: str) -> List[str]:
    raw = re.findall(r"[A-Za-z0-9]+|[\u4e00-\u9fff]+", (text or "").lower())
    return [t for t in raw if len(t) >= 2]


def _build_temp_chunks(state: DeepResearchState) -> List[Dict[str, str]]:
    docs = state.get("user_documents") or []
    out: List[Dict[str, str]] = []
    for d in docs[:10]:
        name = str((d or {}).get("name") or "temp")
        content = str((d or {}).get("content") or "").strip()
        if not content:
            continue
        blocks = [b.strip() for b in re.split(r"\n\s*\n", content) if b.strip()]
        if not blocks:
            blocks = [content]
        for bi, b in enumerate(blocks[:40]):
            chunk = b[:900].strip()
            if len(chunk) < 30:
                continue
            out.append({"name": name, "chunk_id": f"temp::{name}::{bi+1}", "text": chunk})
    return out


def _retrieve_temp_snippets(state: DeepResearchState, query: str, top_k: int = 4) -> List[Dict[str, str]]:
    chunks = _build_temp_chunks(state)
    if not chunks:
        return []
    q_tokens = set(_tokenize_for_overlap(query))
    if not q_tokens:
        return chunks[:top_k]
    scored: List[Tuple[float, Dict[str, str]]] = []
    for c in chunks:
        c_tokens = set(_tokenize_for_overlap(c["text"]))
        overlap = len(q_tokens.intersection(c_tokens))
        if overlap <= 0:
            continue
        score = overlap / max(len(q_tokens), 1)
        scored.append((score, c))
    scored.sort(key=lambda x: x[0], reverse=True)
    if scored:
        return [x[1] for x in scored[:top_k]]
    return chunks[: min(top_k, len(chunks))]


def _accumulate_evidence_chunks(state: DeepResearchState, chunks: List[Any]) -> None:
    """将检索到的 EvidenceChunk 去重后累积到 state，用于后续统一引用替换。"""
    if not chunks:
        return
    existing = state.setdefault("evidence_chunks", [])
    empty_value = state.get("evidence_chunk_empty_value", "")
    seen_ids = {str(getattr(c, "chunk_id", "")) for c in existing if getattr(c, "chunk_id", None)}
    for c in chunks:
        cid = str(getattr(c, "chunk_id", "") or "")
        if cid and cid in seen_ids:
            continue
        compact_chunk = copy.copy(c)
        if isinstance(compact_chunk, dict):
            if "text" in compact_chunk:
                compact_chunk["text"] = empty_value
            if "raw_content" in compact_chunk:
                compact_chunk["raw_content"] = empty_value
        else:
            if hasattr(compact_chunk, "text"):
                setattr(compact_chunk, "text", empty_value)
            if hasattr(compact_chunk, "raw_content"):
                setattr(compact_chunk, "raw_content", empty_value)
        existing.append(compact_chunk)
        if cid:
            seen_ids.add(cid)


def _resolve_text_citations(
    state: DeepResearchState,
    text: str,
    chunks: List[Any],
    include_unreferenced_documents: bool = False,
) -> tuple[str, List[Any]]:
    """
    使用共享的 doc_key->cite_key 映射做 hash 引文替换，确保跨阶段引用键稳定。
    """
    if not text or not chunks:
        return text, []
    from src.collaboration.citation.manager import resolve_response_citations

    doc_key_map = state.setdefault("citation_doc_key_map", {})
    existing_keys_list = state.setdefault("citation_existing_keys", [])
    existing_keys = set(existing_keys_list)
    resolved_text, citations, _ = resolve_response_citations(
        text,
        chunks,
        doc_key_to_cite_key=doc_key_map,
        existing_cite_keys=existing_keys,
        include_unreferenced_documents=include_unreferenced_documents,
    )
    state["citation_existing_keys"] = sorted(existing_keys)
    return resolved_text, citations


def _generate_section_queries(
    state: DeepResearchState,
    section: SectionStatus,
    max_queries: int = 8,
) -> List[str]:
    """Generate section queries using a recall + precision + gap strategy.

    Three query categories (budget from depth preset):
      1. Gap queries     — one per known gap (highest priority)
      2. Recall queries  — short, broad, synonym/variant phrasing (wide net)
      3. Precision queries — long, constrained with method/time/data types (deep evidence)

    This dual-category approach ensures both broad topic coverage and deep
    evidentiary specificity, which is critical for academic writing.
    """
    dashboard = state["dashboard"]
    topic = dashboard.brief.topic
    client, model_override = _resolve_step_client_and_model(state, "research")
    gaps = section.gaps or []
    preset = state.get("depth_preset") or get_depth_preset(state.get("depth", DEFAULT_DEPTH))

    recall_budget = int(preset.get("recall_queries_per_section", 2))
    precision_budget = int(preset.get("precision_queries_per_section", 2))

    # ── Priority 1: gap-targeted queries (directly constructed, no LLM call) ──
    gap_queries: List[str] = []
    for gap in gaps[:max(max_queries // 2, 3)]:
        q = f"{topic} {gap}".strip()
        if q and q not in gap_queries:
            gap_queries.append(q)

    # ── Priority 2+3: LLM-generated recall + precision queries ──
    outline_block = "\n".join(f"- {s.title}" for s in dashboard.sections)
    other_sections = [s.title for s in dashboard.sections if s.title != section.title]
    gaps_block = "\n".join(f"- {g}" for g in gaps) if gaps else "(none)"
    avoid_overlap = ", ".join(other_sections[:4]) if other_sections else "(none)"
    temp_snippets = _retrieve_temp_snippets(state, f"{topic} {section.title}", top_k=3)
    temp_block = "\n\n".join(
        f"[{s['name']}] {s['text'][:350]}" for s in temp_snippets
    ) if temp_snippets else "(none)"

    prompt = _pm.render(
        "generate_queries.txt",
        topic=topic,
        scope=dashboard.brief.scope,
        outline_block=outline_block,
        section_title=section.title,
        gaps_block=gaps_block,
        temp_block=temp_block,
        user_context_block=_build_user_context_block(state, max_chars=1200),
        recall_budget=recall_budget,
        precision_budget=precision_budget,
        avoid_overlap=avoid_overlap,
    )

    recall_queries: List[str] = []
    precision_queries: List[str] = []
    try:
        resp = client.chat(
            messages=[
                {"role": "system", "content": "Output search queries ONLY in the specified format."},
                {"role": "user", "content": prompt},
            ],
            model=model_override,
            max_tokens=500,
        )
        raw = (resp.get("final_text") or "").strip()
        # Parse into recall / precision buckets
        current_bucket: Optional[str] = None
        for line in raw.split("\n"):
            line = line.strip()
            if not line or len(line) <= 3:
                continue
            lower = line.lower().rstrip(":")
            if lower in ("recall", "category a", "recall queries"):
                current_bucket = "recall"
                continue
            if lower in ("precision", "category b", "precision queries"):
                current_bucket = "precision"
                continue
            # Skip header-like lines
            if line.startswith("Category") or line.startswith("##"):
                continue
            # Clean numbering
            cleaned = re.sub(r"^\d+[\.\)]\s*", "", line).strip()
            if len(cleaned) <= 3:
                continue
            seen = set(gap_queries + recall_queries + precision_queries)
            if cleaned in seen:
                continue
            if current_bucket == "precision" and len(precision_queries) < precision_budget:
                precision_queries.append(cleaned)
            elif len(recall_queries) < recall_budget:
                recall_queries.append(cleaned)
            elif len(precision_queries) < precision_budget:
                precision_queries.append(cleaned)
    except Exception:
        # Fallback: simple section query
        recall_queries = [f"{topic} {section.title}".strip()]

    # Assemble: gap → recall → precision (priority order)
    result = gap_queries + recall_queries + precision_queries
    if not result:
        return [f"{topic} {section.title}".strip()]
    return result[:max_queries]


# ────────────────────────────────────────────────
# Local Priority Revise: scan for fresh review signals
# ────────────────────────────────────────────────

def _scan_fresh_revise_signals(state: DeepResearchState) -> None:
    """Check review_waiter for newly submitted 'revise' actions and enqueue them.

    This is called at the start of research_node so that mid-run revisions
    are picked up without waiting for the global review_gate.
    """
    waiter = state.get("review_waiter")
    if not waiter or bool(state.get("skip_draft_review", False)):
        return
    dashboard = state.get("dashboard")
    if not dashboard:
        return

    seen = state.setdefault("review_seen_at", {})
    queue = state.setdefault("revision_queue", [])

    for sec in dashboard.sections:
        if sec.title in queue:
            continue  # already queued
        review = waiter(sec.title)
        if not review:
            continue
        action = str(review.get("action") or "").strip().lower()
        if action != "revise":
            continue
        created_at = float(review.get("created_at") or 0.0)
        last_seen = float(seen.get(sec.title) or 0.0)
        if created_at > last_seen:
            queue.append(sec.title)
            seen[sec.title] = created_at
            _emit_progress(state, "revise_queued", {
                "section": sec.title,
                "message": f"章节 \"{sec.title}\" 已加入优先重写队列。",
                "feedback": str(review.get("feedback") or ""),
            })


def _consume_revision_queue(state: DeepResearchState) -> Optional[str]:
    """Pop the next section from the revision queue (if any).

    Returns the section title to rework, or None if queue is empty.
    """
    queue = state.get("revision_queue") or []
    if not queue:
        return None
    target = queue.pop(0)
    state["revision_queue"] = queue
    return target


# ────────────────────────────────────────────────
# Gap Supplements: load unconsumed supplements for a section
# ────────────────────────────────────────────────

def _load_section_supplements(state: DeepResearchState, section_id: str) -> str:
    """Load unconsumed gap supplements for a specific section and return as context block."""
    job_id = state.get("job_id") or ""
    if not job_id:
        return ""
    try:
        from src.collaboration.research.job_store import list_gap_supplements
        supplements = list_gap_supplements(job_id, section_id=section_id, status="pending")
        if not supplements:
            return ""
        parts: List[str] = []
        for sup in supplements:
            content = sup.get("content") or {}
            text = str(content.get("text") or content.get("info") or "").strip()
            gap = str(sup.get("gap_text") or "").strip()
            stype = str(sup.get("supplement_type") or "material")
            if text:
                label = f"[User Gap Supplement ({stype})]"
                if gap:
                    label += f" For gap: {gap}"
                parts.append(f"{label}\n{text[:1500]}")
        if not parts:
            return ""
        return "\n\nSection-scoped user supplements (HIGH PRIORITY -- incorporate this information):\n" + "\n\n".join(parts)
    except Exception:
        return ""


def _mark_section_supplements_consumed(state: DeepResearchState, section_id: str) -> None:
    """Mark all pending supplements for a section as consumed."""
    job_id = state.get("job_id") or ""
    if not job_id:
        return
    try:
        from src.collaboration.research.job_store import list_gap_supplements, mark_gap_supplement_consumed
        supplements = list_gap_supplements(job_id, section_id=section_id, status="pending")
        for sup in supplements:
            sid = sup.get("id")
            if sid:
                mark_gap_supplement_consumed(int(sid))
        if supplements:
            _emit_progress(state, "gap_supplement_consumed", {
                "section": section_id,
                "count": len(supplements),
                "message": f"已采纳 {len(supplements)} 条用户补充材料进入章节重写。",
            })
    except Exception:
        logger.debug("Failed to mark supplements consumed", exc_info=True)


# ────────────────────────────────────────────────
# Research Insights: record to persistent ledger
# ────────────────────────────────────────────────

def _record_insight(
    state: DeepResearchState,
    insight_type: str,
    text: str,
    section_id: str = "",
    source_context: str = "",
) -> None:
    """Append an insight to the persistent Research Insights Ledger."""
    job_id = state.get("job_id") or ""
    if not job_id or not text:
        return
    try:
        from src.collaboration.research.job_store import append_insight
        append_insight(
            job_id=job_id,
            insight_type=insight_type,
            text=text,
            section_id=section_id,
            source_context=source_context,
        )
    except Exception:
        logger.debug("Failed to record insight", exc_info=True)


# ────────────────────────────────────────────────
# Node 函数
# ────────────────────────────────────────────────

def scoping_node(state: DeepResearchState) -> DeepResearchState:
    """Phase 1: 范围界定 — 生成 Research Brief"""
    _ensure_not_cancelled(state)
    _tick_cost_monitor(state, "scope")
    client, model_override = _resolve_step_client_and_model(state, "scope")
    topic = state["topic"]
    clarification = state.get("clarification_answers") or {}
    clarification_lines = [f"- {k}: {v}" for k, v in clarification.items() if v]
    clarification_block = "\n".join(clarification_lines) if clarification_lines else "(none)"

    prompt = _pm.render(
        "scope_research.txt",
        topic=topic,
        clarification_block=clarification_block,
    )

    try:
        resp = client.chat(
            messages=[
                {"role": "system", "content": "You are a research planning expert. Return JSON only."},
                {"role": "user", "content": prompt},
            ],
            model=model_override,
            max_tokens=1000,
            response_model=_ScopingResponse,
        )
        parsed: Optional[_ScopingResponse] = resp.get("parsed_object")
        if parsed is None:
            raw = (resp.get("final_text") or "").strip()
            if raw:
                parsed = _ScopingResponse.model_validate_json(raw)
        data = parsed.model_dump() if parsed is not None else {}
    except Exception as e:
        logger.warning(f"Scoping failed: {e}")
        data = {}

    brief = ResearchBrief(
        topic=topic,
        scope=data.get("scope", f"Comprehensive review of {topic}"),
        success_criteria=data.get("success_criteria", ["Cover major research directions", "Include recent advances"]),
        key_questions=data.get("key_questions", [topic]),
        exclusions=data.get("exclusions", []),
        time_range=data.get("time_range", ""),
        source_priority=data.get("source_priority", ["peer-reviewed"]),
    )

    dashboard = state.get("dashboard") or ResearchDashboard()
    dashboard.brief = brief
    state["dashboard"] = dashboard

    trajectory = state.get("trajectory") or ResearchTrajectory(topic=topic)
    state["trajectory"] = trajectory
    _emit_progress(state, "scope_done", {"topic": topic, "scope": brief.scope, "key_questions": brief.key_questions})

    return state


def plan_node(state: DeepResearchState) -> DeepResearchState:
    """Phase 2: 规划 — 生成大纲并初始化 Dashboard"""
    _ensure_not_cancelled(state)
    _tick_cost_monitor(state, "plan")
    client, model_override = _resolve_step_client_and_model(state, "plan")
    dashboard = state["dashboard"]
    trajectory = state["trajectory"]

    # Initial retrieval for background context
    svc = _get_retrieval_svc(state)
    dr_filters = dict(state.get("filters") or {})
    dr_filters["use_query_optimizer"] = False
    pack = svc.search(
        query=dashboard.brief.topic,
        mode=state.get("search_mode", "hybrid"),
        top_k=15,
        filters=dr_filters,
    )
    _accumulate_evidence_chunks(state, pack.chunks)
    context = pack.to_context_string(max_chunks=15)

    # Track trajectory
    main_branch = trajectory.add_branch("main", "initial background survey")
    main_branch.status = "done"
    trajectory.add_search_action("main", SearchAction(
        query=dashboard.brief.topic,
        tool="search_hybrid",
        result_summary=f"Retrieved {len(pack.chunks)} relevant chunks",
        source_count=len(pack.chunks),
    ))
    dashboard.total_sources += len(pack.chunks)

    # Generate outline
    prompt = _pm.render(
        "plan_outline.txt",
        topic=dashboard.brief.topic,
        scope=dashboard.brief.scope,
        key_questions=", ".join(dashboard.brief.key_questions),
        context=context[:3000],
    )

    try:
        resp = client.chat(
            messages=[
                {"role": "system", "content": "You are an expert at building academic review outlines."},
                {"role": "user", "content": prompt},
            ],
            model=model_override,
            max_tokens=800,
        )
        raw = resp.get("final_text", "")
    except Exception:
        raw = f"1. Overview of {dashboard.brief.topic}\n2. Research progress\n3. Key findings\n4. Conclusions and outlook"

    # 解析大纲
    sections = []
    for line in raw.strip().split("\n"):
        line = line.strip()
        if not line:
            continue
        m = re.match(r"^[\d\.\-\*]+\s*(.+)$", line)
        if m:
            sections.append(m.group(1).strip())

    if not sections:
        sections = [f"{dashboard.brief.topic} Review"]

    # 初始化 Dashboard 章节
    dashboard.sections = []
    for title in sections:
        dashboard.add_section(title)
        trajectory.add_branch(f"sec_{len(dashboard.sections)}", title)

    # 创建/获取 Canvas
    from src.collaboration.canvas.canvas_manager import create_canvas, get_canvas, update_canvas, upsert_outline
    from src.collaboration.canvas.models import OutlineSection
    canvas_id = state.get("canvas_id")
    if canvas_id:
        canvas = get_canvas(canvas_id)
        if canvas is None:
            canvas = create_canvas(
                session_id=state.get("session_id", ""),
                topic=dashboard.brief.topic,
                user_id=state.get("user_id", ""),
            )
            state["canvas_id"] = canvas.id
    else:
        canvas = create_canvas(
            session_id=state.get("session_id", ""),
            topic=dashboard.brief.topic,
            user_id=state.get("user_id", ""),
        )
        state["canvas_id"] = canvas.id

    # 将 confirmed outline 实时写入 Canvas（前端可立即看到 Outline）
    outline_sections: List[OutlineSection] = []
    for idx, title in enumerate(sections):
        outline_sections.append(
            OutlineSection(
                title=title,
                level=1,
                order=idx,
                status="todo",
            )
        )
    try:
        upsert_outline(state["canvas_id"], outline_sections)
        update_canvas(
            state["canvas_id"],
            stage="outline",
            skip_draft_review=bool(state.get("skip_draft_review", False)),
            skip_refine_review=bool(state.get("skip_refine_review", False)),
            # 将 brief 结构化保存，便于 Explore 阶段展示
            research_brief={
                "scope": dashboard.brief.scope,
                "success_criteria": dashboard.brief.success_criteria,
                "key_questions": dashboard.brief.key_questions,
                "exclusions": dashboard.brief.exclusions,
                "time_range": dashboard.brief.time_range,
                "source_priority": dashboard.brief.source_priority,
                "action_plan": "",
            },
        )
    except Exception as e:
        logger.warning("Failed to initialize canvas outline/brief: %s", e)

    # 同步引用
    from src.collaboration.citation.manager import sync_evidence_to_canvas
    sync_evidence_to_canvas(state["canvas_id"], pack)

    state["markdown_parts"] = [f"# {dashboard.brief.topic}\n"]
    state["sections_completed"] = []
    _emit_progress(
        state,
        "plan_done",
        {
            "outline": sections,
            "initial_sources": len(pack.chunks),
            "sources_used": pack.sources_used,
            "canvas_id": state.get("canvas_id", ""),
        },
    )

    return state


def research_node(state: DeepResearchState) -> DeepResearchState:
    """Phase 3: 递归研究 — 对当前章节执行搜索 + 信息评估"""
    _ensure_not_cancelled(state)
    _tick_cost_monitor(state, "research")
    dashboard = state["dashboard"]
    trajectory = state["trajectory"]
    client = state["llm_client"]

    # ── Local Priority Revise: check for mid-run revise signals ──
    _scan_fresh_revise_signals(state)

    # ── Priority: consume revision queue before picking next section ──
    revise_target = _consume_revision_queue(state)
    if revise_target:
        section = dashboard.get_section(revise_target)
        if section:
            section.status = "researching"
            _emit_progress(state, "revise_started", {
                "section": section.title,
                "message": f"开始优先重写章节：{section.title}",
            })
        else:
            section = dashboard.get_next_section()
    else:
        section = dashboard.get_next_section()
    if section is None:
        return state

    state["current_section"] = section.title
    section.status = "researching"
    section.research_rounds += 1
    dashboard.total_iterations += 1
    state["iteration_count"] = state.get("iteration_count", 0) + 1

    preset = state.get("depth_preset") or get_depth_preset(state.get("depth", DEFAULT_DEPTH))
    _emit_progress(state, "section_research_start", {"section": section.title, "round": section.research_rounds})

    # Build gaps-first focused queries (budget from depth preset: recall + precision + gap buffer)
    recall_q = int(preset.get("recall_queries_per_section", 2))
    precision_q = int(preset.get("precision_queries_per_section", 2))
    max_q = recall_q + precision_q + 2
    queries = _generate_section_queries(state, section, max_queries=max_q)
    temp_hits = _retrieve_temp_snippets(state, f"{dashboard.brief.topic} {section.title}", top_k=4)
    for hit in temp_hits:
        trajectory.add_finding(
            f"sec_{dashboard.sections.index(section) + 1}",
            f"[temp:{hit['name']}] {hit['text'][:180]}"
        )

    # Execute search — tiered top_k: first round (wide net) vs gap-fill (targeted)
    svc = _get_retrieval_svc(state)
    dr_filters = dict(state.get("filters") or {})
    dr_filters["use_query_optimizer"] = False
    if section.research_rounds <= 1:
        search_top_k = int(preset.get("search_top_k_first", 20))
    else:
        search_top_k = int(preset.get("search_top_k_gap", 12))
        # Self-correction: if early rounds already reached high coverage, shrink gap retrieval.
        trigger_cov = float(preset.get("self_correction_trigger_coverage", 0.75))
        min_round = int(preset.get("self_correction_min_round", 3))
        if section.research_rounds >= min_round and float(section.coverage_score or 0.0) >= trigger_cov:
            decay = float(preset.get("search_top_k_gap_decay_factor", 0.7))
            min_k = int(preset.get("search_top_k_gap_min", 6))
            decayed_k = max(min_k, int(search_top_k * max(0.3, min(decay, 1.0))))
            if decayed_k < search_top_k:
                _emit_progress(
                    state,
                    "search_self_correction",
                    {
                        "section": section.title,
                        "round": section.research_rounds,
                        "coverage": round(float(section.coverage_score or 0.0), 3),
                        "top_k_from": search_top_k,
                        "top_k_to": decayed_k,
                        "message": "High coverage detected; reducing gap retrieval top_k to save cost.",
                    },
                )
                search_top_k = decayed_k
                # Also trim query count in high-coverage rounds.
                queries = queries[: max(2, len(queries) // 2)]
    all_chunks = []
    all_sources: set[str] = set()

    for q in queries:
        _ensure_not_cancelled(state)
        pack = svc.search(
            query=q,
            mode=state.get("search_mode", "hybrid"),
            top_k=search_top_k,
            filters=dr_filters,
        )
        all_chunks.extend(pack.chunks)
        all_sources.update(pack.sources_used)
        section.source_count += len(pack.chunks)
        dashboard.total_sources += len(pack.chunks)

        # Track trajectory
        branch_id = f"sec_{dashboard.sections.index(section) + 1}"
        trajectory.add_search_action(branch_id, SearchAction(
            query=q,
            tool="search_hybrid",
            result_summary=f"Retrieved {len(pack.chunks)} chunks",
            source_count=len(pack.chunks),
        ))

    # Adaptive fallback for sparse evidence:
    # Trigger on (a) too few chunks overall, OR (b) too few independent
    # documents (corroboration principle — single-source evidence is fragile).
    def _count_distinct_docs(chunks):
        keys = set()
        for c in chunks:
            if getattr(c, "doi", None):
                keys.add(f"doi:{c.doi}")
            else:
                keys.add(c.doc_group_key)
        return len(keys)

    distinct_docs = _count_distinct_docs(all_chunks)
    needs_fallback = len(all_chunks) < 3 or distinct_docs < 3

    if needs_fallback:
        fallback_reason = (
            "Sparse evidence detected"
            if len(all_chunks) < 3
            else f"Corroboration risk: only {distinct_docs} independent source(s)"
        )
        _emit_progress(
            state,
            "warning",
            {
                "section": section.title,
                "message": f"{fallback_reason}; running adaptive fallback search.",
                "chunks_found": len(all_chunks),
                "distinct_docs": distinct_docs,
            },
        )
        fallback_query = f"{dashboard.brief.topic} {section.title}".strip()
        fallback_mode = state.get("search_mode", "hybrid")
        if fallback_mode == "local":
            fallback_mode = "hybrid"
        try:
            _ensure_not_cancelled(state)
            fallback_pack = svc.search(
                query=fallback_query,
                mode=fallback_mode,
                top_k=int(preset.get("search_top_k_first", 20)),
                filters=dr_filters,
            )
            all_chunks.extend(fallback_pack.chunks)
            all_sources.update(fallback_pack.sources_used)
            section.source_count += len(fallback_pack.chunks)
            dashboard.total_sources += len(fallback_pack.chunks)
            trajectory.add_search_action(branch_id, SearchAction(
                query=fallback_query,
                tool="search_adaptive_fallback",
                result_summary=f"Fallback retrieved {len(fallback_pack.chunks)} chunks",
                source_count=len(fallback_pack.chunks),
            ))
        except Exception as e:
            logger.warning("Adaptive fallback search failed for section '%s': %s", section.title, e)

    distinct_docs_post = _count_distinct_docs(all_chunks)
    if len(all_chunks) < 3 or distinct_docs_post < 3:
        section.evidence_scarce = True
        _emit_progress(
            state,
            "evidence_insufficient",
            {
                "section": section.title,
                "message": "Evidence remains insufficient after fallback search; section may be degraded in writing.",
                "chunks_found": len(all_chunks),
                "distinct_docs": distinct_docs_post,
                "gaps": section.gaps[:3],
            },
        )
    else:
        section.evidence_scarce = False

    # 累积证据块，供 write/synthesize 统一做 hash->cite_key 替换
    _accumulate_evidence_chunks(state, all_chunks)

    # Sync citations
    if all_chunks and state.get("canvas_id"):
        from src.retrieval.evidence import EvidencePack
        from src.collaboration.citation.manager import sync_evidence_to_canvas
        temp_pack = EvidencePack(
            query=section.title,
            chunks=all_chunks,
            total_candidates=len(all_chunks),
            retrieval_time_ms=0,
            sources_used=sorted(all_sources) or ["hybrid"],
        )
        sync_evidence_to_canvas(state["canvas_id"], temp_pack)

    # RE-TRAC: 检查是否需要压缩
    if trajectory.needs_compression():
        compress_trajectory(trajectory, client, model=state.get("model_override"))

    _emit_progress(
        state,
        "section_research_done",
        {
            "section": section.title,
            "queries": queries,
            "chunks_found": len(all_chunks),
            "sources_used": sorted(all_sources),
        },
    )

    return state


def evaluate_node(state: DeepResearchState) -> DeepResearchState:
    """Evaluate information sufficiency and decide if more search is needed."""
    _ensure_not_cancelled(state)
    _tick_cost_monitor(state, "evaluate")
    dashboard = state["dashboard"]
    client, model_override = _resolve_step_client_and_model(state, "evaluate")
    trajectory = state["trajectory"]

    section = dashboard.get_section(state.get("current_section", ""))
    if section is None:
        return state

    # Build current findings context
    branch_id = f"sec_{dashboard.sections.index(section) + 1}"
    branch = trajectory.get_branch(branch_id)
    findings = "\n".join(branch.key_findings[-10:]) if branch else ""

    prompt = _pm.render(
        "evaluate_sufficiency.txt",
        section_title=section.title,
        topic=dashboard.brief.topic,
        source_count=section.source_count,
        findings=findings if findings else "(no key findings yet)",
    )

    try:
        resp = client.chat(
            messages=[
                {"role": "system", "content": "You are an information sufficiency evaluator. Return JSON only."},
                {"role": "user", "content": prompt},
            ],
            model=model_override,
            max_tokens=500,
            response_model=_CoverageEvalResponse,
        )
        parsed: Optional[_CoverageEvalResponse] = resp.get("parsed_object")
        if parsed is None:
            raw = (resp.get("final_text") or "").strip()
            if raw:
                parsed = _CoverageEvalResponse.model_validate_json(raw)
        data = parsed.model_dump() if parsed is not None else {}
    except Exception:
        data = {
            "coverage_score": 0.3,
            "gaps": ["Evaluation failed; evidence may be insufficient and needs additional search."],
            "sufficient": False,
        }
        _emit_progress(
            state,
            "warning",
            {
                "section": section.title,
                "message": "Coverage evaluation failed; using conservative fallback and continuing evidence search.",
            },
        )

    section.coverage_score = float(data.get("coverage_score", 0.5))
    section.gaps = data.get("gaps", [])
    history = state.setdefault("coverage_history", {})
    sec_hist = history.setdefault(section.title, [])
    sec_hist.append(section.coverage_score)
    history[section.title] = sec_hist[-6:]
    coverage_gain = None
    if len(sec_hist) >= 2:
        coverage_gain = float(sec_hist[-1]) - float(sec_hist[-2])

    if section.gaps:
        dashboard.coverage_gaps.extend(section.gaps)
        # ── Record gaps to Research Insights Ledger ──
        for gap_text in section.gaps:
            _record_insight(
                state,
                insight_type="gap",
                text=gap_text,
                section_id=section.title,
                source_context="evaluate_node",
            )
    if section.coverage_score < 0.4:
        _emit_progress(
            state,
            "warning",
            {
                "section": section.title,
                "coverage": section.coverage_score,
                "message": "Low coverage detected. Consider refining keywords or expanding scope.",
                "gaps": section.gaps[:3],
            },
        )
    _emit_progress(
        state,
        "section_evaluate_done",
        {
            "section": section.title,
            "coverage": section.coverage_score,
            "gaps": section.gaps,
            "research_round": int(section.research_rounds),
            "graph_steps": int(state.get("graph_step_count", 0)),
            "coverage_gain": coverage_gain,
        },
    )

    return state


def generate_claims_node(state: DeepResearchState) -> DeepResearchState:
    """Extract 3-5 core claims from section evidence (with [ref_hash]) before writing."""
    _ensure_not_cancelled(state)
    _tick_cost_monitor(state, "generate_claims")
    dashboard = state["dashboard"]
    section = dashboard.get_section(state.get("current_section", ""))
    if section is None:
        return state

    preset = state.get("depth_preset") or get_depth_preset(state.get("depth", DEFAULT_DEPTH))
    dr_filters = dict(state.get("filters") or {})
    write_top_k = _compute_effective_write_k(preset, dr_filters)
    svc = _get_retrieval_svc(state)
    dr_filters["use_query_optimizer"] = False
    pack = svc.search(
        query=f"{dashboard.brief.topic} {section.title}",
        mode=state.get("search_mode", "hybrid"),
        top_k=write_top_k,
        filters=dr_filters,
    )
    evidence_str = pack.to_context_string(max_chunks=write_top_k)

    client, model_override = _resolve_step_client_and_model(state, "write")
    user_content = _pm.render(
        "generate_claims.txt",
        section_title=section.title,
        evidence=evidence_str[:4000],
    )
    try:
        resp = client.chat(
            messages=[
                {"role": "system", "content": "You are an expert at extracting concise, citation-backed claims from evidence. Preserve every [ref_hash] in each claim."},
                {"role": "user", "content": user_content},
            ],
            model=model_override,
            max_tokens=800,
        )
        verified_claims = (resp.get("final_text") or "").strip()
    except Exception as e:
        logger.warning("generate_claims_node LLM call failed: %s", e)
        verified_claims = ""

    state["verified_claims"] = verified_claims
    return state


def write_node(state: DeepResearchState) -> DeepResearchState:
    """Phase 4: 写作 — 生成当前章节内容"""
    _ensure_not_cancelled(state)
    _tick_cost_monitor(state, "write")
    dashboard = state["dashboard"]
    client, model_override = _resolve_step_client_and_model(state, "write")
    trajectory = state["trajectory"]

    section = dashboard.get_section(state.get("current_section", ""))
    if section is None:
        return state

    section.status = "writing"

    # 收集上下文
    context_parts = [dashboard.to_system_prompt()]
    if trajectory.compressed_summaries:
        context_parts.append("\n".join(trajectory.compressed_summaries[-2:]))
    context = "\n\n".join(context_parts)

    # Retrieve section context — adaptive write-stage top_k
    preset = state.get("depth_preset") or get_depth_preset(state.get("depth", DEFAULT_DEPTH))
    dr_filters = dict(state.get("filters") or {})
    write_top_k = _compute_effective_write_k(preset, dr_filters)
    verification_k = int(preset.get("verification_k", max(10, write_top_k)))
    svc = _get_retrieval_svc(state)
    dr_filters["use_query_optimizer"] = False
    pack = svc.search(
        query=f"{dashboard.brief.topic} {section.title}",
        mode=state.get("search_mode", "hybrid"),
        top_k=write_top_k,
        filters=dr_filters,
    )
    evidence_str = pack.to_context_string(max_chunks=write_top_k)
    verify_pack = svc.search(
        query=f"{dashboard.brief.topic} {section.title} data evidence citation verification",
        mode=state.get("search_mode", "hybrid"),
        top_k=verification_k,
        filters=dr_filters,
    )
    verification_evidence_str = verify_pack.to_context_string(max_chunks=verification_k)
    _emit_progress(
        state,
        "write_verification_context",
        {
            "section": section.title,
            "write_top_k": write_top_k,
            "verification_k": verification_k,
            "primary_chunks": len(pack.chunks),
            "verification_chunks": len(verify_pack.chunks),
        },
    )
    temp_snippets = _retrieve_temp_snippets(state, f"{dashboard.brief.topic} {section.title}", top_k=5)
    temp_context = "\n\n".join(
        f"[temp:{s['name']}] {s['text'][:500]}" for s in temp_snippets
    )

    # ── Load section-scoped gap supplements (high priority user input) ──
    supplement_block = _load_section_supplements(state, section.title)

    cov_threshold = float(preset.get("coverage_threshold", 0.6))
    low_coverage = section.coverage_score < cov_threshold
    degraded_mode = bool(section.evidence_scarce) and section.source_count < 3

    if degraded_mode:
        # Hard degrade mode: avoid hallucinated long-form text when evidence is clearly insufficient.
        gaps_md = "\n".join(f"- {g}" for g in (section.gaps or [])[:5]) or "- Evidence retrieval was too sparse to support a reliable synthesis."
        section_text = (
            f"Evidence is currently insufficient to provide a fully supported section for **{section.title}**. "
            "This subsection is intentionally downgraded to avoid overconfident claims.\n\n"
            "Key unresolved gaps:\n"
            f"{gaps_md}\n\n"
            "Recommended next step: broaden data sources, refine terminology variants, and add domain-specific primary studies."
        )
        _emit_progress(
            state,
            "section_degraded",
            {
                "section": section.title,
                "message": "Section downgraded due to sparse evidence; generated a constrained summary instead of full prose.",
                "source_count": section.source_count,
                "coverage": section.coverage_score,
            },
        )
    else:
        caution_block = ""
        if low_coverage:
            caution_block = (
                "\nAdditional low-coverage constraints:\n"
                "- Current evidence coverage is LOW; avoid definitive language.\n"
                "- Use cautious wording (e.g., 'evidence suggests', 'limited data indicate').\n"
                "- If support is weak for a claim, mark it as [evidence limited].\n"
                "- Include a short 'Open Gaps' paragraph at the end listing unresolved evidence gaps.\n"
                "- Target length: 200-350 words (not 400-600) under low coverage.\n"
            )
        numeric_markers = (
            "computed_stats",
            "sample_size",
            "mean",
            "median",
            "std",
            "p-value",
            "p value",
            "confidence interval",
            "effect size",
        )
        numeric_context = f"{evidence_str}\n{verification_evidence_str}".lower()
        has_structured_numeric_data = any(m in numeric_context for m in numeric_markers)
        quantitative_block = ""
        if has_structured_numeric_data:
            quantitative_block = (
                "\nQuantitative strictness (MANDATORY):\n"
                "- Structured numeric evidence (e.g., computed_stats/table-like values) is present.\n"
                "- When any numeric comparison/difference is needed, you MUST call the `run_code` tool to run real Python/Pandas calculation.\n"
                "- Do not estimate, round by intuition, or fabricate any number; only report values that come from tool execution.\n"
                "- If calculation is not possible from available data, explicitly state the data limitation instead of guessing.\n"
            )
        claims_block = ""
        if state.get("verified_claims"):
            claims_block = (
                "\nPre-verified claims for this section (you MUST address each claim):\n"
                f"{state['verified_claims']}\n"
                "Expand each claim into well-supported prose. Do not omit any claim.\n"
            )
        triangulation_block = (
            "\nEvidence Triangulation (MANDATORY):\n"
            "- You MUST synthesize information across multiple independent sources for every major claim.\n"
            "- Do NOT rely on a single paper for any major claim.\n"
            "- If a finding is only supported by one source, explicitly qualify it "
            "(e.g., 'A single study [ref_hash] suggests...' or 'Preliminary evidence from [ref_hash] indicates...').\n"
            "- Actively look for converging evidence from different authors/studies to strengthen conclusions.\n"
        )
        prompt = _pm.render(
            "write_section.txt",
            section_title=section.title,
            language_instruction=_language_instruction(state),
            user_context_block=_build_user_context_block(state, max_chars=1800),
            triangulation_block=triangulation_block,
            caution_block=caution_block,
            quantitative_block=quantitative_block,
            claims_block=claims_block,
            evidence=evidence_str[:4000],
            verification_evidence=verification_evidence_str[:3500],
            temp_context=temp_context if temp_context else "(none)",
            supplement_block=supplement_block,
        )

        try:
            messages = [
                {
                    "role": "system",
                    "content": (
                        "You are an expert academic review writer."
                        " When numeric comparisons are needed, use tools to compute instead of guessing."
                        " You follow the principle that a single-source claim must never be presented as established fact;"
                        " always triangulate across multiple independent references.\n"
                        f"{context[:2000]}"
                    ),
                },
                {"role": "user", "content": prompt},
            ]
            if has_structured_numeric_data:
                from src.llm.react_loop import react_loop
                from src.llm.tools import get_tools_by_names

                react_result = react_loop(
                    messages=messages,
                    tools=get_tools_by_names(["run_code"]),
                    llm_client=client,
                    max_iterations=4,
                    model=model_override,
                    max_tokens=1500,
                )
                section_text = (react_result.final_text or "").strip()
                if not section_text:
                    resp = client.chat(
                        messages=messages,
                        model=model_override,
                        max_tokens=1500,
                    )
                    section_text = (resp.get("final_text") or "").strip()
            else:
                resp = client.chat(
                    messages=messages,
                    model=model_override,
                    max_tokens=1500,
                )
                section_text = (resp.get("final_text") or "").strip()
        except Exception as e:
            section_text = f"(Section '{section.title}' generation failed: {e})"

    # 对章节文本做 hash->cite_key 后处理，并在 state 内保持引用键稳定
    section_chunks = list(pack.chunks) + list(verify_pack.chunks)
    _accumulate_evidence_chunks(state, section_chunks)
    if section_text and section_chunks:
        section_text, section_citations = _resolve_text_citations(
            state,
            section_text,
            section_chunks,
            include_unreferenced_documents=False,
        )
        if section_citations:
            state["citations"] = section_citations

    # 添加到 markdown
    level = "##"
    state.setdefault("markdown_parts", []).append(f"\n{level} {section.title}\n\n{section_text}\n")

    # 实时写回 Canvas Draft（让前端 Drafting 面板在任务进行中可见）
    if state.get("canvas_id"):
        try:
            from src.collaboration.canvas.canvas_manager import get_canvas, upsert_draft, update_canvas
            from src.collaboration.canvas.models import DraftBlock
            canvas = get_canvas(state["canvas_id"])
            section_id = None
            if canvas:
                for s in canvas.outline:
                    if s.title == section.title:
                        section_id = s.id
                        break
            if not section_id:
                # 回退：用章节标题作为 section_id（避免草稿丢失）
                section_id = section.title
            upsert_draft(
                state["canvas_id"],
                DraftBlock(
                    section_id=section_id,
                    content_md=section_text,
                    version=1,
                    used_fragment_ids=[],
                    used_citation_ids=[],
                ),
            )
            update_canvas(state["canvas_id"], stage="drafting")
        except Exception as e:
            logger.warning("Failed to write section draft to canvas: %s", e)

    # ── Mark consumed supplements after successful write ──
    _mark_section_supplements_consumed(state, section.title)

    section.status = "reviewing"  # 写完进入审核
    dashboard.update_confidence()
    _emit_progress(
        state,
        "section_write_done",
        {
            "section": section.title,
            "section_id": section.title,  # 用 title 作为 review id，前后端对齐
            "word_count": len(section_text.split()),
            "coverage": section.coverage_score,
        },
    )

    # 手动介入：每章写完后发出审核事件（不阻塞后续章节）
    if not bool(state.get("skip_draft_review", False)):
        _emit_progress(
            state,
            "waiting_review",
            {
                "section": section.title,
                "section_id": section.title,
                "message": f"等待用户审核章节：{section.title}",
            },
        )

    return state


def verify_node(state: DeepResearchState) -> DeepResearchState:
    """Phase 4b: 验证 — Chain of Verification 验证声明"""
    _ensure_not_cancelled(state)
    _tick_cost_monitor(state, "verify")
    dashboard = state["dashboard"]
    client, model_override = _resolve_step_client_and_model(state, "verify")

    section = dashboard.get_section(state.get("current_section", ""))
    if section is None:
        return state

    # 找到刚写的章节文本
    section_text = ""
    for part in state.get("markdown_parts", []):
        if section.title in part:
            section_text = part
            break

    if not section_text:
        section.status = "done"
        state.setdefault("sections_completed", []).append(section.title)
        _emit_progress(state, "section_verify_done", {"section": section.title, "status": "done", "unsupported_claims": 0})
        return state

    # 执行 CoV
    from src.collaboration.research.verifier import verify_claims
    citations = state.get("citations", [])
    try:
        result = verify_claims(section_text, citations, client, model=model_override)
        section.coverage_score = max(section.coverage_score, {"high": 0.9, "medium": 0.7, "low": 0.4}.get(result.overall_confidence, 0.5))

        # ── 3-tier verification response ──
        # Instead of a single threshold, we handle three severity levels:
        #   light  (< light_threshold):   flag gaps for insight ledger, but don't disrupt flow
        #   medium (light..severe):        gap-fill — record gaps, status stays "reviewing" (not full re-research)
        #   severe (> severe_threshold):   full re-research with expanded queries
        preset = state.get("depth_preset") or get_depth_preset(state.get("depth", DEFAULT_DEPTH))
        light_thr = float(preset.get("verify_light_threshold", 0.20))
        medium_thr = float(preset.get("verify_medium_threshold", 0.30))
        severe_thr = float(preset.get("verify_severe_threshold", 0.40))

        if result.unsupported_claims > 0 and result.total_claims > 0:
            unsup_ratio = result.unsupported_claims / result.total_claims

            # Record supplementary queries as gaps regardless of tier
            if result.supplementary_queries:
                section.gaps = result.supplementary_queries[:4]
                dashboard.coverage_gaps.extend(result.supplementary_queries[:2])

            if unsup_ratio > severe_thr:
                # ── SEVERE: full re-research ──
                section.status = "researching"
                _emit_progress(
                    state,
                    "verify_severe",
                    {
                        "section": section.title,
                        "unsup_ratio": round(unsup_ratio, 2),
                        "message": f"Verification severe ({unsup_ratio:.0%} unsupported) — returning to research.",
                        "unsupported_claims": result.unsupported_claims,
                        "total_claims": result.total_claims,
                    },
                )
                return state

            elif unsup_ratio > light_thr:
                # ── MEDIUM: gap-fill only — record gaps but don't re-research ──
                # This prevents the "infinite verify→research loop" for moderately weak sections.
                _emit_progress(
                    state,
                    "verify_medium",
                    {
                        "section": section.title,
                        "unsup_ratio": round(unsup_ratio, 2),
                        "message": f"Verification medium ({unsup_ratio:.0%} unsupported) — gaps recorded, proceeding.",
                        "gaps": section.gaps[:3],
                    },
                )
                # Record as insights for the limitations section
                for gap in section.gaps[:3]:
                    _record_insight(state, "gap", gap, section.title, "verify_node_medium")
            else:
                # ── LIGHT: just flag ──
                _emit_progress(
                    state,
                    "verify_light",
                    {
                        "section": section.title,
                        "unsup_ratio": round(unsup_ratio, 2),
                        "message": f"Verification light ({unsup_ratio:.0%} unsupported) — minor gaps noted.",
                    },
                )

        conflict = getattr(result, "conflict_notes", None)
        if conflict:
            dashboard.conflict_notes.extend(conflict)
            # ── Record conflicts to Research Insights Ledger ──
            for note in conflict:
                _record_insight(
                    state,
                    insight_type="conflict",
                    text=note,
                    section_id=section.title,
                    source_context="verify_node",
                )
    except Exception as e:
        logger.warning(f"Verification failed for section '{section.title}': {e}")

    section.status = "done"
    state.setdefault("sections_completed", []).append(section.title)
    dashboard.update_confidence()
    _emit_progress(
        state,
        "section_verify_done",
        {
            "section": section.title,
            "status": "done",
            "coverage": section.coverage_score,
        },
    )

    return state


def review_gate_node(state: DeepResearchState) -> DeepResearchState:
    """最终审核门：所有章节审核通过后，才允许进入 synthesize。"""
    _ensure_not_cancelled(state)
    _tick_cost_monitor(state, "review_gate")
    if bool(state.get("force_synthesize", False)):
        state["review_gate_next"] = "synthesize"
        return state
    if bool(state.get("skip_draft_review", False)):
        state["review_gate_next"] = "synthesize"
        return state

    dashboard = state.get("dashboard")
    if dashboard is None or not dashboard.sections:
        state["review_gate_next"] = "synthesize"
        return state

    waiter = state.get("review_waiter")
    if not waiter:
        _emit_progress(
            state,
            "warning",
            {"message": "未配置审核等待器，跳过最终审核门。"},
        )
        state["review_gate_next"] = "synthesize"
        return state

    handled = state.setdefault("review_handled_at", {})
    approved_count = 0
    pending_sections: List[str] = []
    revise_target: Optional[str] = None
    revise_feedback = ""

    for sec in dashboard.sections:
        review = waiter(sec.title)
        if not review:
            pending_sections.append(sec.title)
            continue
        action = str(review.get("action") or "").strip().lower()
        if action == "approve":
            approved_count += 1
            continue
        if action == "revise":
            created_at = float(review.get("created_at") or 0.0)
            last_handled = float(handled.get(sec.title) or 0.0)
            if created_at > last_handled:
                handled[sec.title] = created_at
                revise_target = sec.title
                revise_feedback = str(review.get("feedback") or "")
                break
            # 旧 revise 已处理过，等待用户提交新的审核结果
            pending_sections.append(sec.title)
            continue
        pending_sections.append(sec.title)

    if revise_target:
        section = dashboard.get_section(revise_target)
        if section:
            section.status = "researching"
        state["current_section"] = revise_target
        # ── Record revise feedback as a limitation insight ──
        if revise_feedback:
            _record_insight(
                state,
                insight_type="limitation",
                text=revise_feedback,
                section_id=revise_target,
                source_context="review_gate_node",
            )
        _emit_progress(
            state,
            "review_requeue",
            {
                "section": revise_target,
                "message": "根据审核意见回到该章节重写。",
                "feedback": revise_feedback,
            },
        )
        state["review_gate_next"] = "research"
        return state

    total = len(dashboard.sections)
    if approved_count >= total and not pending_sections:
        _emit_progress(
            state,
            "all_reviews_approved",
            {"approved": approved_count, "total": total},
        )
        state["review_gate_next"] = "synthesize"
        return state

    _emit_progress(
        state,
        "waiting_review_all",
        {
            "approved": approved_count,
            "total": total,
            "pending_sections": pending_sections,
            "message": "等待所有章节审核通过后进入最终整合。",
        },
    )
    interrupt(
        {
            "reason": "waiting_for_review",
            "pending_sections": pending_sections,
        }
    )
    return state


def synthesize_node(state: DeepResearchState) -> DeepResearchState:
    """Phase 5: 全局综合 — 生成摘要 + 不足与展望 + 参考文献"""
    _ensure_not_cancelled(state)
    _tick_cost_monitor(state, "synthesize")
    client, model_override = _resolve_step_client_and_model(state, "synthesize")
    is_zh = (state.get("output_language") or "auto").lower() == "zh"
    dashboard = state.get("dashboard")

    def _dedupe_keep_order(items: List[str]) -> List[str]:
        seen = set()
        out: List[str] = []
        for raw in items:
            v = str(raw or "").strip()
            if not v:
                continue
            k = v.lower()
            if k in seen:
                continue
            seen.add(k)
            out.append(v)
        return out

    def _split_references_block(markdown_text: str) -> Tuple[str, str]:
        """Split body and references, keeping references untouched for coherence pass."""
        if not markdown_text.strip():
            return "", ""
        ref_titles = ["## 参考文献", "## References"]
        for title in ref_titles:
            idx = markdown_text.rfind(f"\n{title}")
            if idx < 0 and markdown_text.startswith(title):
                idx = 0
            if idx >= 0:
                body = markdown_text[:idx].rstrip()
                refs = markdown_text[idx:].strip()
                return (body + "\n") if body else "", refs + "\n"
        return markdown_text.rstrip() + "\n", ""

    def _extract_bracket_tokens(markdown_text: str) -> List[str]:
        """Extract bracketed tokens like [cite_key] / [evidence limited]."""
        if not markdown_text:
            return []
        return [m.strip() for m in re.findall(r"\[([^\[\]\n]{1,120})\]", markdown_text) if m.strip()]

    def _is_protected_reference_token(token: str) -> bool:
        t = (token or "").strip().lower()
        if not t:
            return False
        # Keep epistemic tags stable.
        if t == "evidence limited":
            return True
        # Numeric citation style, e.g., [1], [23]
        if re.fullmatch(r"\d{1,4}", t):
            return True
        # Common cite-key style, e.g., [3dd4798b...], [smith2021], [doi:...]
        if re.fullmatch(r"[a-z0-9_.:/-]{6,120}", t):
            return True
        return False

    def _citation_guard_ok(before_md: str, after_md: str) -> Tuple[bool, Dict[str, Any]]:
        """Guard against coherence pass dropping too many citation/evidence markers."""
        before_tokens = {t.lower() for t in _extract_bracket_tokens(before_md) if _is_protected_reference_token(t)}
        after_tokens = {t.lower() for t in _extract_bracket_tokens(after_md) if _is_protected_reference_token(t)}
        if not before_tokens:
            return True, {"before": 0, "after": len(after_tokens), "missing": 0, "reason": "no_protected_tokens"}

        missing = before_tokens - after_tokens
        before_n = len(before_tokens)
        after_n = len(after_tokens)
        missing_ratio = len(missing) / max(1, before_n)

        # Require at least 75% token retention and <=25% missing ratio.
        if after_n < max(1, int(before_n * 0.75)) or missing_ratio > 0.25:
            return False, {
                "before": before_n,
                "after": after_n,
                "missing": len(missing),
                "missing_examples": list(sorted(missing))[:8],
            }

        ev_before = before_md.lower().count("[evidence limited]")
        ev_after = after_md.lower().count("[evidence limited]")
        if ev_before > 0 and ev_after == 0:
            return False, {
                "before": before_n,
                "after": after_n,
                "missing": len(missing),
                "reason": "all_evidence_limited_tags_removed",
            }
        return True, {"before": before_n, "after": after_n, "missing": len(missing)}

    def _strip_redundant_heading(section_text: str, title_candidates: List[str]) -> str:
        text = (section_text or "").strip()
        if not text:
            return ""
        lines = text.splitlines()
        if not lines:
            return text
        first_raw = lines[0].strip()
        first = first_raw.lstrip("#").strip().lower()
        candidate_keys = [c.strip().lower() for c in title_candidates if str(c or "").strip()]
        if first in candidate_keys:
            lines = lines[1:]
        elif any(k and k in first for k in candidate_keys):
            lines = lines[1:]
        while lines and not lines[0].strip():
            lines = lines[1:]
        return "\n".join(lines).strip()

    def _script_profile(text: str) -> Tuple[int, int]:
        cjk_n = len(re.findall(r"[\u4e00-\u9fff]", text or ""))
        latin_n = len(re.findall(r"[A-Za-z]", text or ""))
        return cjk_n, latin_n

    def _is_mostly_english(text: str) -> bool:
        cjk_n, latin_n = _script_profile(text)
        return latin_n >= 80 and latin_n > cjk_n * 1.8

    def _is_mostly_chinese(text: str) -> bool:
        cjk_n, latin_n = _script_profile(text)
        return cjk_n >= 80 and cjk_n > latin_n * 1.2

    def _coerce_to_target_language(text: str, section_label: str) -> str:
        raw = (text or "").strip()
        if not raw:
            return raw
        lang = (state.get("output_language") or "auto").strip().lower()
        should_translate = False
        target_lang = ""
        if lang == "zh" and _is_mostly_english(raw):
            should_translate = True
            target_lang = "Chinese (中文)"
        elif lang == "en" and _is_mostly_chinese(raw):
            should_translate = True
            target_lang = "English"
        if not should_translate:
            return raw

        prompt = _pm.render(
            "translate_content.txt",
            target_lang=target_lang,
            section_label=section_label,
            raw=raw,
        )
        try:
            resp_lang = client.chat(
                messages=[
                    {"role": "system", "content": "You are a precise academic translator."},
                    {"role": "user", "content": prompt},
                ],
                model=model_override,
                max_tokens=1200,
            )
            translated = (resp_lang.get("final_text") or "").strip()
            return translated or raw
        except Exception:
            logger.debug("Language coercion failed for %s", section_label, exc_info=True)
            return raw

    def _language_consistency_ok(text: str) -> bool:
        lang = (state.get("output_language") or "auto").strip().lower()
        if lang not in ("zh", "en"):
            return True
        cjk_n, latin_n = _script_profile(text or "")
        if lang == "zh":
            # Allow English citation keys/terms, but body should still be Chinese-dominant.
            return cjk_n >= 80 and cjk_n >= int(latin_n * 0.55)
        # English target
        return latin_n >= 120 and latin_n >= int(cjk_n * 1.3)

    # ── Load Research Insights Ledger ──
    insights_by_type: Dict[str, List[str]] = {"gap": [], "conflict": [], "limitation": [], "future_direction": []}
    job_id = state.get("job_id") or ""
    if job_id:
        try:
            from src.collaboration.research.job_store import list_insights, bulk_mark_insights_addressed
            all_insights = list_insights(job_id, status="open")
            for ins in all_insights:
                itype = str(ins.get("insight_type") or "gap")
                text = str(ins.get("text") or "").strip()
                if text and itype in insights_by_type:
                    insights_by_type[itype].append(text)
        except Exception:
            logger.debug("Failed to load insights for synthesize", exc_info=True)

    # ── Aggregate open gaps from multiple channels for future agenda ──
    aggregated_open_gaps: List[str] = []
    aggregated_open_gaps.extend(insights_by_type.get("gap", []))
    if dashboard:
        aggregated_open_gaps.extend(getattr(dashboard, "coverage_gaps", []) or [])
        for sec in getattr(dashboard, "sections", []) or []:
            aggregated_open_gaps.extend(getattr(sec, "gaps", []) or [])
    aggregated_open_gaps = _dedupe_keep_order(aggregated_open_gaps)
    aggregated_conflict_notes: List[str] = []
    aggregated_conflict_notes.extend(insights_by_type.get("conflict", []))
    if dashboard:
        aggregated_conflict_notes.extend(getattr(dashboard, "conflict_notes", []) or [])
    aggregated_conflict_notes = _dedupe_keep_order(aggregated_conflict_notes)

    # 生成摘要
    full_md = "\n".join(state.get("markdown_parts", []))
    prompt = _pm.render(
        "generate_abstract.txt",
        language_instruction=_language_instruction(state),
        full_md=full_md[:5000],
    )

    try:
        resp = client.chat(
            messages=[
                {"role": "system", "content": "You are an academic abstract writing expert."},
                {"role": "user", "content": prompt},
            ],
            model=model_override,
            max_tokens=500,
        )
        abstract = (resp.get("final_text") or "").strip()
    except Exception:
        abstract = ""
    abstract = _strip_redundant_heading(abstract, ["摘要", "abstract"])
    abstract = _coerce_to_target_language(abstract, "abstract")

    # ── Generate "Limitations and Future Directions" section from insights ──
    limitations_section = ""
    scarce_sections = [s.title for s in (dashboard.sections if dashboard else []) if getattr(s, "evidence_scarce", False)]
    has_insights = any(len(v) > 0 for v in insights_by_type.values()) or bool(aggregated_conflict_notes)
    has_scarce_sections = len(scarce_sections) > 0
    if has_insights or has_scarce_sections:
        insight_block_parts: List[str] = []
        if insights_by_type["gap"]:
            insight_block_parts.append("Information Gaps:\n" + "\n".join(f"- {g}" for g in insights_by_type["gap"][:15]))
        if aggregated_conflict_notes:
            insight_block_parts.append(
                "Conflicts/Contradictions with Attribution Clues:\n"
                + "\n".join(f"- {c}" for c in aggregated_conflict_notes[:12])
            )
        if insights_by_type["limitation"]:
            insight_block_parts.append("Reviewer Noted Limitations:\n" + "\n".join(f"- {l}" for l in insights_by_type["limitation"][:10]))
        if has_scarce_sections:
            insight_block_parts.append(
                "Evidence-Scarce Sections (degraded due to sparse retrieval):\n"
                + "\n".join(f"- {s}" for s in scarce_sections[:20])
            )
        insight_block = "\n\n".join(insight_block_parts)

        lim_title = "不足与未来方向" if is_zh else "Limitations and Future Directions"
        lim_prompt = _pm.render(
            "limitations_section.txt",
            lim_title=lim_title,
            full_md=full_md[:3000],
            insight_block=insight_block[:2500],
            language_instruction=_language_instruction(state),
        )

        try:
            resp_lim = client.chat(
                messages=[
                    {"role": "system", "content": "You are an academic review writer specializing in critical analysis."},
                    {"role": "user", "content": lim_prompt},
                ],
                model=model_override,
                max_tokens=800,
            )
            limitations_section = (resp_lim.get("final_text") or "").strip()
        except Exception:
            # Fallback: simple bullet list
            fallback_items = insights_by_type["gap"][:5] + insights_by_type["conflict"][:3] + insights_by_type["limitation"][:3]
            if fallback_items:
                limitations_section = "\n".join(f"- {item}" for item in fallback_items)
        limitations_section = _strip_redundant_heading(
            limitations_section,
            ["不足与未来方向", "Limitations and Future Directions"],
        )
        limitations_section = _coerce_to_target_language(limitations_section, "limitations")

    # ── Structured "Open Gaps -> Future Directions" agenda ──
    open_gap_agenda = ""
    if aggregated_open_gaps:
        gap_lines = "\n".join(f"- {g}" for g in aggregated_open_gaps[:30])
        agenda_title = "开放问题与未来研究议程" if is_zh else "Open Gaps and Future Research Agenda"
        agenda_prompt = _pm.render(
            "open_gaps_agenda.txt",
            agenda_title=agenda_title,
            language_instruction=_language_instruction(state),
            full_md=full_md[:3500],
            gap_lines=gap_lines,
        )
        try:
            resp_agenda = client.chat(
                messages=[
                    {"role": "system", "content": "You are an expert in planning high-impact research roadmaps."},
                    {"role": "user", "content": agenda_prompt},
                ],
                model=model_override,
                max_tokens=1000,
            )
            open_gap_agenda = (resp_agenda.get("final_text") or "").strip()
        except Exception:
            # Deterministic fallback to ensure section always exists
            fallback_gaps = aggregated_open_gaps[:8]
            if fallback_gaps:
                bullet_title = "潜在方向" if is_zh else "Potential Directions"
                gap_label = "问题" if is_zh else "Gap"
                dir_label = "方向" if is_zh else "Direction"
                dir_text = (
                    "构建针对性数据集与实验流程，并执行可重复验证。"
                    if is_zh
                    else "build targeted datasets/protocols and run controlled verification."
                )
                open_gap_agenda = (
                    f"{bullet_title}:\n"
                    + "\n".join(f"- {gap_label}: {g}\n  - {dir_label}: {dir_text}" for g in fallback_gaps)
                )
        open_gap_agenda = _strip_redundant_heading(
            open_gap_agenda,
            ["开放问题与未来研究议程", "Open Gaps and Future Research Agenda"],
        )
        open_gap_agenda = _coerce_to_target_language(open_gap_agenda, "open gaps agenda")

    # 组装最终文档
    parts = state.get("markdown_parts", [])
    if abstract:
        # 在标题后插入摘要
        if len(parts) > 0:
            abstract_title = "摘要" if is_zh else "Abstract"
            parts.insert(1, f"\n## {abstract_title}\n\n{abstract}\n")

    # 添加不足与展望
    if limitations_section:
        lim_title = "不足与未来方向" if is_zh else "Limitations and Future Directions"
        parts.append(f"\n## {lim_title}\n\n{limitations_section}\n")
    if open_gap_agenda:
        gap_title = "开放问题与未来研究议程" if is_zh else "Open Gaps and Future Research Agenda"
        parts.append(f"\n## {gap_title}\n\n{open_gap_agenda}\n")

    # 添加参考文献
    if state.get("canvas_id"):
        try:
            from src.collaboration.citation.formatter import format_reference_list
            from src.collaboration.canvas.canvas_manager import get_canvas_citations
            citations = get_canvas_citations(state["canvas_id"])
            if citations:
                ref_text = format_reference_list(citations)
                ref_title = "参考文献" if is_zh else "References"
                parts.append(f"\n## {ref_title}\n\n{ref_text}\n")
                state["citations"] = citations
        except Exception:
            pass

    # ── Global coherence refinement pass (whole-document editorial integration) ──
    assembled = "\n".join(parts)
    body_md, refs_md = _split_references_block(assembled)
    final_markdown = assembled
    if body_md.strip():
        lang_hard_rule = ""
        if is_zh:
            lang_hard_rule = "Output must remain Chinese (中文). Keep citation tags unchanged."
        elif (state.get("output_language") or "auto").lower() == "en":
            lang_hard_rule = "Output must remain English. Keep citation tags unchanged."
        coherence_prompt = _pm.render(
            "coherence_refine.txt",
            language_instruction=_language_instruction(state),
            lang_hard_rule=lang_hard_rule if lang_hard_rule else "Respect the document's dominant language.",
            body_md=body_md,
        )
        try:
            resp_coherence = client.chat(
                messages=[
                    {"role": "system", "content": "You are an expert in scholarly synthesis and coherence editing."},
                    {"role": "user", "content": coherence_prompt},
                ],
                model=model_override,
                max_tokens=3500,
            )
            refined_body = (resp_coherence.get("final_text") or "").strip()
            if refined_body:
                candidate_markdown = refined_body + ("\n\n" + refs_md.strip() if refs_md.strip() else "")
                ok, guard_diag = _citation_guard_ok(assembled, candidate_markdown)
                lang_ok = _language_consistency_ok(refined_body)
                if ok and lang_ok:
                    final_markdown = candidate_markdown
                    _emit_progress(
                        state,
                        "global_refine_done",
                        {
                            "message": "已完成全文连贯性整合与跨章节一致性优化。",
                            "open_gaps": len(aggregated_open_gaps),
                            "citation_guard": guard_diag,
                        },
                    )
                else:
                    logger.warning(
                        "Global refine fallback (citation/lang guard): citation=%s, lang_ok=%s",
                        guard_diag,
                        lang_ok,
                    )
                    _emit_progress(
                        state,
                        "citation_guard_fallback",
                        {
                            "message": "检测到整合后语言/引用一致性风险，已回退到整合前版本。",
                            "guard": guard_diag,
                            "language_guard_ok": lang_ok,
                        },
                    )
        except Exception as e:
            logger.warning("Global coherence refine failed: %s", e)
            _emit_progress(
                state,
                "warning",
                {"message": "全文连贯性整合失败，已回退到合成稿。"},
            )

    # ── Final citation resolution pass (whole document) ──
    all_evidence_chunks = state.get("evidence_chunks", [])
    if all_evidence_chunks:
        resolved_markdown, resolved_citations = _resolve_text_citations(
            state,
            final_markdown,
            all_evidence_chunks,
            include_unreferenced_documents=False,
        )
        final_markdown = resolved_markdown
        if resolved_citations:
            state["citations"] = resolved_citations
            # 使用最终引用集重建参考文献，避免与正文 cite_key 不一致
            from src.collaboration.citation.formatter import format_reference_list
            body_md, _old_refs_md = _split_references_block(final_markdown)
            ref_title = "参考文献" if is_zh else "References"
            ref_text = format_reference_list(resolved_citations).strip()
            if ref_text:
                final_markdown = body_md.rstrip() + f"\n\n## {ref_title}\n\n{ref_text}\n"

    state["markdown_parts"] = [final_markdown]

    # ── Mark consumed insights as addressed ──
    if job_id and has_insights:
        try:
            from src.collaboration.research.job_store import bulk_mark_insights_addressed
            # Keep "gap" insights open for future research tracking.
            bulk_mark_insights_addressed(job_id, insight_type="conflict")
            bulk_mark_insights_addressed(job_id, insight_type="limitation")
            bulk_mark_insights_addressed(job_id, insight_type="future_direction")
        except Exception:
            logger.debug("Failed to mark insights addressed", exc_info=True)

    # ── Persist human-readable insights to Canvas model ──
    if state.get("canvas_id") and has_insights:
        try:
            from src.collaboration.canvas.canvas_manager import update_canvas
            all_insight_texts = []
            for itype, items in insights_by_type.items():
                for t in items[:10]:
                    all_insight_texts.append(f"[{itype}] {t}")
            update_canvas(state["canvas_id"], research_insights=all_insight_texts)
        except Exception:
            logger.debug("Failed to persist insights to canvas", exc_info=True)

    _emit_progress(
        state,
        "synthesize_done",
        {
            "sections_done": len(state.get("sections_completed", [])),
            "citations": len(state.get("citations", [])),
            "insights_consumed": sum(len(v) for v in insights_by_type.values()),
        },
    )
    return state


# ────────────────────────────────────────────────
# 路由函数
# ────────────────────────────────────────────────


def _write_or_claims(state: DeepResearchState) -> str:
    """When moving to writing: go to generate_claims unless skip_claim_generation or depth is lite."""
    if state.get("skip_claim_generation") or state.get("depth") == "lite":
        return "write"
    return "generate_claims"


def _should_continue_research(state: DeepResearchState) -> str:
    """决定评估后是继续搜索还是进入写作。

    Guard rails (all driven by depth preset):
    1. Global iteration cap (max_iterations)
    2. Per-section research round cap (max_section_research_rounds)
    3. Coverage threshold (coverage_threshold)
    """
    dashboard = state.get("dashboard")
    if dashboard is None:
        return _write_or_claims(state)

    section = dashboard.get_section(state.get("current_section", ""))
    if section is None:
        return _write_or_claims(state)

    preset = state.get("depth_preset") or get_depth_preset(state.get("depth", DEFAULT_DEPTH))

    # Guard 0: forced summarize mode from cost monitor
    # Always short-circuit to write to minimize extra LLM hops under cost pressure.
    if bool(state.get("force_synthesize", False)):
        return "write"

    # Guard 1: global iteration cap (dynamically set in execute_deep_research)
    max_iter = state.get("max_iterations", 30)
    if state.get("iteration_count", 0) >= max_iter:
        logger.info("Global iteration cap reached (%d), moving to write", max_iter)
        return _write_or_claims(state)

    # Guard 2: per-section research round cap
    max_rounds = int(preset.get("max_section_research_rounds", 3))
    if section.research_rounds >= max_rounds:
        logger.info("Section '%s' hit per-section cap (%d rounds), moving to write", section.title, max_rounds)
        return _write_or_claims(state)

    # Guard 3: coverage threshold
    cov_threshold = float(preset.get("coverage_threshold", 0.6))
    if section.coverage_score >= cov_threshold or not section.gaps:
        return _write_or_claims(state)

    # Guard 4: diminishing-return early stop (coverage curve plateau)
    history = (state.get("coverage_history") or {}).get(section.title, [])
    if len(history) >= 3:
        gain_recent = float(history[-1]) - float(history[-2])
        gain_prev = float(history[-2]) - float(history[-3])
        plateau_floor = float(preset.get("coverage_plateau_floor", max(0.7, cov_threshold - 0.05)))
        min_gain = float(preset.get("coverage_plateau_min_gain", 0.02))
        if float(history[-1]) >= plateau_floor and gain_recent < min_gain and gain_prev < min_gain:
            _emit_progress(
                state,
                "coverage_plateau_early_stop",
                {
                    "section": section.title,
                    "coverage": round(float(history[-1]), 3),
                    "gain_recent": round(gain_recent, 4),
                    "gain_prev": round(gain_prev, 4),
                    "message": "Coverage gain curve flattened; early stop triggered for cost efficiency.",
                },
            )
            return _write_or_claims(state)

    # Still have gaps → continue research
    return "research"


def _after_verify(state: DeepResearchState) -> str:
    """验证后，检查是否还有章节待处理或需要补充研究"""
    if bool(state.get("force_synthesize", False)):
        return "synthesize"
    dashboard = state.get("dashboard")
    if dashboard is None:
        return "synthesize"

    # 如果当前章节被打回研究阶段
    section = dashboard.get_section(state.get("current_section", ""))
    if section and section.status == "researching":
        return "research"

    next_section = dashboard.get_next_section()
    if next_section is None:
        if not bool(state.get("skip_draft_review", False)):
            return "review_gate"
        return "synthesize"

    return "research"


def _after_review_gate(state: DeepResearchState) -> str:
    if bool(state.get("force_synthesize", False)):
        return "synthesize"
    return str(state.get("review_gate_next") or "review_gate")


# ────────────────────────────────────────────────
# 构建 LangGraph
# ────────────────────────────────────────────────

def build_research_graph(include_scope_plan: bool = True) -> StateGraph:
    """构建 Deep Research Agent 的 LangGraph

    Scope → Plan → Research → Evaluate → Write → Verify → (next section / Synthesize)
    """
    graph = StateGraph(DeepResearchState)

    # 添加节点
    graph.add_node("scope", scoping_node)
    graph.add_node("plan", plan_node)
    graph.add_node("research", research_node)
    graph.add_node("evaluate", evaluate_node)
    graph.add_node("generate_claims", generate_claims_node)
    graph.add_node("write", write_node)
    graph.add_node("verify", verify_node)
    graph.add_node("review_gate", review_gate_node)
    graph.add_node("synthesize", synthesize_node)

    # 添加边
    if include_scope_plan:
        graph.set_entry_point("scope")
        graph.add_edge("scope", "plan")
        graph.add_edge("plan", "research")
    else:
        graph.set_entry_point("research")
    graph.add_edge("research", "evaluate")
    graph.add_conditional_edges("evaluate", _should_continue_research, {
        "research": "research",
        "write": "write",
        "generate_claims": "generate_claims",
    })
    graph.add_edge("generate_claims", "write")
    graph.add_edge("write", "verify")
    graph.add_conditional_edges("verify", _after_verify, {
        "research": "research",
        "review_gate": "review_gate",
        "synthesize": "synthesize",
    })
    graph.add_conditional_edges("review_gate", _after_review_gate, {
        "review_gate": "review_gate",
        "research": "research",
        "synthesize": "synthesize",
    })
    graph.add_edge("synthesize", END)

    return graph


def _build_initial_state(
    *,
    topic: str,
    llm_client: Any,
    canvas_id: Optional[str] = None,
    session_id: str = "",
    user_id: str = "",
    search_mode: str = "hybrid",
    filters: Optional[Dict[str, Any]] = None,
    model_override: Optional[str] = None,
    max_iterations: int = 30,
    output_language: str = "auto",
    clarification_answers: Optional[Dict[str, str]] = None,
    user_context: str = "",
    user_context_mode: str = "supporting",
    user_documents: Optional[List[Dict[str, str]]] = None,
    step_models: Optional[Dict[str, Optional[str]]] = None,
    step_model_strict: bool = False,
    progress_callback: Optional[Callable[[str, Dict[str, Any]], None]] = None,
    cancel_check: Optional[Callable[[], bool]] = None,
    review_waiter: Optional[Callable[[str], Optional[Dict[str, Any]]]] = None,
    skip_draft_review: bool = False,
    skip_refine_review: bool = False,
    skip_claim_generation: bool = False,
    job_id: str = "",
    depth: str = DEFAULT_DEPTH,
) -> DeepResearchState:
    resolved_depth = depth if depth in DEPTH_PRESETS else DEFAULT_DEPTH
    preset = get_depth_preset(resolved_depth)
    # max_iterations is set as a placeholder here; the real value is computed
    # dynamically in execute_deep_research() after num_sections is known:
    #   max_iterations = max_iterations_per_section × num_sections
    return {
        "topic": topic,
        "dashboard": ResearchDashboard(),
        "trajectory": ResearchTrajectory(topic=topic),
        "canvas_id": canvas_id or "",
        "session_id": session_id,
        "user_id": user_id,
        "search_mode": search_mode,
        "filters": filters or {},
        "current_section": "",
        "sections_completed": [],
        "markdown_parts": [],
        "citations": [],
        "evidence_chunks": [],
        "evidence_chunk_empty_value": "",
        "citation_doc_key_map": {},
        "citation_existing_keys": [],
        "iteration_count": 0,
        "max_iterations": max_iterations,  # placeholder; overridden in execute_deep_research()
        "llm_client": llm_client,
        "model_override": model_override,
        "output_language": output_language,
        "clarification_answers": clarification_answers or {},
        "user_context": user_context or "",
        "user_context_mode": user_context_mode or "supporting",
        "user_documents": user_documents or [],
        "step_models": step_models or {},
        "step_model_strict": bool(step_model_strict),
        "progress_callback": progress_callback,
        "cancel_check": cancel_check,
        "review_waiter": review_waiter,
        "skip_draft_review": bool(skip_draft_review),
        "skip_refine_review": bool(skip_refine_review),
        "skip_claim_generation": bool(skip_claim_generation),
        "verified_claims": "",
        "depth": resolved_depth,
        "depth_preset": preset,
        "review_gate_rounds": 0,
        "review_gate_unchanged": 0,
        "review_gate_last_snapshot": "",
        "review_gate_next": "review_gate",
        "review_handled_at": {},
        "revision_queue": [],
        "review_seen_at": {},
        "graph_step_count": 0,
        "cost_warned": False,
        "force_synthesize": False,
        "coverage_history": {},
        "last_cost_tick_step": 0,
        "job_id": job_id,
        "error": None,
    }


def start_deep_research(
    topic: str,
    llm_client: Any,
    canvas_id: Optional[str] = None,
    session_id: str = "",
    user_id: str = "",
    search_mode: str = "hybrid",
    filters: Optional[Dict[str, Any]] = None,
    clarification_answers: Optional[Dict[str, str]] = None,
    output_language: str = "auto",
    model_override: Optional[str] = None,
    step_models: Optional[Dict[str, Optional[str]]] = None,
    step_model_strict: bool = False,
    max_iterations: int = 30,
) -> Dict[str, Any]:
    """Phase 1 only: scope + plan. Return outline and brief for confirmation."""
    state = _build_initial_state(
        topic=topic,
        llm_client=llm_client,
        canvas_id=canvas_id,
        session_id=session_id,
        user_id=user_id,
        search_mode=search_mode,
        filters=filters,
        model_override=model_override,
        max_iterations=max_iterations,
        output_language=output_language,
        clarification_answers=clarification_answers,
        step_models=step_models,
        step_model_strict=step_model_strict,
    )
    try:
        state = scoping_node(state)
        state = plan_node(state)
        dashboard = state["dashboard"]
        return {
            "topic": topic,
            "session_id": state.get("session_id", ""),
            "canvas_id": state.get("canvas_id", ""),
            "brief": {
                "topic": dashboard.brief.topic,
                "scope": dashboard.brief.scope,
                "success_criteria": dashboard.brief.success_criteria,
                "key_questions": dashboard.brief.key_questions,
                "exclusions": dashboard.brief.exclusions,
                "time_range": dashboard.brief.time_range,
                "source_priority": dashboard.brief.source_priority,
            },
            "outline": [s.title for s in dashboard.sections],
            "initial_stats": {
                "total_sources": dashboard.total_sources,
                "total_iterations": dashboard.total_iterations,
            },
        }
    except Exception as e:
        logger.error("start_deep_research failed: %s", e)
        return {
            "topic": topic,
            "session_id": session_id,
            "canvas_id": canvas_id or "",
            "brief": {
                "topic": topic,
                "scope": f"Comprehensive review of {topic}",
                "success_criteria": [],
                "key_questions": [topic],
                "exclusions": [],
                "time_range": "",
                "source_priority": [],
            },
            "outline": [topic],
            "initial_stats": {"total_sources": 0, "total_iterations": 0},
            "error": str(e),
        }


def build_deep_research_result_from_state(
    state: Dict[str, Any],
    *,
    topic: str,
    elapsed_ms: float,
    fallback_outline: Optional[List[str]] = None,
) -> Dict[str, Any]:
    """Build stable API result from a terminal graph state."""
    final_state: Dict[str, Any] = state if isinstance(state, dict) else {}
    final_dashboard = final_state.get("dashboard", ResearchDashboard())
    final_markdown = "\n".join(final_state.get("markdown_parts", []))
    final_citations = final_state.get("citations", [])
    all_chunks = final_state.get("evidence_chunks", [])
    if final_markdown and all_chunks:
        final_markdown, resolved_citations = _resolve_text_citations(
            final_state,
            final_markdown,
            all_chunks,
            include_unreferenced_documents=False,
        )
        if resolved_citations:
            final_citations = resolved_citations
    if isinstance(final_dashboard, ResearchDashboard):
        outline = [s.title for s in final_dashboard.sections]
        dashboard_dict = final_dashboard.to_dict()
    else:
        outline = list(fallback_outline or [])
        dashboard_dict = final_dashboard if isinstance(final_dashboard, dict) else {}
    return {
        "markdown": final_markdown,
        "canvas_id": final_state.get("canvas_id", ""),
        "outline": outline,
        "citations": final_citations,
        "dashboard": dashboard_dict,
        "total_time_ms": elapsed_ms,
        "topic": topic,
    }


def prepare_deep_research_runtime(
    topic: str,
    llm_client: Any,
    confirmed_outline: List[str],
    confirmed_brief: Optional[Dict[str, Any]] = None,
    canvas_id: Optional[str] = None,
    session_id: str = "",
    user_id: str = "",
    search_mode: str = "hybrid",
    filters: Optional[Dict[str, Any]] = None,
    model_override: Optional[str] = None,
    max_iterations: int = 30,
    output_language: str = "auto",
    step_models: Optional[Dict[str, Optional[str]]] = None,
    step_model_strict: bool = False,
    user_context: str = "",
    user_context_mode: str = "supporting",
    user_documents: Optional[List[Dict[str, str]]] = None,
    progress_callback: Optional[Callable[[str, Dict[str, Any]], None]] = None,
    cancel_check: Optional[Callable[[], bool]] = None,
    review_waiter: Optional[Callable[[str], Optional[Dict[str, Any]]]] = None,
    skip_draft_review: bool = False,
    skip_refine_review: bool = False,
    skip_claim_generation: bool = False,
    job_id: str = "",
    depth: str = DEFAULT_DEPTH,
) -> Dict[str, Any]:
    """Prepare compiled graph runtime for deep-research execution/resume."""
    t0 = time.perf_counter()
    resolved_topic = topic.strip()
    preset = get_depth_preset(depth)
    graph = build_research_graph(include_scope_plan=False)
    checkpointer = MemorySaver()
    compiled = graph.compile(checkpointer=checkpointer)
    compiled.recursion_limit = int(preset["recursion_limit"])
    initial_state = _build_initial_state(
        topic=resolved_topic,
        llm_client=llm_client,
        canvas_id=canvas_id,
        session_id=session_id,
        user_id=user_id,
        search_mode=search_mode,
        filters=filters,
        model_override=model_override,
        max_iterations=max_iterations,
        output_language=output_language,
        user_context=user_context,
        user_context_mode=user_context_mode,
        user_documents=user_documents,
        step_models=step_models,
        step_model_strict=step_model_strict,
        progress_callback=progress_callback,
        cancel_check=cancel_check,
        review_waiter=review_waiter,
        skip_draft_review=skip_draft_review,
        skip_refine_review=skip_refine_review,
        skip_claim_generation=skip_claim_generation,
        job_id=job_id,
        depth=depth,
    )

    _emit_progress(initial_state, "depth_resolved", {"depth": depth, "preset": preset})
    dashboard = initial_state["dashboard"]
    brief = confirmed_brief or {}
    dashboard.brief = ResearchBrief(
        topic=brief.get("topic", resolved_topic),
        scope=brief.get("scope", f"Comprehensive review of {resolved_topic}"),
        success_criteria=brief.get("success_criteria", []),
        key_questions=brief.get("key_questions", [resolved_topic]),
        exclusions=brief.get("exclusions", []),
        time_range=brief.get("time_range", ""),
        source_priority=brief.get("source_priority", ["peer-reviewed"]),
    )

    outline = [s.strip() for s in (confirmed_outline or []) if s and s.strip()]
    if not outline:
        outline = [resolved_topic]
    dashboard.sections = []
    for idx, title in enumerate(outline):
        dashboard.add_section(title)
        initial_state["trajectory"].add_branch(f"sec_{idx+1}", title)

    iter_per_sec = int(preset.get("max_iterations_per_section", 5))
    num_sections = len(outline)
    scaled_max_iter = iter_per_sec * num_sections
    initial_state["max_iterations"] = scaled_max_iter
    logger.info(
        "Depth=%s | sections=%d | max_iterations=%d (%d × %d)",
        depth, num_sections, scaled_max_iter, iter_per_sec, num_sections,
    )
    initial_state["markdown_parts"] = [f"# {dashboard.brief.topic}\n"]

    if initial_state.get("canvas_id"):
        try:
            from src.collaboration.canvas.canvas_manager import upsert_outline, update_canvas
            from src.collaboration.canvas.models import OutlineSection
            sections_payload = [
                OutlineSection(title=title, level=1, order=idx, status="todo")
                for idx, title in enumerate(outline)
            ]
            upsert_outline(initial_state["canvas_id"], sections_payload)
            update_canvas(
                initial_state["canvas_id"],
                stage="outline",
                skip_draft_review=bool(skip_draft_review),
                skip_refine_review=bool(skip_refine_review),
                research_brief={
                    "scope": dashboard.brief.scope,
                    "success_criteria": dashboard.brief.success_criteria,
                    "key_questions": dashboard.brief.key_questions,
                    "exclusions": dashboard.brief.exclusions,
                    "time_range": dashboard.brief.time_range,
                    "source_priority": dashboard.brief.source_priority,
                    "action_plan": "",
                },
            )
        except Exception as e:
            logger.warning("Failed to sync execute outline/brief to canvas: %s", e)

    _emit_progress(initial_state, "execute_started", {"outline": outline, "topic": dashboard.brief.topic})
    thread_id = job_id.strip() if isinstance(job_id, str) else ""
    if not thread_id:
        seed = f"{session_id}:{resolved_topic}:{int(time.time() * 1000)}"
        thread_id = f"dr-{abs(hash(seed))}"
    config = {"configurable": {"thread_id": thread_id}}
    return {
        "compiled": compiled,
        "config": config,
        "initial_state": initial_state,
        "outline": outline,
        "topic": dashboard.brief.topic,
        "started_at_perf": t0,
    }


def execute_deep_research(
    topic: str,
    llm_client: Any,
    confirmed_outline: List[str],
    confirmed_brief: Optional[Dict[str, Any]] = None,
    canvas_id: Optional[str] = None,
    session_id: str = "",
    user_id: str = "",
    search_mode: str = "hybrid",
    filters: Optional[Dict[str, Any]] = None,
    model_override: Optional[str] = None,
    max_iterations: int = 30,
    output_language: str = "auto",
    step_models: Optional[Dict[str, Optional[str]]] = None,
    step_model_strict: bool = False,
    user_context: str = "",
    user_context_mode: str = "supporting",
    user_documents: Optional[List[Dict[str, str]]] = None,
    progress_callback: Optional[Callable[[str, Dict[str, Any]], None]] = None,
    cancel_check: Optional[Callable[[], bool]] = None,
    review_waiter: Optional[Callable[[str], Optional[Dict[str, Any]]]] = None,
    skip_draft_review: bool = False,
    skip_refine_review: bool = False,
    skip_claim_generation: bool = False,
    job_id: str = "",
    depth: str = DEFAULT_DEPTH,
) -> Dict[str, Any]:
    """Phase 2: run research loop with confirmed brief/outline."""
    runtime = prepare_deep_research_runtime(
        topic=topic,
        llm_client=llm_client,
        confirmed_outline=confirmed_outline,
        confirmed_brief=confirmed_brief,
        canvas_id=canvas_id,
        session_id=session_id,
        user_id=user_id,
        search_mode=search_mode,
        filters=filters,
        model_override=model_override,
        max_iterations=max_iterations,
        output_language=output_language,
        step_models=step_models,
        step_model_strict=step_model_strict,
        user_context=user_context,
        user_context_mode=user_context_mode,
        user_documents=user_documents,
        progress_callback=progress_callback,
        cancel_check=cancel_check,
        review_waiter=review_waiter,
        skip_draft_review=skip_draft_review,
        skip_refine_review=skip_refine_review,
        skip_claim_generation=skip_claim_generation,
        job_id=job_id,
        depth=depth,
    )
    compiled = runtime["compiled"]
    config = runtime["config"]
    initial_state = runtime["initial_state"]

    # 将 job_id 注入 LangGraph RunnableConfig 的 tags/metadata，供 LangSmith 追踪使用
    if job_id:
        existing_tags = list(config.get("tags") or [])
        if f"job:{job_id}" not in existing_tags:
            existing_tags.append(f"job:{job_id}")
        existing_metadata = dict(config.get("metadata") or {})
        existing_metadata.update({"job_id": job_id, "topic": topic})
        config = {
            **config,
            "tags": existing_tags,
            "metadata": existing_metadata,
        }

    try:
        compiled.invoke(initial_state, config=config)
        state_snapshot = compiled.get_state(config)
    except Exception as e:
        logger.error(f"Deep Research execution failed: {e}")
        return {
            "markdown": f"# {topic}\n\nDeep Research execution failed: {e}",
            "canvas_id": canvas_id or "",
            "outline": runtime.get("outline") or [],
            "citations": [],
            "dashboard": {},
            "total_time_ms": (time.perf_counter() - float(runtime.get("started_at_perf") or time.perf_counter())) * 1000,
        }

    if getattr(state_snapshot, "next", ()):
        return {
            "status": "waiting_review",
            "markdown": "\n".join(initial_state.get("markdown_parts", [])),
            "canvas_id": initial_state.get("canvas_id", ""),
            "outline": runtime.get("outline") or [],
            "citations": initial_state.get("citations", []) or [],
            "dashboard": (initial_state.get("dashboard").to_dict() if initial_state.get("dashboard") else {}),
            "total_time_ms": (time.perf_counter() - float(runtime.get("started_at_perf") or time.perf_counter())) * 1000,
        }

    elapsed = (time.perf_counter() - float(runtime.get("started_at_perf") or time.perf_counter())) * 1000
    final_state = getattr(state_snapshot, "values", {}) or {}
    return build_deep_research_result_from_state(
        final_state,
        topic=str(runtime.get("topic") or topic),
        elapsed_ms=elapsed,
        fallback_outline=runtime.get("outline") or [],
    )


# ────────────────────────────────────────────────
# 入口函数
# ────────────────────────────────────────────────

def run_deep_research(
    topic: str,
    llm_client: Any,
    canvas_id: Optional[str] = None,
    session_id: str = "",
    user_id: str = "",
    search_mode: str = "hybrid",
    filters: Optional[Dict[str, Any]] = None,
    model_override: Optional[str] = None,
    max_iterations: int = 30,
    clarification_answers: Optional[Dict[str, str]] = None,
    output_language: str = "auto",
    step_models: Optional[Dict[str, Optional[str]]] = None,
    progress_callback: Optional[Callable[[str, Dict[str, Any]], None]] = None,
    depth: str = DEFAULT_DEPTH,
    step_model_strict: bool = False,
) -> Dict[str, Any]:
    """
    执行 Deep Research Agent。

    Args:
        depth: Research depth — "lite" (fast, ~3-10 min) or "comprehensive" (thorough, ~15-40 min).

    Returns:
        {
            "markdown": str,
            "canvas_id": str,
            "outline": list[str],
            "citations": list,
            "dashboard": dict,
            "total_time_ms": float,
        }
    """
    start_result = start_deep_research(
        topic=topic,
        llm_client=llm_client,
        canvas_id=canvas_id,
        session_id=session_id,
        user_id=user_id,
        search_mode=search_mode,
        filters=filters,
        clarification_answers=clarification_answers,
        output_language=output_language,
        model_override=model_override,
        step_models=step_models,
        max_iterations=max_iterations,
        step_model_strict=step_model_strict,
    )
    return execute_deep_research(
        topic=topic,
        llm_client=llm_client,
        confirmed_outline=start_result.get("outline", []),
        confirmed_brief=start_result.get("brief"),
        canvas_id=start_result.get("canvas_id") or canvas_id,
        session_id=session_id,
        user_id=user_id,
        search_mode=search_mode,
        filters=filters,
        model_override=model_override,
        max_iterations=max_iterations,
        output_language=output_language,
        step_models=step_models,
        progress_callback=progress_callback,
        depth=depth,
        step_model_strict=step_model_strict,
    )
</file>

</files>
