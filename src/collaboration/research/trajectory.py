"""
RE-TRAC (Recursive Trajectory Compression)

è§£å†³é•¿ç ”ç©¶é“¾çš„ context drift é—®é¢˜ï¼š
- å½“ context æ¥è¿‘ä¸Šé™æ—¶ï¼Œå°†è¯¦ç»†è½¨è¿¹å‹ç¼©ä¸ºé«˜å±‚æ‘˜è¦
- ä¿ç•™æˆ˜ç•¥å‘ç°ã€æœªæ¢ç´¢åˆ†æ”¯ã€å…³é”®å¼•ç”¨
- ä¸¢å¼ƒé‡å¤å†…å®¹ã€ä¸­é—´æ¨ç†æ­¥éª¤
"""

from __future__ import annotations

import json
import time
from dataclasses import dataclass, field
from typing import Any, Dict, List, Optional

from src.log import get_logger
from src.utils.prompt_manager import PromptManager

_pm = PromptManager()

logger = get_logger(__name__)


@dataclass
class SearchAction:
    """ä¸€æ¬¡æœç´¢åŠ¨ä½œçš„è®°å½•"""
    query: str
    tool: str  # search_local / search_web / search_scholar / explore_graph
    result_summary: str  # å‹ç¼©åçš„ç»“æœæ‘˜è¦
    source_count: int = 0
    timestamp: float = field(default_factory=time.time)


@dataclass
class ResearchBranch:
    """ä¸€æ¡ç ”ç©¶åˆ†æ”¯"""
    id: str
    title: str
    status: str = "pending"  # pending | in_progress | done | discarded
    key_findings: List[str] = field(default_factory=list)
    search_actions: List[SearchAction] = field(default_factory=list)
    sub_questions: List[str] = field(default_factory=list)


@dataclass
class ResearchTrajectory:
    """
    ç ”ç©¶è½¨è¿¹ï¼šè®°å½•æ•´ä¸ªç ”ç©¶è¿‡ç¨‹çš„çŠ¶æ€ã€‚

    RE-TRAC çš„æ ¸å¿ƒæ•°æ®ç»“æ„ï¼Œç»´æŠ¤"å·²çŸ¥/æœªçŸ¥/å·²æ’é™¤"çš„å…¨å±€è§†å›¾ã€‚
    """
    topic: str
    branches: List[ResearchBranch] = field(default_factory=list)
    compressed_summaries: List[str] = field(default_factory=list)
    known_facts: List[str] = field(default_factory=list)
    open_questions: List[str] = field(default_factory=list)
    discarded: List[str] = field(default_factory=list)
    total_sources: int = 0
    compression_count: int = 0

    def add_branch(self, branch_id: str, title: str) -> ResearchBranch:
        branch = ResearchBranch(id=branch_id, title=title)
        self.branches.append(branch)
        return branch

    def get_branch(self, branch_id: str) -> Optional[ResearchBranch]:
        for b in self.branches:
            if b.id == branch_id:
                return b
        return None

    def add_search_action(self, branch_id: str, action: SearchAction) -> None:
        branch = self.get_branch(branch_id)
        if branch:
            branch.search_actions.append(action)
            self.total_sources += action.source_count

    def add_finding(self, branch_id: str, finding: str) -> None:
        branch = self.get_branch(branch_id)
        if branch:
            branch.key_findings.append(finding)
            self.known_facts.append(finding)

    def estimate_token_count(self) -> int:
        """ç²—ä¼°å½“å‰è½¨è¿¹å ç”¨çš„ token æ•°ï¼ˆ1 char â‰ˆ 0.5 token for CJK, 0.25 for ENï¼‰"""
        total_chars = len(self.topic)
        for b in self.branches:
            total_chars += len(b.title) + sum(len(f) for f in b.key_findings)
            for a in b.search_actions:
                total_chars += len(a.query) + len(a.result_summary)
        for s in self.compressed_summaries:
            total_chars += len(s)
        total_chars += sum(len(f) for f in self.known_facts)
        total_chars += sum(len(q) for q in self.open_questions)
        return int(total_chars * 0.4)  # ç²—ä¼°

    def needs_compression(self, max_tokens: int = 30000) -> bool:
        return self.estimate_token_count() > max_tokens

    def to_context_string(self) -> str:
        """å°†è½¨è¿¹åºåˆ—åŒ–ä¸ºå¯æ³¨å…¥ system prompt çš„å­—ç¬¦ä¸²"""
        parts = [f"## ç ”ç©¶è½¨è¿¹: {self.topic}"]
        parts.append(f"å·²å¤„ç†æ¥æº: {self.total_sources} | å‹ç¼©æ¬¡æ•°: {self.compression_count}")

        if self.compressed_summaries:
            parts.append("\n### å·²å‹ç¼©çš„ç ”ç©¶æ‘˜è¦")
            for i, s in enumerate(self.compressed_summaries):
                parts.append(f"[æ‘˜è¦ {i+1}] {s}")

        active_branches = [b for b in self.branches if b.status != "discarded"]
        if active_branches:
            parts.append("\n### ç ”ç©¶åˆ†æ”¯")
            for b in active_branches:
                status_icon = {"pending": "â³", "in_progress": "ğŸ”", "done": "âœ…"}.get(b.status, "")
                parts.append(f"\n**{b.title}** ({status_icon}{b.status})")
                if b.key_findings:
                    for f in b.key_findings[-5:]:  # åªä¿ç•™æœ€è¿‘ 5 ä¸ª
                        parts.append(f"  - {f}")

        if self.open_questions:
            parts.append("\n### å¾…å›ç­”çš„é—®é¢˜")
            for q in self.open_questions[-10:]:
                parts.append(f"  - {q}")

        if self.discarded:
            parts.append(f"\n### å·²æ’é™¤æ–¹å‘ ({len(self.discarded)})")
            for d in self.discarded[-5:]:
                parts.append(f"  - {d}")

        return "\n".join(parts)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# RE-TRAC å‹ç¼©
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€



def compress_trajectory(
    trajectory: ResearchTrajectory,
    llm_client: Any,
    model: Optional[str] = None,
) -> str:
    """
    å½“ context æ¥è¿‘ä¸Šé™æ—¶ï¼Œæ‰§è¡Œ RE-TRAC å‹ç¼©ã€‚

    å°†æ‰€æœ‰åˆ†æ”¯çš„è¯¦ç»†æœç´¢è®°å½•å‹ç¼©ä¸ºé«˜å±‚æ‘˜è¦ï¼Œé‡Šæ”¾ context ç©ºé—´ã€‚
    """
    # æ„å»ºè¦å‹ç¼©çš„è¯¦ç»†å†…å®¹
    detail_parts = []
    for branch in trajectory.branches:
        if branch.status == "discarded":
            continue
        detail_parts.append(f"\nåˆ†æ”¯: {branch.title} ({branch.status})")
        for action in branch.search_actions:
            detail_parts.append(f"  æœç´¢ [{action.tool}]: {action.query}")
            detail_parts.append(f"  ç»“æœ: {action.result_summary[:300]}")
        for finding in branch.key_findings:
            detail_parts.append(f"  å‘ç°: {finding}")

    trajectory_text = "\n".join(detail_parts)

    prompt = _pm.render("trajectory_compress.txt", trajectory=trajectory_text)

    try:
        resp = llm_client.chat(
            messages=[
                {"role": "system", "content": _pm.render("trajectory_compress_system.txt")},
                {"role": "user", "content": prompt},
            ],
            model=model,
            max_tokens=1500,
        )
        summary = (resp.get("final_text") or "").strip()
    except Exception as e:
        logger.warning(f"RE-TRAC compression failed: {e}")
        # é™çº§ï¼šæ‰‹åŠ¨æ‹¼æ¥å…³é”®å‘ç°
        summary = "å‹ç¼©å¤±è´¥ï¼Œä¿ç•™å…³é”®å‘ç°:\n" + "\n".join(trajectory.known_facts[-20:])

    # æ‰§è¡Œå‹ç¼©ï¼šæ¸…é™¤è¯¦ç»†è®°å½•ï¼Œä¿ç•™æ‘˜è¦
    trajectory.compressed_summaries.append(summary)
    trajectory.compression_count += 1

    # æ¸…é™¤å·²å‹ç¼©åˆ†æ”¯çš„è¯¦ç»†æœç´¢è®°å½•
    for branch in trajectory.branches:
        if branch.status in ("done", "in_progress"):
            branch.search_actions = []  # æ¸…é™¤è¯¦ç»†æœç´¢è®°å½•
            # ä¿ç•™ key_findings çš„æœ€è¿‘å‡ æ¡
            branch.key_findings = branch.key_findings[-3:]

    logger.info(f"RE-TRAC compression #{trajectory.compression_count}: "
                f"{len(summary)} chars, {trajectory.estimate_token_count()} est tokens remaining")

    return summary
